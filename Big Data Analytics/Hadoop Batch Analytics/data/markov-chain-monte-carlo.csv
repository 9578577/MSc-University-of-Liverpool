,10.1016/j.ijepes.2018.09.041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054174923&doi=10.1016%2fj.ijepes.2018.09.041&partnerID=40&md5=acf17587ac4bf67191332358f62e7b9a,"Ever-increasing rate of wind power generation as an uncertain and variable source of energy, face new challenges in power system operation. For this reason, optimal real-time operation of wind integrated power systems in profit based markets considering the effects of probabilistic future variations of wind speed is one of the main concerns of system operators. For this purpose, some solutions such as demand response (DR) programs and energy storage systems (ESS) are widely being used to cover and manage wind generation uncertainty. But some of them such as DR programs can add some extra sources of uncertainty due to unpredictable customer's behavior. To this end, this paper proposes an online model-based predictive control approach for optimal real-time operation of wind integrated power systems including DR and ESS facilities. Discrete-time manner, re-optimization characteristic, and adaptability are the main features of the proposed MPC method which make it well-suited to address high uncertainties regarding wind power generation and customer's behavior. Besides, MPC considers all interactive effects of the control facilities in accordance with the expected wind farm output power in the future prediction horizon to maximize wind power utilization and so enhance social welfare. In addition, the uncertain nature of wind power is modeled using Markov chain Monte Carlo method. For efficiency evaluation of the proposed approach, simulation is implemented in MATLAB software using YALMIP optimization toolbox for the 8-bus test system. Results confirm the acceptable performance of the proposed approach in reducing operation cost through optimal uncertainties management. © 2018 Elsevier Ltd"
,10.1016/j.inffus.2018.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046730150&doi=10.1016%2fj.inffus.2018.04.001&partnerID=40&md5=23ef6dde461676fbb0fbe45fb8d33e66,"Conversational data in social media contain a great deal of useful information, and conversation anomaly detection is an important research direction in the field of sentiment analysis. Each user has his or her own specific emotional characteristic, and by studying the distribution and sampling the users’ emotional transitions, we can simulate specific emotional transitions in the conversations. Anomaly detection in conversation data refers to detecting users’ abnormal opinions and sentiment patterns as well as special temporal aspects of such patterns. This paper proposes a hybrid model that combines the convolutional neural network long short-term memory (CNN-LSTM) with a Markov chain Monte Carlo (MCMC) method to identify users’ emotions, sample users’ emotional transition and detect anomalies according to the transition tensor. The emotional transition sampling is implemented by improving the MCMC algorithm and the anomalies are detected by calculating the similarity between the normal transition tensor and the current transition tensor of the user. The experiment was carried on four corpora, and the results show that emotions can be well sampled to conform to user's characteristics and anomaly can be detected by the proposed method. The model proposed can be used in intelligent conversation systems, such as simulating the emotional transition and detecting the abnormal emotions. © 2018 Elsevier B.V."
,10.1016/j.ymssp.2018.08.050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053181074&doi=10.1016%2fj.ymssp.2018.08.050&partnerID=40&md5=186e66b433517feb6795faffa906aee0,"This paper introduces an improved version of a novel inverse approach for the quantification of multivariate interval uncertainty for high dimensional models under scarce data availability. Furthermore, a conceptual and practical comparison of the method with the well-established probabilistic framework of Bayesian model updating via Transitional Markov Chain Monte Carlo is presented in the context of the DLR-AIRMOD test structure. First, it is shown that the proposed improvements of the inverse method alleviate the curse of dimensionality of the method with a factor up to 105. Furthermore, the comparison with the Bayesian results revealed that the selection ofthe most appropriate method depends largely on the desired information and availability of data. In case large amounts of data are available, and/or the analyst desires full (joint)-probabilistic descriptors of the model parameter uncertainty, the Bayesian method is shown to be the most performing. On the other hand however, when such descriptors are not needed (e.g., for worst-case analysis), and only scarce data are available, the interval method is shown to deliver more objective and robust bounds on the uncertain parameters. Finally, also suggestions to aid the analyst in selecting the most appropriate method for inverse uncertainty quantification are given. © 2018 Elsevier Ltd"
,10.1016/j.cam.2018.08.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052538043&doi=10.1016%2fj.cam.2018.08.012&partnerID=40&md5=bcd16988ebc2de6540b3e9a19ed63289,"A new generalization of the Lindley distribution, called the power Lindley distribution was proposed by Ghitany et al., (2013), which offers a more flexible distribution for modeling lifetime data, such as in reliability. They studied classical inferences for the model based on complete data sets. However, we may deal with record breaking data sets in which only values smaller (or larger) than the current extreme value are reported. In this paper, by using record values and inter-record times, we develop inference procedures for the estimation of the parameters and prediction of future record values for the power Lindley distribution. First, the maximum likelihood estimate of the parameters and their asymptotic confidence intervals are obtained. Next, we consider Bayes estimation under the symmetric (squared error) and asymmetric (linear-exponential (LINEX)) loss functions by using the joint bivariate density function. Since the closed forms of the estimates are not available, we encounter some computational difficulties to evaluate the Bayes estimates of the parameters involved in the model. For this reason, we use Tierney and Kadane's method as well as Markov Chain Monte Carlo (MCMC) procedure to compute approximate Bayes estimates. We further consider the non-Bayesian and Bayesian prediction for future lower record arising from the power Lindley distribution based on record data. The comparison of the derived predictors is carried out by using Monte Carlo simulations. A real data set is analyzed for illustration purposes. © 2018 Elsevier B.V."
,10.1016/j.ejor.2018.07.022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051047931&doi=10.1016%2fj.ejor.2018.07.022&partnerID=40&md5=4c90dc27d615e3397022daf94a6d74da,"In this paper, we study reliability based measures and prognostic problems of a K-out-of-N system in which the failure process of each component depends not only on its intrinsic characteristic but also on its operating environment conditions. The system reliability and the expected remaining useful lifetime are calculated. Under the periodic inspection policy, the system asymptotic availability is derived. We aim at providing explicit expressions for these quantities. The model allows us to incorporate the observation information of the environment in the evaluation of the system performances. Numerical examples show the efficiency and accuracy of our method by comparing with the Monte-Carlo simulations. It is pointed out that the environment condition has significant effect on the system reliability based measures and the system prognostic analysis. © 2018 Elsevier B.V."
,10.1016/j.physa.2018.09.067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053836898&doi=10.1016%2fj.physa.2018.09.067&partnerID=40&md5=6bf68ea1b784366c47648719d2db08d7,"As green behavior plays an increasingly important role today, in this paper, we aimed at investigating the spreading process of city resident green behavior. We applied classical threshold model and modified contagion model into a multiplex network to illustrate this dynamic process. Since people always act in conformity with the majorities when they make decisions or take actions, we consider the effects on negative individuals from their positive friends. Only when the proportion of centrist's positive friends surpassed the local awareness ratio α will he become positive. Also, we consider the influence from green awareness, which facilitate the green behavior. Last but not least, compulsory policy is also taken into account because it will force specific negative individuals to take such green behavior. The theoretical analysis is conducted by microscopic Markov chain approach and the numerical simulations are performed based on the Monte Carlo simulation. The results show that the intensity of policy regulation θ plays a vital role in both spreading threshold and final green behavior size. We attain a new explanation on the extinction of some behaviors and may provide advice on policy modification. © 2018 Elsevier B.V."
,10.1016/j.jmva.2018.08.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053157378&doi=10.1016%2fj.jmva.2018.08.014&partnerID=40&md5=d8fecacf78ada9336a63a7ff156cba17,"When performing Bayesian data analysis using a general linear mixed model, the resulting posterior density is almost always analytically intractable. However, if proper conditionally conjugate priors are used, there is a simple two-block Gibbs sampler that is geometrically ergodic in nearly all practical settings, including situations where p>n (Abrahamsen and Hobert, 2017). Unfortunately, the (conditionally conjugate) multivariate Gaussian prior on β does not perform well in the high-dimensional setting where p≫n. In this paper, we consider an alternative model in which the multivariate Gaussian prior is replaced by the normal-gamma shrinkage prior developed by Griffin and Brown (2010). This change leads to a much more complex posterior density, and we develop a simple MCMC algorithm for exploring it. This algorithm, which has both deterministic and random scan components, is easier to analyze than the more obvious three-step Gibbs sampler. Indeed, we prove that the new algorithm is geometrically ergodic in most practical settings. © 2018 Elsevier Inc."
,10.1016/j.strusafe.2018.05.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051537147&doi=10.1016%2fj.strusafe.2018.05.005&partnerID=40&md5=6fd7208085a72d30244c50a18e4d9a07,"This paper studies a non-random-walk Markov Chain Monte Carlo method, namely the Hamiltonian Monte Carlo (HMC) method in the context of Subset Simulation used for reliability analysis. The HMC method relies on a deterministic mechanism inspired by Hamiltonian dynamics to propose samples following a target probability distribution. The method alleviates the random walk behavior to achieve a more effective and consistent exploration of the probability space compared to standard Gibbs or Metropolis-Hastings techniques. After a brief review of the basic concepts of HMC method and its computational details, two algorithms are proposed to facilitate the application of HMC method to Subset Simulation in reliability analysis. Next, the behavior of the two HMC algorithms is illustrated using simple probability distribution models. Finally, the accuracy and efficiency of Subset Simulation employing the two HMC algorithms are tested using various reliability examples in both Gaussian and non-Gaussian spaces. © 2018 Elsevier Ltd"
,10.1016/j.cageo.2018.10.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055337361&doi=10.1016%2fj.cageo.2018.10.006&partnerID=40&md5=cb2a31985db57870ac41cbeca12c67b7,"For many geoscience applications, prediction requires building complex 3D surface models. Because of such complexity, often only a single model is built, possibly with a small set of variants to represent the uncertainty. Recent advancement in implicit modeling has made the construction of 3D geological models simpler; however, automatic assessment and visualization of uncertainty constrained by input geological rules and data constraints is still an active research topic. In this paper, we propose a new method that directly assesses and visualizes the uncertainty of geological surfaces by the means of stochastic motion. We represent the geological surfaces as the addition of stochastic implicit conceptual models and residual functions subject to the constraints of data and geological age relationships. Two sampling approaches to create the stochastic motion are proposed: Monte Carlo and Markov chain Monte Carlo (McMC). The uncertainty is assessed by independent realizations drawn by Monte Carlo sampling. The uncertainty is visualized by a “smooth” movie of gradually evolving geological surfaces that have the same stationary distribution as Monte Carlo, sampled by Markov chain Monte Carlo (McMC). This idea is integrated into the level set equation. Level sets are an ideal way to represent mathematically complex surfaces without explicit grid representations, thereby having the advantage of avoiding tedious topological computations such as defining the connectivity of a surface. We illustrate this new idea with simple synthetic 3D examples, taking the constraints of data and geological age relationships into consideration. Finally, we illustrate the idea using a synthetic data set from a copper deposit, where dense drillholes constrain an ore body with seven different lithologies. Our method provides a direct assessment and visualization of the uncertainty of 3D geological surfaces. © 2018 Elsevier Ltd"
,10.1016/j.sigpro.2018.09.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053758525&doi=10.1016%2fj.sigpro.2018.09.023&partnerID=40&md5=5bf544382bc17f44240981fd8824ff53,"In this paper, an adaptive distributed particle filter (ADPF) is proposed for single acoustic source tracking in distributed microphone networks (DMNs). To deal with spurious effects due to the reverberation and noise, a modified multiple-hypothesis model is first investigated by exploiting the generalized cross-correlation (GCC) function. Based on this model, the time-delay of arrival (TDOA) selection is performed for constituting the local observation. Then the acoustic source tracking is formulated as a Bayesian filtering problem under the assumption on the Langevin dynamic model of the source motion. Next, an adaptive distributed particle filter (ADPF) is presented to solve the Bayesian filtering problem for distributed acoustic source tracking. To improve the tracking performance, in the proposed ADPF, an adaptive and distributed computation method of the optimal proposal function is designed based on the Gaussian approximation, implemented by utilizing a Markov Chain Monte Carlo (MCMC) sampler and a consensus filter. The main advantage of the proposed acoustic source tracking method is the combination of the strength of the modified TDOA multiple-hypothesis model and the ADPF. Both simulation and real-world recording experiment results show that, the proposed ADPF has a relatively good tracking performance under different SNR conditions and reverberation environments. © 2018 Elsevier B.V."
,10.1016/j.csda.2018.07.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052476781&doi=10.1016%2fj.csda.2018.07.007&partnerID=40&md5=961b83680af5277c2bd4f83271b12b88,"Multidimensional scaling methods are frequently used by researchers and practitioners to project high dimensional data into a low dimensional space. However, it is a challenge to integrate side information which is available along with the dissimilarities to perform such dimension reduction analysis. A novel Bayesian integrative multidimensional scaling procedure, namely Bayesian multidimensional scaling with variable selection, is proposed to incorporate external information on the objects into the analysis through the use of a latent multivariate regression structure. The proposed Bayesian procedure allows the incorporation of covariate information into the dimension reduction analysis through the use of a variable selection strategy. An efficient computational algorithm to implement the procedure is also developed. A series of simulation experiments and a real data analysis are conducted, and the proposed model is shown to outperform several benchmark models based on some measures commonly used in the literature. © 2018 Elsevier B.V."
,10.1016/j.strusafe.2018.09.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054435699&doi=10.1016%2fj.strusafe.2018.09.004&partnerID=40&md5=ade4053039f788ccd7438d2ab6c1778d,"Civil engineering structures are commonly monitored to assess their structural behaviour, using alarm thresholds to indicate when contingency actions are needed to improve safety. However, there is a need for guidelines on how to establish thresholds that ensure sufficient safety. This paper therefore proposes a general computational algorithm for establishment of reliability-based alarm thresholds for civil engineering structures. The algorithm is based on Subset simulation with independent-component Markov chain Monte Carlo simulation and applicable with both analytical structural models and finite element models. The reliability-based alarm thresholds can straightforwardly be used in the monitoring plans that are developed in the design phase of a construction project, in particular for sequentially loaded structures such as staged construction of embankments. With the reliability-based alarm thresholds, contingency actions will only be implemented when they are needed to satisfy the target probability of failure. © 2018 The Authors"
,10.1007/978-3-319-99272-3_28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052213633&doi=10.1007%2f978-3-319-99272-3_28&partnerID=40&md5=d818679f1c8181042be98bf0a741813e,"The analyzed problem is the identification of fault parameters taking into account the stochastic characteristic of the system. The objective is to estimate the unbalance parameters, as the unbalance moment, phase angle and axial position of the unbalance force applied to the rotor. Therefore, experimental tests with the rotor to obtain the unbalance response is performed. This work aims the comparison between Bayesian inference with Markov Chain Monte Carlo method (MCMC), using Delayed Rejection Adaptive Metropolis algorithm (DRAM), and Stochastic Collocation through Generalized polynomial chaos expansion. This method has computational cost smaller than the MCMC methods, and it could be used as an alternative method for stochastic simulation. The Bayesian inference with MCMC and DRAM is based on previous works. However, the application of the MCMC have a high computational cost. Therefore, the Stochastic collocation is introduced into the likelihood function of the Bayes theorem for a faster convergence rate. The low computational cost of the collocation is evaluated and the results of both methods are compared to determine the convergence and precision of the collocation method. © 2019, Springer Nature Switzerland AG."
,10.1016/j.geothermics.2018.09.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054256522&doi=10.1016%2fj.geothermics.2018.09.012&partnerID=40&md5=4799fe74db6938023cbfccb2ab79d6f9,"In the evaluation of low- to medium-enthalpy geothermal resources on the island of Ireland, some of the most interesting targets are the deep sedimentary basins of Northern Ireland. The deepest of these is the Rathlin Basin, where Permian and Triassic reservoir sediments are known to exist to at least 2300 m depth. Two deep boreholes within the basin provide evidence of elevated temperatures at depth that are atypical within Ireland, prompting further geophysical exploration of the basin as one component of the IRETHERM project. The magnetotelluric (MT) method was selected as the investigative geophysical tool as it is capable of sensing and defining electrically conductive porous sediments beneath overlying resistive strata, in this case flood basalt sequences. MT data were acquired on a rectangular grid of 39 sites across almost half of the onshore basin to investigate the composition and spatial variation of the basin's formations. One-dimensional stochastic inverse modelling of the observed MT data was with a reversible-jump Markov chain Monte Carlo 1D inversion code, resulting in ensembles of models for each site. The use of model ensembles rather than single models avoids the pitfall of over-reliant interpretation on non-unique resistivity models, increasing the robustness of the interpretation. Interpreted models compare very favourably with nearby deep borehole records, and interpolation of the complete set of ensemble interpretations results in a conservative reservoir volume of approx. 32 km3 of combined Permian and Triassic sandstones beneath the MT survey. Based upon new, high quality temperature data available in the Ballinlea 1 borehole, an approximate estimation of thermal energy in place as a function of final reservoir temperature has been performed for the interpreted MT resistivity model volume. A final minimum temperature of 25 °C (being the temperature that comparable estimates have been made for adjacent geothermal prospects) results in a minimum estimated Indicated Geothermal Reserve (IGR) of 2.9 × 1018 J beneath the MT survey area. The modelling results suggest that exploitation of the maximum volume of sediments would occur for a final temperature of ≈55 °C. © 2018 Elsevier Ltd"
,10.1016/j.physe.2018.08.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053436586&doi=10.1016%2fj.physe.2018.08.028&partnerID=40&md5=bd6ad2a8698e740b5803cb75db359998,"A Markov model of semiconductor nanolaser is constructed in order to describe finely the effects of quantum fluctuations in the dynamics of the laser, in particular by considering the transition to lasing. Nanolasers are expected to contain only a small number of emitters, whose semiconductor bands are simulated using true carrier energy states. The model takes into account carrier-carrier interactions in the conduction and valence bands, but the result is a huge Markov chain that is often too demanding for direct Monte-Carlo simulation. We introduce here a technique to split the whole chain into two subchains, one referring to thermalization events within the bands and the other to laser photonic events of interest. The model is applied to the analysis of laser transition and enlightens the coexistence of a pulse regime triggered by the quantum nature of the photon with the birth of the known coherent cw regime. This conclusion is highlighted by calculated time traces. We show that on the ultrasmall scale of nanolasers, we are unable to define perfectly the threshold. © 2018 Elsevier B.V."
,10.1016/j.strusafe.2018.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050075353&doi=10.1016%2fj.strusafe.2018.07.001&partnerID=40&md5=e9d658c043af71e599553df6ca9ec24e,"The computation of the probability of a rare (failure) event is a common task in structural reliability analysis. In most applications, the numerical model defining the rare event is nonlinear and the resulting failure domain often multimodal. One strategy for estimating the probability of failure in this context is the importance sampling method. The efficiency of importance sampling depends on the choice of the importance sampling density. A near-optimal sampling density can be found through application of the cross entropy method. The cross entropy method is an adaptive sampling approach that determines the sampling density through minimizing the Kullback-Leibler divergence between the theoretically optimal importance sampling density and a chosen parametric family of distributions. In this paper, we investigate the suitability of the multivariate normal distribution and the Gaussian mixture model as importance sampling densities within the cross entropy method. Moreover, we compare the performance of the cross entropy method to sequential importance sampling, another recently proposed adaptive sampling approach, which uses the Gaussian mixture distribution as a proposal distribution within a Markov Chain Monte Carlo algorithm. For the parameter updating of the Gaussian mixture within the cross entropy method, we propose a modified version of the expectation-maximization algorithm that works with weighted samples. To estimate the number of distributions in the mixture, the density-based spatial clustering of applications with noise (DBSCAN) algorithm is adapted to the use of weighted samples. We compare the performance of the different methods in several examples, including component reliability problems, system reliability problems and reliability in varying dimensions. The results show that the cross entropy method using a single Gaussian outperforms the cross entropy method using Gaussian mixture and that both distribution types are not suitable for high dimensional reliability problems. © 2018 Elsevier Ltd"
,10.1016/j.envsoft.2018.09.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054236136&doi=10.1016%2fj.envsoft.2018.09.004&partnerID=40&md5=8078123e09bc38ffa176449632a09ede,"Dynamic global vegetation models (DGVMs) are of crucial importance for understanding and predicting vegetation, carbon, nitrogen and water dynamics of ecosystems in response to climate change. Their complexity, however, creates challenges for model analysis and data integration. A solution is to interface DGVMs with established statistical computing environments. Here we introduce rLPJGUESS, an R-package that couples the widely used DGVM LPJ-GUESS with the R environment for statistical computing, making existing R-packages and functions readily available to perform complex analyses with this model. We demonstrate the advantages of this framework by using rLPJGUESS to perform several otherwise laborious tasks: first, a set of single simulations, followed by global and local sensitivity analyses, a Bayesian calibration with a Markov-Chain Monte Carlo (MCMC) algorithm, and a predictive simulation with multiple climate scenarios. Our example highlights the opportunities of interfacing existing models in earth and environmental sciences with state-of-the-art computing environments such as R. © 2018 Elsevier Ltd"
,10.1007/978-3-319-78187-7_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049505586&doi=10.1007%2f978-3-319-78187-7_3&partnerID=40&md5=62718e2492dc7740fb08d286355baea2,"A reliable estimation of regional ground motion plays a critical role in probabilistic seismic hazard analysis (PSHA). The earthquake resistant design of structures within a region of a small spatial scale is often based on the assumption of relatively uniform form-factors which leads to the assumption of same station condition. However, for some small-scale regions this may not be the case. In this study, we propose a new Bayesian Hierarchical Model (BHM) for peak ground acceleration (PGA) records from two small-aperture Icelandic strong-motion arrays. The proposed BHM characterizes source effect, local station effect, source-station effect, and an error term that represents the measurement error and other unaccounted factors, separately. Posterior inference is based on a Markov chain Monte Carlo algorithm that uses the Metropolis algorithm. Uncertainty in unknown parameters is assessed through their joint posterior density. Analysis of PGA records based on the proposed BHM will improve the comprehensive understanding of the source effects, localized station conditions, and wave propagation. © Springer International Publishing AG, part of Springer Nature 2019."
,10.1007/978-3-319-78187-7_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049538092&doi=10.1007%2f978-3-319-78187-7_2&partnerID=40&md5=1f70d90b99840e62867d8c0dbf8b0fbc,"In this study, probabilistic seismic hazard assessment (PSHA) for North Iceland is explored in terms of its sensitivity to one of its key elements, the selected ground-motion models (GMMs). The GMMs in previous PSHA studies for Iceland are reviewed and in some cases recalibrated to the Icelandic dataset using a Markov Chain Monte Carlo (MCMC) algorithm which is useful in regions where the earthquake records are scarce. To show the ground motion model variability as it is manifested in PSHA uncertainties, the hazard maps of standard deviation and coefficient of variation (CV) of PGA at two hazard levels for GMMs before and after recalibrating are shown. The results indicate that the recalibrated models are promising candidates to be applied for future hazard studies in Iceland, but more importantly they show how to what extent and how the epistemic uncertainty of the GMMs contribute to patches of heightened hazard uncertainties, especially at near- and far-fault distances where there is a particular lack of data. © Springer International Publishing AG, part of Springer Nature 2019."
,10.1007/978-3-319-91086-4_1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053768966&doi=10.1007%2f978-3-319-91086-4_1&partnerID=40&md5=66daa9bca00d38e2856e20085ff045c7,"Simulated Annealing (SA) is one of the simplest and best-known metaheuristic method for addressing difficult black box global optimization problems whose objective function is not explicitly given and can only be evaluated via some costly computer simulation. It is massively used in real-life applications. The main advantage of SA is its simplicity. SA is based on an analogy with the physical annealing of materials that avoids the drawback of the Monte-Carlo approach (which can be trapped in local minima), thanks to an efficient Metropolis acceptance criterion. When the evaluation of the objective-function results from complex simulation processes that manipulate a large-dimension state space involving much memory, population-based algorithms are not applicable and SA is the right answer to address such issues. This chapter is an introduction to the subject. It presents the principles of local search optimization algorithms, of which simulated annealing is an extension, and the Metropolis algorithm, a basic component of SA. The basic SA algorithm for optimization is described together with two theoretical properties that are fundamental to SA: statistical equilibrium (inspired from elementary statistical physics) and asymptotic convergence (based on Markov chain theory). The chapter surveys the following practical issues of interest to the user who wishes to implement the SA algorithm for its particular application: finite-time approximation of the theoretical SA, polynomial-time cooling, Markov-chain length, stopping criteria, and simulation-based evaluations. To illustrate these concepts, this chapter presents the straightforward application of SA to two classical and simple classical NP-hard combinatorial optimization problems: the knapsack problem and the traveling salesman problem. The overall SA methodology is then deployed in detail on a real-life application: a large-scale aircraft trajectory planning problem involving nearly 30,000 flights at the European continental scale. This exemplifies how to tackle nowadays complex problems using the simple scheme of SA by exploiting particular features of the problem, by integrating astute computer implementation within the algorithm, and by setting user-defined parameters empirically, inspired by the SA basic theory presented in this chapter. © 2019, Springer International Publishing AG, part of Springer Nature."
,10.1016/j.ejor.2018.05.053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049566717&doi=10.1016%2fj.ejor.2018.05.053&partnerID=40&md5=103d4ae053bb6dd15f9c39e0e57171a9,"Estimation of banking efficiency and productivity is essential for regulatory purposes and for testing various theories in the context of banking such as the quiet life hypothesis, the bad management hypothesis etc. In such studies it is, therefore, important to place as few restrictions as possible on the functional forms subject to global satisfaction of the theoretical properties relating to monotonicity and concavity. In this paper, we propose an alternative to nonparametric segmented concave least squares. We use a differentiable approximation to an arbitrary functional form based on smoothly mixing Cobb-Douglas anchor functions over the data space. Estimation is based on Bayesian techniques organized around Markov Chain Monte Carlo. The approximation properties of the new functional form are investigated in a Monte Carlo experiment where the true functional form is a Symmetric Generalized McFadden. The new techniques are applied to a large U.S banking data set as well as a global banking data set. © 2018 Elsevier B.V."
,10.1016/j.agrformet.2018.09.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053793802&doi=10.1016%2fj.agrformet.2018.09.002&partnerID=40&md5=6ddf17121fa5ad08ce2005bb12ae5396,"Reliable data-driven models designed to accurately estimate cotton yield, an important agricultural commodity, can be adopted by farmers, agricultural system modelling experts and agricultural policy-makers in strategic decision-making processes. In this paper a hybrid genetic programing model integrated with the Markov Chain Monte Carlo (MCMC) based Copula technique is developed to incorporate climate-based inputs as the predictors of cotton yield, for selected study regions: Faisalabad (31.4504 °N, 73.1350 °E), Multan (30.1984 °N, 71.4687 °E) and Nawabshah (26.2442 °N, 68.4100 °E), as important cotton growing hubs in the developing nation of Pakistan. Several different types of GP-MCMC-copula models were developed, each with the well-known copula families (i.e., Gaussian, student t, Clayton, Gumble Frank and Fischer-Hinzmann functions) to screen and utilize an optimal cotton yield forecast model for the present study region. The results of the GP-MCMC based hybrid copula model were evaluated with a standalone GP and the MCMC based copula model in accordance with statistical analysis of the predicted yield based on correlation coefficient (r), Willmott's index (WI), Nash-Sutcliffe coefficient (NSE), root mean squared error (RMSE) and mean absolute error (MAE) in the independent test phase. Further performance preciseness was evaluated by the Akiake Information Criterion (AIC), the Bayesian Information Criterion (BIC) and the Maximum Likelihood (MaxL) for the GP-MCMC based copula as well as the MCMC based copula model. GP-MCMC-Clayton copula model generated the most accurate result for the Multan station. For the optimal GP-MCMC-Clayton copula model, the acquired model evaluation metrics for Multan were: (LM≈0.952; RRMSE≈2.107%; RRMAE≈1.771%) followed by the MCMC based Gaussian copula model (LM≈0.895; RRMSE≈4.541%; RRMAE≈0.3.214%) and the standalone GP model (LM≈0.132; RRMSE≈23.638%; RRMAE≈22.652%), indicating the superiority of the GP-MCMC-Clayton copula model in respect to the other benchmark models. The performance of GP-MCMC based copula model was also found to be superior in the case of Faisalabad and Nawabshah station as confirmed by AIC, BIC, MaxL metrics, including a larger value of the Legates-McCabe's (LM) index, utilized in conjunction with the relative percentage RRMSE and the relative mean absolute error (RMAE). Accordingly, it is averred that the developed GP-MCMC copula model can be considered as a pertinent data-intelligent tool used for accurate prediction of cotton yield, utilizing the readily available climate datasets in agricultural regions and is of relevance to agricultural yield simulation and sectoral decision-making. © 2018 Elsevier B.V."
,10.1016/j.jcp.2018.08.049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052906458&doi=10.1016%2fj.jcp.2018.08.049&partnerID=40&md5=de4a8254a06b0badc63008e03d087788,"We present a systematic study of the nested sampling algorithm based on the example of the Potts model. This model, which exhibits a first order phase transition for q&gt;4, exemplifies a generic numerical challenge in statistical physics: The evaluation of the partition function and thermodynamic observables, which involve high dimensional sums of sharply structured multi-modal density functions. It poses a major challenge to most standard numerical techniques, such as Markov Chain Monte Carlo. In this paper we will demonstrate that nested sampling is particularly suited for such problems and it has a couple of advantages. For calculating the partition function of the Potts model with N sites: a) one run stops after O(N) moves, so it takes O(N2) operations for the run, b) only a single run is required to compute the partition function along with the assignment of confidence intervals, c) the confidence intervals of the logarithmic partition function decrease with 1/N and d) a single run allows to compute quantities for all temperatures while the autocorrelation time is very small, irrespective of temperature. Thermodynamic expectation values of observables, which are completely determined by the bond configuration in the representation of Fortuin and Kasteleyn, like the Helmholtz free energy, the internal energy as well as the entropy and heat capacity, can be calculated in the same single run needed for the partition function along with their confidence intervals. In contrast, thermodynamic expectation values of magnetic properties like the magnetization and the magnetic susceptibility require sampling the additional spin degree of freedom. Results and performance are studied in detail and compared with those obtained with multi-canonical sampling. Eventually the implications of the findings on a parallel implementation of nested sampling are outlined. © 2018 Elsevier Inc."
,10.1080/00949655.2018.1523410,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053520378&doi=10.1080%2f00949655.2018.1523410&partnerID=40&md5=172d648970a50091ee668008889e9b38,"In this study, we discuss the classical, Bayesian, and generalized inference of the reliability parameter Rs,k = Pr (At least s of the (X1, X2, . . . , Xk) exceed Y) = Pr(Xk−s+1:k &gt; Y) of an s-out-of-k:G system with strength components X1, X2, . . . , Xk subjected to a common stress Y whose probability densities are independent two-parameter general class of exponentiated inverted exponential distributions. These statistical analyses are carried out based on the progressively type-II right censored data with uniformly random removals. Under squared error and LINEX loss functions, Bayes estimates are developed by using Lindley's approximation and the Markov Chain Monte Carlo method due to the lack of closed forms of the posterior distributions. Generalized inferences are performed based on the generalized variable method. Simulation studies and real-world data analyses are given to illustrate the proposed procedures. The size of the test, adjusted and unadjusted power of the test, coverage probability and expected confidence lengths of the confidence interval, and biases of the estimator are also discussed. Comparison and contrast among the classical, Bayesian, and generalized inferences of the reliability parameter in the multicomponent stress-strength model are performed. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1061/(ASCE)HE.1943-5584.0001720,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054990653&doi=10.1061%2f%28ASCE%29HE.1943-5584.0001720&partnerID=40&md5=427e01f4e2132246e2ff32475420d9a2,"This study presents a probabilistic framework that considers both the water quality improvement capability and reliability of alternative total maximum daily load (TMDL) pollutant allocations. Generalized likelihood uncertainty estimation and Markov chain Monte Carlo techniques were used to assess the relative uncertainty and reliability of two alternative TMDL pollutant allocations that were developed to address a fecal coliform (FC) bacteria impairment in a rural watershed in western Virginia. The allocation alternatives, developed using the Hydrological Simulation Program-FORTRAN, specified differing levels of FC bacteria reduction from different sources. While both allocations met the applicable water-quality criteria, the approved TMDL allocation called for less reduction in the FC source that produced the greatest uncertainty (cattle directly depositing feces in the stream), suggesting that it would be less reliable than the alternative, which called for a greater reduction from that same source. The approach presented in this paper illustrates a method to incorporate uncertainty assessment into TMDL development, thereby enabling stakeholders to engage in more informed decision making. © ASCE."
,10.1016/j.csda.2018.07.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050977723&doi=10.1016%2fj.csda.2018.07.005&partnerID=40&md5=ace032cb65eabfeea1e9ec056ca5c70a,"The reversible jump Markov chain Monte Carlo (RJMCMC) method offers an across-model simulation approach for Bayesian estimation and model comparison, by exploring the sampling space that consists of several models of possibly varying dimensions. A naive implementation of RJMCMC to models like Gibbs random fields suffers from computational difficulties: the posterior distribution for each model is termed doubly-intractable since computation of the likelihood function is rarely available. Consequently, it is simply impossible to simulate a transition of the Markov chain in the presence of likelihood intractability. A variant of RJMCMC is presented, called noisy RJMCMC, where the underlying transition kernel is replaced with an approximation based on unbiased estimators. Based on previous theoretical developments, convergence guarantees for the noisy RJMCMC algorithm are provided. The experiments show that the noisy RJMCMC algorithm can be much more efficient than other exact methods, provided that an estimator with controlled Monte Carlo variance is used, a fact which is in agreement with the theoretical analysis. © 2018 Elsevier B.V."
,10.1016/j.compstruct.2018.08.074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054027278&doi=10.1016%2fj.compstruct.2018.08.074&partnerID=40&md5=7bc260d8ab153584398632dba9a21d29,"This paper presents a novel stochastic framework to quantify the knock down in strength from out-of-plane wrinkles at the coupon level. The key innovation is a Markov Chain Monte Carlo algorithm which rigorously derives the stochastic distribution of wrinkle defects directly informed from image data of defects. The approach significantly reduces uncertainty in the parameterization of stochastic numerical studies on the effects of defects. To demonstrate our methodology, we present an original stochastic study to determine the distribution of strength of corner bend samples with random out-of-plane wrinkle defects. The defects are parameterized by stochastic random fields defined using Karhunen-Loéve (KL) modes. The distribution of KL coefficients are inferred from misalignment data extracted from B-Scan data using a modified version of Multiple Field Image Analysis. The strength distribution is estimated by embedding wrinkles into high fidelity FE simulations using the high performance toolbox dune-composites from which we observe severe knockdowns to 74% of structural strength with a probability of 1/200. Supported by the literature our results highlight the strong correlation between maximum misalignment and knockdown in coupon strength. This observation allows us to define a surrogate model providing fast assessment of predicted strength informed from stochastic simulations utilizing both observed wrinkle data and high fidelity finite element models. © 2018"
,10.1016/j.ejor.2018.05.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048809412&doi=10.1016%2fj.ejor.2018.05.026&partnerID=40&md5=620235a887587788e7cf7b1fdab89c61,"A method is presented for real-time forecasting of product returns in remanufacturing. It determines the quantity of imminent returns and quality features such as age distribution and number of past cycles. Required data in real-time include the mean age of stock, a scaled quantity (population average) reliably monitored, even from small-size or decentralized stock samples, the maximum and minimum age in return samples and past volumes of net demand or sales. The characteristic parameters of the return distribution (center axis and spread) are updated in real-time. The method sequentially determines the retention probability in each time period, a key random variable that unties the dynamic closed-loop-supply chain knot. The retention probability sequence is used in explicit expressions for the product return flow and age distribution (a quality index), based on Markov representation of stock and flows. The model allows for arbitrarily random early loss and non-stationarities, uncertain demand and varying utilization of reusable returns. Markov-chain Monte-Carlo simulation enables assessment of the efficacy of the forecasting method. Exploiting reliable, current information, the method may provide improved estimates of product returns compared to linear models that relate returns to past levels of sales and/or returns, and utilize conventional regression, recursive least squares, or adaptive identification methods. Forecasting efficiency is higher as measured by mean or integral absolute error, and particularly so, regarding peaks and lows of the return flow. The results may be useful for enhanced acquisition of returns with reduced stock inventories and efficient planning of remanufacturing operations. © 2018 Elsevier B.V."
1,10.1016/j.automatica.2018.07.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053166670&doi=10.1016%2fj.automatica.2018.07.024&partnerID=40&md5=c2107a8e93805b393d87acead42a9022,"In our previous work, we proposed a particle Gaussian mixture (PGM-I) filter for nonlinear estimation. The PGM-I filter uses the transition kernel of the state Markov chain to sample from the propagated prior. It constructs a Gaussian mixture representation of the propagated prior density by clustering the samples. The measurement data are incorporated by updating individual mixture modes using the Kalman measurement update. However, the Kalman measurement update is inexact when the measurement function is nonlinear and leads to the restrictive assumption that the number of modes remains fixed during the measurement update. In this paper, we introduce an alternate PGM-II filter that employs parallelized Markov Chain Monte Carlo (MCMC) sampling to perform the measurement update. The PGM-II filter update is asymptotically exact and does not enforce any assumptions on the number of Gaussian modes. The PGM-II filter is employed in the estimation of two test case systems. The results indicate that the PGM-II filter is suitable for handling nonlinear/non-Gaussian measurement update. © 2018 Elsevier Ltd"
,10.1111/jiec.12698,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035207702&doi=10.1111%2fjiec.12698&partnerID=40&md5=36c92f2381757f9eb1abf30ed66072d9,"Material flow analysis (MFA) is widely used to study the life cycles of materials from production, through use, to reuse, recycling, or disposal, in order to identify environmental impacts and opportunities to address them. However, development of this type of analysis is often constrained by limited data, which may be uncertain, contradictory, missing, or over-aggregated. This article proposes a Bayesian approach, in which uncertain knowledge about material flows is described by probability distributions. If little data is initially available, the model predictions will be rather vague. As new data is acquired, it is systematically incorporated to reduce the level of uncertainty. After reviewing previous approaches to uncertainty in MFA, the Bayesian approach is introduced, and a general recipe for its application to material flow analysis is developed. This is applied to map the global production of steel using Markov Chain Monte Carlo simulations. As well as aiding the analyst, who can get started in the face of incomplete data, this incremental approach to MFA also supports efforts to improve communication of results by transparently accounting for uncertainty throughout. © 2017 The Authors. Journal of Industrial Ecology, published by Wiley Periodicals, Inc., on behalf of Yale University."
,10.1186/s13634-017-0524-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040464639&doi=10.1186%2fs13634-017-0524-6&partnerID=40&md5=f4ca3fe2f14d18462d31663f7ba7d5ae,"Monte Carlo methods have become essential tools to solve complex Bayesian inference problems in different fields, such as computational statistics, machine learning, and statistical signal processing. In this work, we introduce a novel class of adaptive Monte Carlo methods, called adaptive independent sticky Markov Chain Monte Carlo (MCMC) algorithms, to sample efficiently from any bounded target probability density function (pdf). The new class of algorithms employs adaptive non-parametric proposal densities, which become closer and closer to the target as the number of iterations increases. The proposal pdf is built using interpolation procedures based on a set of support points which is constructed iteratively from previously drawn samples. The algorithm’s efficiency is ensured by a test that supervises the evolution of the set of support points. This extra stage controls the computational cost and the convergence of the proposal density to the target. Each part of the novel family of algorithms is discussed and several examples of specific methods are provided. Although the novel algorithms are presented for univariate target densities, we show how they can be easily extended to the multivariate context by embedding them within a Gibbs-type sampler or the hit and run algorithm. The ergodicity is ensured and discussed. An overview of the related works in the literature is also provided, emphasizing that several well-known existing methods (like the adaptive rejection Metropolis sampling (ARMS) scheme) are encompassed by the new class of algorithms proposed here. Eight numerical examples (including the inference of the hyper-parameters of Gaussian processes, widely used in machine learning for signal processing applications) illustrate the efficiency of sticky schemes, both as stand-alone methods to sample from complicated one-dimensional pdfs and within Gibbs samplers in order to draw from multi-dimensional target distributions. © 2018, The Author(s)."
,10.1093/gji/ggy362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054749227&doi=10.1093%2fgji%2fggy362&partnerID=40&md5=da0ae1f330e434b6d25dcce8a7372cdf,"Seismic surfacewave tomography is a tried and tested method to reveal the subsurface structure of the Earth.However, the conventional 2-step scheme of inverting first for 2-Dmaps of surface wave phase or group velocity and then inverting for the 3-D spatial velocity structure preserves little information about lateral spatial correlations, and introduces additional uncertainties and errors into the 3-D result. We introduce a 1-step 3-D non-linear surface wave tomography method that removes these effects by inverting for 3-D spatial structure directly from frequencydependent traveltime measurements. We achieve this using the reversible jump Markov chain Monte Carlo (McMC) algorithm with a fully 3-D model parametrization. Synthetic tests show that the method estimates the velocity model and associated uncertainties significantly better than the conventional 2-step McMC method, and that the computational cost seems to be comparable with 2-step McMC methods. The resulting uncertainties are more intuitively reasonable than those from the 2-step method, and provide directly interpretable uncertainty on volumetrics of structures of interest. © The Author(s) 2018. Published by Oxford University Press on behalf of The Royal Astronomical Society."
,10.1016/j.cam.2018.04.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047254709&doi=10.1016%2fj.cam.2018.04.028&partnerID=40&md5=879e3f25bcd73b08721ae237459ccc6d,"In this article, we consider the problem of estimation and prediction on unknown parameters of a Lomax distribution when the lifetime data are observed in the presence of progressively type-I hybrid censoring scheme. In the classical scenario, the Expectation–Maximization (EM) algorithm is utilized to derive the maximum likelihood estimates (MLEs) for the unknown parameters and associated confidence intervals. Under the Bayesian framework, the point estimates of unknown parameters with respect to different symmetric, asymmetric and balanced loss functions are obtained using Tierney–Kadane's approximation and Markov Chain Monte Carlo (MCMC) technique. Also, the highest posterior density (HPD) credible intervals for the parameters are reckoned using importance sampling procedure. Simulation experiments are performed to compare the different proposed methods. Further, the predictive estimates of censored observations and the corresponding prediction intervals are also provided. One real-life data example is presented to illustrate the derived results. © 2018 Elsevier B.V."
,10.1016/j.jhydrol.2018.10.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055130504&doi=10.1016%2fj.jhydrol.2018.10.004&partnerID=40&md5=cb7c515255a30e9df0b062cf6476872e,"Fracture-scale heterogeneity plays an important role in driving dispersion, mixing and heat transfer in fractured rocks. Current approaches to characterize fracture scale flow and transport processes largely rely on indirect information based on the interpretation of tracer tests. Geophysical techniques used in parallel with tracer tests can offer time-lapse images indicative of the migration of electrically-conductive tracers away from the injection location. In this study, we present a methodology to invert time-lapse ground penetrating radar reflection monitoring data acquired during a push-pull tracer test to infer fracture-scale transport patterns and aperture distribution. We do this by using a probabilistic inversion based on a Markov chain Monte Carlo algorithm. After demonstration on a synthetic dataset, we apply the new inversion method to field data. Our main findings are that the marginal distribution of local fracture apertures is well resolved and that the field site is characterized by strong flow channeling, which is consistent with interpretations of heat tracer tests in the same injection fracture. © 2018 Elsevier B.V."
,10.1007/s00180-018-0801-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042227577&doi=10.1007%2fs00180-018-0801-3&partnerID=40&md5=c165d68bb145fc590c85a12458a3ff57,"A flexible Bayesian periodic autoregressive model is used for the prediction of quarterly and monthly time series data. As the unknown autoregressive lag order, the occurrence of structural breaks and their respective break dates are common sources of uncertainty these are treated as random quantities within the Bayesian framework. Since no analytical expressions for the corresponding marginal posterior predictive distributions exist a Markov Chain Monte Carlo approach based on data augmentation is proposed. Its performance is demonstrated in Monte Carlo experiments. Instead of resorting to a model selection approach by choosing a particular candidate model for prediction, a forecasting approach based on Bayesian model averaging is used in order to account for model uncertainty and to improve forecasting accuracy. For model diagnosis a Bayesian sign test is introduced to compare the predictive accuracy of different forecasting models in terms of statistical significance. In an empirical application, using monthly unemployment rates of Germany, the performance of the model averaging prediction approach is compared to those of model selected Bayesian and classical (non)periodic time series models. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1016/j.cam.2018.05.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047760683&doi=10.1016%2fj.cam.2018.05.013&partnerID=40&md5=f967bf8753a1c82217d7cde96ad3222b,"A competing risks model based on Kumaraswamy distribution is discussed under progressive censoring. When the latent lifetime model of failure causes features different and common parameters, maximum likelihood estimates for unknown parameters are established where the existence and uniqueness of the estimates are provided, and the approximate confidence intervals are also constructed via the observed fisher information matrix. Moreover, Bayes estimates and associated highest posterior density credible intervals are also obtained based on Monte-Carlo Markov chain sampling methods. In addition, to test the equivalence of parameters between the competing risks, likelihood ratio test is also proposed. Finally, simulation studies and real-life example are presented for illustration purpose. © 2018 Elsevier B.V."
,10.1038/s41598-018-24648-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045963174&doi=10.1038%2fs41598-018-24648-w&partnerID=40&md5=d5cab9a05fec2982b889a2535c1ee018,"We present a framework to simulate SIR processes on networks using weighted shortest paths. Our framework maps the SIR dynamics to weights assigned to the edges of the network, which can be done for Markovian and non-Markovian processes alike. The weights represent the propagation time between the adjacent nodes for a particular realization. We simulate the dynamics by constructing an ensemble of such realizations, which can be done by using a Markov Chain Monte Carlo method or by direct sampling. The former provides a runtime advantage when realizations from all possible sources are computed as the weighted shortest paths can be re-calculated more efficiently. We apply our framework to three empirical networks and analyze the expected propagation time between all pairs of nodes. Furthermore, we have employed our framework to perform efficient source detection and to improve strategies for time-critical vaccination. © 2018 The Author(s)."
,10.1016/j.sigpro.2018.07.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051832245&doi=10.1016%2fj.sigpro.2018.07.028&partnerID=40&md5=5299a027e20da3bb3c2e0f1071fab26b,"Reversible jump Markov chain Monte Carlo (RJMCMC) is a Bayesian model estimation method, which has been generally used for trans-dimensional sampling and model order selection studies in the literature. In this study, we draw attention to unexplored potentials of RJMCMC beyond trans-dimensional sampling. the proposed usage, which we call trans-space RJMCMC exploits the original formulation to explore spaces of different classes or structures. This provides flexibility in using different types of candidate classes in the combined model space such as spaces of linear and nonlinear models or of various distribution families. As an application, we looked into a special case of trans-space sampling, namely trans-distributional RJMCMC in impulsive data modeling. In many areas such as seismology, radar, image, using Gaussian models is a common practice due to analytical ease. However, many noise processes do not follow a Gaussian character and generally exhibit events too impulsive to be successfully described by the Gaussian model. We test the proposed usage of RJMCMC to choose between various impulsive distribution families to model both synthetically generated noise processes and real-life measurements on power line communications impulsive noises and 2-D discrete wavelet transform coefficients. © 2018 Elsevier B.V."
,10.1016/j.jad.2018.07.044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051484252&doi=10.1016%2fj.jad.2018.07.044&partnerID=40&md5=47a42fbf2aa5511d1f338abef7553324,"Background: : Postpartum depression negatively affects the whole family and its prevalence in Sweden ranges between 6–10% for fathers and 13–16% for mothers. However, only mothers in Sweden are currently routinely screened. Aim: : The aim of this study was to determine if a postpartum depression screening for fathers in Stockholm County could be cost-effective. Methods: : National Swedish databases were used to find registry data and a literature review was undertaken to identify the model data inputs associated with postpartum depression in Sweden. The generated evidence was used to build a Markov model in TreeAge. One-way and probabilistic sensitivity analyses were performed to account for parameter uncertainties. Alternative scenario analyses were further undertaken to test the assumptions in the base case analysis. Results: : A postpartum screening for depression in fathers is cost-effective in base case and alternative scenarios. The results indicate that the screening program is associated with lower costs and higher health effects. The results were sensitive to variables of quality adjusted life years for the depressed fathers, probabilities of remission in treatment and no treatment groups and start age and productivity losses. The probabilistic sensitivity analysis resulted in a 70% probability of the postnatal depression screening intervention being cost-effective. Limitations: : The current study only uses secondary data; therefore future research should assess the cost-effectiveness of screening fathers for depression. Conclusion: : The postpartum screening intervention for fathers could be cost-effective compared to no screening. Future research should replicate the potential cost-effectiveness for screening fathers for postpartum depression. © 2018"
,10.1016/j.copbio.2018.01.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043587135&doi=10.1016%2fj.copbio.2018.01.023&partnerID=40&md5=34fbd2f28c735b3d6fed20c9991c4d98,"In all organisms, chromatin is packed to fulfil structural constraints and functional requirements. The hierarchical model of chromatin organization in the 3D nuclear space encompasses different topologies at diverse scale lengths, with chromosomes occupying distinct volumes, further organized in compartments, inside which the chromatin fibers fold into large domains and short-range loops. In the recent years, the combination of chromosome conformation capture (3C) techniques and high-throughput sequencing allowed probing chromatin spatial organization at the whole genome-scale. 3C-based methods produce enormous amounts of genomic data that are analyzed using ad-hoc computational procedures. Here, we review the common pipelines and methods for the analysis of genome-wide chromosome conformation capture data, highlighting recent developments in key steps for the identification of chromatin structures. © 2018 Elsevier Ltd"
,10.1007/s00180-018-0799-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042223829&doi=10.1007%2fs00180-018-0799-6&partnerID=40&md5=2bf5ced10ab31aba590de2c15b5a6148,"Using recent developments in econometrics and computational statistics we consider the estimation of the fractional Ornstein–Uhlenbeck process under a flow sampling scheme. To address the problem, we adopt throughout the paper an exact discretization approach. A flow sampling scheme arises, for example, naturally in modelling asset prices in continuous time since the time integral over successive observations defines the observable increments of asset log-prices. Exact discretization delivers an ARIMA(1,1,1) model for log-prices with a fractional driving noise. Building on the resulting exact discretization formulae and covariance function, a new Markov Chain Monte Carlo scheme is proposed and apply it to investigate the properties of both the time and frequency domain likelihoods/posteriors. For the exact discrete model, we adopt a general sampling interval of length h. This allows us to determine the optimal choice of h independent of the sample size. To illustrate the methods, with no ambition to a comprehensive data analysis, we use high frequency stock price data showing the relevance of aggregation over time issues in modelling asset prices. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1007/s00180-018-0805-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043462586&doi=10.1007%2fs00180-018-0805-z&partnerID=40&md5=3c4ceb75009f99c54db7cb8ce9338813,"This paper describes the package sppmix for the statistical environment R. The sppmix package implements classes and methods for modeling spatial point patterns using inhomogeneous Poisson point processes, where the intensity surface is assumed to be a multiple of a finite additive mixture of normal components and the number of components is a finite, fixed or random integer. Extensions to the marked inhomogeneous Poisson point processes case are also presented. We provide an extensive suite of R functions that can be used to simulate, visualize and model point patterns, estimate the parameters of the models, assess convergence of the algorithms and perform model selection and checking in the proposed modeling context. In addition, several approaches have been implemented in order to handle the standard label switching issue which arises in any modeling approach involving mixture models. We adapt a hierarchical Bayesian framework in order to model the intensity surfaces and have implemented two major algorithms in order to estimate the parameters of the mixture models involved: the data augmentation and the birth–death Markov chain Monte Carlo (DAMCMC and BDMCMC). We used C++ (via the Rcpp package) in order to implement the most computationally intensive algorithms. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1016/j.dark.2018.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053439820&doi=10.1016%2fj.dark.2018.09.001&partnerID=40&md5=d07925af26c6936d70394c262dd44076,"The generalized Chaplygin gas could be considered as the unified dark fluid model because it might describe the past decelerating matter dominated era and at present time it provides an accelerating expansion of the Universe. In this paper, we employed the Planck 2015 cosmic microwave background anisotropy, type-Ia supernovae, observed Hubble parameter data sets to measure the full parameter space of the generalized Chaplygin gas as an unified dark matter and dark energy model. The model parameters Bs and α determine the evolutional history of this unified dark fluid model by influencing the energy density ρGCG=ρGCG0[Bs+(1−Bs)a−3(1+α)]1∕(1+α). We assume the pure adiabatic perturbation of unified generalized Chaplygin gas. In the light of Markov Chain Monte Carlo method, we found that Bs=0.759−0.032−0.046 +0.020+0.051 and α=0.0801−0.0801−0.0801 +0.0208+0.1087 at 2σ level. The model parameter α is very close to zero, the nature of GCG model is very similar to cosmological standard model ΛCDM. © 2018 Elsevier B.V."
,10.1186/s40649-018-0051-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050802342&doi=10.1186%2fs40649-018-0051-0&partnerID=40&md5=30680a990cd47c571159f063e81ea829,"Background: In the framework of network sampling, random walk (RW) based estimation techniques provide many pragmatic solutions while uncovering the unknown network as little as possible. Despite several theoretical advances in this area, RW based sampling techniques usually make a strong assumption that the samples are in stationary regime, and hence are impelled to leave out the samples collected during the burn-in period. Methods: This work proposes two sampling schemes without burn-in time constraint to estimate the average of an arbitrary function defined on the network nodes, for example, the average age of users in a social network. The central idea of the algorithms lies in exploiting regeneration of RWs at revisits to an aggregated super-node or to a set of nodes, and in strategies to enhance the frequency of such regenerations either by contracting the graph or by making the hitting set larger. Our first algorithm, which is based on reinforcement learning (RL), uses stochastic approximation to derive an estimator. This method can be seen as intermediate between purely stochastic Markov chain Monte Carlo iterations and deterministic relative value iterations. The second algorithm, which we call the Ratio with Tours (RT)-estimator, is a modified form of respondent-driven sampling (RDS) that accommodates the idea of regeneration. Results: We study the methods via simulations on real networks. We observe that the trajectories of RL-estimator are much more stable than those of standard random walk based estimation procedures, and its error performance is comparable to that of respondent-driven sampling (RDS) which has a smaller asymptotic variance than many other estimators. Simulation studies also show that the mean squared error of RT-estimator decays much faster than that of RDS with time. Conclusion: The newly developed RW based estimators (RL- and RT-estimators) allow to avoid burn-in period, provide better control of stability along the sample path, and overall reduce the estimation time. Our estimators can be applied in social and complex networks. © 2018, The Author(s)."
,10.1016/j.neucom.2018.08.071,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053159085&doi=10.1016%2fj.neucom.2018.08.071&partnerID=40&md5=4255325c87b8122e813cde24e9b7475d,"A great number of machine learning algorithms strongly depend on the underlying distance metric for representing the important correlations of input data. Distance metric learning is defined as learning an appropriate similarity or distance metric for all input data pairs. Metric learning algorithms are of supervised and unsupervised categories with different deterministic and probabilistic approaches. One of the objectives of unsupervised metric learning is to project data points into a new space in such a way that high clustering accuracy is provided. This is obtainable by maximizing between-clusters separation. There exist some deterministic metric learning methods to serve this purpose. In this article, a probabilistic method for unsupervised distance metric learning is proposed which aims to maximize the separability among different clusters in the projected space. In this proposed method, distance metric learning and fuzzy c-means clustering are jointly formulated in a sense that FCM provides clusters, and distance metric learning algorithm applies the obtained clusters to materialize the maximum separability among all; moreover, Markov Chain Monte Carlo (MCMC) algorithm is applied to infer the latent variables. This proposed method, not only can obtain a low dimensional projection with specified number of dimensions, but also it can learn the proper number of reduced dimensions for each dataset in an automated sense. The experimental results reveal the out-performance of this method on different real-world datasets against its counterparts. © 2018 Elsevier B.V."
,10.1093/mnras/sty2144,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054062183&doi=10.1093%2fmnras%2fsty2144&partnerID=40&md5=0a9f3b9a61d47316811d5758c1be76a6,"The present day spectrum of the extragalactic background light (EBL) in UV, optical, and IR wavelengths is the integral result of multiple astrophysical processes going on throughout the evolution of the Universe. The relevant processes include star formation, stellar evolution, light absorption, and emission by the cosmic dust. The properties of these processes are known with uncertainties, which contribute to the EBL spectrum precision. In this paper, we develop a numerical model of the EBL spectrum while maintaining the explicit dependence on the astrophysical parameters involved.We constructed a Markov Chain in the parameter space by using the likelihood function built with the up-to-date upper and lower bounds on the EBL intensity. The posterior distributions built with the Markov Chain Monte Carlo method are used to determine an allowed range of the individual parameters of the model. Consequently, the star formation rate multiplication factor is constrained in the range 1.01 &lt; Csfr &lt; 1.69 at 68 per cent C.L. The method also results in the bounds on the lifetime, radius, dust particle density, and opacity of the molecular clouds that have large ambiguity otherwise. It is shown that there is a reasonable agreement between the model and the intensity bounds while the astrophysical parameters of the best-fitting model are close to their estimates from literature. © 2018 The Author(s). Published by Oxford University Press on behalf of the Royal Astronomical Society."
,10.1093/mnras/sty2326,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054084560&doi=10.1093%2fmnras%2fsty2326&partnerID=40&md5=58387c5297ffe866017b5b0a3eba0afa,"The well-known Bayes theorem assumes that a posterior distribution is a probability distribution. However, the posterior distribution may no longer be a probability distribution if an improper prior distribution (non-probability measure) such as an unbounded uniform prior is used. Improper priors are often used in the astronomical literature to reflect a lack of prior knowledge, but checking whether the resulting posterior is a probability distribution is sometimes neglected. It turns out that 23 out of 75 articles (30.7 per cent) published online in two renowned astronomy journals (ApJ and MNRAS) between 2017 Jan 1 and Oct 15 make use of Bayesian analyses without rigorously establishing posterior propriety. A disturbing aspect is that a Gibbs-type Markov chain Monte Carlo (MCMC) method can produce a seemingly reasonable posterior sample even when the posterior is not a probability distribution (Hobert & Casella 1996). In such cases, researchers may erroneously make probabilistic inferences without noticing that the MCMC sample is from a non-existing probability distribution. We review why checking posterior propriety is fundamental in Bayesian analyses, and discuss how to set up scientifically motivated proper priors. © 2018 The Author(s). Published by Oxford University Press on behalf of the Royal Astronomical Society."
,10.1108/MRR-11-2017-0377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049564282&doi=10.1108%2fMRR-11-2017-0377&partnerID=40&md5=860426a8fba26f3c658200dd3acba248,"Purpose: This paper aims to popularize the Bayesian methods among novice management researchers. The paper interprets the results of Bayesian method of confirmatory factor analysis (CFA), structural equation modelling (SEM), mediation and moderation analysis, with the intention that the novice researchers will apply this method in their research. The paper made an attempt in discussing various complex mathematical concepts such as Markov Chain Monte Carlo, Bayes factor, Bayesian information criterion and deviance information criterion (DIC), etc. in a lucid manner. Design/methodology/approach: Data collected from 172 pharmaceutical sales representatives were used. The study will help the management researchers to perform Bayesian CFA, Bayesian SEM, Bayesian moderation analysis and Bayesian mediation analysis using SPSS AMOS software. Findings: The interpretation of the results of Bayesian CFA, Bayesian SEM and Bayesian mediation analysis were discussed. Practical implications: The management scholars are non-statisticians and are not much aware of the benefits offered by Bayesian methods. Hitherto, the management scholars use predominantly traditional SEM in validating their models empirically, and this study will give an exposure to “Bayesian statistics” that has practical advantages. Originality/value: This is one paper, which discusses the following four concepts: Bayesian method of CFA, SEM, mediation and moderation analysis. © 2018, Emerald Publishing Limited."
,10.1080/02664763.2018.1435633,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042221260&doi=10.1080%2f02664763.2018.1435633&partnerID=40&md5=2a45803b1a9b5c4881b3f45110b83f77,"Word clouds constitute one of the most popular statistical tools for the visual analysis of text documents because they provide users with a quick and intuitive understanding of the content. Despite their popularity for visualizing single documents, word clouds are not appropriate to compare different text documents. Independently generating word clouds for each document leads to configurations where the same word is typically located in widely different positions. This makes it very difficult to compare two or more word clouds. This paper introduces COWORDS, a new stochastic algorithm to create multiple word clouds, including one for each document. The shared words in multiple documents are placed in the same position in all clouds. Similar documents produce similar and compact clouds, making it easier to simultaneously compare and interpret several word clouds. The algorithm is based on a probability distribution in which the most probable configurations are those with a desirable visual aspect, such as a low value for the total distance between the words in all clouds. The algorithm output is a set of word clouds that are randomly selected from this probability distribution. The selection procedure uses a Markov chain Monte Carlo simulation method. We present several examples that illustrate the performance and visual results that can be obtained by our algorithm. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
3,10.1016/j.atmosres.2018.07.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049460229&doi=10.1016%2fj.atmosres.2018.07.005&partnerID=40&md5=0a55d723e94ce9e6f61c1e6082983cdc,"To ameliorate agricultural impacts due to persistent drought-risks by promoting sustainable utilization and pre-planning of water resources, accurate rainfall forecasting models, addressing the dynamic nature of drought phenomenon, is crucial. In this paper, a multi-stage probabilistic machine learning model is designed and evaluated for forecasting monthly rainfall. The multi-stage hybrid MCMC-Cop-Bat-OS-ELM model utilizing online-sequential extreme learning machines integrated with Markov Chain Monte Carlo (MCMC) based bivariate-copula and the Bat algorithm is employed to incorporate significant antecedent rainfall (t–1) as the model's predictor in the training phase. After computing the partial autocorrelation function (PACF) at the first stage, twenty-five MCMC based copulas (i.e., Gaussian, t, Clayton, Gumble, Frank and Fischer-Hinzmann etc.) are adopted to determine the dependence of antecedent month's rainfall with the current and future rainfall at the second stage of the model design. Bat algorithm is applied to sort the optimal MCMC-copula model by a feature selection strategy at the third stage. At the fourth stage, PACF's of the optimal MCMC-copula model are computed to couple the output with OS-ELM algorithm to forecast future rainfall values in an independent test dataset. As a benchmarking process, standalone extreme learning machine (ELM) and random forest (RF) is also integrated with MCMC based copulas and the Bat algorithm, yielding a hybrid MCMC-Cop-Bat-ELM and a MCMC-Cop-Bat-RF models. The proposed multi-stage hybrid model is tested in agricultural belt region in Faisalabad, Jhelum and Multan, located in Pakistan. The testing performance of all three hybridized models, according to robust statistical error metrics, is satisfactory in comparison to the standalone counterparts, however the multi-stage, hybridized MCMC-Cop-Bat-OS-ELM model is found to be a superior tool for forecasting monthly rainfall. This multi-stage probabilistic learning model can be explored as a pertinent decision-support tool for agricultural water resources management in arid and semi-arid regions where a statistically significant relationship with antecedent rainfall exists. © 2018 Elsevier B.V."
,10.1016/j.chemolab.2018.09.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054834438&doi=10.1016%2fj.chemolab.2018.09.004&partnerID=40&md5=2999a26a10726b0bc5eb68fbcd771f3a,"A tutorial and a user-friendly program for evaluating risks of false decisions in conformity assessment of a multicomponent material or object due to measurement uncertainty, based on a Bayesian approach, are presented. The developed program consists of two separate MS-Excel spreadsheets. It allows calculation of the consumer's and producer's risks concerning each component of the material whose concentration was tested (‘particular risks’) as well as concerning the material as a whole (‘total risks’). According to the Bayesian framework, probability density functions of the actual/‘true’ component concentrations (prior pdfs) and likelihood functions (likelihoods) of the corresponding test results are used to model the knowledge about the material or object. Both cases of independent and correlated variables (the actual concentrations and the test results) are treated in the present work. Spreadsheets provide an estimate of the joint posterior pdf for the actual component concentrations as the normalized product of the multivariate prior pdf and the likelihood, starting from normal or log-normal prior pdfs and normal likelihoods, using Markov chain Monte Carlo (MCMC) simulations by the Metropolis-Hastings algorithm. The principles of Bayesian inference and MCMC are described for users with basic knowledge in statistics, necessary for correct formulation of a task and interpretation of the calculation results. The spreadsheet program was validated by comparison of the obtained results with analytical results calculated in the R programming environment. The developed program allows estimation of risks greater than 0.003% with standard deviations of such estimates spreading from 0.001% to 1.5%, depending on the risk value. Such estimation characteristics are satisfactory, taking into account known variability in measurement uncertainty associated with the test results of multicomponent materials. © 2018 Elsevier B.V."
,10.1080/0022250X.2017.1396985,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033672992&doi=10.1080%2f0022250X.2017.1396985&partnerID=40&md5=db7816f2ed11558a61fc26c3d299cc9d,"Generation of deviates from random graph models with nontrivial edge dependence is an increasingly important problem. Here, we introduce a method which allows perfect sampling from random graph models in exponential family form (“exponential family random graph” models), using a variant of Coupling From The Past. We illustrate the use of the method via an application to the Markov graphs, a family that has been the subject of considerable research. We also show how the method can be applied to a variant of the biased net models, which are not exponentially parameterized. © 2017 Taylor & Francis."
,10.1016/j.ecss.2018.07.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049775859&doi=10.1016%2fj.ecss.2018.07.012&partnerID=40&md5=98437a6722e149e68f908df2053808d6,"Differences in species diversity in ecosystems have long and often been discussed and interrogated in ecological research. A two year study on zooplankton diversity in the Meghna River and its estuary of Bangladesh found that diversity is comparatively higher in the estuary than the river. This study examines whether the biotic interactions of species can explain the diversity difference between these two aquatic habitats. The study is based on several species diversity hypotheses related to biotic interactions (i.e. low interspecific interactions, comparatively higher disturbance, higher species recruitments and higher intransitivities cause higher species diversity). A first order Markov chain model was used to estimate the biotic interactions i.e. species displacement ability, disturbance, colonization and intransitivities. Monte Carlo Markov chain (MCMC) simulations were performed to estimate species interactions from the Markov chain model. Results suggest low inter-specific interactions, comparatively higher disturbance rate, higher species recruitment and intransitivies in the Meghna estuary have caused higher zooplankton than the Meghna riverine ecosystem. In addition, it is evident that the negative association of species colonization with species displacement ability and displacement risk also led to a comparatively higher diversity in the Meghna estuary than the Meghna River. It is apparent from the zooplankton abundance data, that biotic interactions can explain the zooplankton species diversity difference in the Meghna aquatic ecosystems of Bangladesh. With these findings the current study provides valuable insights into zooplankton diversity differences in tropical ecosystems. © 2018 Elsevier Ltd"
,10.1016/j.tecto.2018.09.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053936110&doi=10.1016%2fj.tecto.2018.09.005&partnerID=40&md5=c02e8901e0068c899204b44569e23cc1,"Monte Carlo Uncertainty Estimation (MCUE) is an emerging heuristic uncertainty propagation method designed to provide reliable and time/cost efficient estimates of geometrical uncertainties in 3D geological modeling. MCUE is a subtype of Bayesian Monte Carlo method similar to geostatistical simulation. The methods described here rely on disturbance probability distributions that are parameterized to best represent individual input uncertainty. Essentially, disturbance distributions quantify the error about the location (x, y, z) and orientation (dip and azimuth) of observed geological structures. The disturbance distributions are sampled either independently or via a Markov-Chain to produce many plausible alternative datasets. These plausible datasets are then input to a 3D geological modeling engine to build a series of plausible alternative model realizations. Further processing may be applied to the series of plausible models to provide valuable decision aids such as probabilistic models, reliability models, or uncertainty reduction hotspot maps. In this paper, a complete and comprehensive MCUE procedure for common drillhole path and log uncertainty propagation is proposed. Basic concepts of drillhole uncertainty are introduced and are applied to a Markov Chain scheme. Appropriate disturbance distributions for the different parts of the problem and their respective parameterization are discussed. The method proposed is demonstrated on three separate proof of concept case studies of increasing complexity. Results demonstrate that the method is able to propagate path and log uncertainty appropriately. First order interpretation indicates that both path and log uncertainty increase with depth and angle of attack to the geological interfaces. Ignoring drillhole uncertainty was found to be detrimental to the understanding of a modeled area which is most likely due to the over-constraining effect brought by “perfect” drillholes. The third case study (Mansfield) hints that uncertainty is better reduced when drillholes intersect the “triple line” that partitions three distinct lithologies. In cross-sections, triples lines appear as triple points. © 2018 The Authors"
,10.1080/00949655.2018.1504944,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052104064&doi=10.1080%2f00949655.2018.1504944&partnerID=40&md5=31dbf81f6de0540882ee70165a110ae5,"Interval-censored survival data arise often in medical applications and clinical trials [Wang L, Sun J, Tong X. Regression analyis of case II interval-censored failure time data with the additive hazards model. Statistica Sinica. 2010;20:1709–1723]. However, most of existing interval-censored survival analysis techniques suffer from challenges such as heavy computational cost or non-proportionality of hazard rates due to complicated data structure [Wang L, Lin X. A Bayesian approach for analyzing case 2 interval-censored data under the semiparametric proportional odds model. Statistics & Probability Letters. 2011;81:876–883; Banerjee T, Chen M-H, Dey DK, et al. Bayesian analysis of generalized odds-rate hazards models for survival data. Lifetime Data Analysis. 2007;13:241–260]. To address these challenges, in this paper, we introduce a flexible Bayesian non-parametric procedure for the estimation of the odds under interval censoring, case II. We use Bernstein polynomials to introduce a prior for modeling the odds and propose a novel and easy-to-implement sampling manner based on the Markov chain Monte Carlo algorithms to study the posterior distributions. We also give general results on asymptotic properties of the posterior distributions. The simulated examples show that the proposed approach is quite satisfactory in the cases considered. The use of the proposed method is further illustrated by analyzing the hemophilia study data [McMahan CS, Wang L. A package for semiparametric regression analysis of interval-censored data; 2015. http://CRAN.R-project.org/package=ICsurv. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/13658816.2018.1504219,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052941492&doi=10.1080%2f13658816.2018.1504219&partnerID=40&md5=d4bec8c0ea992b2abb2b902e5ecd3496,"In this research, we match web-based activity diary data with daily mobility information recorded by GPS trackers for a sample of 709 residents in a 7-day survey in Beijing in 2012 to investigate activity satisfaction. Given the complications arising from the irregular time intervals of GPS-integrated diary data and the associated complex dependency structure, a direct application of standard (spatial) panel data econometric approaches is inappropriate. This study develops a multi-level temporal autoregressive modelling approach to analyse such data, which conceptualises time as continuous and examines sequential correlations via a time or space-time weights matrix. Moreover, we manage to simultaneously model individual heterogeneity through the inclusion of individual random effects, which can be treated flexibly either as independent or dependent. Bayesian Markov chain Monte Carlo (MCMC) algorithms are developed for model implementation. Positive sequential correlations and individual heterogeneity effects are both found to be statistically significant. Geographical contextual characteristics of sites where activities take place are significantly associated with daily activity satisfaction, controlling for a range of situational characteristics and individual socio-demographic attributes. Apart from the conceivable urban planning and development implications of our study, we demonstrate a novel statistical methodology for analysing semantic GPS trajectory data in general. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1109/TIE.2018.2815941,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043763344&doi=10.1109%2fTIE.2018.2815941&partnerID=40&md5=7f95dbc7d0369259b3bb391d19cf26ac,"The new-generation high-efficiency video coding (HEVC) standard has recently been developed by the Joint Collaborative Team on Video Coding to provide significant improvement in picture quality, especially for high-resolution videos. However, one of the most important challenges in HEVC is a high degree of computational complexity. This problem is addressed in a novel way considering skip detection and coding unit termination as two-class decision making problems. A Bayesian classifier is used for both of these approaches. Prior and class conditional probability values for a Bayesian classifier are not known at the time of encoding a video frame. Therefore, the Markov chain Monte Carlo model is used. Experimental results show that the proposed method provides significant time reduction for encoding with reasonably low loss in video quality. © 1982-2012 IEEE."
1,10.3150/17-BEJ976,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046714336&doi=10.3150%2f17-BEJ976&partnerID=40&md5=9869ab53d5b63b9031ee46dd957bb8d8,"The purpose of this paper is to introduce a new Markov chain Monte Carlo method and to express its effectiveness by simulation and high-dimensional asymptotic theory. The key fact is that our algorithm has a reversible proposal kernel, which is designed to have a heavy-tailed invariant probability distribution. A high-dimensional asymptotic theory is studied for a class of heavy-tailed target probability distributions. When the number of dimensions of the state space passes to infinity, we will show that our algorithm has a much higher convergence rate than the pre-conditioned Crank–Nicolson (pCN) algorithm and the random-walk Metropolis algorithm. © 2018 ISI/BS."
1,10.1016/j.dsp.2018.07.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051273711&doi=10.1016%2fj.dsp.2018.07.007&partnerID=40&md5=de27b7421f972ddaefde865c88b00943,"Bayesian methods and their implementations by means of sophisticated Monte Carlo techniques have become very popular in signal processing over the last years. Importance Sampling (IS) is a well-known Monte Carlo technique that approximates integrals involving a posterior distribution by means of weighted samples. In this work, we study the assignation of a single weighted sample which compresses the information contained in a population of weighted samples. Part of the theory that we present as Group Importance Sampling (GIS) has been employed implicitly in different works in the literature. The provided analysis yields several theoretical and practical consequences. For instance, we discuss the application of GIS into the Sequential Importance Resampling framework and show that Independent Multiple Try Metropolis schemes can be interpreted as a standard Metropolis–Hastings algorithm, following the GIS approach. We also introduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS. The first one, named Group Metropolis Sampling method, produces a Markov chain of sets of weighted samples. All these sets are then employed for obtaining a unique global estimator. The second one is the Distributed Particle Metropolis–Hastings technique, where different parallel particle filters are jointly used to drive an MCMC algorithm. Different resampled trajectories are compared and then tested with a proper acceptance probability. The novel schemes are tested in different numerical experiments such as learning the hyperparameters of Gaussian Processes, two localization problems in a wireless sensor network (with synthetic and real data) and the tracking of vegetation parameters given satellite observations, where they are compared with several benchmark Monte Carlo techniques. Three illustrative Matlab demos are also provided. © 2018 Elsevier Inc."
1,10.1016/j.cageo.2018.07.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050894096&doi=10.1016%2fj.cageo.2018.07.004&partnerID=40&md5=56f4e3055094cc114e850be067e9814e,"AMORPH utilizes a new Bayesian statistical approach to interpreting X-ray diffraction results of samples with both crystalline and amorphous components. AMORPH fits X-ray diffraction patterns with a mixture of narrow and wide components, simultaneously inferring all of the model parameters and quantifying their uncertainties. The program simulates background patterns previously applied manually, providing reproducible results, and significantly reducing inter- and intra-user biases. This approach allows for the quantification of amorphous and crystalline materials and for the characterization of the amorphous component, including properties such as the centre of mass, width, skewness, and nongaussianity of the amorphous component. Results demonstrate the applicability of this program for calculating amorphous contents of volcanic materials and independently modeling their properties in compositionally variable materials. © 2018 Elsevier Ltd"
,10.1016/j.jeconom.2018.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053813655&doi=10.1016%2fj.jeconom.2018.08.001&partnerID=40&md5=69cca64c3bc3a8b8c33f6a2ba880af26,"Two test statistics are proposed to determine model specification after a model is estimated by an MCMC method. The first test is the MCMC version of IOSA test and its asymptotic null distribution is normal. The second test is motivated from the power enhancement technique of Fan et al. (2015). It combines a component (J1) that tests a null point hypothesis in an expanded model and a power enhancement component (J0) obtained from the first test. It is shown that J0 converges to zero when the null model is correctly specified and diverges when the null model is misspecified. Also shown is that J1 is asymptotically χ2-distributed, suggesting that the second test is asymptotically pivotal, when the null model is correctly specified. The main feature of the first test is that no alternative model is needed. The second test has several properties. First, its size distortion is small and hence bootstrap methods can be avoided. Second, it is easy to compute from MCMC output and hence is applicable to a wide range of models, including latent variable models for which frequentist methods are difficult to use. Third, when the test statistic rejects the null model and J1 takes a large value, the test suggests the source of misspecification. The finite sample performance is investigated using simulated data. The method is illustrated in a linear regression model, a linear state-space model, and a stochastic volatility model using real data. © 2018 Elsevier B.V."
,10.1016/j.autcon.2018.08.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052646219&doi=10.1016%2fj.autcon.2018.08.014&partnerID=40&md5=d202114b68ad92a5cdace1dae02895fa,"Crosshole ground-penetrating radar (GPR) is a widely used measurement technique to help inspect the structural integrity of man-made underground structures. In a previous paper, we have introduced a Bayesian framework for inversion of crosshole GPR experiments to help back out defects in concrete underground structures. Here, we evaluate the practical usefulness of our inversion framework by application to waveform data from a real-world GPR survey of a diaphragm wall panel with two embedded structure defects. We also use this case study to further refine our methodology by introducing the elements of a two-stage inversion method to help delineate the exact location and shape of small structure defects. Herein, a low-resolution inversion composed of relatively few inversion coefficients (stage-1) is used to determine roughly the presence of structure defects, followed by a second inversion (stage-2) with much enhanced spatial resolution in those areas classified with anomalous or suspicious permittivity values. This two-stage inversion approach uses more wisely CPU-resources by focusing primarily on those areas of the concrete structure that have been classified as anomalies. We investigate the benefits of this two-stage inversion scheme using a synthetic and real-world case study involving waveform data of a diaphragm wall panel measured with crosshole GPR. Our results demonstrate that the proposed two-stage inversion method recovers successfully the location and shape of structure defects, at a computational cost that is considerably lower than the original inversion framework. © 2018"
,10.1016/j.mbs.2018.08.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053711844&doi=10.1016%2fj.mbs.2018.08.011&partnerID=40&md5=45ed9a7707f765dd2b23236e5e7d2a8d,"Background and ObjectiveBayesian State Space models are recent advancement in stochastic modeling which capture the randomness of a hidden background process by scrutinizing the prior knowledge and likelihood of observed data. This article elucidate the scope of Bayesian state space modeling on predicting the future expression values of a longitudinal micro array data. MethodsThe study conveniently makes use of longitudinally collected clinical trial data (GSE30531) from NCBI Gene Expression Omnibus (GEO) data repository. Multiple testing methodology using t-test is used for selecting differentially expressed genes between groups for fitting the model. The parameter values of the predictive model and future expression levels are estimated by drawing samples from the posterior joint distribution using a stochastic Markov Chain Monte Carlo (MCMC) algorithm which relies on Gibbs Sampling. The study also made an attempt to get estimates and its 95% Credible Interval through assumptions of different covariance structures like Variance Components, First order Auto Regressive and Unstructured variance–covariance structure to showcase the flexibility of the algorithm. Results72 Distinct genes with significantly different expression levels where selected for model fitting. Parameter estimates showed almost similar trends under different covariance structure assumption. Cross tabulation of gene frequencies having minimum credible interval under each covariance structure and study group showed a significant P value of 0.02. ConclusionsPresent study reveals that Bayesian state space models can be effectively used to explain and predict a complex data like gene expression data. © 2018 Elsevier Inc."
,10.1016/j.jhydrol.2018.08.047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053828433&doi=10.1016%2fj.jhydrol.2018.08.047&partnerID=40&md5=42da371335dc597d1a67c8384507a876,"Carbon-14 (14C) has been measured in groundwater for over half a century and remains a widely used tool for understanding groundwater flow systems. Ultimately, the usefulness of 14C as a groundwater tracer relies on the ability to distinguish between changes in concentration due to various chemical/physical processes (e.g. chemical reactions with solid carbonate material, conditions at the water table), and changes due to ageing along flow paths, the latter being most informative of groundwater flow conditions. To this end, a number of correction methodologies have been developed to account for chemical modifications in groundwater systems. In this paper, we implement two different single sample correction models, one for closed and one for open system carbonate dissolution in conjunction with a Markov chain Monte Carlo (MCMC) approach at two sites; the sedimentary Port Willunga Formation Aquifer in South Australia and a fractured rock aquifer in the Hamersley Basin, northwest Australia. For comparison, we include argon-39 (39Ar) data taken from some of the wells sampled and use a mixing envelope constraint in the MCMC procedure. We found that considering all of the errors associated with 14C correction resulted in a distribution of values to consider for groundwater dating procedures. When accounting for all parameters associated with single sample correction techniques, the associated error was 10 times greater than the analytical errors. Additionally, inclusion of the 39Ar data produced mixed results, with little improvement observed in the Port Willunga Aquifer (closed system correction), and a significant improvement observed at the Hamersley site (open system). This is most likely due to the mixing caused by long screens and the sensitivity of the open system correction model. Our results highlight the importance of considering all sources of error in groundwater dating studies. © 2018 Elsevier B.V."
,10.1002/for.2546,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053503795&doi=10.1002%2ffor.2546&partnerID=40&md5=d6bde55295833c0310822301a8c7b5db,"This paper examines a method of filtering the volatility dynamics of the KOSPI200 index under a stochastic volatility model. This study applies a particle filter algorithm for sequential estimation of volatility dynamics. In order to improve our estimation, the cross-asset class approach is adopted by adding option price information to the model. The entire estimation procedure including the derivation of theoretical option price is based on Bayesian Markov chain Monte Carlo methods, so the method presented in this paper can be applied to diversified volatility models. Through the simulation study, we confirm that this method can estimate unknown volatility dynamics correctly, and the use of additional option prices improves both the accuracy and efficiency of volatility filtering. The sequential one-step-ahead prediction of the distribution of the KOSPI 200 index and index option prices shows that the additional option price information also enhances the prediction performance. © 2018 John Wiley & Sons, Ltd."
,10.1002/wics.1441,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054701188&doi=10.1002%2fwics.1441&partnerID=40&md5=8c69703bb4d3bfcf51714b11a512157b,"The analysis of small-area health data is often a focus of epidemiological research. While it is possible to provide smoothed risk estimates for disease maps, it is often important to consider underlying structure in the risk outcome that is suggested by understanding of the etiology of disease processes. To address this more complex problem, it can be important to consider latent structure in the disease risk. Latent structure can take a variety of forms, from basic random effects to more complex latent variables models. In this paper I will review outline the basic approaches to the problem of latent structure for spatio-temporal health outcome data and the solutions that Bayesian modeling can offer. This article is categorized under: Applications of Computational Statistics > Computational Climate Change and Numerical Weather Forecasting Statistical Learning and Exploratory Methods of the Data Sciences > Modeling Methods Statistical and Graphical Methods of Data Analysis > Bayesian Methods and Theory Statistical and Graphical Methods of Data Analysis > Markov Chain Monte Carlo (MCMC). © 2018 Wiley Periodicals, Inc."
1,10.1115/1.4038475,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049388189&doi=10.1115%2f1.4038475&partnerID=40&md5=12249e70d09cd8a441c3e893ed3baa3a,"In this paper, we present a method to determine the quantitative stability level of a lean-premixed combustor from dynamic pressure data. Specifically, we make use of the autocorrelation function of the dynamic pressure signal acquired in a combustor where a turbulent flame acts as a thermoacoustic driver. In the proposed approach, the unfiltered pressure signal including several modes is analyzed by an algorithm based on Bayesian statistics. For this purpose, a Gibbs sampler is used to calculate parameters like damping rates and eigenfrequencies in the form of probability density functions (PDF) by a Markov-chain Monte Carlo (MCMC) method. The method provides a robust solution algorithm for fitting problems without requiring initial values. A further advantage lies in the nature of the statistical approach since the results can be assessed regarding its quality by means of the PDF and its standard deviation for each of the obtained parameters. First, a simulation of a stochastically forced van-der-Pol oscillator with preset input values is carried out to demonstrate accuracy and robustness of the method. In this context, it is shown that, despite a large amount of uncorrelated background noise, the identified damping rates are in a good agreement with the simulated parameters. Second, this technique is applied to measured pressure data. By doing so, the combustor is initially operated under stable conditions before the thermal power is gradually increased by adjusting the fuel mass flow rate until a limit-cycle oscillation is established. It is found that the obtained damping rates are qualitatively in line with the amplitude levels observed during operation of the combustor. Copyright © 2018 by ASME."
,10.1016/j.nucengdes.2018.08.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052301925&doi=10.1016%2fj.nucengdes.2018.08.016&partnerID=40&md5=8e87dd6358450b48dff7269955bb0f93,"In seismic risk assessment, the fragility curve is used to estimate the reliability of structures and equipment under seismic loads. The shape of fragility curves is usually approximated by the cumulative distribution function of a lognormal distribution. The estimation of the parameters of the fragility curves requires gathering different sources of information and quantifying the uncertainties coming from these sources. This paper proposes a methodology for the computation of fragility curves for nuclear power plant equipment, based on a Bayesian updating framework that combines the results of numerical simulations and damage data. An artificial neural network is trained iteratively by optimizing its prediction uncertainties over the ground motion sample space, and it is used to conduct numerical simulations. The results of the numerical simulations provide a prior estimation of the seismic capacity of the equipment. The estimation of the uncertainty related to the equipment capacity is taken from the literature. Damage data, collected from the in situ observation and the database of the seismic qualification utility group (SQUG), are used to construct the likelihood function for the Bayesian updating. The posterior equipment capacity is evaluated by Markov chain Monte Carlo simulation and posterior fragility curves are, then, obtained. The main contributions of the work are: (i) proposal of an adaptive training algorithm of artificial neural networks to improve the design of experiments for finite element simulations; (ii) proposal of a two-step transformation method to construct the likelihood function with existing damage data from the SQUG database. The methodology is applied to compute the fragility curves of a low-voltage switchgear of a nuclear power plant, within the so-called KARISMA benchmark. © 2018 Elsevier B.V."
,10.1109/TASLP.2018.2852500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049342264&doi=10.1109%2fTASLP.2018.2852500&partnerID=40&md5=40aedb01cdc2cd5cb8bfe875941e3223,"We develop a parallelizable Markov chain Monte Carlo sampler for a Dirichlet process mixture of mixtures model. Our sampler jointly infers a codebook and clusters. The codebook is a global collection of components. Clusters are mixtures, defined over the codebook. We combine a nonergodic Gibbs sampler with two layers of split and merge samplers on codebook and mixture level to form a valid ergodic chain. We design an additional switch sampler for components that supports convergence in our experimental results. In the use case of unsupervised subword modeling, we show that our method infers complex classes from real speech feature vectors that consistently show higher quality on several evaluation metrics. At the same time, we infer fewer classes that represent subword units more consistently and show longer durations, compared to a standard Dirichlet process mixture model sampler. © 2014 IEEE."
,10.1007/s11128-018-2078-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053852552&doi=10.1007%2fs11128-018-2078-4&partnerID=40&md5=9a921e34330aacc0f6abf0f2fca5294a,"We investigate the quantum parameter estimation in circuit quantum electrodynamics via dispersive measurement. Based on the Metropolis–Hastings algorithm and the Markov chain Monte Carlo (MCMC) integration, a new algorithm is proposed to calculate the Fisher information by the stochastic master equation. The Fisher information is expressed in the form of log-likelihood functions and further approximated by the MCMC integration. Numerical results show that the evolution of the Fisher information can approach the quantum Fisher information in a short time interval. These results demonstrate the effectiveness of the proposed algorithm. Finally, based on the proposed algorithm, we consider the effects of the measurement operator and the measurement efficiency on the Fisher information. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
,10.1016/j.ejor.2017.10.059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034420961&doi=10.1016%2fj.ejor.2017.10.059&partnerID=40&md5=962e5a6b7fcf4c55c29aa160e2171d33,"Shelf out-of-stock (OOS) is a salient problem that causes non-trivial profit loss in retailing. To tackle shelf-OOS that plagues customers, retailers, and suppliers, we develop a decision support model for managers who aim to fix the recurring issue of shelf-OOS through data-driven audits. Specifically, we propose a point-of-sale (POS) data analytics approach and use consecutive zero sales observations in POS data as signals to develop an optimal audit policy. The proposed model considers relevant cost factors, conditional probability of shelf-OOS, and conditional expectation of shelf-OOS duration. We then analyze the impact of relevant cost factors, stochastic transition from non-OOS to OOS, zero sale probability of the underlying demand, managers’ perceived OOS likelihood, and even random fixes of shelf-OOS on optimal decisions. We also uncover interesting dynamics between decisions, costs, and probability estimates. After analyzing model behaviors, we perform extensive simulations to validate the economic utility of the proposed data-driven audits, which can be a cost-efficient complement to existing shelf inventory control. We further outline implementation details for the sake of model validation. Particularly, we use Bayesian inference and Markov chain Monte Carlo to develop an estimation framework that ensures all model parameters are empirically grounded. We conclude by articulating practical and theoretical implications of our data-driven audit policy design for retail managers. © 2017 Elsevier B.V."
,10.1016/j.ijpe.2018.09.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054827079&doi=10.1016%2fj.ijpe.2018.09.020&partnerID=40&md5=c00b302a4841162d4ef925b8fa816ead,"National culture matters in business as it affects managerial attitudes, values, behaviors, and efficacy in organizations. It contributes to our understanding of environmental management issues as well. Previous studies investigating the link between national culture and environmental performance had mixed findings as they overlooked the intervening mechanism between the two—firm environmental management practice (EMP) adoption. This paper considers the missing link, lays out a complete theoretical framework, and empirically tests the effects of national culture on firm EMP adoption and EMP effectiveness. The analysis uses data collected during the 5th round of the Global Manufacturing Research Group (GMRG) survey, Hofstede cultural dimensions, and World Bank Database. Hierarchical linear models (HLMs) using the Bayesian Markov chain Monte Carlo (MCMC) approach are employed to examine the cross-level relationships in the framework. The study finds out that certain cultural traits are significantly related to corporate EMP adoption and how effectively EMPs are implemented after adoption. The study provides meanings insights into the role of national culture in the context of environmental management. © 2018 Elsevier B.V."
,10.1016/j.infrared.2018.09.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053769640&doi=10.1016%2fj.infrared.2018.09.005&partnerID=40&md5=b9c40157dc22146d679a8f1acfa96933,"The micro-Doppler (MD) effect of weak vibration target is obvious in infrared laser detection. This provides the foundation for precise estimation of micro-motion parameters, and makes the target classification and recognition possible. The multi-targets or multi-scattering points existing in the detecting field will generate the single-channel multi-component (SCMC) signal in laser detection. Further, the similar micro-motion parameters will lead to the feature overlapping in time-frequency domain, which will increase the difficulty of parameter estimation. In this paper, a separate parameter estimator based on the maximum likelihood framework and singular value decomposition is proposed to deal with this mixed signal. First, an improved singular value ratio (SVR) spectrum with detailed period scanning is presented to locate the vibration frequency. The amplitude ratio information of each component is also extracted from the SVR spectrum. Then, the analytic expression of the maximum likelihood estimation (MLE) of micro-motion parameters is derived. To solve the high nonlinear problem in laser MD signal, a new likelihood function (LF) is designed in the derivation process. The Robustness and efficiency are both increased with this new LF. The Markov chain Monte Carlo (MCMC) sampling is employed to implement the MLE. Finally, the simulation results verifies the validity of the proposed method. The comparison with the Cramer-Rao bound shows the ability of accurate estimation of the proposed method. © 2018 Elsevier B.V."
,10.1111/rssc.12280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045730479&doi=10.1111%2frssc.12280&partnerID=40&md5=1bef3a1210e4d6c7e0feb96e10d7f901,"Identifying a peptide on the basis of a scan from a mass spectrometer is an important yet highly challenging problem. To identify peptides, we present a Bayesian approach which uses prior information about the average relative abundances of bond cleavages and the prior probability of any particular amino acid sequence. The scoring function proposed is composed of two overall distance measures, which measure how close an observed spectrum is to a theoretical scan for a peptide. Our use of our scoring function, which approximates a likelihood, has connections to the generalization presented by Bissiri and co-workers of the Bayesian framework. A Markov chain Monte Carlo algorithm is employed to simulate candidate choices from the posterior distribution of the peptide sequence. The true peptide is estimated as the peptide with the largest posterior density. © 2018 Royal Statistical Society"
,10.1007/s11222-017-9787-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033405542&doi=10.1007%2fs11222-017-9787-x&partnerID=40&md5=8c71ba05cf84ea2f142e2b7dc9bd4ae7,"Queueing networks describe complex stochastic systems of both theoretical and practical interest. They provide the means to assess alterations, diagnose poor performance and evaluate robustness across sets of interconnected resources. In the present paper, we focus on the underlying continuous-time Markov chains induced by these networks, and we present a flexible method for drawing parameter inference in multi-class Markovian cases with switching and different service disciplines. The approach is directed towards the inferential problem with missing data, where transition paths of individual tasks among the queues are often unknown. The paper introduces a slice sampling technique with mappings to the measurable space of task transitions between the service stations. This can address time and tractability issues in computational procedures, handle prior system knowledge and overcome common restrictions on service rates across existing inferential frameworks. Finally, the proposed algorithm is validated on synthetic data and applied to a real data set, obtained from a service delivery tasking tool implemented in two university hospitals. © 2017, Springer Science+Business Media, LLC."
1,10.1007/s11222-017-9786-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032711994&doi=10.1007%2fs11222-017-9786-y&partnerID=40&md5=9f686b689d296f8cea69335b09844301,"In cluster analysis interest lies in probabilistically capturing partitions of individuals, items or observations into groups, such that those belonging to the same group share similar attributes or relational profiles. Bayesian posterior samples for the latent allocation variables can be effectively obtained in a wide range of clustering models, including finite mixtures, infinite mixtures, hidden Markov models and block models for networks. However, due to the categorical nature of the clustering variables and the lack of scalable algorithms, summary tools that can interpret such samples are not available. We adopt a Bayesian decision theoretical approach to define an optimality criterion for clusterings and propose a fast and context-independent greedy algorithm to find the best allocations. One important facet of our approach is that the optimal number of groups is automatically selected, thereby solving the clustering and the model-choice problems at the same time. We consider several loss functions to compare partitions and show that our approach can accommodate a wide range of cases. Finally, we illustrate our approach on both artificial and real datasets for three different clustering models: Gaussian mixtures, stochastic block models and latent block models for networks. © 2017, The Author(s)."
,10.1016/j.insmatheco.2018.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053081962&doi=10.1016%2fj.insmatheco.2018.06.002&partnerID=40&md5=e23b1b3cdd5070891bdf9a3075c00472,"Standard regression models are often insufficient to describe the complex relationships that exist in healthcare claims. A Bayesian nonparametric regression approach is presented as a flexible regression model that relaxes the assumption of Gaussianity. The details for implementation are presented. Bayesian nonparametric regression is applied to a dataset of claims by episode treatment group (ETG) with a specific focus on prediction of new observations. It is shown that the predictive accuracy improves when compared both to standard linear model assumptions and the more flexible Generalized Beta regression. Of the 347 different ETGs, the nonparametric regression outperformed both the standard linear and generalized beta regression on all but 11. By studying Conjunctivitis and Lung Transplants specifically, it is shown that this approach can handle complex characteristics of the regression error distribution such as skewness, thick tails, outliers, and bimodality. © 2018 Elsevier B.V."
,10.1016/j.jpowsour.2018.09.091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054415075&doi=10.1016%2fj.jpowsour.2018.09.091&partnerID=40&md5=1efe33aba84c9511d5b88bf2ba331f82,"A curve fitting is the most important process in the analysis of electrochemical impedance spectra to evaluate the ionic conductivity of materials. To analyze the impedance spectra, a gradient method such as steepest descent has been used so far. However, the parameter solution by using the gradient method is often trapped into local minima, and the curve fitting strongly depends on the initial parameters. In this study, to avoid the local minima issue, we propose a random walk Metropolis Hastings algorithm to analyze impedance spectra, where we can provide a unique solution of the impedance spectra. As an example, we measured the solid-state oxide electrolyte of (La0.62Li0.15)TiO3, (La0.53Li0.40)TiO3 and (La0.31Li0.07)NbO3 polycrystal and we uniquely identify the respective lithium ion conductivity at the bulk and the grain boundary by using the random walk Metropolis Hastings algorithm. The present algorithm is free from the choice of the initial values of the fitting parameters and moreover the estimated accuracy of the Li-ion conductivity is better than 5%. © 2018 Elsevier B.V."
,10.3150/17-BEJ938,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045889269&doi=10.3150%2f17-BEJ938&partnerID=40&md5=b919720434a41b9ce7e94444dbec6183,"Perturbation theory for Markov chains addresses the question of how small differences in the transition probabilities of Markov chains are reflected in differences between their distributions. We prove powerful and flexible bounds on the distance of the nth step distributions of two Markov chains when one of them satisfies a Wasserstein ergodicity condition. Our work is motivated by the recent interest in approximate Markov chain Monte Carlo (MCMC) methods in the analysis of big data sets. By using an approach based on Lyapunov functions, we provide estimates for geometrically ergodic Markov chains under weak assumptions. In an autoregressive model, our bounds cannot be improved in general. We illustrate our theory by showing quantitative estimates for approximate versions of two prominent MCMC algorithms, the Metropolis–Hastings and stochastic Langevin algorithms. © 2018 ISI/BS."
1,10.1002/qre.2329,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053644777&doi=10.1002%2fqre.2329&partnerID=40&md5=32bedc5901d82cea7659d034e41772d1,"Very recently, a new degradation process model, named the transformed gamma process, has been proposed to describe Markovian degradation processes whose increments over disjoint intervals are not independent, so that the degradation growth over a future time interval can depend both on the current age and the current state (degradation level) of the unit. This paper introduces a Bayesian estimation approach for such a process, based on prior information on physical characteristics of the observed degradation process. Several different prior distributions are then proposed, reflecting different degrees of knowledge of the analyst on the observed phenomenon. A Monte Carlo Markov Chain technique is adopted to estimate the transformed gamma parameters and some functions thereof, such as the residual reliability of a unit, as well as to predict future degradation growth and residual lifetime. Finally, the proposed approach is applied to a real dataset consisting of wear measures of the liners of the 8-cylinder engine which equips a cargo ship. © 2018 John Wiley & Sons, Ltd."
1,10.1007/s11222-017-9789-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033456573&doi=10.1007%2fs11222-017-9789-8&partnerID=40&md5=d73c68a42fc8fd54350d1461cd36448e,"Fitting stochastic kinetic models represented by Markov jump processes within the Bayesian paradigm is complicated by the intractability of the observed-data likelihood. There has therefore been considerable attention given to the design of pseudo-marginal Markov chain Monte Carlo algorithms for such models. However, these methods are typically computationally intensive, often require careful tuning and must be restarted from scratch upon receipt of new observations. Sequential Monte Carlo (SMC) methods on the other hand aim to efficiently reuse posterior samples at each time point. Despite their appeal, applying SMC schemes in scenarios with both dynamic states and static parameters is made difficult by the problem of particle degeneracy. A principled approach for overcoming this problem is to move each parameter particle through a Metropolis-Hastings kernel that leaves the target invariant. This rejuvenation step is key to a recently proposed SMC 2 algorithm, which can be seen as the pseudo-marginal analogue of an idealised scheme known as iterated batch importance sampling. Computing the parameter weights in SMC 2 requires running a particle filter over dynamic states to unbiasedly estimate the intractable observed-data likelihood up to the current time point. In this paper, we propose to use an auxiliary particle filter inside the SMC 2 scheme. Our method uses two recently proposed constructs for sampling conditioned jump processes, and we find that the resulting inference schemes typically require fewer state particles than when using a simple bootstrap filter. Using two applications, we compare the performance of the proposed approach with various competing methods, including two global MCMC schemes. © 2017, The Author(s)."
,10.1016/j.aap.2018.08.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054056389&doi=10.1016%2fj.aap.2018.08.021&partnerID=40&md5=2d491f15de24621d8f44252b1850c788,"Background: The Fatality Analysis Reporting System (FARS) provides important data for studying the role of marijuana in motor vehicle crashes. However, marijuana testing data are available for only 34% of drivers in the FARS, which represents a major barrier in the use of the data. Methods: We developed a multiple imputation (MI) procedure for estimating marijuana positivity among drivers with missing marijuana test results, using a Bayesian multilevel model that allows a nonlinear association with blood alcohol concentrations (BACs), accounts for correlations among drivers in the same states, and includes both individual-level and state-level covariates. We generated 10 imputations for the missing marijuana-testing data using Markov chain Monte Carlo simulations and estimated positivity rates of marijuana in the nation and each state. Results: Drivers who were at older age, female, using seatbelt at the time of crash, having valid license, or operating median/heavy trucks were less likely to test positive for marijuana. There was a reverse U-shaped association between BACs and positivity of marijuana, with lower positivity when BACs < 0.01 g/dL or ≥0.15 g/dL. The MI data estimated a lower positivity rate of marijuana in the nation and each of the state than the observed data, with a national positivity rate of 11.7% (95% CI: 11.1, 12.4) versus 14.8% using the observed data in 2013. Conclusions: Our MI procedure appears to be a valid approach to addressing missing marijuana data in the FARS and may help strengthen the capacity of the FARS for monitoring the epidemic of drugged driving and understanding the role of marijuana in fatal motor vehicle crashes in the United States. © 2018 Elsevier Ltd"
,10.1016/j.ijrobp.2018.06.033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053198037&doi=10.1016%2fj.ijrobp.2018.06.033&partnerID=40&md5=5b1aa408e7363443407b8db9bb7b206e,"Purpose: A priori identification of the small proportion of radiation therapy patients who prove to be severely radiosensitive is a long-held goal in radiation oncology. A number of published studies indicate that analysis of the DNA damage response after ex vivo irradiation of peripheral blood lymphocytes, using the γ-H2AX assay to detect DNA damage, provides a basis for a functional assay for identification of the small proportion of severely radiosensitive cancer patients undergoing radiotherapy. Methods and Materials: We introduce a new, more rigorous, integrated approach to analysis of radiation-induced γ-H2AX response, using Bayesian statistics. Results: This approach shows excellent discrimination between radiosensitive and non-radiosensitive patient groups described in a previously reported data set. Conclusions: Bayesian statistical analysis provides a more appropriate and reliable methodology for future prospective studies. © 2018 Elsevier Inc."
,10.1007/s11222-017-9788-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032971018&doi=10.1007%2fs11222-017-9788-9&partnerID=40&md5=9793d1c32c0dcb93936d608f05be02cf,"The stochastic block model (SBM) is widely used for modelling network data by assigning individuals (nodes) to communities (blocks) with the probability of an edge existing between individuals depending upon community membership. In this paper, we introduce an autoregressive extension of the SBM, based on continuous-time Markovian edge dynamics. The model is appropriate for networks evolving over time and allows for edges to turn on and off. Moreover, we allow for the movement of individuals between communities. An effective reversible-jump Markov chain Monte Carlo algorithm is introduced for sampling jointly from the posterior distribution of the community parameters and the number and location of changes in community membership. The algorithm is successfully applied to a network of mice. © 2017, Springer Science+Business Media, LLC."
1,10.1002/env.2519,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050633642&doi=10.1002%2fenv.2519&partnerID=40&md5=7dd5b83f9dd349e48c1d3a85c9525e6b,"While new mobile monitoring technology has revolutionized our ability to measure pollutant levels over large regions, statistical methods for making inferences from data collected by these mobile systems are still being developed. We introduce a new capture–recapture model to answer key inferential questions from data collected by mobile monitoring systems. We apply our new method to characterize populations of natural gas (NG) leaks in urban areas using data collected by atmospheric methane analyzers placed on Google Street View cars. Leaks in urban NG distribution systems correspond to an economic loss, are a potential safety hazard, and are climate altering because NG is primarily composed of methane, a potent greenhouse gas. The new calibration capture–recapture (CCR) model combines data from controlled methane release experiments and data collected from mobile air monitors to enable inference for several NG leak population characteristics, including the number of undetected leaks and the total methane output rate in a surveyed region. Our methodology is a novel application of capture–recapture modeling. The CCR model addresses challenges associated with using a capture–recapture model to analyze data collected by a mobile monitoring system such as a variable sampling effort. We develop a Markov chain Monte Carlo algorithm for parameter estimation and apply the CCR model to data collected in two U.S. cities. The CCR model provides a new framework for inferring the total number of leaks in NG distribution systems and offers critical insights for informing intelligent infrastructure repair policy that is both cost effective and environmentally friendly. © 2018 John Wiley & Sons, Ltd."
,10.1061/(ASCE) WW.1943-5460.0000472,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053062165&doi=10.1061%2f%28ASCE%29+WW.1943-5460.0000472&partnerID=40&md5=1039441538a174db6092381ac87d3586,"A Bayesian inverse framework is developed to optimize the skill of a predictive numerical model via interpolation of bathymetric measurements to provide the most probable bathymetric surface. The numerical model is a coupled wave flow model and predicts wave and hydrodynamic information (e.g., significant wave height and longshore velocity). The Bayesian method, coupled with Markov chain Monte Carlo (MCMC) optimization, is used to find the bathymetric field, which serves to minimize the residual errors between measured data and the corresponding numerical model results. By using a Bayesian approach, the range of probable model parameters is inferred from the observed data. Monte Carlo simulation is also applied to this numerical model to perform the uncertainty analysis of the model output fields (wave height and flow velocity). This analysis is performed by taking random samples from the probability distribution function (PDF) of inputs and running the model as required until the desired precision (±0.05 m for significant wave height) in output fields is achieved. The case study used in this analysis is the DUCK94 experiment, which was conducted at the US Army Field Research Facility at Duck, North Carolina, in the fall of 1994. The unknown model parameters for the hydrodynamic model involve those controlling bathymetric resolution. Furthermore, the ability of the statistical model to estimate the observed data is tested by running the forward model for two sets of input parameters: the estimated input parameters updated by the previously mentioned statistical model and the prior (noninformative) parameters. Using the model parameters estimated from the Bayesian analysis leads to improved comparisons to data. Using the presented method, the relative errors between the model outputs and the observed data for significant wave height at nearshore gauges is reduced by 30%. © 2018 American Society of Civil Engineers."
2,10.1016/j.gsf.2017.10.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044648949&doi=10.1016%2fj.gsf.2017.10.014&partnerID=40&md5=4d6424ab2bacfbad6e111a66ff6373a3,"Determining soil–water characteristic curve (SWCC) at a site is an essential step for implementing unsaturated soil mechanics in geotechnical engineering practice, which can be measured directly through various in-situ and/or laboratory tests. Such direct measurements are, however, costly and time-consuming due to high standards for equipment and procedural control and limits in testing apparatus. As a result, only a limited number of data points (e.g., volumetric water content vs. matric suction) on SWCC at some values of matric suction are obtained in practice. How to use a limited number of data points to estimate the site-specific SWCC and to quantify the uncertainty (or degrees-of-belief) in the estimated SWCC remains a challenging task. This paper proposes a Bayesian approach to determine a site-specific SWCC based on a limited number of test data and prior knowledge (e.g., engineering experience and judgment). The proposed Bayesian approach quantifies the degrees-of-belief on the estimated SWCC according to site-specific test data and prior knowledge, and simultaneously selects a suitable SWCC model from a number of candidates based on the probability logic. To address computational issues involved in Bayesian analyses, Markov Chain Monte Carlo Simulation (MCMCS), specifically Metropolis-Hastings (M-H) algorithm, is used to solve the posterior distribution of SWCC model parameters, and Gaussian copula is applied to evaluating model evidence based on MCMCS samples for selecting the most probable SWCC model from a pool of candidates. This removes one key limitation of the M-H algorithm, making it feasible in Bayesian model selection problems. The proposed approach is illustrated using real data in Unsaturated Soil Database (UNSODA) developed by U.S. Department of Agriculture. It is shown that the proposed approach properly estimates the SWCC based on a limited number of site-specific test data and prior knowledge, and reflects the degrees-of-belief on the estimated SWCC in a rational and quantitative manner. © 2017 China University of Geosciences (Beijing) and Peking University"
,10.1038/s41437-018-0125-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052585483&doi=10.1038%2fs41437-018-0125-7&partnerID=40&md5=2fe3800e5edbd51d2c8bc4d5d115d333,"Fitness landscapes map the relationship between genotypes and fitness. However, most fitness landscape studies ignore the genetic architecture imposed by the codon table and thereby neglect the potential role of synonymous mutations. To quantify the fitness effects of synonymous mutations and their potential impact on adaptation on a fitness landscape, we use a new software based on Bayesian Monte Carlo Markov Chain methods and re-estimate selection coefficients of all possible codon mutations across 9 amino acid positions in Saccharomyces cerevisiae Hsp90 across 6 environments. We quantify the distribution of fitness effects of synonymous mutations and show that it is dominated by many mutations of small or no effect and few mutations of larger effect. We then compare the shape of the codon fitness landscape across amino acid positions and environments, and quantify how the consideration of synonymous fitness effects changes the evolutionary dynamics on these fitness landscapes. Together these results highlight a possible role of synonymous mutations in adaptation and indicate the potential mis-inference when they are neglected in fitness landscape studies. © 2018, The Genetics Society."
,10.1080/02664763.2018.1431208,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041625565&doi=10.1080%2f02664763.2018.1431208&partnerID=40&md5=4f8e9d1a264080a935106df9c625b268,"In this paper, we develop a conditional model for analyzing mixed bivariate continuous and ordinal longitudinal responses. We propose a quantile regression model with random effects for analyzing continuous responses. For this purpose, an Asymmetric Laplace Distribution (ALD) is allocated for continuous response given random effects. For modeling ordinal responses, a cumulative logit model is used, via specifying a latent variable model, with considering other random effects. Therefore, the intra-association between continuous and ordinal responses is taken into account using their own exclusive random effects. But, the inter-association between two mixed responses is taken into account by adding a continuous response term in the ordinal model. We use a Bayesian approach via Markov chain Monte Carlo method for analyzing the proposed conditional model and to estimate unknown parameters, a Gibbs sampler algorithm is used. Moreover, we illustrate an application of the proposed model using a part of the British Household Panel Survey data set. The results of data analysis show that gender, age, marital status, educational level and the amount of money spent on leisure have significant effects on annual income. Also, the associated parameter is significant in using the best fitting proposed conditional model, thus it should be employed rather than analyzing separate models. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/03610918.2017.1359289,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029436661&doi=10.1080%2f03610918.2017.1359289&partnerID=40&md5=fb6f7db0e9892084bf13680d2c30bdfd,"In this article, we propose a new distribution by mixing normal and Pareto distributions, and the new distribution provides an unusual hazard function. We model the mean and the variance with covariates for heterogeneity. Estimation of the parameters is obtained by the Bayesian method using Markov Chain Monte Carlo (MCMC) algorithms. Proposal distribution in MCMC is proposed with a defined working variable related to the observations. Through the simulation, the method shows a dependable performance of the model. We demonstrate through establishing model under a real dataset that the proposed model and method can be more suitable than the previous report. © 2018, © 2018 Taylor & Francis Group, LLC."
,10.1080/03610918.2017.1359286,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033723707&doi=10.1080%2f03610918.2017.1359286&partnerID=40&md5=3af4e21c565affe0b208f2236b750a11,"Recently, the Bayesian nonparametric approaches in survival studies attract much more attentions. Because of multimodality in survival data, the mixture models are very common. We introduce a Bayesian nonparametric mixture model with Burr distribution (Burr type XII) as the kernel. Since the Burr distribution shares good properties of common distributions on survival analysis, it has more flexibility than other distributions. By applying this model to simulated and real failure time datasets, we show the preference of this model and compare it with Dirichlet process mixture models with different kernels. The Markov chain Monte Carlo (MCMC) simulation methods to calculate the posterior distribution are used. © 2018, © 2018 Taylor & Francis Group, LLC."
,10.1016/j.physa.2018.05.064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047824118&doi=10.1016%2fj.physa.2018.05.064&partnerID=40&md5=2a13efd63cd79da85aad31fe06a57dfe,"Our paper studies the casual relationship between oil and major bilateral exchange rates against US dollar via a novel Bayesian, graph-based approach. This approach is shown to be quite effective in dealing with identification in Vector Autoregression (VAR) model, in which the temporal causal structure is represented by a graph sampled by Markov Chain Monte Carlo (MCMC) method. Empirical evidence demonstrates that oil price leads the exchange market in the after-crisis period whereas vice versa before crisis, implying a potential impact from financial crisis on the causality between these two markets. We further show that in general, oil-market specific shock affects the dependence structure most, while aggregate demand shock plays a weaker role and supply shock contributes least. Specifically, these three oil shocks take effect during different periods, thus capturing some invisible information about market evolutions. © 2018 Elsevier B.V."
,10.1016/j.scitotenv.2018.05.169,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047265427&doi=10.1016%2fj.scitotenv.2018.05.169&partnerID=40&md5=de894051dee701b7d91a0715cb33c6ce,"Knowledge of global C cycle implications from changes to fire regime and climate are of growing importance. Studies on the role of the fire regime in combination with climate change on soil C pools are lacking. We used Bayesian modelling to estimate the soil % total C (% CTot) and % recalcitrant pyrogenic C (% RPC) from field samples collected using a stratified sampling approach. These observations were derived from the following scenarios: 1. Three fire frequencies across three distinctive climate regions in a homogeneous dry sclerophyll forest in south-eastern Australia over four decades. 2. The effects of different fire intensity combinations from successive wildfires. We found climate had a stronger effect than fire frequency on the size of the estimated mineral soil C pool. The largest soil C pool was estimated to occur under a wet and cold (WC) climate, via presumed effects of high precipitation, an adequate growing season temperature (i.e. resulting in relatively high NPP) and winter conditions sufficiently cold to retard seasonal soil respiration rates. The smallest soil C pool was estimated in forests with lower precipitation but warmer mean annual temperature (MAT). The lower precipitation and higher temperature was likely to have retarded NPP and litter decomposition rates but may have had little effect on relative soil respiration. Small effects associated with fire frequency were found, but both their magnitude and direction were climate dependent. There was an increase in soil C associated with a low intensity fire being followed by a high intensity fire. For both fire frequency and intensity the response of % RPC mirrored that of % CTot: i.e. it was effectively a constant across all combinations of climate and fire regimes sampled. © 2018"
,10.1002/eqe.3093,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050655385&doi=10.1002%2feqe.3093&partnerID=40&md5=fd60fee485dd38485b78b918dd28b2d3,"Two new algorithms are presented for efficiently selecting suites of ground motions that match a target multivariate distribution or conditional intensity measure target. The first algorithm is a Markov chain Monte Carlo (MCMC) approach in which records are sequentially added to a selected set such that the joint probability density function (PDF) of the target distribution is progressively approximated by the discrete distribution of the selected records. The second algorithm derives from the concept of the acceptance ratio within MCMC but does not involve any sampling. The first method takes advantage of MCMC's ability to efficiently explore a sampling distribution through the implementation of a traditional MCMC algorithm. This method is shown to enable very good matches to multivariate targets to be obtained when the numbers of records to be selected is relatively large. A weaker performance for fewer records can be circumvented by the second method that uses greedy optimisation to impose additional constraints upon properties of the target distribution. A preselection approach based upon values of the multivariate PDF is proposed that enables near-optimal record sets to be identified with a very close match to the target. Both methods are applied for a number response analyses associated with different sizes of record sets and rupture scenarios. Comparisons are made throughout with the Generalised Conditional Intensity Measure (GCIM) approach. The first method provides similar results to GCIM but with slightly worse performance for small record sets, while the second method outperforms method 1 and GCIM for all considered cases. © 2018 John Wiley & Sons, Ltd."
,10.3847/1538-4357/aadcf3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055161073&doi=10.3847%2f1538-4357%2faadcf3&partnerID=40&md5=c2c13bb1ff6275e5549bc05e58e5f03e,"We perform Markov chain Monte Carlo analyses to put constraints on the nonflat φCDM inflation model using Planck 2015 cosmic microwave background (CMB) anisotropy data and baryon acoustic oscillation distance measurements. The φCDM model is a consistent dynamical dark energy model in which the currently accelerating cosmological expansion is powered by a scalar field φ slowly rolling down an inverse power-law potential energy density. We also use a physically consistent power spectrum for energy density inhomogeneities in this nonflat model. We find that, like the closed-ΛCDM and closed-XCDM models, the closed-φCDM model provides a better fit to the lower multipole region of the CMB temperature anisotropy data compared to that provided by the tilted flat-ΛCDM model. Also, like the other closed models, this model reduces the tension between the Planck and the weak lensing σ 8 constraints. However, the higher multipole region of the CMB temperature anisotropy data are better fit by the tilted flat-Λ model than by the closed models. © 2018. The American Astronomical Society. All rights reserved.."
,10.5194/bg-15-5801-2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054855321&doi=10.5194%2fbg-15-5801-2018&partnerID=40&md5=3c158c7f908652bd0aa9a25856a43d73,"Data-model integration plays a critical role in assessing and improving our capacity to predict ecosystem dynamics. Similarly, the ability to attach quantitative statements of uncertainty around model forecasts is crucial for model assessment and interpretation and for setting field research priorities. Bayesian methods provide a rigorous data assimilation framework for these applications, especially for problems with multiple data constraints. However, the Markov chain Monte Carlo (MCMC) techniques underlying most Bayesian calibration can be prohibitive for computationally demanding models and large datasets. We employ an alternative method, Bayesian model emulation of sufficient statistics, that can approximate the full joint posterior density, is more amenable to parallelization, and provides an estimate of parameter sensitivity. Analysis involved informative priors constructed from a meta-analysis of the primary literature and specification of both model and data uncertainties, and it introduced novel approaches to autocorrelation corrections on multiple data streams and emulating the sufficient statistics surface. We report the integration of this method within an ecological workflow management software, Predictive Ecosystem Analyzer (PEcAn), and its application and validation with two process-based terrestrial ecosystem models: SIPNET and ED2. In a test against a synthetic dataset, the emulator was able to retrieve the true parameter values. A comparison of the emulator approach to standard q MCMC involving multiple data constraints showed that the emulator method was able to constrain the faster and simpler SIPNET model's parameters with comparable performance to the brute-force approach but reduced computation time by more than 2 orders of magnitude. The emulator was then applied to calibration of the ED2 model, whose complexity precludes standard (brute-force) Bayesian data assimilation techniques. Both models are constrained after assimilation of the observational data with the emulator method, reducing the uncertainty around their predictions. Performance metrics showed increased agreement between model predictions and data. Our study furthers efforts toward reducing model uncertainties, showing that the emulator method makes it possible to efficiently calibrate complex models. © 2018 Author(s)."
,10.1080/02664763.2017.1421916,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041120642&doi=10.1080%2f02664763.2017.1421916&partnerID=40&md5=59aa41fc0d8a8b815481ef935421f2f3,"This paper presents a new method for the reconciliation of data described by arbitrary continuous probability distributions, with the focus on nonlinear constraints. The main idea, already applied to linear constraints in a previous paper, is to restrict the joint prior probability distribution of the observed variables with model constraints to get a joint posterior probability distribution. Because in general the posterior probability density function cannot be calculated analytically, it is shown that it has decisive advantages to sample from the posterior distribution by a Markov chain Monte Carlo (MCMC) method. From the resulting sample of observed and unobserved variables various characteristics of the posterior distribution can be estimated, such as the mean, the full covariance matrix, marginal posterior densities, as well as marginal moments, quantiles, and HPD intervals. The procedure is illustrated by examples from material flow analysis and chemical engineering. © 2018, © 2018 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/24725854.2018.1455117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048165545&doi=10.1080%2f24725854.2018.1455117&partnerID=40&md5=05a94548d1c61e736d89fff12432b870,"Metrology data are crucial to quality control of three-dimensional (3D) printed parts. Low-cost measurement systems are often unreliable due to their low resolutions, whereas high-resolution measurement systems usually induce high measurement costs. To balance the measurement cost and accuracy, a new cost-effective and reliable measurement strategy is proposed in this article, which jointly uses two-resolution measurement systems. Specifically, only a small sample of base parts are measured by both the low- and high-resolution measurement systems in order to save costs. The measurement accuracy of most parts with only low-resolution metrology data is improved by effectively integrating high-resolution metrology data of the base parts. A Bayesian generative model parameterizes a part-independent bias and variance pattern of the low-resolution metrology data and facilitates a between-part data integration via an efficient Markov chain Monte Carlo sampling algorithm. This multi-part two-resolution metrology data integration highlights the novelty and contribution of this article compared with the existing one-part data integration methods in the literature. Finally, an intensive experimental study involving a laser scanner and a machine visual system has validated the effectiveness of our measurement strategy in acquisition of reliable metrology data of 3D printed parts. © 2018, Copyright © 2018 “IISE”."
,10.1088/1742-6596/1087/2/022004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054904702&doi=10.1088%2f1742-6596%2f1087%2f2%2f022004&partnerID=40&md5=5b768dcf5eb8c63e9e47e94be4e4faaf,"Metropolis-Hastings algorithm which keeps the detailed balance is the basic element for most Markov chain Monte Carlo sampling algorithms in which undermined Markov processes are reversible. Previous research shows that nonreversible Markov processes have a faster rate of convergence than reversible ones. Taking advantage of the ""lifting"" idea, this paper develops a general framework for designing Metropolis-Hastings algorithms breaking detailed balance and implements two new nonreversible Metropolis-Hastings algorithms based on the Gaussian proposal conditional probability and Langevin dynamics in the zero-mass limit respectively. Numerical simulations in one and two dimensions demonstrate that new nonreversible Metropolis-Hastings algorithms can speed up the convergence to target stationary distributions, which supports the theoretical finding and the design of our new algorithms. © Published under licence by IOP Publishing Ltd."
,10.1016/j.ress.2018.06.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049329553&doi=10.1016%2fj.ress.2018.06.005&partnerID=40&md5=ce07df8666645597ce93337c40abeffc,"In networked systems such as communication networks or power grids, graph separation from node failures can damage the overall operation severely. One of the most important goals of network attackers is thus to separate nodes so that the sizes of connected components become small. In this work, we consider the problem of finding a minimum α-separator, that partitions the graph into connected components of sizes at most αn, where n is the number of nodes. To solve the α-separator problem, we develop a random walk algorithm based on Metropolis chain. We characterize the conditions for the first passage time (to find an optimal solution) of our algorithm. We also find an optimal cooling schedule, under which the random walk converges to an optimal solution almost surely. Furthermore, we generalize our algorithm to non-uniform node weights. We show through extensive simulations that the first passage time is less than O(n3), thereby validating our analysis. The solution found by our algorithm allows us to identify the weakest points in the network that need to be strengthened. Simulations in real topologies show that attacking a dense area is often not an efficient solution for partitioning a network into small components. © 2018"
,10.1107/S1600576718011597,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054101410&doi=10.1107%2fS1600576718011597&partnerID=40&md5=b2b8ba4edc4e414ff4f3ed705b361385,"Assemblies of nanosheets are often characterized by extensive layer-position disorder. Coupled with the often minute coherent scattering domain size and relaxation of the nanosheet structure itself, unambiguous interpretation of X-ray and neutron scattering data from such materials is non-trivial. This work demonstrates a general approach towards refinement of layer-disorder information from atomic pair distribution function (PDF) data for materials that span the gap between turbostratism and ordered stacking arrangements. X-ray total scattering data typical of a modern rapid-acquisition PDF instrument are simulated for a hypothetical graphene-like structure using the program DIFFaX, from which atomic PDFs are extracted. Small 1 × 1 × 20 supercell models representing the stacking of discrete layer types are combined to model a continuous distribution of layer-position disorder. Models optimized using the differential evolution algorithm demonstrate improved fit quality over 75 Å when a single mean layer-type model is replaced with a constrained 31-layertype model. Posterior distribution analyses using the Markov chainMonte Carlo algorithm demonstrate that the influence of layer disorder and finite particle size are correlated. However, the refined mean stacking vectors match well with the generative parameter set. © 2018 International Union of Crystallography."
,10.1016/j.ndteint.2018.02.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047862987&doi=10.1016%2fj.ndteint.2018.02.004&partnerID=40&md5=d479f21e25d6ad2bc518786d0e972038,"Flaw characterization in eddy current testing usually requires to solve a non-linear inverse problem. Due to high computational cost, Markov Chain Monte Carlo (MCMC) methods are hardly employed since often needing many forward evaluations. However, they have good potential in dealing with complicated forward models and they do not reduce to only providing the parameters sought. Here, we introduce a computationally-cheap surrogate forward model into a MCMC algorithm for eddy current flaw characterization. Due to the use of a database trained off-line, we benefit from the MCMC algorithm for getting more information and we do not suffer from the computational burden. Numerous experiments are carried out to validate the approach. The results include not only the estimated parameters, but also standard deviations, marginal densities and correlation coefficients between two parameters of interest. © 2018 Elsevier Ltd"
1,10.1016/j.ijthermalsci.2018.06.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049355475&doi=10.1016%2fj.ijthermalsci.2018.06.006&partnerID=40&md5=b2521d05cb65b4420bca11706570b77a,"This work deals with the solution of an inverse heat conduction problem aiming at the detection of contact failures in layered composites through the estimation of the contact conductance between the layers. The spatially varying contact conductance is estimated using a Bayesian formulation of the problem and a Markov chain Monte Carlo method, with infrared camera measurements of the transient temperature field on the surface of the body. The inverse analysis is formulated using a data compression scheme, where the temperature measurements are integral transformed with respect to the spatial variable. The present approach is evaluated using synthetic measurements and experimental data from controlled laboratory experiments. It is shown that only few transformed modes of the data are required for solving the inverse problem, thus providing substantial reduction of the computational time in the Markov chain Monte Carlo method, as well as regularization of the ill-posed problem. © 2018 Elsevier Masson SAS"
,10.1016/j.dsp.2018.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050687771&doi=10.1016%2fj.dsp.2018.07.001&partnerID=40&md5=9299ce9dae822c383f6e338007d9656b,"In many applications in engineering, one is interested in tracking a dynamic system whose state evolves on a manifold. Solutions to such problems frequently must resort to nonlinear filtering techniques as many manifolds can be described as equality restrictions on higher-dimensional embedding spaces. We propose in this paper a new particle filtering (PF) method to track the states of dynamic systems that evolve according to a random walk on the unit sphere. We derive an approximation to the intractable optimal importance function and develop a Markov Chain Monte Carlo (MCMC) method to sample from it. The system state variable is then estimated via a Monte Carlo approximation of its intrinsic mean on the sphere, obtained from the Karcher mean of the particle set. As we verify via computer simulations, the proposed method shows improved performance compared to previous Constrained Extended Kalman filters and Bootstrap PF solutions. © 2018 Elsevier Inc."
,10.1016/j.jhydrol.2018.08.082,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053050730&doi=10.1016%2fj.jhydrol.2018.08.082&partnerID=40&md5=72a3896fe5c666fc470ebf3a5fdf169c,"In this work, we conducted two laboratory column experiments on an undisturbed sandy soil. The first deals with a percolation-drainage experiment whereas the second deals with an infiltration of a constant water flux at the surface of the unsaturated soil. A Bayesian assessment of the soil parameters is performed for both experiments with the Markov Chain Monte Carlo (MCMC) method using measurements of pressure head inside the column and cumulative outflow collected during the experiments. The results show that both experiments can be well reproduced with the mathematical model based on the Richards equation and the van-Genchten/Mualem models. Furthermore, the inversion of the two laboratory experiments yields similar results in terms of mean estimated parameter values but strong discrepancies occur between confidence intervals used to quantify uncertainty on the estimated parameters. Compared to the percolation-drainage experiment, the infiltration experiment yields more accurate parameters with narrower uncertainty regions. © 2018 Elsevier B.V."
,10.1002/qre.2284,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053443606&doi=10.1002%2fqre.2284&partnerID=40&md5=d750300ced36abc9625e45c09c93909e,"The concept of a Bayesian probability of agreement was recently introduced to give the posterior probabilities that the response surfaces for two different groups are within δ of one another. For example, a difference of less than δ in the mean response at fixed levels of the predictor variables might be thought to be practically unimportant. In such a case, we would say that the mean responses are in agreement. The posterior probability of this is called the Bayesian probability of agreement. In this article, we quantify the probability that new response observations from two groups will be within δ for a continuous response, and the probability that the two responses agree completely for categorical cases such as logistic regression and Poisson regression. We call these Bayesian comparative predictive probabilities, with the former being the predictive probability of agreement. We use Markov chain Monte Carlo simulation to estimate the posterior distribution of the model parameters and then the predictive probability of agreement. We illustrate the use of this methodology with three examples and provide a freely available R Shiny app that automates the computation and estimation associated with the methodology. © 2018 John Wiley & Sons, Ltd."
1,10.1016/j.compgeo.2017.11.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039913933&doi=10.1016%2fj.compgeo.2017.11.012&partnerID=40&md5=0383deff7332534377ba981f040956be,"An efficient probabilistic back estimation method for characterization of spatial variability is proposed by integration of the Karhunen–Loève (K-L) expansion method, the Polynomial Chaos Expansion (PCE) method and the Markov Chain Monte Carlo (MCMC) method. To reduce the dimension of back estimation, the spatially varied soil property is simulated using the K-L expansion method and the basic random variables of K-L terms are parameters to be estimated. To further reduce computation load, a PCE surrogate model is constructed to substitute the original model. The proposed method is applied on an example where a randomly heterogeneous soil slope is subject to surface infiltration. The pressure responses are used to estimate the spatial variability of the saturated coefficient permeability. The results show that the spatial variability can be satisfactorily estimated. The coefficient of variation of the estimation is less than 5%. © 2017 Elsevier Ltd"
,10.1007/s10463-017-0615-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028894602&doi=10.1007%2fs10463-017-0615-z&partnerID=40&md5=45faa8480b603a2b8731e4a69ad6dcba,"Exact conditional goodness-of-fit tests for discrete exponential family models can be conducted via Monte Carlo estimation of p values by sampling from the conditional distribution of multiway contingency tables. The two most popular methods for such sampling are Markov chain Monte Carlo (MCMC) and sequential importance sampling (SIS). In this work we consider various ways to hybridize the two schemes and propose one standout strategy as a good general purpose method for conducting inference. The proposed method runs many parallel chains initialized at SIS samples across the fiber. When a Markov basis is unavailable, the proposed scheme uses a lattice basis with intermittent SIS proposals to guarantee irreducibility and asymptotic unbiasedness. The scheme alleviates many of the challenges faced by the MCMC and SIS schemes individually while largely retaining their strengths. It also provides diagnostics that guide and lend credibility to the procedure. Simulations demonstrate the viability of the approach. © 2017, The Institute of Statistical Mathematics, Tokyo."
,10.1007/s00477-018-1555-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047386843&doi=10.1007%2fs00477-018-1555-8&partnerID=40&md5=b998ed9cf79243aea75a88b2ab82533c,"Uncertainty and its propagation in computer models has relevance in many disciplines, including hydrology, environmental engineering, ecology and climate change. Error propagation in a model results in uncertainty in prediction due to uncertainties in model inputs and parameters. Common methods for quantifying error propagation are reviewed, namely Differential Error Analysis and Monte Carlo Simulation, including underlying principles, together with a discussion on their differences, advantages and disadvantages. The separate case of uncertainty in the model calibration process is different to error propagation in a fixed model in that it is associated with a dynamic process of iterative parameter adjustment, and is compared in the context of non-linear regression and Bayesian approaches, such as Markov Chain Monte Carlo Simulation. Error propagation is investigated for a soil model representing the organic carbon depth profile and also a streamflow model using probabilistic simulation. Different sources of error are compared, including uncertainty in inputs, parameters and geometry. The results provided insights into error propagation and its computation in systems and models in general. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1016/j.rse.2018.07.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050106933&doi=10.1016%2fj.rse.2018.07.017&partnerID=40&md5=c11383a12ca9e20958306e94432ea819,"Drought is the costliest hazard among all natural disasters. Despite the significant improvements in drought modeling over the last decade, accurate provisions of drought conditions in a timely manner is still a major research challenge. In order to improve the current drought monitoring skills, this study presents a land data assimilation system by merging the remotely sensed surface soil moisture with the model simulations with the use of a recently developed particle Markov chain Monte Carlo (PMCMC) method. To cope with the computational complexity, a modular parallel particle filtering framework (PPFF) is developed which allows a large ensemble size in PMCMC applications. The implementation of the proposed system is demonstrated with the 2012 summer flash drought case study over the Contiguous United States (CONUS). Results from both synthetic and real case studies suggest that the land data assimilation system improves the soil moisture predictions and the drought monitoring skills. Compared with the U.S. Drought Monitoring (USDM), the land data assimilation can better capture the drought onset on May 2012 and the drought severity in June and July 2012. This study recommends that the proposed land data assimilation system based on a high-performance computing (HPC) infrastructure can better facilitate the drought preparation and response actions. © 2018 Elsevier Inc."
1,10.1016/j.chemosphere.2018.06.118,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048968287&doi=10.1016%2fj.chemosphere.2018.06.118&partnerID=40&md5=64bfd5290b18eead60048e40670c3720,"Environmental factors may increase colon cancer (CC) risk. It has been suggested that pesticides could play a significant role in the etiology of this malignancy. As agriculture is one of the mainstays of the Brazilian economy, this country has become the largest pesticides consumer worldwide. The CC burden is also increasing in Brazil. Herein, we examined data from the Brazilian Federal Government to determine whether CC mortality and pesticide consumption may be associated. Database of the Ministry of Health provided CC mortality data in Brazil, while pesticide usage was accessed at the website of Brazilian Institute of Environment and Renewable Natural Resources. The CC mortality in the Brazilian states was calculated as standard mortality rates (SMR). All Bayesian analysis was performed using a Markov chain Monte Carlo method in WinBUGS software. We observed that CC mortality has exhibited a steady increase for more than a decade, which correlated with the amount of sold pesticides in the country. Both observations are concentrated in the Southern and the Southeast regions of Brazil. Although ecological studies like ours have methodological limitations, the current dataset suggests the possibility that pesticide exposure may be a risk factor for CC. It warrants further investigation. © 2018 Elsevier Ltd"
,10.1007/s00024-018-1881-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054696084&doi=10.1007%2fs00024-018-1881-2&partnerID=40&md5=0d925cef8a78522daa33001165ace8b1,"Ambient noise seismic tomography has been widely used to study crustal and upper-mantle shear velocity structures. Most studies, however, concentrate on short period (&lt; 50 s) surface wave from ambient noise, while studies using long period surface wave from ambient noise are limited. In this paper, we demonstrate the feasibility of using long-period surface wave from ambient noise to study the lithospheric structure on a continental scale. We use broadband Rayleigh wave phase velocities to obtain a 3-D VS structures beneath the contiguous United States at period band of 10–150 s. During the inversion, 1-D shear wave velocity profile is parameterized using B-spline at each grid point and is inverted with nonlinear Markov Chain Monte Carlo method. Then, a 3-D shear velocity model is constructed by assembling all the 1-D shear velocity profiles. Our model is overall consistent with existing models which are based on multiple datasets or data from earthquakes. Our model along with the other post-USArray models reveal lithosphere structures in the upper mantle, which are consistent with the geological tectonic background (e.g., the craton root and regional upwelling provinces). The model has comparable resolution on lithosphere structures compared with many published results and can be used for future detailed regional or continental studies and analysis. © 2018, Springer International Publishing AG, part of Springer Nature."
,10.1016/j.ress.2018.05.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048279401&doi=10.1016%2fj.ress.2018.05.016&partnerID=40&md5=425b305460b028b3ecc59c227178f42d,"This paper presents a new modeling approach, computational algorithm, and an example application for health monitoring and learning in on-line System Health Management (SHM). A hybrid Dynamic Bayesian Network (DBN) is introduced to represent complex engineering systems with underlying physics of failure by modeling a theoretical or empirical degradation model with continuous variables. The methodology is designed to be flexible and intuitive, and scalable from small, localized functionality to large complex dynamic systems. Markov Chain Monte Carlo (MCMC) inference is optimized using a pre-computation strategy and dynamic programming for on-line monitoring of system health. Proposed Monitoring and Anomaly Detection algorithm uses pattern recognition to improve failure detection and estimation of Remaining Useful Life (RUL). Pre-computation inference database enables efficient on-line learning and maintenance decision-making. The proposed methodology and algorithm are demonstrated with an Unmanned Aerial Vehicle (UAV) application. © 2018 Elsevier Ltd"
,10.1016/j.econlet.2018.08.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051537403&doi=10.1016%2fj.econlet.2018.08.004&partnerID=40&md5=aa13d5c2ca09e92ffdf8bbbd99b55419,"We propose a Poisson regression model that controls for three potential sources of persistence in panel count data; dynamics, latent heterogeneity and serial correlation in the idiosyncratic errors. We also account for the initial conditions problem. For model estimation, we develop a Markov Chain Monte Carlo algorithm. The proposed methodology is illustrated by a real example on the number of patents granted. © 2018 Elsevier B.V."
,10.1016/j.jeconom.2018.06.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049466754&doi=10.1016%2fj.jeconom.2018.06.006&partnerID=40&md5=4898979de2c0e84a814f6638bd0a5b33,"Ergodic theorem shows that ergodic averages of the posterior draws converge in probability to the posterior mean under the stationarity assumption. The literature also shows that the posterior distribution is asymptotically normal when the sample size of the original data considered goes to infinity. To the best of our knowledge, there is little discussion on the large sample behaviour of the posterior mean. In this paper, we aim to fill this gap. In particular, we extend the posterior mean idea to the conditional mean case, which is conditioning on a given vector of summary statistics of the original data. We establish a new asymptotic theory for the conditional mean estimator for the case when both the sample size of the original data concerned and the number of Markov chain Monte Carlo iterations go to infinity. Simulation studies show that this conditional mean estimator has very good finite sample performance. In addition, we employ the conditional mean estimator to estimate a GARCH(1,1) model for S&P 500 stock returns and find that the conditional mean estimator performs better than quasi-maximum likelihood estimation in terms of out-of-sample forecasting. © 2018 Elsevier B.V."
,10.1016/j.camwa.2018.07.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050686549&doi=10.1016%2fj.camwa.2018.07.027&partnerID=40&md5=cd9ae66d98e1c30ec9791dd94896ebf7,"This paper considers the valuation of a CDS (credit default swap) contract. To find out a more accurate CDS price, we work on an extended Merton's model by assuming that the price of the reference asset follows a regime switching Black–Scholes model, and moreover, the reference asset can default at any time before the expiry time. A general pricing formula for the CDS containing the unknown no default probability is derived first. It is then subsequently shown that the no default probability is equivalent to the price of a down-and-out binary option written on the same reference asset. By simulating the Markov chain with the Monte-Carlo technique, we obtain an approximation formula for the down-and-out binary option, with the availability of which, the calculation of the CDS price becomes straightforward. Finally, some numerical experiments are conducted to examine the accuracy of the approximation approach as well as the impacts of the introduction of the regime switching mechanics on the CDS price. © 2018 Elsevier Ltd"
,10.5705/ss.202017.0016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054552439&doi=10.5705%2fss.202017.0016&partnerID=40&md5=d6583ef6e5b7bcd1bf95ea9f873a9908,"Nonresponse is an important practical problem in epidemiological surveys and clinical trials. Common methods for dealing with missing data rely on untestable assumptions. In particular, non-ignorable modeling, which derives inference from the likelihood function based on a joint distribution of the variables and the missingness indicators, can be sensitive to misspecification of this distribution and may also have problems with identifying the parameters. Nonresponse two-phase sampling (NTS), which re-contacts and collects data from a subsample of the initial nonrespondents, has been used to reduce nonresponse bias. The additional data collected in phase II provide important information for identifying the parameters in the non-ignorable models. We propose a Bayesian selection model which utilizes the additional data from phase II and develop an efficient Markov chain Monte Carlo algorithm for the posterior computation. We illustrate the proposed model on simulation studies and a Quality of Life (QOL) dataset. © Institute of Statistical Science. All rights reserved."
,10.1111/jvp.12677,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050491648&doi=10.1111%2fjvp.12677&partnerID=40&md5=c6194877eb2a263e085aeea274aa17f5,"Bayesian population pharmacokinetic models of florfenicol in healthy pigs were developed based on retrospective data in pigs either via intravenous (i.v.) or intramuscular (i.m.) administration. Following i.v. administration, the disposition of florfenicol was best described by a two-compartment open model with the typical values of half-life at α phase (t1/2α), half-life at β phase (t1/2β), total body clearance (Cl), and volume of distribution (Vd) were 0.132 ± 0.0289, 2.78 ± 0.166 hr, 0.215 ± 0.0102, and 0.841 ± 0.0289 L kg−1, respectively. The disposition of florfenicol after i.m. administration was best described by a one-compartment open model. The typical values of maximum concentration of drug in serum (Cmax), elimination half-life (t1/2Kel), Cl, and Volume (V) were 5.52 ± 0.605 μg/ml, 9.96 ± 1.12 hr, 0.228 ± 0.0154 L hr−1 kg−1, and 3.28 ± 0.402 L/kg, respectively. The between-subject variabilities of all the parameters after i.m. administration were between 25.1%–92.1%. Florfenicol was well absorbed (94.1%) after i.m. administration. According to Monte Carlo simulation, 8.5 and 6 mg/kg were adequate to exert 90% bactericidal effect against Actinobacillus pleuropneumoniae after i.v. and i.m. administration. © 2018 John Wiley & Sons Ltd"
,10.1007/s10237-018-1036-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047664641&doi=10.1007%2fs10237-018-1036-5&partnerID=40&md5=e16d8c5c2626fc1233dccd3616242c0c,"Cell migration plays an essential role in cancer metastasis. In cancer invasion through confined spaces, cells must undergo extensive deformation, which is a capability related to their metastatic potentials. Here, we simulate the deformation of the cell and nucleus during invasion through a dense, physiological microenvironment by developing a phenomenological computational model. In our work, cells are attracted by a generic emitting source (e.g., a chemokine or stiffness signal), which is treated by using Green’s Fundamental solutions. We use an IMEX integration method where the linear parts and the nonlinear parts are treated by using an Euler backward scheme and an Euler forward method, respectively. We develop the numerical model for an obstacle-induced deformation in 2D or/and 3D. Considering the uncertainty in cell mobility, stochastic processes are incorporated and uncertainties in the input variables are evaluated using Monte Carlo simulations. This quantitative study aims at estimating the likelihood for invasion and the length of the time interval in which the cell invades the tissue through an obstacle. Subsequently, the two-dimensional cell deformation model is applied to simplified cancer metastasis processes to serve as a model for in vivo or in vitro biomedical experiments. © 2018, The Author(s)."
2,10.1016/j.apenergy.2017.08.181,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028584694&doi=10.1016%2fj.apenergy.2017.08.181&partnerID=40&md5=aaffe48891cc56d1acf8715a6a3a6106,"Air-conditioners (AC) usually consume the most electricity among all of the auxiliary components in an electric bus, over 30% of the battery power at maximum. On-board passengers carried by the electric bus are important but random heat sources, which are obsessional disturbances for the cabin temperature control and energy management of the AC system. This paper aims to improve the AC energy efficiency via passenger amount variation analysis and forecast in a model predictive control (MPC) framework. Three forecasting approaches are proposed to realize the passenger amount variation prediction in real-time, namely, stochastic prediction based on Monte Carlo, radial basis function neural network (RBF-NN) prediction, and Markov-chain prediction. A sample passenger number database along a typical bus line in Beijing is built for passenger variation pattern analysis and forecast. A comparative study of the above three prediction approaches with different prediction lengths (bus stops in this case) is conducted, from both the energy consumption and temperature control perspectives. A predictive AC controller is developed, and evaluated by comparing with Dynamic Programming (DP) and a commonly used rule-based control strategy. Simulation results show that all the three forecasting methods integrated within the MPC framework are able to achieve more stable temperature performance. The energy consumptions of MPC with Markov-chain prediction, RBF-NN forecast and Monte Carlo prediction are 6.01%, 5.88% and 5.81% lower than rule-based control, respectively, on the Beijing bus route studied in this paper. © 2017"
,10.1109/TWC.2018.2861870,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051405233&doi=10.1109%2fTWC.2018.2861870&partnerID=40&md5=c2dafd4bdced2387385c7553b2f05fd6,"The distributed spatial modulation (DSM) protocol, which allows relays to forward the source's data while simultaneously allowing the relays to transmit their own data, has been proposed by Narayanan et al. In this paper, we introduce two new protocols for enabling the DSM, consisting of single-antenna network nodes, with simultaneous wireless information and power transfer capability: power splitting-based DSM (PS-DSM) and energy recycling-based DSM (ER-DSM). More specifically, the PS-DSM relies on power splitters at the relay nodes to harvest energy transmitted from the source. On the other hand, the ER-DSM, by exploiting the inactive cooperating relays in DSM-based protocols, recycles part of the transmitted energy in the network, without relying on power splitters or time switches at the relays to harvest energy. This leads to an increase in the average harvested energy at the relays with reduced hardware complexity. Both the PS-DSM and the ER-DSM also retain all the original features of DSM. Due to its particular operating principle and specific advantages, we select the ER-DSM as the candidate for further mathematical analysis. More specifically, by considering a multi-state battery model, we propose an analytical framework based on a Markov chain formulation for modeling the charging/discharging behavior of the batteries at the relay nodes in the ER-DSM. Furthermore, based on the derived Markov chain model, we introduce a mathematical framework for computing the error probability of the ER-DSM, by explicitly taking into account, the effect of finite-sized batteries. The frameworks are substantiated with the aid of Monte Carlo simulations for various system setups. © 2002-2012 IEEE."
,10.1016/j.expthermflusci.2018.04.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047605965&doi=10.1016%2fj.expthermflusci.2018.04.026&partnerID=40&md5=5f484dc8eb5239b53120bf66cd07b32b,"In this work, an inverse methodology is developed for estimating the local heat transfer coefficients on a vertical plate embedded with the three discrete heat sources, under steady state natural convection, with the temperatures measured at the adiabatic surface without disturbing the fluid flow, using simple conduction/surrogate model and Bayesian inference. Liquid crystal thermography (LCT), an optical measurement method based on the colour-temperature relationship of thermochromic liquid crystal sheet (TLC) is used to determine the temperature field of the adiabatic surface. Bayesian framework with Metropolis Hastings-Markov chain Monte Carlo (MH-MCMC) sampling method is considered for exploring the posterior distribution to estimate the parameters in terms of point estimates like mean, Maximum a posteriori (MAP) and standard deviation. A parity plot between simulated (using retrieved parameters) and measured TLC temperatures shows good agreement. © 2018 Elsevier Inc."
,10.1007/s40273-018-0688-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050953934&doi=10.1007%2fs40273-018-0688-4&partnerID=40&md5=640957dcc135aa2bc172967e9bade493,"Background: Heart failure affects over 1 million people in Germany and contributes to morbidity, mortality, and high healthcare costs. A recent large randomized controlled trial compared the novel compound sacubitril/valsartan (LCZ696) with the angiotensin-converting enzyme (ACE) inhibitor enalapril and found a 16% reduction in mortality hazard. In Germany, sacubitril/valsartan was launched at the beginning of 2016. Objective: The purpose of this study was to conduct a post hoc analysis of the cost effectiveness, budget impact, and disease burden reduction of sacubitril/valsartan compared with ACE inhibitors for patients with heart failure from the perspective of the German social health insurance (SHI), based on the results of this trial. Methods: A Markov (cohort) state transition model was constructed to simulate treatment over a remaining lifetime. Based on the Markov model, a dynamic population model was developed that projects the incidence, prevalence, mortality, and healthcare costs of heart failure in the SHI population from 2017 to 2060. The population model follows prevalent and incident cohorts over time. Each year a new cohort is added, while the existing cohorts age by 1 year or die. To test for sensitivity of results, a Monte Carlo simulation was run. Results: Based on the price negotiated between manufacturer and representatives of the SHI, the base-case incremental cost-effectiveness ratio (ICER) of sacubitril/valsartan versus ACE inhibitors is €23,401 per life-year gained (in 2018 Euros). At a price of zero, the cost-effectiveness ratio is already €9594 per life-year gained due to high background costs of heart failure. Annual budget impact and reduction of disease burden reach a maximum at 4–8 years after launch (€221 million and 2.9%, respectively, in the base case). Conclusions: The ICER of sacubitril/valsartan is projected to be at or below the level of other accepted interventions for the treatment of asymptomatic to severe heart failure in Germany. Projected budget impact leads to an increase in SHI expenditures by < 0.04% per year. © 2018, Springer Nature Switzerland AG."
,10.1016/j.engstruct.2018.06.040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048432351&doi=10.1016%2fj.engstruct.2018.06.040&partnerID=40&md5=e86978a0278e07b6daa7bf90068dfbde,"In this work, an inverse analysis procedure adopting a Bayesian approach is proposed as a numerical tool to investigate the causes that have led a masonry arch bridge to be in a certain pathological condition. Within this framework, the damaged condition investigation is formulated as a parameter estimation problem. A nonlinear finite element model is developed, and the implementation of plausible loading scenarios together with possible initial undamaged configurations of the bridge is then carried out. Computer model predictions are subsequently compared against real, measured geometrical data. The aim of the identification problem is to obtain the distribution of the most likely values of the parameters of the mechanical model so that the numerical predictions reproduce with the highest accuracy the existing damage pattern. The posterior probability distributions of the unknown parameters are estimated via the use of simulation techniques, namely, the Markov chain Monte Carlo (MCMC) method. The computational burden associated with both the MCMC sampling procedure and the time-consuming numerical model is alleviated by the adoption of a Gaussian process emulator. The feasibility of the practical implementation of the method is tested on a real case study located in Kakodiki village on the island of Crete (Greece). The results indicate that reasonable inferences about the original geometry of the bridge, as well as possible damage loading scenarios, can be made, resulting in a nearly identical crack pattern with respect to the present damaged state. The possibility of exploiting the posterior distributions of the model updating parameters for subsequent structural assessment tasks is also shown, allowing probabilistic simulation outcomes from which a more reliable judgement of the actual bridge safety condition can be established. The application of the proposed methodology would result in a better understanding of the underlying mechanisms triggering damage while also providing useful guidelines for decision making, such as those related to the planning of adequate maintenance actions and the selection of optimal strengthening measures. © 2018 Elsevier Ltd"
,10.1016/j.ijforecast.2018.04.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050795529&doi=10.1016%2fj.ijforecast.2018.04.008&partnerID=40&md5=b18aaf752b9b4302e0e52e55d8c77229,"This paper analyzes the drivers of financial distress that were experienced by small Italian cooperative banks during the latest deep recession, focusing mainly on the importance of bank capital as a predictor of bankruptcy for Italian nonprofit banks. The analysis aims to build an early-warning model that is suitable for this type of bank. The results reveal non-monotonic effects of bank capital on the probability of failure. In contrast to distress models for for-profit banks, non-performing loans, profitability, liquidity, and management quality have a negligible predictive value. The findings also show that unreserved impaired loans have an important impact on the probability of bank distress. Moreover, the loan–loss ratio provision on substandard loans constitutes a suitable antibody against bank distress. Overall, the results are robust in terms of both the methodology (i.e., frequentist and Bayesian approaches) and the sample used (i.e., cooperative banks in Italy and euro-area countries). © 2018 International Institute of Forecasters"
,10.1109/TNNLS.2017.2782711,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040636748&doi=10.1109%2fTNNLS.2017.2782711&partnerID=40&md5=9657de243ff6f87baa504f9b003faeb9,"The mixture of Gaussian processes (GPS) is capable of learning any general stochastic process based on a given set of (sample) curves for the regression and prediction problems. However, it is ineffective for curve clustering and prediction, when the sample curves are derived from different stochastic processes as independent sources linearly mixed together. In this paper, we propose a two-layer mixture model of GP functional regressions (GPFRs) to describe such a mixture of general stochastic processes or independent sources, especially for curve clustering and prediction. Specifically, in the lower layer, the mixture of GPFRs (MGPFRs) is developed for a cluster (or class) of curves within the input space. In the higher layer, the mixture of MGPFRs is further established to divide the curves into clusters according to its components in the output space. For the parameter estimation of the two-layer mixture of GPFRs, we develop a Monte Carlo EM algorithm based on a Monte Carlo Markov chain (MCMC) method, in short, the MCMC EM algorithm. We validate the hierarchical mixture of GPFRs and MCMC EM algorithm using synthetic and real-world data sets. Our results show that our new model outperforms the conventional mixture models in curve clustering and prediction. © 2012 IEEE."
,10.1002/qre.2314,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053450633&doi=10.1002%2fqre.2314&partnerID=40&md5=7a1b06172fa0c4f3e634a345128763ba,"Two-parameter (shifted) exponential distribution is widely applied in many areas such as reliability modeling and analysis where time to failure is protected by a guaranty period that induces an origin parameter in the exponential model. Despite a large volume of works on inferential aspects of two-parameter exponential distribution, only few studies are done from the perspective of process monitoring. In the modern production process, where items come with a warranty, we often encounter shifted-exponential time between events from consumers' perspective, and therefore, in this paper, we propose two CUSUM schemes for joint monitoring of the origin and scale parameters based on the Maximum Likelihood estimators. We study the in-control behavior of the proposed procedures via Markov chain approach as well as applying Monte Carlo. We provide detailed implementation strategies of the two schemes along with the follow-up procedures to identify the source of shifts when an out-of-control signal is obtained. We examine the performance properties of CUSUM schemes and find that the two proposed schemes offer performance advantages over the Shewhart-type schemes especially for monitoring small to moderate shifts. Further, we provide some guidance for choosing the appropriate schemes and study the effect of reference parameter k of the CUSUM schemes. We also investigate the optimal design of reference values both in known and unknown shift cases. Finally, two examples are given to illustrate the implementation of the proposed approach. © 2018 John Wiley & Sons, Ltd."
,10.1109/TGRS.2018.2825608,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046706999&doi=10.1109%2fTGRS.2018.2825608&partnerID=40&md5=e8077b926b8266915f09a040e47baa84,"The ages of terrains on other planetary bodies are chiefly determined using crater size-frequency distributions. However, primary impacts can generate numerous secondary craters that can affect the crater population. Classifying impact craters as primary or secondary is commonly done via time-consuming manual inspection, which limits the areas that can be analyzed at high resolution. We present a parametric model for characterizing small (100-600 m diameter) impact craters, where the model parameters have implications for describing the physical processes involved in their formation and modification. We infer these parameters from craters in images captured by the high-resolution imaging science experiment (HiRISE) camera onboard the Mars Reconnaissance Orbiter. For each crater within the appropriate size range, our algorithm creates a 3-D surface for a parametrically modeled crater and a 2-D rendering using illumination metadata, including emission, phase, and solar incidence angles at the time when the image was captured. A function describes the likelihood of each set of model parameters in terms of the geometry of craters in a given HiRISE image. These values are then optimized using a Metropolis-Hasting Markov chain Monte Carlo sampler. We evaluated three different prior probability distributions over the parameter space and two different likelihoods: one for digital terrain models and the other for images. We show that after applying t-distributed stochastic neighbor embedding (t-SNE) over the inferred crater parameters, t-SNE is able to project the multidimensional crater parameters into a 2-D space where secondary craters cluster together and are separable from primary craters. © 1980-2012 IEEE."
,10.1051/0004-6361/201833436,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055006385&doi=10.1051%2f0004-6361%2f201833436&partnerID=40&md5=2fbd22ebee6c5ba926a2cf565f6e80ef,"Context. The Kepler Object of Interest Network (KOINet) is a multi-site network of telescopes around the globe organised to follow up transiting planet-candidate Kepler objects of interest (KOIs) with large transit timing variations (TTVs). Its main goal is to complete their TTV curves, as the Kepler telescope no longer observes the original Kepler field. Aims. Combining Kepler and new ground-based transit data we improve the modelling of these systems. To this end, we have developed a photodynamical model, and we demonstrate its performance using the Kepler-9 system as an example. Methods. Our comprehensive analysis combines the numerical integration of the system's dynamics over the time span of the observations along with the transit light curve model. This provides a coherent description of all observations simultaneously. This model is coupled with a Markov chain Monte Carlo algorithm, allowing for the exploration of the model parameter space. Results. Applied to the Kepler-9 long cadence data, short cadence data, and 13 new transit observations collected by KOINet between the years 2014 and 2017, our modelling provides well constrained predictions for the next transits and the system's parameters. We have determined the densities of the planets Kepler-9b and 9c to the very precise values of ρb = 0.439 ± 0.023 g cm-3 and ρc = 0.322 ± 0.017 g cm-3. Our analysis reveals that Kepler-9c will stop transiting in about 30 yr due to strong dynamical interactions between Kepler-9b and 9c, near 2:1 resonance, leading to a periodic change in inclination. Conclusions. Over the next 30 years, the inclination of Kepler-9c (-9b) will decrease (increase) slowly. This should be measurable by a substantial decrease (increase) in the transit duration, in as soon as a few years' time. Observations that contradict this prediction might indicate the presence of additional objects in this system. If this prediction turns out to be accurate, this behaviour opens up a unique chance to scan the different latitudes of a star: High latitudes with planet c and low latitudes with planet b. © 2018 ESO."
1,10.1051/0004-6361/201832924,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054952645&doi=10.1051%2f0004-6361%2f201832924&partnerID=40&md5=1ab07bf31650ae8b1d5e7143003decb8,"Context. Evolutionary models are widely used to infer the mass of stars, brown dwarfs, and giant planets. Their predictions are thought to be less reliable at young ages (&lt; 200 Myr) and in the low-mass regime (&lt; 1 M⊙). GJ 2060 AB and TWA 22 AB are two rare astrometric M-dwarf binaries, respectively members of the AB Doradus (AB Dor) and Beta Pictoris (β Pic) moving groups. As their dynamical mass can be measured to within a few years, they can be used to calibrate the evolutionary tracks and set new constraints on the age of young moving groups. Aims. We provide the first dynamical mass measurement of GJ 2060 and a refined measurement of the total mass of TWA 22. We also characterize the atmospheric properties of the individual components of GJ 2060 that can be used as inputs to the evolutionary models. Methods. We used NaCo and SPHERE observations at VLT and archival Keck/NIRC2 data to complement the astrometric monitoring of the binaries. We combined the astrometry with new HARPS radial velocities (RVs) and FEROS RVs of GJ 2060. We used a Markov chain Monte-Carlo (MCMC) module to estimate posteriors on the orbital parameters and dynamical masses of GJ 2060 AB and TWA 22 AB from the astrometry and RVs. Complementary data obtained with the integral field spectrograph VLT/SINFONI were gathered to extract the individual near-infrared (1.1-2.5 μm) medium-resolution (R ∼ 1500-2000) spectra of GJ 2060 A and B. We compared the spectra to those of known objects and to grids of BT-SETTL model spectra to infer the spectral type, bolometric luminosities, and temperatures of those objects. Results. We find a total mass of 0.18±0.02 M⊙ for TWA 22, which is in good agreement with model predictions at the age of the β Pic moving group. We obtain a total mass of 1.09±0.10 M⊙ for GJ 2060. We estimate a spectral type of M1±0.5, L/L⊙ =-1.20±0.05 dex, and Teff = 3700±100 K for GJ 2060 A. The B component is a M3±0.5 dwarf with L/L⊙ =-1.63±0.05 dex and Teff = 3400±100 K. The dynamical mass of GJ 2060 AB is inconsistent with the most recent models predictions (BCAH15, PARSEC) for an AB Dor age in the range 50-150 Myr. It is 10%-20% (1-2σ, depending on the assumed age) above the model's predictions, corresponding to an underestimation of 0.10-0.20 M⊙. Coevality suggests a young age for the system (∼50 Myr) according to most evolutionary models. Conclusions. TWA 22 validates the predictions of recent evolutionary tracks at ∼20 Myr. On the other hand, we evidence a 1-2σ mismatch between the predicted and observed mass of GJ 2060 AB. This slight departure may indicate that one of the stars hosts a tight companion. Alternatively, this would confirm the model's tendency to underestimate the mass of young low-mass stars. © ESO 2018."
,10.1016/j.jmp.2018.08.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053770333&doi=10.1016%2fj.jmp.2018.08.005&partnerID=40&md5=20189e4ce54e289ec632f8683f05228d,"Measuring shared beliefs, expert consensus, or the details of a crime in eyewitness testimony represents a psychometric challenge. In expert interviews, for example, the correct responses representing the expert consensus (i.e., the answer key) are initially unknown and experts may differ in their contribution to this consensus. I propose the variable-response model, an extension of latent-trait models. The model allows the estimation of the answer key and the latent trait for continuous, categorical, or mixed responses. I describe some minimal requirements for the addition of new response formats to the model. I further propose a Markov chain Monte Carlo algorithm to estimate the model parameters. The results of a simulation study demonstrate that the algorithm accurately recovers the data-generating parameters. I also present an application of the variable-response model to the empirical data of a Geography test. In this application, the parameter estimates correspond well with the true answer key. © 2018 Elsevier Inc."
,10.1093/molbev/msy147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054892383&doi=10.1093%2fmolbev%2fmsy147&partnerID=40&md5=640c396b3d7408788342ee866b910fee,"The multispecies coalescent provides a natural framework for accommodating ancestral genetic polymorphism and coalescent processes that can cause different genomic regions to have different genealogical histories. The Bayesian program BPP includes a full-likelihood implementation of the multispecies coalescent, using transmodel Markov chain Monte Carlo to calculate the posterior probabilities of different species trees. BPP is suitable for analyzing multilocus sequence data sets and it accommodates the heterogeneity of gene trees (both the topology and branch lengths) among loci and gene tree uncertainties due to limited phylogenetic information at each locus. Here, we provide a practical guide to the use of BPP in species tree estimation. BPP is a command-line program that runs on linux, macosx, and windows. This protocol shows how to use both BPP 3.4 (http://abacus.gene.ucl.ac.uk/software/) and BPP 4.0 (https://github.com/bpp/)."
1,10.1093/cz/zox076,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047531305&doi=10.1093%2fcz%2fzox076&partnerID=40&md5=dcc55f6f47d9b95b1fb5d0d1b6351a8f,"Inbreeding negatively affects various life-history traits, with inbred individuals typically having lower fitness than outbred individuals (=inbreeding depression). Inbreeding depression is often emphasized under environmental stress, but the underlying mechanisms and potential long-lasting consequences of such inbreeding-environment interactions remain poorly understood. Here, we hypothesize that inbreeding-environment interactions that occur early in life have long-term physiological effects, in particular on the adult oxidative balance. We applied a unique experimental design to manipulate early life conditions of inbred and outbred songbirds (Serinus canaria) that allowed us to separate prenatal and postnatal components of early life conditions and their respective importance in inbreeding-environment interactions.We measured a wide variety of markers of oxidative status in adulthood, resulting in a comprehensive account for oxidative balance. Using a Bayesian approach with Markov chain Monte Carlo, we found clear sex-specific effects and we also found only in females small yet significant long-term effects of inbreeding-environment interactions on adult oxidative balance. Postnatal components of early life conditions were most persuasively reflected on adult oxidative balance, with inbred females that experienced disadvantageous postnatal conditions upregulating enzymatic antioxidants in adulthood. Our study provides some evidence that adult oxidative balance can reflect inbreeding-environment interactions early in life, but given the rather small effects that were limited to females, we conclude that oxidative stress might have a limited role asmechanism underlying inbreeding-environment interactions. © The Author (2017). Published by Oxford University Press."
3,10.1093/mnras/sty1835,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051545334&doi=10.1093%2fmnras%2fsty1835&partnerID=40&md5=e3cdd68dcedd3a3598bc8cf4447b0df9,"We study reionization in two non-flat (n-ary logical and)CDM inflation models that best fit the Planck 2015 cosmic microwave background (CMB) anisotropy observations, ignoring or in conjunction with baryon acoustic oscillation distance measurements.We implement a principal component analysis (PCA) to estimate the uncertainties in the reionization history from a joint quasar- CMB data set. A thorough Markov Chain Monte Carlo analysis is done over the parameter space of the PCAmodes for both non-flat(n-ary logical and)CDMinflationmodels aswell as the original Planck 2016 tilted, spatially flat (n-ary logical and)CDM inflation model. Although both flat and non-flat models can closely match the low-redshift (z ≲ 6) observations, we notice a possible tension between high-redshift (z ~ 8) Lyman a emitter data and the non-flat models. This is solely due to the fact that the closed models have a relatively higher reionization optical depth compared to the flat one, which in turn demands more high-redshift ionizing sources and favours an extended reionization starting as early as z ≈ 14. We conclude that as opposed to flat cosmology, for the non-flat cosmology models (i) the escape fraction needs steep redshift evolution and even unrealistically high values at some redshifts and (ii) most of the physical parameters require to have non-monotonic redshift evolution, especially apparent when Lyman a emitter data are included in the analysis. © 2018 The Author(s). Published by Oxford University Press on behalf of The Royal Astronomical Society."
,10.1111/gean.12152,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055344331&doi=10.1111%2fgean.12152&partnerID=40&md5=2dbeab7f9813f4651bff0aaf72b4a9ad,"We extend the heterogeneous coefficients spatial autoregressive panel model (HSAR) from Aquaro, Bailey, and Pesaran (2015) to the case of a heterogeneous coefficients matrix exponential spatial specification (HMESS). The HSAR is capable of producing parameter estimates for each region in the sample, that follow a spatial autoregressive process. Spatial autoregressive processes apply geometric decay of influence to higher-order neighboring regions. The HMESS takes a similar approach as the HSAR to produce estimates for each region in the sample, but relies on a matrix exponential function to apply exponential decay to higher-order neighbors. The MESS introduced by LeSage and Pace (2007) for the case of cross-sectional spatial data samples has some potential computational advantages over the spatial autoregressive specification. In addition, the spatial dependence parameter in the MESS ranges from minus to plus infinity, which allows for use of normal priors assigned to this parameter in a Bayesian setting. We extend the cross-sectional MESS to the case of a heterogeneous coefficients model, and describe Bayesian Markov Chain Monte Carlo estimation. We illustrate the HMESS model with a panel wage curve relationship using quarterly unemployment and wage rates from 261 counties centered on the Bakken shale oil region in North Dakota and Montana. © 2017 The Ohio State University"
1,10.1093/mnras/sty1820,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051458887&doi=10.1093%2fmnras%2fsty1820&partnerID=40&md5=046598d397de3f14714775b907a5a575,"With the Hubble Frontier Fields program, gravitational lensing has provided a powerful way to extend the study of the ultraviolet luminosity function (LF) of galaxies at z ~ 6 down to unprecedented magnitude limits. At the same time, significant discrepancies between different studies were found at the very faint end of the LF. In an attempt to understand such disagreements, we present a comprehensive assessment of the uncertainties associated with the lensing models and the size distribution of galaxies. We use end-to-end simulations from the source plane to the final LF that account for all lensing effects and systematic uncertainties by comparing several mass models. In addition to the size distribution, the choice of lens model leads to large differences at magnitudes fainter than MUV = -15 AB mag, where the magnification factor becomes highly uncertain. We perform Markov Chain Monte Carlo (MCMC) simulations that include all these uncertainties at the individual galaxy level to compute the final LF, allowing, in particular, a crossover between magnitude bins. The best LF fit, using a modified Schechter function that allows for a turnover at faint magnitudes, gives a faint-end slope of α = -2.01+0.12 -0.14, a curvature parameter of β = 0.48+0.49 -0.25, and a turnover magnitude ofMT = -14.93+0.61 -0.52. Most importantly, our procedure shows that robust constraints on the LF at magnitudes fainter than MUV = -15 AB remain unrealistic, as the 95 per cent confidence interval accommodates both a turnover and a steep faint-end slope. More accurate lens modeling and future observations of lensing clusters with the James Webb Space Telescope can reliably extend the ultraviolet (UV) LF to fainter magnitudes. © 2018 The Author(s). Published by Oxford University Press on behalf of The Royal Astronomical Society."
,10.1016/j.neuroimage.2018.06.073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049460748&doi=10.1016%2fj.neuroimage.2018.06.073&partnerID=40&md5=cc16f2650da95f6ff3ee9c9d2c75a51f,"A recently introduced hierarchical generative model unified the inference of effective connectivity in individual subjects and the unsupervised identification of subgroups defined by connectivity patterns. This hierarchical unsupervised generative embedding (HUGE) approach combined a hierarchical formulation of dynamic causal modelling (DCM) for fMRI with Gaussian mixture models and relied on Markov chain Monte Carlo (MCMC) sampling for inference. While well suited for the inversion of complex hierarchical models, MCMC-based sampling suffers from a computational burden that is prohibitive for many applications. To address this problem, this paper derives an efficient variational Bayesian (VB) inversion scheme for HUGE that simultaneously provides approximations to the posterior distribution over model parameters and to the log model evidence. The face validity of the VB scheme was tested using two synthetic fMRI datasets with known ground truth. Additionally, an empirical fMRI dataset of stroke patients and healthy controls was used to evaluate the practical utility of the method in application to real-world problems. Our analyses demonstrate good performance of our VB scheme, with a marked speed-up of model inversion by two orders of magnitude compared to MCMC, while maintaining a similar level of accuracy. Notably, additional acceleration would be possible if parallel computing techniques were applied. Generally, our VB implementation of HUGE is fast enough to support multi-start procedures for whole-group analyses, a useful strategy to ameliorate problems with local extrema. HUGE thus represents a potentially useful practical solution for an important problem in clinical neuromodeling and computational psychiatry, i.e., the unsupervised detection of subgroups in heterogeneous populations that are defined by effective connectivity. © 2018 The Authors"
,10.3847/1538-4357/aadba5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054796521&doi=10.3847%2f1538-4357%2faadba5&partnerID=40&md5=f6586ee3bb7d63b30f4b6b4c640218b7,"We develop and apply a model to quantify the global efficiency of radial orbit migration among stars in the Milky Way disk. This model parameterizes the possible star formation and enrichment histories and radial birth profiles, and combines them with a migration model that relates present-day orbital radii to birth radii through a Gaussian probability, broadening with age τ as . Guided by observations, we assume that stars are born with an initially tight age-metallicity relation at given radius, which becomes subsequently scrambled by radial orbit migration, thereby providing a direct observational constraint on radial orbit migration strength . We fit this model with Markov Chain Monte Carlo sampling of the observed age-metallicity distribution of low-α red clump stars with Galactocentric radii between 5 and 14 kpc from APOGEE DR12, sidestepping the complex spatial selection function and accounting for the considerable age uncertainties. This simple model reproduces the observed data well, and we find a global (in radius and time) radial orbit migration efficiency in the Milky Way of = 3.6 ± 0.1 kpc when marginalizing over all other aspects of the model. This shows that radial orbit migration in the Milky Way's main disk is indeed rather strong, in line with theoretical expectations: stars migrate by about a half-mass radius over the age of the disk. The model finds the Sun's birth radius at ∼5.2 kpc. If such strong radial orbit migration is typical, this mechanism indeed plays an important role in setting the structural regularity of disk galaxies. © 2018. The American Astronomical Society. All rights reserved."
,10.1016/j.agee.2018.06.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049477360&doi=10.1016%2fj.agee.2018.06.029&partnerID=40&md5=591ed275f9755c1592e6c22524b15e8a,"As has been widely reported, climate change will be felt throughout Europe, though effects are likely to vary dramatically across European regions. While all areas are expected to experience elevated atmospheric CO2 concentrations (↑C) and higher temperatures (↑T), the north east will get considerably wetter (↑W) while the south much drier (↓W). It is likely that these changes will have an impact on pastures and consequently on grazing livestock. This study aims to evaluate the expected changes to pasture yield and quality caused by ↑C, ↑T, ↑W and ↓W across the different European regions and across different plant functional groups (PFGs). Data was collected from 143 studies giving a total of 998 observations. Mixed models were used to estimate expected changes in above ground dry weight (AGDW) and nitrogen (N) concentrations and were implemented using Markov Chain Monte Carlo simulations. The results showed an increase in AGDW under ↑C, particularly for shrubs (+71.6%), though this is likely to be accompanied by a reduction in N concentrations (−4.8%). ↑T will increase yields in Alpine and northern areas (+82.6%), though other regions will experience little change or else decreases. ↑T will also reduce N concentrations, especially for shrubs (−13.6%) and forbs (−18.5%). ↓W will decrease AGDW for all regions and PFGs, though will increase N concentrations (+11.7%). Under ↑W there was a 33.8% increase in AGDW. While there is a need for further research to get a more complete picture of future pasture conditions, this analysis provides a general overview of expected changes and thus can help European farmers prepare to adapt their systems to meet the challenges presented by a changing climate. © 2018"
,10.1371/journal.pone.0205889,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055071100&doi=10.1371%2fjournal.pone.0205889&partnerID=40&md5=0d5ac3a26d6cd52a671cc5ab706a81d0,"Japan experienced a nationwide rubella epidemic from 2012 to 2013, mostly in urban prefectures with large population sizes. The present study aimed to capture the spatiotemporal patterns of rubella using a parsimonious metapopulation epidemic model and examine the potential usefulness of spatial vaccination. Methodology/Principal findings A metapopulation epidemic model in discrete time and space was devised and applied to rubella notification data from 2012 to 2013. Employing a piecewise constant model for the linear growth rate in six different time periods, and using the particle Markov chain Monte Carlo method, the effective reproduction numbers were estimated at 1.37 (95% CrI: 1.12, 1.77) and 1.37 (95% CrI: 1.24, 1.48) in Tokyo and Osaka groups, respectively, during the growing phase of the epidemic in 2013. The rubella epidemic in 2012 involved substantial uncertainties in its parameter estimates and forecasts. We examined multiple scenarios of spatial vaccination with coverages of 1%, 3% and 5% for all of Japan to be distributed in different combinations of prefectures. Scenarios indicated that vaccinating the top six populous urban prefectures (i.e., Tokyo, Kanagawa, Osaka, Aichi, Saitama and Chiba) could potentially be more effective than random allocation. However, greater uncertainty was introduced by stochasticity and initial conditions such as the number of infectious individuals and the fraction of susceptibles. Conclusions While the forecast in 2012 was accompanied by broad uncertainties, a narrower uncertainty bound of parameters and reliable forecast were achieved during the greater rubella epidemic in 2013. By better capturing the underlying epidemic dynamics, spatial vaccination could substantially outperform the random vaccination. © 2018 Saito et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.3847/1538-3881/aad45b,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054801851&doi=10.3847%2f1538-3881%2faad45b&partnerID=40&md5=61eca243aa576fc80de32e873b0a11e4,"The occultation of the radio galaxy 0141+268 by the asteroid (372) Palma on 2017 May 15 was observed using six antennas of the Very Long Baseline Array (VLBA). The shadow of Palma crossed the VLBA station at Brewster, Washington. Owing to the wavelength used, and the size and the distance of the asteroid, a diffraction pattern in the Fraunhofer regime was observed. The measurement retrieves both the amplitude and the phase of the diffracted electromagnetic wave. This is the first astronomical measurement of the phase shift caused by diffraction. The maximum phase shift is sensitive to the effective diameter of the asteroid. The bright spot at the shadow's center, the so called Arago-Poisson spot, is clearly detected in the amplitude time-series, and its strength is a good indicator of the closest angular distance between the center of the asteroid and the radio source. A sample of random shapes constructed using a Markov chain Monte Carlo algorithm suggests that the silhouette of Palma deviates from a perfect circle by 26 ± 13%. The best-fitting random shapes resemble each other, and we suggest their average approximates the shape of the silhouette at the time of the occultation. The effective diameter obtained for Palma, 192.1 ± .8 km, is in excellent agreement with recent estimates from thermal modeling of mid-infrared photometry. Finally, our computations show that because of the high positional accuracy, a single radio interferometric occultation measurement can reduce the long-term ephemeris uncertainty by an order of magnitude. © 2018. The American Astronomical Society. All rights reserved."
,10.1007/s10709-018-0027-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048376903&doi=10.1007%2fs10709-018-0027-x&partnerID=40&md5=f4c2dd2f99b9627a8f171d272527bd10,"Genomic prediction is feasible for estimating genomic breeding values because of dense genome-wide markers and credible statistical methods, such as Genomic Best Linear Unbiased Prediction (GBLUP) and various Bayesian methods. Compared with GBLUP, Bayesian methods propose more flexible assumptions for the distributions of SNP effects. However, most Bayesian methods are performed based on Markov chain Monte Carlo (MCMC) algorithms, leading to computational efficiency challenges. Hence, some fast Bayesian approaches, such as fast BayesB (fBayesB), were proposed to speed up the calculation. This study proposed another fast Bayesian method termed fast BayesC (fBayesC). The prior distribution of fBayesC assumes that a SNP with probability γ has a non-zero effect which comes from a normal density with a common variance. The simulated data from QTLMAS XII workshop and actual data on large yellow croaker were used to compare the predictive results of fBayesB, fBayesC and (MCMC-based) BayesC. The results showed that when γ was set as a small value, such as 0.01 in the simulated data or 0.001 in the actual data, fBayesB and fBayesC yielded lower prediction accuracies (abilities) than BayesC. In the actual data, fBayesC could yield very similar predictive abilities as BayesC when γ ≥ 0.01. When γ = 0.01, fBayesB could also yield similar results as fBayesC and BayesC. However, fBayesB could not yield an explicit result when γ ≥ 0.1, but a similar situation was not observed for fBayesC. Moreover, the computational speed of fBayesC was significantly faster than that of BayesC, making fBayesC a promising method for genomic prediction. © 2018, Springer International Publishing AG, part of Springer Nature."
,10.1088/1742-6596/1090/1/012014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054488552&doi=10.1088%2f1742-6596%2f1090%2f1%2f012014&partnerID=40&md5=0208f5a951e78b2dc49e838a1931f8dd,"The assessment and comparison of income inequality and poverty can be supported by estimating the probability distribution of income. Income distributions which are typically heavy-tailed and positively skewed have been estimated both parametric and nonparametric approach. In parametric approach, finite mixtures distributions have been usefully implemented in the modelling of income distributions which has the multimodal characteristic. The Markov Chain Monte Carlo (MCMC) approach is one of the estimation methods which has a good performance in estimating the parameter of Bayesian finite mixture model. The convergence of the MCMC sampler to the posterior distribution is typically assessed using standard diagnostics methods, i.e., Gelman-Rubin method, Geweke method, Raftery-Lewis method and Heidelberger-Welch method. Those methods can give different results to conclude MCMC convergence condition. In this paper, a real sample income data from the Indonesian Family Life Survey (IFLS) 2015 and BidikMisi 2015 are employed to demonstrate the performance of diagnostics tools that assess convergence of the MCMC algorithm in estimating the parameter of Bayesian finite mixture models. © Published under licence by IOP Publishing Ltd."
,10.1088/1742-6596/1090/1/012072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054554265&doi=10.1088%2f1742-6596%2f1090%2f1%2f012072&partnerID=40&md5=13a6c899b6044b5f826b70b9eb1831a8,"This research has a purpose to develop Bernoulli Mixture model for Bidikmisi data modelling using Bayesian approach. Model development is done by considering the specificity in the data acceptance of Bidikmisi scholarship prototype in East Java Province. Bidikmisi acceptance status having a binary type (0 and 1) coupled with the main criteria factor of parent income and the number of dependents family produces a structure of Bernoulli mixture distribution with two components. The characteristics of each component can be identified through the Bernoulli Mixture modelling by involving the covariates of Bidikmisi scholarship recipients. The estimating parameter was performed using Bayesian Markov Chain Monte Carlo (MCMC) couple with the Gibbs Sampling algorithm. This model is applied to data registrants Bidikmisi districts/cities in the province of East Java as many as 44,489 students. This model shows the smallest value of Deviance Information Criteria (DIC) compared with Bayesian binary logistic regression. © Published under licence by IOP Publishing Ltd."
,10.1103/PhysRevD.98.063533,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054478048&doi=10.1103%2fPhysRevD.98.063533&partnerID=40&md5=f04aad4ea762b03e8ed1492848842a9e,"In this work we investigate the holographic dark energy models with slowly time-varying model parameter defined based on the current Hubble horizon length scale. While the previous studies on the three popular holographic dark energy models defined based on the future event horizon, Ricci scale and Granda-Oliveros IR cutoffs showed that these models cannot fit the observational data [I. A. Akhlaghi, M. Malekjani, S. Basilakos, and H. Haghi, Mon. Not. R. Astron. Soc. 477, 3659 (2018)]MNRAA40035-871110.1093/mnras/sty903, in this work we show that the holographic dark energy models with time-varying model parameter defined on the current Hubble radius are well favored by observations. Using the standard χ2 minimization in the context of Markov Chain Monte Carlo method, we compare the ability of holographic dark energy models with time-varying c2 parameter constructed on the current Hubble length scale against different sets of observational data namely expansion data, growth rate data, and expansion+growth rate data, respectively. Based on the values of Akaike and Bayesian information criteria, we find that these types of holographic dark energy models are well fitted to both expansion and growth rate observations as equal to ΛCDM cosmology. We also put constraints on the cosmological parameters and show that the transition epoch form early decelerated to current accelerated expansion calculated in holographic dark energy models with time-varying model parameter defined on the Hubble length is consistent with observations. © 2018 American Physical Society."
,10.1515/jqas-2015-0076,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049726965&doi=10.1515%2fjqas-2015-0076&partnerID=40&md5=2f5da0d1b17daf63b62bd5376d31ee33,"A Bayesian model is used to evaluate the probability that a given skill performed in a specified area of the field will lead to a predetermined outcome by using discrete absorbing Markov chains. The transient states of the Markov process are defined by unique skill-area combinations. The absorbing states of the Markov process are defined by a shot, turnover, or bad turnover. Defining the states in this manner allows the probability of a transient state leading to an absorbing state to be derived. A non-informative prior specification of transition counts is used to permit the data to define the posterior distribution. A web application was created to collect play-by-play data from 34 Division 1 NCAA Women's soccer matches for the 2013-2014 seasons. A prudent construction of updated transition probabilities facilitates a transformation through Monte Carlo simulation to obtain marginal probability estimates of each unique skill-area combination leading to an absorbing state. For each season, marginal probability estimates for given skills are compared both across and within areas to determine which skills and areas of the field are most advantageous. © 2018 Walter de Gruyter GmbH, Berlin/Boston."
,10.1515/jqas-2017-0066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053147720&doi=10.1515%2fjqas-2017-0066&partnerID=40&md5=6c685d8ca8a5188eee0e454c62aa80a5,"Although there is no consensus on how to measure and quantify individual performance in any sport, there has been less development in this area for soccer than for other major sports. And only once this measurement is defined, does modeling for predictive purposes make sense. We use the player ratings provided by a popular Italian fantasy soccer game as proxies for the players' performance; we discuss the merits and flaws of a variety of hierarchical Bayesian models for predicting these ratings, comparing the models on their predictive accuracy on hold-out data. Our central goals are to explore what can be accomplished with a simple freely available dataset comprising only a few variables from the 2015-2016 season in the top Italian league, Serie A, and to focus on a small number of interesting modeling and prediction questions that arise. Among these, we highlight the importance of modeling the missing observations and we propose two models designed for this task. We validate our models through graphical posterior predictive checks and we provide out-of-sample predictions for the second half of the season, using the first half as a training set. We use Stan to sample from the posterior distributions via Markov chain Monte Carlo. © 2018 Walter de Gruyter GmbH, Berlin/Boston."
,10.1080/00949655.2018.1490418,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049088622&doi=10.1080%2f00949655.2018.1490418&partnerID=40&md5=aedac1c83e42d9480c98291f96a2a872,"Feature selection arises in many areas of modern science. For example, in genomic research, we want to find the genes that can be used to separate tissues of different classes (e.g. cancer and normal). One approach is to fit regression/classification models with certain penalization. In the past decade, hyper-LASSO penalization (priors) have received increasing attention in the literature. However, fully Bayesian methods that use Markov chain Monte Carlo (MCMC) for regression/classification with hyper-LASSO priors are still in lack of development. In this paper, we introduce an MCMC method for learning multinomial logistic regression with hyper-LASSO priors. Our MCMC algorithm uses Hamiltonian Monte Carlo in a restricted Gibbs sampling framework. We have used simulation studies and real data to demonstrate the superior performance of hyper-LASSO priors compared to LASSO, and to investigate the issues of choosing heaviness and scale of hyper-LASSO priors. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/00949655.2018.1487441,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048857826&doi=10.1080%2f00949655.2018.1487441&partnerID=40&md5=7feb6d01ca04785138c86dbb9b9c927c,"The inverted (or inverse) distributions are sometimes very useful to explore additional properties of the phenomenons which non-inverted distributions cannot. We introduce a new inverted model called the inverted Nadarajah–Haghighi distribution which exhibits decreasing and unimodal (right-skewed) density while the hazard rate shapes are decreasing and upside-down bathtub. Our main focus is the estimation (from both frequentist and Bayesian points of view) of the unknown parameters along with some mathematical properties of the new model. The Bayes estimators and the associated credible intervals are obtained using Markov Chain Monte Carlo techniques under squared error loss function. The gamma priors are adopted for both scale and shape parameters. The potentiality of the distribution is analysed by means of two real data sets. In fact, it is found to be superior in its ability to sufficiently model the data as compared to the inverted Weibull, inverted Rayleigh, inverted exponential, inverted gamma, inverted Lindley and inverted power Lindley models. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1088/1757-899X/418/1/012089,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054205221&doi=10.1088%2f1757-899X%2f418%2f1%2f012089&partnerID=40&md5=cc115cd9c5c6b33f6e3da96dbffdad49,"The continual demand for vehicle weight reduction, improved fuel efficiency and crashworthiness has driven the automotive industry to increasingly fabricate automotive body parts from advanced high strength steel (AHSS) sheet, such as dual phase (DP) and transformation induced plasticity (TRIP) steels. It is therefore essential to carefully investigate the forming behaviour of these sheet materials under various forming conditions. In this work, the quasi-static tensile flow behaviour of DP600 and TRIP780 sheet specimens was obtained in three orientations (RD, DD, and TD) with respect to the sheet rolling direction. A 3-parameter Voce hardening function was then fitted to each flow curve in order to determine true stress and true strain based on constant amount of plastic work per unit volume to calculate the normalized yield stress as well as the r-value for each material orientation. Yoshida's 6th-order polynomial anisotropic yield function, expressed as a function of the second and third invariants of the deviatoric stress tensor (J 2 and J 3, respectively), was used to predict the mechanical response of these two sheet materials. A new optimization method based on the Markov chain Monte Carlo (MCMC) MetropolisHastings (MH) algorithm was employed to calibrate the anisotropic yield function and determine the anisotropic coefficients. The yield loci for both materials were then derived as a function of only, and also as a function of both J2 and . The performance of each function is evaluated and validated by comparing the numerical predictions of r-value and flow stress directionality with the experimental results. And the effects of J 2 and J 3 in predicting the shape of the yield locus of DP600 and TRIP780 are also discussed. © Published under licence by IOP Publishing Ltd."
,10.1093/mnras/sty1691,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051462796&doi=10.1093%2fmnras%2fsty1691&partnerID=40&md5=466e6dda9f07694db8abef9260fcb096,"We introduce PHI, a fully Bayesian Markov chain Monte Carlo algorithm designed for the structural decomposition of galaxy images. PHI uses a triple layer approach to effectively and efficiently explore the complex parameter space. Combining this with the use of priors to prevent non-physical models, PHI offers a number of significant advantages for estimating surface brightness profile parameters over traditional optimization algorithms. We apply PHI to a sample of synthetic galaxies with Sloan Digital Sky Survey (SDSS)-like image properties to investigate the effect of galaxy properties on our ability to recover unbiased and wellconstrained structural parameters. In two-component bulge+disc galaxies, we find that the bulge structural parameters are recovered less well than those of the disc, particularly when the bulge contributes a lower fraction to the luminosity, or is barely resolved with respect to the pixel scale or point spread function (PSF). There are few systematic biases, apart from for bulge+disc galaxies with large bulge Sérsic parameter, n. On application to SDSS images, we find good agreement with other codes, when run on the same images with the same masks, weights, and PSF. Again, we find that bulge parameters are the most difficult to constrain robustly. Finally, we explore the use of a Bayesian information criterion method for deciding whether a galaxy has one or two components. © 2018 The Author(s). Published by Oxford University Press on behalf of the Royal Astronomical Society."
,10.1103/PhysRevD.98.063011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054474767&doi=10.1103%2fPhysRevD.98.063011&partnerID=40&md5=ead5452224b4f384e726fb1a185b6d89,"Isolated nonaxisymmetric rotating neutron stars producing continuous-gravitational-wave signals may undergo occasional spin-up events known as glitches. If unmodeled by a search, these glitches can result in continuous wave signals being missed or misidentified as detector artifacts. We outline a semicoherent glitch-robust search method that allows identification of continuous wave signal candidates that contain glitches and inferences about the model parameters. We demonstrate how this can be applied to the follow-up of candidates found by wide-parameter space searches. We find that a Markov chain Monte Carlo method outperforms a grid-based method in speed and accuracy. © 2018 authors. Published by the American Physical Society."
,10.3847/1538-4357/aadadc,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053902151&doi=10.3847%2f1538-4357%2faadadc&partnerID=40&md5=64fe0c511fa02a663308e1bb2abac585,"We report individual dynamical masses for the brown dwarfs ϵ Indi B and C, which have spectral types of T1.5 and T6, respectively, measured from astrometric orbit mapping. Our measurements are based on a joint analysis of astrometric data from the Carnegie Astrometric Planet Search and the Cerro Tololo Inter-American Observatory Parallax Investigation, as well as archival high-resolution imaging, and use a Markov chain Monte Carlo method. We find dynamical masses of 75.0 ±0.82 M Jup for the T1.5 B component and 70.1 ±0.68 M Jup for the T6 C component. These masses are surprisingly high for such cool objects and challenge our understanding of substellar structure and evolution. We discuss several evolutionary scenarios proposed in the literature and find that while none of them can provide conclusive explanations for the high substellar masses, evolutionary models incorporating lower atmospheric opacities come closer to approximating our results. We discuss the details of our astrometric model, its algorithm implementation, and how we determine parameter values via Markov chain Monte Carlo Bayesian inference. © 2018. The American Astronomical Society. All rights reserved."
,10.1103/PhysRevC.98.035802,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053849917&doi=10.1103%2fPhysRevC.98.035802&partnerID=40&md5=e07830bed5cb1678fd7470dc15b4cbe7,"If the thermal evolution of the hot young neutron star in the supernova remnant HESS J1731-347 is driven by neutrino emission, it provides a stringent constraint on the coupling of light (mass 10keV) axion-like particles to neutrons. Using Markov-Chain Monte Carlo we find that for the values of axion-neutron coupling gann2>7.7×10-20 (90% c.l.) the axion cooling from the bremsstrahlung reaction n+n→n+n+a is too rapid to account for the high observed surface temperature. This implies that the Pecci-Quinn scale or axion decay constant fa>6.7×107GeV for KSVZ axions and fa>1.7×109GeV for DFSZ axions. The high temperature of this neutron star also allows us to tighten constraints on the size of the nucleon pairing gaps. © 2018 American Physical Society."
,10.3847/1538-4357/aad95d,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053897152&doi=10.3847%2f1538-4357%2faad95d&partnerID=40&md5=b73d8e2759abad05b222619d779af308,"Interior characterization traditionally relies on individual planetary properties, ignoring correlations between different planets of the same system. For multiplanetary systems, planetary data are generally correlated. This is because the differential masses and radii are better constrained than absolute planetary masses and radii. We explore such correlations and data specific to the multiplanetary system of TRAPPIST-1 and study their value for our understanding of planet interiors. Furthermore, we demonstrate that the rocky interior of planets in a multiplanetary system can be preferentially probed by studying the densest planet representing a rocky interior analog. Our methodology includes a Bayesian inference analysis that uses a Markov chain Monte Carlo scheme. Our interior estimates account for the anticipated variability in the compositions and layer thicknesses of core, mantle, water oceans, and ice layers, as well as a gas envelope. Our results show that (1) interior estimates significantly depend on available abundance proxies and (2) the importance of interdependent planetary data for interior characterization is comparable to changes in data precision by 30%. For the interiors of TRAPPIST-1 planets, we find that possible water mass fractions generally range from 0% to 25%. The lack of a clear trend of water budgets with orbital period or planet mass challenges possible formation scenarios. While our estimates change relatively little with data precision, they critically depend on data accuracy. If planetary masses varied within ±24%, interiors would be consistent with uniform (∼7%) or an increasing water mass fractions with orbital period (∼2%-12%). © 2018. The American Astronomical Society. All rights reserved."
,10.1186/s12936-018-2478-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053528747&doi=10.1186%2fs12936-018-2478-z&partnerID=40&md5=61ac5ba6e95fc308eab580bc15650dc9,"Background: Viet Nam has made tremendous progress towards reducing mortality and morbidity associated with malaria in recent years. Despite the success in malaria control, there has been a recent increase in cases in some provinces. In order to understand the changing malaria dynamics in Viet Nam and measure progress towards elimination, the aim of this study was to describe and quantify spatial and temporal trends of malaria by species at district level across the country. Methods: Malaria case reports at the Viet Nam National Institute of Malariology, Parasitology, and Entomology were reviewed for the period of January 2009 to December 2015. The population of each district was obtained from the Population and Housing Census-2009. A multivariate (insecticide-treated mosquito nets [ITN], indoor residual spraying [IRS], maximum temperature), zero-inflated, Poisson regression model was developed with spatial and spatiotemporal random effects modelled using a conditional autoregressive prior structure, and with posterior parameters estimated using Bayesian Markov chain Monte Carlo simulation with Gibbs sampling. Covariates included in the models were coverage of intervention (ITN and IRS) and maximum temperature. Results: There was a total of 57,713 Plasmodium falciparum and 32,386 Plasmodium vivax cases during the study period. The ratio of P. falciparum to P. vivax decreased from 4.3 (81.0% P. falciparum; 11,121 cases) in 2009 to 0.8 (45.0% P. falciparum; 3325 cases) in 2015. Coverage of ITN was associated with decreased P. falciparum incidence, with a 1.1% (95% credible interval [CrI] 0.009%, 1.2%) decrease in incidence for 1% increase in the ITN coverage, but this was not the case for P. vivax, nor was it the case for IRS coverage. Maximum temperature was associated with increased incidence of both species, with a 4% (95% CrI 3.5%, 4.3%) and 1.6% (95% CrI 0.9%, 2.0%) increase in P. falciparum and P. vivax incidence for a temperature increase of 1 °C, respectively. Temporal trends of P. falciparum and P. vivax incidence were significantly higher than the national average in Central and Central-Southern districts. Conclusion: Interventions (ITN distribution) and environmental factors (increased temperature) were associated with incidence of P. falciparum and P. vivax during the study period. The factors reviewed were not exhaustive, however the data suggest distribution of resources can be targeted to areas and times of increased malaria transmission. Additionally, changing distribution of the two predominant malaria species in Viet Nam will require different programmatic approaches for control and elimination. © 2018 The Author(s)."
,10.1088/1361-6382/aadc36,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053905352&doi=10.1088%2f1361-6382%2faadc36&partnerID=40&md5=0a81ca4a5a6cd15868f8578a4aaa102d,"Everpresent Λ is a cosmological scenario in which the observed cosmological 'constant' Λ fluctuates between positive and negative values with a vanishing mean, and with a magnitude comparable to the critical density at any epoch. In accord with a longstanding heuristic prediction of causal set theory, it postulates that Λ is a stochastic function of cosmic time that will vary from one realization of the scenario to another. Herein, we consider two models of 'dark energy' that exhibit these features. Via Monte Carlo Markov chains, we explore the space of cosmological parameters and the set of stochastic realizations of these models, finding that Everpresent Λ can fit current cosmological observations as well as the ΛCDM model does. Furthermore, it removes the observational tensions that ΛCDM experiences in relation to low redshift measurements of the Hubble constant, and to the baryonic acoustic oscillations (BAO) in Lyman-α forest at -3. It does not, however, help significantly with the early growth of ultramassive black holes, or with the Lithium problem in Big Bang nucleosynthesis. Future measurements of 'dark energy' at high redshifts will further test the viability of Everpresent Λ as an alternative to the ΛCDM cosmology. © 2018 IOP Publishing Ltd."
,10.1016/j.ejor.2018.02.037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043503189&doi=10.1016%2fj.ejor.2018.02.037&partnerID=40&md5=ff034510540c01f8262a91f7e24d2cc7,"We propose a novel multivariate approach for dependence analysis in the energy market. The methodology is based on tree copulas and GARCH type processes. We use it to study the dependence structure among the main factors affecting energy price, and to perform portfolio risk evaluation. The temporal dynamic of the examined variables is described via a set of GARCH type models where the joint distribution of the standardised residuals is represented via suitable tree copula structures. Working in a Bayesian framework, we perform both qualitative and quantitative learning. Posterior summaries of the quantities of interest are obtained via MCMC methods. © 2018 Elsevier B.V."
,10.1016/j.physa.2018.03.096,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046170598&doi=10.1016%2fj.physa.2018.03.096&partnerID=40&md5=29536f02103dff44cc68bea70d890b56,"Sampling from complicated and unknown distributions has wide-ranging applications. Standard Monte Carlo techniques are designed for known distributions and are difficult to adapt when the distribution is unknown. Markov Chain Monte Carlo (MCMC) techniques are designed for unknown distributions, but when the underlying state space is complex and not continuous, the application of MCMC becomes challenging and no longer straightforward. Both of these techniques have been proposed for the astronomically large redistricting application that is characterized by an extremely complex and idiosyncratic state space. We explore the theoretic applicability of these methods and evaluate their empirical performance. © 2018 Elsevier B.V."
,10.1080/03610918.2017.1341525,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025833700&doi=10.1080%2f03610918.2017.1341525&partnerID=40&md5=ea3cdf56436f0d95f633c1565fd41aa3,"In this article, to reduce computational load in performing Bayesian variable selection, we used a variant of reversible jump Markov chain Monte Carlo methods, and the Holmes and Held (HH) algorithm, to sample model index variables in logistic mixed models involving a large number of explanatory variables. Furthermore, we proposed a simple proposal distribution for model index variables, and used a simulation study and real example to compare the performance of the HH algorithm with our proposed and existing proposal distributions. The results show that the HH algorithm with our proposed proposal distribution is a computationally efficient and reliable selection method. © 2018, © 2018 Taylor & Francis Group, LLC."
,10.1186/s12859-018-2347-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053317941&doi=10.1186%2fs12859-018-2347-3&partnerID=40&md5=25b5bd81c4fd7efca86597266d1a9fd1,"Background: Conventional phylogenetic clustering approaches rely on arbitrary cutpoints applied a posteriori to phylogenetic estimates. Although in practice, Bayesian and bootstrap-based clustering tend to lead to similar estimates, they often produce conflicting measures of confidence in clusters. The current study proposes a new Bayesian phylogenetic clustering algorithm, which we refer to as DM-PhyClus (Dirichlet-Multinomial Phylogenetic Clustering), that identifies sets of sequences resulting from quick transmission chains, thus yielding easily-interpretable clusters, without using any ad hoc distance or confidence requirement. Results: Simulations reveal that DM-PhyClus can outperform conventional clustering methods, as well as the Gap procedure, a pure distance-based algorithm, in terms of mean cluster recovery. We apply DM-PhyClus to a sample of real HIV-1 sequences, producing a set of clusters whose inference is in line with the conclusions of a previous thorough analysis. Conclusions: DM-PhyClus, by eliminating the need for cutpoints and producing sensible inference for cluster configurations, can facilitate transmission cluster detection. Future efforts to reduce incidence of infectious diseases, like HIV-1, will need reliable estimates of transmission clusters. It follows that algorithms like DM-PhyClus could serve to better inform public health strategies. © 2018 The Author(s)."
,10.3390/s18093057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053662413&doi=10.3390%2fs18093057&partnerID=40&md5=c019f009c20d0b665bf5c5d8cb2edb62,"We focus on a Bayesian inference framework for finite element (FE) model updating of a long-span cable-stayed bridge using long-term monitoring data collected from a wireless sensor network (WSN). A robust Bayesian inference method is proposed which marginalizes the prediction-error precisions and applies Transitional Markov Chain Monte Carlo (TMCMC) algorithm. The proposed marginalizing error precision is compared with other two treatments of prediction-error precisions, including the constant error precisions and updating error precisions through theoretical analysis and numerical investigation based on a bridge FE model. TMCMC is employed to draw samples from the posterior probability density function (PDF) of the structural model parameters and the uncertain prediction-error precision parameters if required. It is found that the proposed Bayesian inference method with prediction-error precisions marginalized as “nuisance” parameters produces an FE model with more accurate posterior uncertainty quantification and robust modal property prediction. When applying the identified modal parameters from acceleration data collected during a one-year period from the large-scale WSN on the bridge, we choose two candidate model classes using different parameter grouping based on the clustering results from a sensitivity analysis and apply Bayes’ Theorem at the model class level. By implementing the TMCMC sampler, both the posterior distributions of the structural model parameters and the plausibility of the two model classes are characterized given the real data. Computation of the posterior probabilities over the candidate model classes provides a procedure for Bayesian model class assessment, where the computation automatically implements Bayesian Ockham razor that trades off between data-fitting and model complexity, which penalizes model classes that “over-fit” the data. The results of FE model updating and assessment based on the real data using the proposed method show that the updated FE model can successfully predict modal properties of the structural system with high accuracy. © 2018 by the authors. Licensee MDPI, Basel, Switzerland."
,10.1109/RAM.2018.8463028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054150819&doi=10.1109%2fRAM.2018.8463028&partnerID=40&md5=8e902fea2e0bb63fab3e724c3f3b33cb,"This paper deals with selective maintenance of a multistate series system working under time-varying environmental (or operational) conditions. The environmental conditions are evolving dynamically during the mission and influence the degradation rate of each component and the whole system. We assume that the environmental conditions vary as a continuous-time Markov chain. The components are maintained during the maintenance break between two consecutive missions performing maintenance actions: do-nothing, imperfect, and perfect maintenance. The selective maintenance optimization problem is used to find the optimal maintenance strategy in order to maximize the expected system reliability in the next mission subjected to maintenance time and budget limitations. Monte Carlo simulation is used to evaluate the reliability of the system at the end of the next mission considering variable environmental/operational conditions. An example is provided to demonstrate the importance of considering the uncertainty in environmental (or operational) conditions. © 2018 IEEE."
,10.1109/ICASSP.2018.8461412,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054275010&doi=10.1109%2fICASSP.2018.8461412&partnerID=40&md5=c7f9f45d0730ce01ba8f3a40c427130d,"Channel gain cartography relies on sensor measurements to construct maps providing the attenuation profile between arbitrary transmitter-receiver locations. Existing approaches capitalize on tomographic models, where shadowing is the weighted integral of a spatial loss field (SLF) depending on the propagation environment. Currently, the SLF is learned via regularization methods tailored to the propagation environment. However, the effectiveness of existing approaches remains unclear especially when the propagation environment involves heterogeneous characteristics. To cope with this, the present work considers a piecewise homogeneous SLF with a hidden Markov random field (MRF) model under the Bayesian framework. Efficient field estimators are obtained by using samples from Markov chain Monte Carlo (MCMC). Furthermore, an uncertainty sampling algorithm is developed to adaptively collect measurements. Real data tests demonstrate the capabilities of the novel approach. © 2018 IEEE."
,10.1109/ICASSP.2018.8462438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054259859&doi=10.1109%2fICASSP.2018.8462438&partnerID=40&md5=0223f294b009094bffc20192611e04c0,"This work addresses the problem of segmentation in time series data with respect to a statistical parameter of interest in Bayesian models. It is common to assume that the parameters are distinct within each segment. As such, many Bayesian change point detection models do not exploit the segment parameter patterns, which can improve performance. This work proposes a Bayesian mean-shift change point detection algorithm that makes use of repetition in segment parameters, by introducing segment class labels that utilise a Dirichlet process prior. The performance of the proposed approach was assessed on both synthetic and real world data, highlighting the enhanced performance when using parameter labelling. © 2018 IEEE."
,10.1109/ICASSP.2018.8462197,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054257655&doi=10.1109%2fICASSP.2018.8462197&partnerID=40&md5=f706bfe932e0d9c053755fc0e762d740,"Supervised classification and spectral unmixing are two methods to extract information from hyperspectral images. However, despite their complementarity, they have been scarcely considered jointly. This paper presents a new hierarchical Bayesian model to perform simultaneously both analysis in order to ensure that they benefit from each other. A linear mixture model is proposed to described the pixel measurements. Then a clustering is performed to identify groups of statistically similar abundance vectors. A Markov random field (MRF) is used as prior for the corresponding cluster labels. It promotes a spatial regularization through a Potts-Markov potential and also includes a local potential induced by the classification. Finally, the classification exploits a set of possibly corrupted labeled data provided by the end-user. Model parameters are estimated thanks to a Markov chain Monte Carlo (MCMC) algorithm. The interest of the proposed model is illustrated on synthetic and real data. © 2018 IEEE."
,10.1080/02664763.2017.1420147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039854958&doi=10.1080%2f02664763.2017.1420147&partnerID=40&md5=ebf4fef11dbd2a4f7b928856efcc2f50,"In this paper, we provide a full Bayesian analysis for Cox's proportional hazards model under different hazard rate shape assumptions. To this end, we select the modified Weibull distribution family to model failure rates. A novel Markov chain Monte Carlo method allows one to tackle both exact and right-censored failure time data. Both simulated and real data are used to illustrate the methods. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1109/ICASSP.2018.8462457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054267190&doi=10.1109%2fICASSP.2018.8462457&partnerID=40&md5=0062976ec02c788c542caca67bb55d3c,"The Non-homogeneous Poisson process is a point process with time-varying intensity across its domain, the use of which arises in numerous areas in signal processing and machine learning. However, applications are largely limited by the intractable likelihood function and the high computational cost of existing inference schemes. We present a sequential inference framework that utilises generative Poisson data and sequential Markov Chain Monte Carlo (SMCMC) algorithm to enable online inference in various applications. The proposed model is compared to competing methods on synthetic datasets and tested with real-world financial data. © 2018 IEEE."
,10.23919/ICIF.2018.8455750,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054062058&doi=10.23919%2fICIF.2018.8455750&partnerID=40&md5=0d503b2b858041844bed0a32a80c0206,"Relying on the idea of importance sampling for substantiating the Bayesian filtering recursion, particle filters may become prohibitively inefficient even for moderate state dimensions and likewise whenever the signal to noise ratio is relatively high, as is the case with nearly deterministic state dynamics or random parameters. Markov chain Monte Carlo particle filters completely avoid importance sampling and by that circumvent many of the deficiencies associated with conventional particle filters. These methods may nevertheless suffer from slow convergence rate once inadequate or computationally intractable proposal distributions are used for generating new candidate samples in the underlying Markov chain. In this work, we devise a new Markov chain Monte Carlo particle filter whose sampling mechanism employs jumping Gaussian distributions. This technique enhances the underlying sampling efficiency and leads to significant reduction in the computational cost. The newly derived filter is shown to outperform the conventional (regularised) particle filter both in terms of accuracy and computational overhead, particularly when applied to estimation in systems with low intensity noise or of relatively high state dimensions. © 2018 ISIF"
,10.23919/ICIF.2018.8455349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054086524&doi=10.23919%2fICIF.2018.8455349&partnerID=40&md5=3c9b81101242e671299545ca296eea8d,"Particle filters are powerful and general tools for performing nonlinear, non-Gaussian filtering. They provide an estimate of the distribution on target state at the time of the last measurement. However, it is often desirable to compute the posterior distribution on the target's path over an interval of time given all the measurements received in that interval, i.e., a smoothed estimate of the target's path. The process of computing this distribution is called smoothing. This paper presents a Markov Chain Monte Carlo (MCMC) approach to smoothing when the target motion is given by a Generalized Random Tour (GRT) model, a non-Gaussian motion model. This model is particularly appropriate in maritime tracking situations which often involve non-linear measurements. Since the filter is non-linear and non-Gaussian, one cannot apply a Kalman smoother. It is easy and natural to simulate target paths using a GRT model, but the transition function does not have a closed analytic form. As a result, one cannot use standard methods for particle filter smoothing. In this paper, we describe a method for performing MCMC smoothing for GRT particle filters and demonstrate the results of using this method in examples. © 2018 ISIF"
,10.23919/ICIF.2018.8455307,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054082435&doi=10.23919%2fICIF.2018.8455307&partnerID=40&md5=5460ec357a0ceedb5cb5073165ae3d4f,"In our previous work, we proposed a particle Gaussian mixture (PGM-I) filter for nonlinear estimation. The PGM-I filter uses the transition kernel of the state Markov chain to sample from the propagated prior. It constructs a Gaussian mixture representation of the propagated prior density by clustering the samples. The measurement data is incorporated by updating individual mixture modes using the Kalman measurement update. However, the Kalman measurement update is inexact when the measurement function is nonlinear and leads to the restrictive assumption that the number of modes remain fixed during the measurement update. In this paper, we introduce an alternate PGM-II filter that employs parallelized Markov Chain Monte Carlo sampling to perform the measurement update. The PGM-II filter update is asymptotically exact and does not enforce any assumptions on the number of Gaussian modes. The PGM-II filter is employed in the estimation of two test case systems. The results indicate that the PGM-II filter is suitable for handling nonlinear/non-Gaussian measurement update. © 2018 ISIF"
,10.1016/j.vaccine.2018.07.053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050650362&doi=10.1016%2fj.vaccine.2018.07.053&partnerID=40&md5=33b55b6f68987fd0991fa21c85417a3a,"Background: Norovirus is thought to be responsible for a fifth of all acute gastroenteritis cases globally each year. The population level transmission dynamics of this very common virus are still poorly understood, in part because illness is under-reported. With vaccines undergoing clinical trials, there is a growing need for appropriate, empirically grounded models, to predict the likely impact of vaccination. Methods: We developed a dynamic age-specific mathematical model of norovirus transmission and vaccination, informed by available data, particularly age-stratified time series case notification data. We introduce the use of a self-reporting Markov model to account for variation by age and over time in the statutory reporting of norovirus in Germany. We estimated the model using a sequential Monte Carlo particle filter. We then extended and applied our estimated model to investigate the potential impact of a range of immunisation strategies. We performed sensitivity analyses on the mode of vaccine action and other vaccine-related parameters. Results: We find that routine immunisation could reduce the incidence of norovirus by up to 70.5% even when those vaccines do not provide complete protection from disease. Furthermore, we find that the relative efficiency of alternative strategies targeting different age groups are dependant on the outcome we consider and are sensitive to assumptions on the mode of vaccine action. Strategies that target infants and toddler are more efficient in preventing infection but targeting older adults is preferable for preventing severe outcomes. Conclusions: Our model provides a robust estimate of a dynamic transmission model for norovirus at the population level. Vaccination may be an effective strategy in preventing disease but further work is required to ascertain norovirus vaccine efficacy, its mode of action and to estimate the cost-effectiveness of immunisation against norovirus. © 2018 The Authors"
,10.1021/acs.est.8b03217,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052400614&doi=10.1021%2facs.est.8b03217&partnerID=40&md5=b78b33026633cc6817ddd84f673ec52c,"We estimate postmeter methane (CH4) emissions from California's residential natural gas (NG) system using measurements and analysis from a sample of homes and appliances. Quiescent whole-house emissions (i.e., pipe leaks and pilot lights) were measured using a mass balance method in 75 California homes, while CH4 to CO2 emission ratios were measured for steady operation of individual combustion appliances and, separately, for transient operation of three tankless water heaters. Measured quiescent whole-house emissions are typically &lt;1 g CH4/day, though they exhibit long-tailed gamma distributions containing values &gt;10 g CH4/day. Most operating appliances yield undetectable CH4 to CO2 enhancements in steady operation (&lt;0.01% of gas consumed), though storage water heaters and stovetops exhibit long-tailed gamma distributions containing high values (∼1-3% of gas consumed), and transients are observed for the tankless heaters. Extrapolating results to the state-level using Bayesian Markov chain Monte Carlo sampling combined with California housing statistics and gas use information suggests quiescent house leakage of 23.4 (13.7-45.6, at 95% confidence) Gg CH4, with pilot lights contributing ∼30%. Emissions from steady operation of appliances and their pilots are 13.3 (6.6-37.1) Gg CH4/yr, an order of magnitude larger than current inventory estimates, with transients likely increasing appliance emissions further. Together, emissions from residential NG are 35.7 (21.7-64.0) Gg CH4/yr, equivalent to ∼15% of California's NG CH4 emissions, suggesting leak repair, improvement of combustion appliances, and adoption of nonfossil energy heating sources can help California meet its 2050 climate goals. © 2018 American Chemical Society."
,10.1103/PhysRevLett.121.101101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053313875&doi=10.1103%2fPhysRevLett.121.101101&partnerID=40&md5=d9876a023668069502fd55c340cece94,"Gravitational torques among objects orbiting a supermassive black hole drive the rapid reorientation of orbital planes in nuclear star clusters (NSCs), a process known as vector resonant relaxation. In this Letter, we determine the statistical equilibrium of systems with a distribution of masses, semimajor axes, and eccentricities. We average the interaction over the apsidal precession time and construct a Monte Carlo Markov chain method to sample the microcanonical ensemble of the NSC. We examine the case of NSCs formed by 16 episodes of star formation or globular cluster infall. We find that the massive stars and stellar mass black holes form a warped disk, while low mass stars resemble a spherical distribution with a possible net rotation. This explains the origin of the clockwise disk in the Galactic center and predicts a population of black holes (BHs) embedded within this structure. The rate of mergers among massive stars, tidal disruption events of massive stars by BHs, and BH-BH mergers are highly increased in such disks. The first two may explain the origin of the observed G1 and G2 clouds, the latter may be important for gravitational wave detections with LIGO and VIRGO. More generally, black holes are expected to settle in disks in all dense spherical stellar systems assembled by mergers of smaller systems including globular clusters. © 2018 American Physical Society."
,10.1088/1475-7516/2018/09/002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054524370&doi=10.1088%2f1475-7516%2f2018%2f09%2f002&partnerID=40&md5=378abdc1e78b2e6e412c8a914de092f4,"Future neutrino detectors will obtain high-statistics data from a nearby core-collapse supernova. We study the mixing with eV-mass sterile neutrinos in a supernova environment and its effects on the active neutrino fluxes as detected by Hyper-Kamiokande and IceCube. Using a Markov Chain Monte Carlo analysis, we make projections for how accurately these experiments will measure the active-sterile mixing angle θs given that there are substantial uncertainties on the expected luminosity and spectrum of active neutrinos from a galactic supernova burst. We find that Hyper-Kamiokande can reconstruct the sterile neutrino mixing and mass in many different situations, provided the neutrino luminosity of the supernova is known precisely. Crucially, we identify a degeneracy between the mixing angle and the overall neutrino luminosity of the supernova. This means that it will only be possible to determine the luminosity if the presence of sterile neutrinos with θs 0.1 can be ruled out independently. We discuss ways in which this degeneracy may be broken in the future. © 2018 IOP Publishing Ltd and Sissa Medialab."
,10.1088/1475-7516/2018/09/001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054470005&doi=10.1088%2f1475-7516%2f2018%2f09%2f001&partnerID=40&md5=d95c214d4035292ab45e6939fc7e28a5,"It is a well known fact that galaxies are biased tracers of the distribution of matter in the Universe. The galaxy bias is usually factored as a function of redshift and scale, and approximated as being scale-independent on large, linear scales. In cosmologies with massive neutrinos, the galaxy bias defined with respect to the total matter field (cold dark matter, baryons, and non-relativistic neutrinos) also depends on the sum of the neutrino masses Mν, and becomes scale-dependent even on large scales. This effect has been usually neglected given the sensitivity of current surveys. However, it becomes a severe systematic for future surveys aiming to provide the first detection of non-zero Mν. The effect can be corrected for by defining the bias with respect to the density field of cold dark matter and baryons, rather than the total matter field. In this work, we provide a simple prescription for correctly mitigating the neutrino-induced scale-dependent bias effect in a practical way. We clarify a number of subtleties regarding how to properly implement this correction in the presence of redshift-space distortions and non-linear evolution of perturbations. We perform a Markov Chain Monte Carlo analysis on simulated galaxy clustering data that match the expected sensitivity of the Euclid survey. We find that the neutrino-induced scale-dependent bias can lead to important shifts in both the inferred mean value of Mν, as well as its uncertainty, and provide an analytical explanation for the magnitude of the shifts. We show how these shifts propagate to the inferred values of other cosmological parameters correlated with Mν, such as the cold dark matter physical density Ωcdm h2 and the scalar spectral index ns. In conclusion, we find that correctly accounting for the neutrino-induced scale-dependent bias will be of crucial importance for future galaxy clustering analyses. We encourage the cosmology community to correctly account for this effect using the simple prescription we present in our work. The tools necessary to easily correct for the neutrino-induced scale-dependent bias will be made publicly available in an upcoming release of the Boltzmann solver CLASS. © 2018 IOP Publishing Ltd and Sissa Medialab."
1,10.1080/15732479.2017.1402064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035124243&doi=10.1080%2f15732479.2017.1402064&partnerID=40&md5=1de0664775a85512bd7432f0e169c3e9,"Existing performance models developed for interurban pavements are not applicable to urban pavements due to differences in traffic demands and deterioration trends. The objective of the study was to develop performance models for the management of urban pavement networks. Markov chains and Monte Carlo simulation were applied to account for the probabilistic nature of pavements deterioration over time, using data collected in the field. One of the advantages of this methodology is that it can be used by local agencies with scarce technical resources and historical data. Eight performance models were developed and successfully validated for asphalt and concrete pavements in humid, dry and Mediterranean climates with different functional hierarchies. The resulting models evidence the impact of design, traffic demand, climate and construction standards on urban pavements performance. Predicted service life of asphalt and concrete pavements in primary networks are consistent with design standards. However, pavements in secondary and local networks present shorter and longer service life compared to design life, respectively. Climate is a relevant factor for asphalt pavements, where higher deterioration was observed compared to that expected. Opposite to this, no relevant differences between design and performance can be attributed to climate in concrete pavements. © 2017, © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/03610926.2017.1367814,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032696075&doi=10.1080%2f03610926.2017.1367814&partnerID=40&md5=d10781c9fe2235bcff02409b2326cced,"In the literature, assuming independence of random variables X and Y, statistical estimation of the stress–strength parameter R = P(X > Y) is intensively investigated. However, in some real applications, the strength variable X could be highly dependent on the stress variable Y. In this paper, unlike the common practice in the literature, we discuss on estimation of the parameter R where more realistically X and Y are dependent random variables distributed as bivariate Rayleigh model. We derive the Bayes estimates and highest posterior density credible intervals of the parameters using suitable priors on the parameters. Because there are not closed forms for the Bayes estimates, we will use an approximation based on Laplace method and a Markov Chain Monte Carlo technique to obtain the Bayes estimate of R and unknown parameters. Finally, simulation studies are conducted in order to evaluate the performances of the proposed estimators and analysis of two data sets are provided. © 2018, © 2018 Taylor & Francis Group, LLC."
,10.1080/15732479.2017.1418009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039560400&doi=10.1080%2f15732479.2017.1418009&partnerID=40&md5=c8390c8b34e5729acfd650da96029415,"The main purpose of this study is to develop an estimation procedure of seismic design level setting for reinforced concrete (RC) piers considering aftershock-induced seismic hazards. This work develops an assessment method of the seismic hazards induced by aftershocks and takes an example of the Chi–Chi Earthquake in Taiwan. The number of aftershocks is assumed to follow the modified Gutenberg–Richter law with lower and upper bounds when analysing the cumulative density function of the magnitude of the aftershock within a specified post-mainshock period for the earthquake. Additionally, this work considers the spatial uncertainty in the hypocentres of aftershocks to assess the aftershock-induced seismic hazards. Fragility curves and residual factors of damaged RC piers are used in the transition probability matrix of Markov Chain model for considering the cumulative damage induced by aftershocks by incorporating uncertainty into aftershock events, as well as into structural capacity and residual factors corresponding to a specified damage state, the exceedance probabilities for various damage states can be estimated using Markov Chain model and Monte Carlo Simulation. Finally, in the case study, the proposed procedure is used to determine the important factor in the preliminary seismic design of typical RC piers for the Chi–Chi Earthquake in Taiwan. © 2017, © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1109/TSIPN.2017.2756563,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051597888&doi=10.1109%2fTSIPN.2017.2756563&partnerID=40&md5=48bdf8563c6b119d8d24d45c0aca1b8e,"Measurements in wireless sensor networks (WSNs) are often correlated both in space and in time. This paper focuses on tracking multiple targets in WSNs by taking into consideration these measurement correlations. A sequential Markov Chain Monte Carlo (SMCMC) approach is proposed in which a Metropolis within Gibbs refinement step and a likelihood gradient proposal are introduced. This SMCMC filter is applied to case studies with cellular network received signal strength data in which the shadowing component correlations in space and time are estimated. The efficiency of the SMCMC approach compared to particle filtering, as well as the gradient proposal compared to a basic prior proposal, are demonstrated through numerical simulations. The accuracy improvement with the gradient-based SMCMC is above 90\% when using a low number of particles. Thanks to its sequential nature, the proposed approach can be applied to various WSN applications, including traffic mobility monitoring and prediction. © 2015 IEEE."
,10.1016/j.matcom.2018.03.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046133836&doi=10.1016%2fj.matcom.2018.03.014&partnerID=40&md5=8bc51f6e77b4f8414d314b04fa37c385,"For improved prediction of subsurface flows and representation of the uncertainties of geostatistical properties, we use the framework of Bayesian statistical interface in combination with the Markov Chain Monte Carlo (MCMC) method which needs many fine-scale simulations. Hence it is essential to apply cheap screening stages, such as coarse-scale simulation to remove irrelevant proposals of the generated Markov chain, reduce fine-scale computational cost and increase the acceptance rate of MCMC. We propose a screening step, that is examination of subsurface characteristics around injection/production wells, aiming at accurate breakthrough capturing as well as above mentioned efficiency goals. However this short time simulation needs fine-scale structure of the geological model around wells and running a fine-scale model is not as cheap as necessary for screening steps. On the other hand applying it on a coarse-scale model declines important data around wells and causes inaccurate results, particularly accurate breakthrough capturing which is important for prediction applications. Therefore we propose a multi-scale grid which preserves the fine-scale model around wells (as well as high permeable regions and fractures) and coarsens rest of the field and keeps efficiency and accuracy for the screening well stage and coarse-scale simulation, as well. A discrete wavelet transform is used as a powerful tool to generate the desired unstructured multi-scale grid efficiently. Finally an accepted proposal on coarse-scale models (screening well stage and coarse-scale simulation) will be assessed by fine-scale simulation. Accepted proposals are saved for prediction. Numerical results admit increment in acceptance rate, improvement in breakthrough capturing and significant reduction in computational cost by avoiding many forward simulations. © 2018 International Association for Mathematics and Computers in Simulation (IMACS)"
,10.1007/s11222-017-9780-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031117409&doi=10.1007%2fs11222-017-9780-4&partnerID=40&md5=102313459d872b362d9d939fa093197d,"Parallelizable Markov chain Monte Carlo (MCMC) generates multiple proposals and parallelizes the evaluations of the likelihood function on different cores at each MCMC iteration. Inspired by Calderhead (Proc Natl Acad Sci 111(49):17408–17413, 2014), we introduce a general ‘waste-recycling’ framework for parallelizable MCMC, under which we show that using weighted samples from waste-recycling is preferable to resampling in terms of both statistical and computational efficiencies. We also provide a simple-to-use criteria, the generalized effective sample size, for evaluating efficiencies of parallelizable MCMC algorithms, which applies to both the waste-recycling and the vanilla versions. A moment estimator of the generalized effective sample size is provided and shown to be reasonably accurate by simulations. © 2017, Springer Science+Business Media, LLC."
,10.1515/mcma-2018-0018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050857221&doi=10.1515%2fmcma-2018-0018&partnerID=40&md5=63592c7eeb036dcc3ae09321c0f435a8,"We consider the problem of sampling from high-dimensional likelihood functions with large amounts of non-identifiabilities via Markov-Chain Monte-Carlo algorithms. Non-identifiabilities are problematic for commonly used proposal densities, leading to a low effective sample size. To address this problem, we introduce a regularization method using an artificial prior, which restricts non-identifiable parts of the likelihood function. This enables us to sample the posterior using common MCMC methods more efficiently. We demonstrate this with three MCMC methods on a likelihood based on a complex, high-dimensional blood coagulation model and a single series of measurements. By using the approximation of the artificial prior for the non-identifiable directions, we obtain a sample quality criterion. Unlike other sample quality criteria, it is valid even for short chain lengths. We use the criterion to compare the following three MCMC variants: The Random Walk Metropolis Hastings, the Adaptive Metropolis Hastings and the Metropolis adjusted Langevin algorithm. © 2018 Walter de Gruyter GmbH, Berlin/Boston."
,10.1037/met0000155,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035000253&doi=10.1037%2fmet0000155&partnerID=40&md5=214e88badc526432225f2a0e0eafd10b,"The STARTS (Stable Trait, AutoRegressive Trait, and State) model decomposes individual differences in psychological measurement across time into 3 sources of variation: a time-invariant stable component, a time-varying autoregressive component, and an occasion-specific state component. Previous simulation research and applications of the STARTS model have shown that serious estimation problems such as nonconvergence or inadmissible estimates (e.g., negative variances) frequently occur for STARTS model parameters. This article introduces a general approach to estimating the parameters of the STARTS model by employing Bayesian methods that use Markov Chain Monte Carlo (MCMC) techniques. With the specification of appropriate prior distributions, the Bayesian approach offers the advantage that the model estimates will be within the admissible range, and it should be possible to avoid estimation problems. Furthermore, we show how Bayesian methods can be used to stabilize STARTS model estimates by specifying weakly informative prior distributions for the model parameters. In a simulation study, the statistical properties (bias, root mean square error, coverage rate) of the parameter estimates obtained from the Bayesian approach are compared with those of the maximum-likelihood approach. A data example is presented to illustrate how the Bayesian approach can be used to estimate the STARTS model. Finally, further extensions of the STARTS model are discussed, and suggestions for applied research are made. © 2017 American Psychological Association."
,10.1007/s00180-017-0759-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027690095&doi=10.1007%2fs00180-017-0759-6&partnerID=40&md5=95fc68974263f87499fdd9cb227df977,"In Bayesian analysis of multidimensional scaling model with MCMC algorithm, we encounter the indeterminacy of rotation, reflection and translation of the parameter matrix of interest. This type of indeterminacy may be seen in other multivariate latent variable models as well. In this paper, we propose to address this indeterminacy problem with a novel, offline post-processing method that is easily implemented using easy-to-use Markov chain Monte Carlo (MCMC) software. Specifically, we propose a post-processing method based on the generalized extended Procrustes analysis to address this problem. The proposed method is compared with four existing methods to deal with indeterminacy thorough analyses of artificial as well as real datasets. The proposed method achieved at least as good a performance as the best existing method. The benefit of the offline processing approach in the era of easy-to-use MCMC software is discussed. © 2017, Springer-Verlag GmbH Germany."
1,10.1007/s11222-017-9778-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030726080&doi=10.1007%2fs11222-017-9778-y&partnerID=40&md5=284bbf624068b87866c2ab1915004d74,"The Integrated Nested Laplace Approximation (INLA) has established itself as a widely used method for approximate inference on Bayesian hierarchical models which can be represented as a latent Gaussian model (LGM). INLA is based on producing an accurate approximation to the posterior marginal distributions of the parameters in the model and some other quantities of interest by using repeated approximations to intermediate distributions and integrals that appear in the computation of the posterior marginals. INLA focuses on models whose latent effects are a Gaussian Markov random field. For this reason, we have explored alternative ways of expanding the number of possible models that can be fitted using the INLA methodology. In this paper, we present a novel approach that combines INLA and Markov chain Monte Carlo (MCMC). The aim is to consider a wider range of models that can be fitted with INLA only when some of the parameters of the model have been fixed. We show how new values of these parameters can be drawn from their posterior by using conditional models fitted with INLA and standard MCMC algorithms, such as Metropolis–Hastings. Hence, this will extend the use of INLA to fit models that can be expressed as a conditional LGM. Also, this new approach can be used to build simpler MCMC samplers for complex models as it allows sampling only on a limited number of parameters in the model. We will demonstrate how our approach can extend the class of models that could benefit from INLA, and how the R-INLA package will ease its implementation. We will go through simple examples of this new approach before we discuss more advanced applications with datasets taken from the relevant literature. In particular, INLA within MCMC will be used to fit models with Laplace priors in a Bayesian Lasso model, imputation of missing covariates in linear models, fitting spatial econometrics models with complex nonlinear terms in the linear predictor and classification of data with mixture models. Furthermore, in some of the examples we could exploit INLA within MCMC to make joint inference on an ensemble of model parameters. © 2017, Springer Science+Business Media, LLC."
,10.1016/j.envsoft.2018.05.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048199531&doi=10.1016%2fj.envsoft.2018.05.021&partnerID=40&md5=13172a6970e8dd0696b52e58bc04e7a9,"Human interventions to optimise river functions are often contentious, disruptive, and expensive. To analyse the expected impact of an intervention before implementation, decision makers rely on computations with complex physics-based hydraulic models. The outcome of these models is known to be sensitive to uncertain input parameters, but long model runtimes render full probabilistic assessment infeasible with standard computer resources. In this paper we propose an alternative, efficient method for uncertainty quantification for impact analysis that significantly reduces the required number of model runs by using a subsample of a full Monte Carlo ensemble to establish a probabilistic relationship between pre- and post-intervention model outcome. The efficiency of the method depends on the number of interventions, the initial Monte Carlo ensemble size and the desired level of accuracy. For the cases presented here, the computational cost was decreased by 65%. © 2018 Elsevier Ltd"
,10.1109/TPWRS.2018.2803044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041508989&doi=10.1109%2fTPWRS.2018.2803044&partnerID=40&md5=937451885679c8766323729454675b09,"The concept of virtual power plant (VPP) has been proposed to manage distributed renewable energy sources as packaging to engage in the energy and reserve planning on contemporary generating portfolios. In this context, an efficient tool is needed to support analysis of generating flexibility of conventional units in combination with VPPs to cooperatively counterbalance the fluctuation of net demand. In this paper, an adaptive importance sampling method is proposed, intentionally for efficiently evaluating specific indices capturing the possibility and severity of rare inadequate spinning reserve events of a deployed unit comment schedule for a generating system incorporating VPPs, in terms of short-term stochastic unit failures and power fluctuation of VPPs. The proposed method is based on the standard cross-entropy (CE) method, which newly introduces a mathematical transformation aiming at diverting evaluations of customized risk indices to a generic rare-event probability estimation problem. A Markov chain Monte Carlo method is employed to train the proposal density, to efficiently gain on the best, owing to avoiding the iterative parameter-updating mechanism of the standard CE method. The efficacy of the proposed method is tested on a modified RTS-96 generating system emulating a portfolio of conventional and renewable generating sources modeled as VPPs. © 1969-2012 IEEE."
,10.1016/j.apm.2018.05.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047524319&doi=10.1016%2fj.apm.2018.05.004&partnerID=40&md5=bb15e1bab0f6b826caeabf3bd0f19d8c,"The present work is associated with Bayesian finite element (FE) model updating using modal measurements based on maximizing the posterior probability instead of any sampling based approach. Such Bayesian updating framework usually employs normal distribution in updating of parameters, although normal distribution has usual statistical issues while using non-negative parameters. These issues are proposed to be dealt with incorporating lognormal distribution for non-negative parameters. Detailed formulations are carried out for model updating, uncertainty-estimation and probabilistic detection of changes/damages of structural parameters using combined normal-lognormal probability distribution in this Bayesian framework. Normal and lognormal distributions are considered for eigen-system equation and structural (mass and stiffness) parameters respectively, while these two distributions are jointly considered for likelihood function. Important advantages in FE model updating (e.g. utilization of incomplete measured modal data, non-requirement of mode-matching) are also retained in this combined normal-lognormal distribution based proposed FE model updating approach. For demonstrating the efficiency of this proposed approach, a two dimensional truss structure is considered with multiple damage cases. Satisfactory performances are observed in model updating and subsequent probabilistic estimations, however level of performances are found to be weakened with increasing levels in damage scenario (as usual). Moreover, performances of this proposed FE model updating approach are compared with the typical normal distribution based updating approach for those damage cases demonstrating quite similar level of performances. The proposed approach also demonstrates better computational efficiency (achieving higher accuracy in lesser computation time) in comparison with two prominent Markov Chain Monte Carlo (MCMC) techniques (viz. Metropolis-Hastings algorithm and Gibbs sampling). © 2018 Elsevier Inc."
,10.1002/wics.1435,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051442339&doi=10.1002%2fwics.1435&partnerID=40&md5=d1a49973fe87692a95cf12071408af5b,"Markov chain Monte Carlo algorithms are used to simulate from complex statistical distributions by way of a local exploration of these distributions. This local feature avoids heavy requests on understanding the nature of the target, but it also potentially induces a lengthy exploration of this target, with a requirement on the number of simulations that grows with the dimension of the problem and with the complexity of the data behind it. Several techniques are available toward accelerating the convergence of these Monte Carlo algorithms, either at the exploration level (as in tempering, Hamiltonian Monte Carlo and partly deterministic methods) or at the exploitation level (with Rao–Blackwellization and scalable methods). This article is categorized under: Statistical and Graphical Methods of Data Analysis > Markov Chain Monte Carlo (MCMC) Algorithms and Computational Methods > Algorithms Statistical and Graphical Methods of Data Analysis > Monte Carlo Methods. © 2018 The Authors. WIREs Computational Statistics published by Wiley Periodicals, Inc."
,10.1214/17-AOAS1132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053352791&doi=10.1214%2f17-AOAS1132&partnerID=40&md5=d3ed3448fb96770ee26008aca06b1cf4,"In this paper we study data on discrete labor market transitions from Austria. In particular, we follow the careers of workers who experience a job displacement due to plant closure and observe—over a period of 40 quarters— whether these workers manage to return to a steady career path. To analyse these discrete-valued panel data, we apply a new method of Bayesian Markov chain clustering analysis based on inhomogeneous first order Markov transition processes with time-varying transition matrices. In addition, a mixture-of-experts approach allows us to model the probability of belonging to a certain cluster as depending on a set of covariates via a multinomial logit model. Our cluster analysis identifies five career patterns after plant closure and reveals that some workers cope quite easily with a job loss whereas others suffer large losses over extended periods of time. © Institute of Mathematical Statistics, 2018."
,10.1016/j.jmbbm.2018.05.037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048805088&doi=10.1016%2fj.jmbbm.2018.05.037&partnerID=40&md5=23eabe1a6ee88f5ecea22e837265de91,"The results of a study comparing model calibration techniques for Ogden's constitutive model that describes the hyperelastic behavior of brain tissue are presented. One and two-term Ogden models are fit to two different sets of stress-strain experimental data for brain tissue using both least squares optimization and Bayesian estimation. For the Bayesian estimation, the joint posterior distribution of the constitutive parameters is calculated by employing Hamiltonian Monte Carlo (HMC) sampling, a type of Markov Chain Monte Carlo method. The HMC method is enriched in this work to intrinsically enforce the Drucker stability criterion by formulating a nonlinear parameter constraint function, which ensures the constitutive model produces physically meaningful results. Through application of the nested sampling technique, 95% confidence bounds on the constitutive model parameters are identified, and these bounds are then propagated through the constitutive model to produce the resultant bounds on the stress-strain response. The behavior of the model calibration procedures and the effect of the characteristics of the experimental data are extensively evaluated. It is demonstrated that increasing model complexity (i.e., adding an additional term in the Ogden model) improves the accuracy of the best-fit set of parameters while also increasing the uncertainty via the widening of the confidence bounds of the calibrated parameters. Despite some similarity between the two data sets, the resulting distributions are noticeably different, highlighting the sensitivity of the calibration procedures to the characteristics of the data. For example, the amount of uncertainty reported on the experimental data plays an essential role in how data points are weighted during the calibration, and this significantly affects how the parameters are calibrated when combining experimental data sets from disparate sources. © 2018"
,10.1016/j.jconhyd.2018.08.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052886023&doi=10.1016%2fj.jconhyd.2018.08.005&partnerID=40&md5=6ee26fd95d3cde70f07fe32182e6d4c8,"Groundwater reactive transport models that consider the coupling of hydraulic and biochemical processes are vital tools for predicting the fate of groundwater contaminants and effective groundwater management. The models involve a large number of parameters whose specification greatly affects the model performance. Thus model parameters calibration is crucial to its successful application. The Bayesian inference framework implemented by Markov chain Monte Carlo (MCMC) sampling provides a comprehensive framework to estimate the model parameters. However, its application is hampered by the large computational requirements caused by repeated evaluations of the model in MCMC sampling. This study develops an adaptive Kriging-based MCMC method to overcome the bottleneck of Bayesian inference by replacing the simulation model with a computationally inexpensive Kriging surrogate model. In the adaptive Kriging-based MCMC method, instead of constructing a globally accurate surrogate of the simulation model, we sequentially build a locally accurate surrogate with an iterative refinement to the high probability regions. The performance of the proposed method is demonstrated using a synthetic groundwater reactive transport model for describing sequential Kinetic degradation of Tetrachloroethene (PCE), whose hydraulic and biochemical parameters are jointly estimated. The results suggest that the adaptive Kriging-based MCMC method is able to achieve an accurate Bayesian inference with a hundredfold reduction in the computational cost compared to the conventional MCMC method. © 2018"
,10.3390/a11090142,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053777536&doi=10.3390%2fa11090142&partnerID=40&md5=338fa293ae7402f733aca6bdd83f5685,"The satisfiability modulo theories (SMT) problem is to decide the satisfiability of a logical formula with respect to a given background theory. This work studies the counting version of SMT with respect to linear integer arithmetic (LIA), termed SMT(LIA). Specifically, the purpose of this paper is to count the number of solutions (volume) of a SMT(LIA) formula, which has many important applications and is computationally hard. To solve the counting problem, an approximate method that employs a recent Markov Chain Monte Carlo (MCMC) sampling strategy called ""flat histogram"" is proposed. Furthermore, two refinement strategies are proposed for the sampling process and result in two algorithms, MCMC-Flat1/2 and MCMC-Flat1/t, respectively. In MCMC-Flat1/t, a pseudo sampling strategy is introduced to evaluate the flatness of histograms. Experimental results show that our MCMC-Flat1/t method can achieve good accuracy on both structured and random instances, and our MCMC-Flat1/2 is scalable for instances of convex bodies with up to 7 variables. © 2018 by the authors."
,10.1016/j.matcom.2018.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046151927&doi=10.1016%2fj.matcom.2018.04.001&partnerID=40&md5=ab9a6c1afe18edc62724c9bb1baf103f,"A mathematical model of heat–moisture transfer within textiles and a corresponding inverse problem of textile material design (IPTMD) are reformulated. A stability theorem for the forward problem is given to show wellposedness of the heat–moisture transfer model. A Bayesian inference approach is presented to solve the IPTMD based on thermal comfort of clothing. The triple parameters (thickness, thermal conductivity, porosity of textiles) are simultaneously determined in the sense of the statistical point estimation by the likelihood function. The Bayesian techniques based on Markov chain Monte Carlo (MCMC) methods are employed to simultaneously determine three parameters in IPTMD, where the Metropolis–Hastings algorithm is applied in the inversion process. The interpolated likelihood function reduces significantly the computational cost associated with the implementation of MCMC method without loss of accuracy in the parameters estimation. Numerical experiments confirm that Bayesian inference method can provide more accurate solutions to the IPTMD. © 2018 International Association for Mathematics and Computers in Simulation (IMACS)"
,10.1016/j.astropartphys.2018.04.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045421562&doi=10.1016%2fj.astropartphys.2018.04.002&partnerID=40&md5=10a8806f60bf133292e82f43233e5536,"We present a flavor and energy inference analysis for each down-going high-energy astrophysical neutrino event observed by the IceCube observatory during six years of data taking. Our goal is to obtain, for the first time, an estimate of the posterior probability distribution for the most relevant properties, such as the neutrino energy and flavor, of the neutrino-nucleon interactions producing shower and track events in the IceCube detector. For each event the main observables in the IceCube detector are the deposited energy and the event topology (showers or tracks) produced by the Cherenkov light by the transit through a medium of charged particles created in neutrino interactions. It is crucial to reconstruct from these observables the properties of the neutrino which generated such event. Here we describe how to achieve this goal using Bayesian inference and Markov chain Monte Carlo methods. © 2018 Elsevier B.V."
,10.1007/s12205-017-1727-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039839142&doi=10.1007%2fs12205-017-1727-y&partnerID=40&md5=f584cb47cbabd5148377514fcd4400af,"Traffic monitoring, and particularly maximum vehicle load, is a very important for predicting the remaining service time of either long span or short-to-medium span bridges. Using weigh in motion (WIM) data from the Nanxi Yangtze river bridge, a novel maximum load estimation model of vehicle load was constructed. The novel model is based on the extended Burr XII (EBurr) distribution, which includes the Weibull, generalized Pareto (GPD) and log-logistic distributions. Thus, the traditional GPD model is a special form of the proposed novel model. The correlation of vehicle load is extracted using a peak over threshold method, and a Markov chain Monte Carlo Bayesian method is applied to estimate the parameters. The proposed novel model is compared with other traditional models. The 95th percentile of the load distribution is considered as the evaluation point for overloaded trucks. In addition, vehicle loads collected on highway station are used to verify the novel model’s applicability. The results show: The EBurr distribution is more suitable to capture sparse extreme points than other traditional distributions according to the value of SSE closely to 0 and R2 closely to 1. When the assessment reference period T changes from 100 to 30 years, the deceased ration of the evaluation load weight is 15.17% of EBurr and 10% of GPD of the Nanxi Yangtze river bridge, where it is 12.17% of EBurr and 0.84% of GPD of the bridge near La linhe highway station. The deceased ration of the evaluation load weight using EBurr is larger than that using GPD. Moreover, the deceased ration of the evaluation load weight using GPD in La linhe highway station has a little change. Hence, using EBurr distribution to model the evaluation load is more correspond to fact. © 2018, Korean Society of Civil Engineers."
,10.1109/TCSVT.2017.2727963,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028875742&doi=10.1109%2fTCSVT.2017.2727963&partnerID=40&md5=74b2bb0c2ff08ae37e74816e79a6ad1a,"For semantic analysis of activities and events in videos, it is important to capture the spatio-temporal relation among objects in the 3D space. In this paper, we present a probabilistic method that extracts 3D trajectories of objects from 2D videos, captured from a monocular moving camera. Compared with existing methods that rely on restrictive assumptions, we propose a method that can extract 3D trajectories with much less restriction by adopting new example-based techniques, which compensate the lack of information. Here, we estimate the focal length of the camera based on similar candidates, and use it to compute depths of detected objects. Contrary to other 3D trajectory extraction methods, our method is able to process videos taken from a stable camera as well as a non-calibrated moving camera without restrictions. For this, we modify Reversible Jump Markov Chain Monte Carlo particle filtering to be more suitable for camera odometry without relying on geometrical feature points. Moreover, our method decreases time consumption by reducing the number of object detections with keypoint matching. Finally, we evaluate our method on known data sets by showing the robustness of our system and demonstrating its efficiency in dealing with different kind of videos. © 1991-2012 IEEE."
,10.1175/MWR-D-17-0366.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053609370&doi=10.1175%2fMWR-D-17-0366.1&partnerID=40&md5=68afbf48fb9ec0d0cba64c04e8c76673,"A probabilistic forecasting method to predict thunderstorms in the European eastern Alps is developed. A statistical model links lightning occurrence from the ground-based Austrian Lightning Detection and Information System (ALDIS) detection network to a large set of direct and derived variables from a numerical weather prediction (NWP) system. The NWP system is the high-resolution run (HRES) of the European Centre for Medium-Range Weather Forecasts (ECMWF) with a grid spacing of 16 km. The statistical model is a generalized additive model (GAM) framework, which is estimated by Markov chain Monte Carlo (MCMC) simulation. Gradient boosting with stability selection serves as a tool for selecting a stable set of potentially nonlinear terms. Three grids from 64 × 64 to 16 × 16 km2 and five forecast horizons from 5 days to 1 day ahead are investigated to predict thunderstorms during afternoons (1200-1800 UTC). Frequently selected covariates for the nonlinear terms are variants of convective precipitation, convective potential available energy, relative humidity, and temperature in the midlayers of the troposphere, among others. All models, even for a lead time of 5 days, outperform a forecast based on climatology in an out-of-sample comparison. An example case illustrates that coarse spatial patterns are already successfully forecast 5 days ahead. © 2018 American Meteorological Society."
,10.3390/rs10091400,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053633426&doi=10.3390%2frs10091400&partnerID=40&md5=2b8d13518db0cb9d9b972736d9599953,"Seismogenic fault geometry, especially for a blind fault, is usually difficult to derive, based only on the distribution of aftershocks and interference fringes of Interferometric Synthetic Aperture Radar (InSAR). To better constrain the fault geometry of the 2017 Jiuzhaigou Mw 6.5 earthquake, we first carried out a nonlinear inversion for a single fault source using multi-peak particle swarm optimization (MPSO), Monte Carlo (MC), and Markov Chain Monte Carlo (MCMC) algorithms, respectively, with constraints of InSAR data in multiple SAR viewing geometries. The fault geometry models retrieved with different methods were highly consistent and mutually verifiable, showing that a blind faulting with a strike of ~154° and a dip angle of ~77° was responsible for the Jiuzhaigou earthquake. Based on the optimal fault geometry model, the fault slip distribution jointly inverted from the InSAR and Global Positioning System (GPS) data by the steepest descent method (SDM) and the MC method showed that the slip was mainly concentrated at the depth of 1-15 km, and only one slip center appeared at the depth of 5-9 km with a maximum slip of about 1.06 m, some different from previous studies. Taking the shear modulus of μ = 32 GPa, the seismic moment derived from the distributed slip model was about 7.85 × 1018 Nm, equivalent to Mw 6.54, which was slightly larger than that from the focal mechanism solutions. The fault spatial geometry and slip distribution could be further validated with the spatial patterns of the immediate aftershocks. Most of the off-fault aftershocks with the magnitude &gt; M2 within one year after the mainshock occurred in the stress positive stress change area, which coincided with the stress triggering theory. The static Coulomb stress, triggered by the mainshock, significantly increased at the Tazang fault (northwest to the epicenter), and at the hidden North Huya fault, and partial segments of the Minjiang fault (west of the epicenter). © 2018 by the authors."
,10.1177/0265813516688688,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041550024&doi=10.1177%2f0265813516688688&partnerID=40&md5=8a7c425f43cd4c4bbd78f3dbb1955f58,"This paper discusses a project on the completion of a database of socio-economic indicators across the European Union for the years from 1990 onward at various spatial scales. Thus the database consists of various time series with a spatial component. As a substantial amount of the data was missing a method of imputation was required to complete the database. A Markov Chain Monte Carlo approach was opted for. We describe the Markov Chain Monte Carlo method in detail. Furthermore, we explain how we achieved spatial coherence between different time series and their observed and estimated data points. © The Author(s) 2017."
,10.1016/j.buildenv.2018.06.045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048999694&doi=10.1016%2fj.buildenv.2018.06.045&partnerID=40&md5=8b54cb03bfb1013d55a03cdcbeaac577,"A concept known as ‘nudge’ has recently received attention in many application domains. It implies influencing the behavior and decision-making of individuals by making indirect suggestions through the presentation of adequate information. We apply such a perspective to improve the value of a space. It can be measured by the number of visitors, and the predicted thermal sensation is considered as information offered to potential visitors. In the present study, we explain how to generate the information required for a successful nudge. This information must be specifically tailored towards personalized characteristics, rather than a one-fits-all approach. This study presents a new data-driven method for predicting individuals' thermal sensation by formulating the effect of both measured (thermal) and non-measured factors on thermal sensation votes. The proposed model is explicitly encoded based on a major premise that “different individuals have different thermal sensation characteristics; however, all individuals also have a common trend.” The inference model uses a Bayesian approach, and is hierarchically structured to represent dependencies across model parameters of the personalized characteristics of individual-level and the typical trend of group-level thermal sensations. The Markov chain Monte Carlo approach is used to approximate the posterior distribution and draw inferences on the model parameters. The results, based on data collected from outdoor spaces, show that the proposed model provides accurate predictions for personalized thermal sensation and improves the efficiency of parameter estimates. Our approach provides fresh insight into statistical models for predicting thermal sensation. © 2018 The Authors"
1,10.1007/s10687-018-0330-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048681260&doi=10.1007%2fs10687-018-0330-z&partnerID=40&md5=9a7674004e86f61799697d6fb6717cae,"This paper concerns our approach to the EVA2017 challenge, the aim of which was to predict extreme precipitation quantiles across several sites in the Netherlands. Our approach uses a Bayesian hierarchical structure, which combines Gamma and generalised Pareto distributions. We impose a spatio-temporal structure in the model parameters via an autoregressive prior. Estimates are obtained using Markov chain Monte Carlo techniques and spatial interpolation. This approach has been successful in the context of the challenge, providing reasonable improvements over the benchmark. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
,10.1111/anzs.12241,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052814743&doi=10.1111%2fanzs.12241&partnerID=40&md5=201df678a4f111dea75a5aefe500ffe3,"We demonstrate the use of our R package, gammSlice, for Bayesian fitting and inference in generalised additive mixed model analysis. This class of models includes generalised linear mixed models and generalised additive models as special cases. Accurate Bayesian inference is achievable via sufficiently large Markov chain Monte Carlo (MCMC) samples. Slice sampling is a key component of the MCMC scheme. Comparisons with existing generalised additive mixed model software shows that gammSlice offers improved inferential accuracy, albeit at the cost of longer computational time. © 2018 Australian Statistical Publishing Association Inc. Published by John Wiley & Sons Australia Pty Ltd."
,10.1111/rssb.12269,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042587941&doi=10.1111%2frssb.12269&partnerID=40&md5=c0da2873f775845b886003c59560803a,"We introduce a new family of Markov chain Monte Carlo samplers that combine auxiliary variables, Gibbs sampling and Taylor expansions of the target density. Our approach permits the marginalization over the auxiliary variables, yielding marginal samplers, or the augmentation of the auxiliary variables, yielding auxiliary samplers. The well-known Metropolis-adjusted Langevin algorithm MALA and preconditioned Crank–Nicolson–Langevin algorithm pCNL are shown to be special cases. We prove that marginal samplers are superior in terms of asymptotic variance and demonstrate cases where they are slower in computing time compared with auxiliary samplers. In the context of latent Gaussian models we propose new auxiliary and marginal samplers whose implementation requires a single tuning parameter, which can be found automatically during the transient phase. Extensive experimentation shows that the increase in efficiency (measured as the effective sample size per unit of computing time) relative to (optimized implementations of) pCNL, elliptical slice sampling and MALA ranges from tenfold in binary classification problems to 25 fold in log-Gaussian Cox processes to 100 fold in Gaussian process regression, and it is on a par with Riemann manifold Hamiltonian Monte Carlo sampling in an example where that algorithm has the same complexity as the aforementioned algorithms. We explain this remarkable improvement in terms of the way that alternative samplers try to approximate the eigenvalues of the target. We introduce a novel Markov chain Monte Carlo sampling scheme for hyperparameter learning that builds on the auxiliary samplers. The MATLAB code for reproducing the experiments in the paper is publicly available and an on-line supplement to this paper contains additional experiments and implementation details. © 2018 Royal Statistical Society"
,10.1007/s40995-016-0124-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051107074&doi=10.1007%2fs40995-016-0124-6&partnerID=40&md5=d2168fa3ec86bb15b0ad96347013ac4b,"The study deals with the analysis of Type-II hybrid censored data from the modified Weibull distribution. We provide maximum-likelihood estimates of the parameters, reliability, and hazard rate functions along with their standard errors. The confidence intervals along with their widths have also been obtained. Assuming gamma and Jeffrey’s invariant priors for the unknown parameters, Bayes estimates along with its posterior errors and highest posterior density credible intervals are obtained. The Markov Chain Monte Carlo technique has been used to simulate draws from the complicated posterior densities of the parameters. A simulation study is conducted to compare the performances of classical and Bayesian methods of estimation. Finally, a real data analysis is performed for illustrative purpose. © 2016, Shiraz University."
,10.1111/1365-2435.13180,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050389546&doi=10.1111%2f1365-2435.13180&partnerID=40&md5=d46e175d1a7a6c430cd799432eeab565,"Characterising the spatiotemporal variation of animal behaviour can elucidate the way individuals interact with their environment and allocate energy. Increasing sophistication of tracking technologies paired with novel analytical approaches allows the characterisation of movement dynamics even when an individual is not directly observable. In this study, high-resolution movement data collected via global positioning system (GPS) tracking in three dimensions were paired with topographical information and used in a Bayesian state-space model to describe the flight modes of migrating golden eagles (Aquila chrysaetos) in eastern North America. Our model identified five functional behavioural states, two of which were previously undescribed variations on thermal soaring. The other states comprised gliding, perching and orographic soaring. States were discriminated by movement features in the horizontal (step length and turning angle) and vertical (change in altitude) planes and by the association with ridgelines promoting wind deflection. Tracked eagles spent 2%, 31%, 38%, 9% and 20% of their daytime in directed thermal soaring, gliding, convoluted thermal soaring, perching and orographic soaring, respectively. The analysis of the relative occurrence of these flight modes highlighted yearly, seasonal, age, individual and sex differences in flight strategy and performance. Particularly, less energy-efficient orographic soaring was more frequent in autumn, when thermals were less available. Adult birds were also better at optimising energy efficiency than subadults. Our approach represents the first example of a state-space model for bird flight mode using altitude data in conjunction with horizontal locations and is applicable to other flying organisms where similar data are available. The ability to describe animal movements in a three-dimensional habitat is critical to advance our understanding of the functional processes driving animals’ decisions. A plain language summary is available for this article. © 2018 The Authors. Functional Ecology © 2018 British Ecological Society"
1,10.1214/17-AOAS1129,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053334214&doi=10.1214%2f17-AOAS1129&partnerID=40&md5=1509a700574bcdd4675258971fbceb8f,"Gene expression is largely controlled by transcription factors (TFs) in a collaborative manner. Therefore, an understanding of TF collaboration is crucial for the elucidation of gene regulation. The co-activation of TFs can be represented by networks. These networks are dynamic in diverse biological conditions and heterogeneous across the genome within each biological condition. Existing methods for construction of TF networks lack solid statistical models, analyze each biological condition separately, and enforce a single network for all genomic locations within one biological condition, resulting in low statistical power and misleading spurious associations. In this paper, we present a novel Bayesian nonparametric dynamic Poisson graphical model for inference on TF networks. Our approach automatically teases out genome heterogeneity and borrows information across conditions to improve signal detection from very few replicates, thus offering a valid and efficient measure of TF co-activations. We develop an efficient parallel Markov chain Monte Carlo algorithm for posterior computation. The proposed approach is applied to study TF associations in ENCODE cell lines and provides novel findings. © Institute of Mathematical Statistics, 2018."
,10.7845/kjm.2018.8044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054685305&doi=10.7845%2fkjm.2018.8044&partnerID=40&md5=8e98fa94fd084e65758130dd77ed7b0e,"Avian influenza recently damaged the poultry industry, which suffered a huge economic loss reaching billions of U.S. dollars in South Korea. Transmission routes of the pathogens would help plan to control and limit the spread of the devastating biological tragedy. Phylogenetic analyses of pathogen's DNA sequences could sketch transmission trees relating hosts with directed edges. The last decade has seen the methodological development of inferring transmission trees using epidemiological as well as genetic data. Here, I reanalyzed the DNA sequence data that had originated in the highly pathogenic avian influenza H5N8 outbreak of South Korea in 2014. The H5N8 viruses spread geographically contiguously from the origin of the outbreak, Jeonbuk. The Jeonbuk origin viruses were known to spread to four provinces neighboring Jeonbuk. I estimated the transmission tree of the host domestic and migratory wild birds after combining multiple runs of Markov chain Monte Carlo using a Bayesian method for inferring transmission trees. The estimated transmission tree, albeit with a rather large uncertainty in the directed edges, showed that the viruses spread from Jeonbuk through Chungnam to Gyeonggi. Domestic birds of breeder or broiler ducks were estimated to appear to be at the terminal nodes of the transmission tree. This observation confirmed that migratory wild birds played an important role as one of the main infection mediators in the avian influenza H5N8 outbreak of South Korea in 2014. © 2018, The Microbiological Society of Korea.최근 양계업에 막대한 피해를 끼치는 조류독감은 한국에서 수천억원의 거대한 경제적 손실을 초래하였다. 병원균의 전염 경로를 파악할 수 있다면 막대한 손해를 끼치는 생물학적 피 해의 확산을 막고 일부 지역으로 제한하는데 큰 도움이 될 것 이다. 병원균 DNA 서열의 계통학적인 분석을 통하여 감염된 숙주들을 방향성이 있는 연결선으로 연관짓는 전염 계통수를 얻을 수 있다. 지난 10여년간 유전적 데이터뿐만 아니라 역학 데이터를 이용한 전염 계통수 추론의 방법론적 발전이 이루어 졌다. 이에, 본 연구에서는 전염 계통수 추론 방법을 이용하여 지난 2014년 한국에 발병한 고병원성 조류독감 H5N8에서 유 래한 DNA 서열을 재분석하였다. 당시, H5N8 바이러스는 전 라북도에서 시작하여 지역적으로 접해있는 4개의 지역으로 확산되어 나갔던 것으로 알려져 있다. 전염 계통수를 추론하 는 베이지언 통계 방법인 Markov chain Monte Carlo를 반복적 으로 시행하고 이를 종합하여 철새 외래종과 국내종 조류 숙 주들의 전염 계통수를 추정하였다. 비록 연결선의 불확실성은 높았으나 추정된 전염 계통수를 통하여 당시 H5N8 바이러스 는 전라북도에서 시작하고 충청남도를 거쳐 경기도로 퍼져나 간 것을 확인할 수 있었다. 사육하는 오리와 같은 국내종 조류 는 전염 계통수의 말단 노드에 위치하는 것으로 추정되었다. 이러한 결과를 통하여 야생 철새종이 2014년 한국의 H5N8 조 류독감의 감염 매개자로 주된 역할을 하였다는 것을 재확인하 였다."
,10.1002/gepi.22133,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052931521&doi=10.1002%2fgepi.22133&partnerID=40&md5=cbc001b95470abdd1d9f7dfaaf660234,"Multipoint linkage analysis is an important approach for localizing disease-associated loci in pedigrees. Linkage analysis, however, is sensitive to misspecification of marker allele frequencies. Pedigrees from recently admixed populations are particularly susceptible to this problem because of the challenge of accurately accounting for population structure. Therefore, increasing emphasis on use of multiethnic samples in genetic studies requires reevaluation of best practices, given data currently available. Typical strategies have been to compute allele frequencies from the sample, or to use marker allele frequencies determined by admixture proportions averaged over the entire sample. However, admixture proportions vary among pedigrees and throughout the genome in a family-specific manner. Here, we evaluate several approaches to model admixture in linkage analysis, providing different levels of detail about ancestral origin. To perform our evaluations, for specification of marker allele frequencies, we used data on 67 Caribbean Hispanic admixed families from the Alzheimer's Disease Sequencing Project. Our results show that choice of admixture model has an effect on the linkage analysis results. Variant-specific admixture proportions, computed for individual families, provide the most detailed regional admixture estimates, and, as such, are the most appropriate allele frequencies for linkage analysis. This likely decreases the number of false-positive results, and is straightforward to implement. © 2018 WILEY PERIODICALS, INC."
1,10.1111/sjos.12312,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042183382&doi=10.1111%2fsjos.12312&partnerID=40&md5=581b0aed6fdab8d5569f79d6a5e17c98,"Bayesian hierarchical formulations are utilized by the U.S. Bureau of Labor Statistics (BLS) with respondent-level data for missing item imputation because these formulations are readily parameterized to capture correlation structures. BLS collects survey data under informative sampling designs that assign probabilities of inclusion to be correlated with the response on which sampling-weighted pseudo posterior distributions are estimated for asymptotically unbiased inference about population model parameters. Computation is expensive and does not support BLS production schedules. We propose a new method to scale the computation that divides the data into smaller subsets, estimates a sampling-weighted pseudo posterior distribution, in parallel, for every subset and combines the pseudo posterior parameter samples from all the subsets through their mean in the Wasserstein space of order 2. We construct conditions on a class of sampling designs where posterior consistency of the proposed method is achieved. We demonstrate on both synthetic data and in application to the Current Employment Statistics survey that our method produces results of similar accuracy as the usual approach while offering substantially faster computation. Published 2018. This article is a U.S. Government work and is in the public domain in the USA."
1,10.1214/18-AOAS1139,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053347780&doi=10.1214%2f18-AOAS1139&partnerID=40&md5=c9842d19bb57d41e9a69f81477efeab3,"The randomized response technique (RRT) is a classical and effective method used to mitigate the distortion arising from dishonest answers. The traditional RRT usually focuses on the case of a single sensitive attribute, and discussion of the case of multiple sensitive attributes is limited. Here, we study a business case to identify some individual and organizational determinants driving information systems (IS) resource misuse in the workplace. People who actually engage in IS resource misuse are probably not willing to provide honest answers, given the sensitivity of the topic. Yet, to develop the causal relationship between IS resource misuse and its determinants, a version of the RRT for multivariate analysis is required. To implement the RRT with multiple sensitive attributes, we propose a Bayesian approach for estimating covariance matrices with incomplete information (re-sulting from the randomization procedure in the RRT case). The proposed approach (i) accommodates the positive definite condition and other intrinsic parameter constraints in the posterior to improve statistical precision, (ii) incorporates Bayesian shrinkage estimation for covariance matrices despite incomplete information, and (iii) adopts a quasi-likelihood method to achieve Bayesian semiparametric inference for enhancing flexibility. We show the effectiveness of the proposed method in a simulation study. We also apply the Bayesian RRT method and structural equation modeling to identify the causal relationship between IS resource misuse and its determinants. © Institute of Mathematical Statistics, 2018."
,10.1007/s11336-017-9594-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034215692&doi=10.1007%2fs11336-017-9594-5&partnerID=40&md5=3172c076cf16db538d58f720d82e69ba,"Piecewise growth mixture models are a flexible and useful class of methods for analyzing segmented trends in individual growth trajectory over time, where the individuals come from a mixture of two or more latent classes. These models allow each segment of the overall developmental process within each class to have a different functional form; examples include two linear phases of growth, or a quadratic phase followed by a linear phase. The changepoint (knot) is the time of transition from one developmental phase (segment) to another. Inferring the location of the changepoint(s) is often of practical interest, along with inference for other model parameters. A random changepoint allows for individual differences in the transition time within each class. The primary objectives of our study are as follows: (1) to develop a PGMM using a Bayesian inference approach that allows the estimation of multiple random changepoints within each class; (2) to develop a procedure to empirically detect the number of random changepoints within each class; and (3) to empirically investigate the bias and precision of the estimation of the model parameters, including the random changepoints, via a simulation study. We have developed the user-friendly package BayesianPGMM for R to facilitate the adoption of this methodology in practice, which is available at https://github.com/lockEF/BayesianPGMM. We describe an application to mouse-tracking data for a visual recognition task. © 2017, The Psychometric Society."
1,10.1214/17-AOAS1123,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053344866&doi=10.1214%2f17-AOAS1123&partnerID=40&md5=d0812ea52c9feba86e50f92db10f4162,"Tumors are heterogeneous. A tumor sample usually consists of a set of subclones with distinct transcriptional profiles and potentially different degrees of aggressiveness and responses to drugs. Understanding tumor heterogeneity is therefore critical for precise cancer prognosis and treatment. In this paper we introduce BayCount—a Bayesian decomposition method to infer tumor heterogeneity with highly over-dispersed RNA sequencing count data. Using negative binomial factor analysis, BayCount takes into account both the between-sample and gene-specific random effects on raw counts of sequencing reads mapped to each gene. For the posterior inference, we develop an efficient compound Poisson-based blocked Gibbs sampler. Simulation studies show that BayCount is able to accurately estimate the subclonal inference, including the number of subclones, the proportions of these subclones in each tumor sample, and the gene expression profiles in each subclone. For real world data examples, we apply BayCount to The Cancer Genome Atlas lung cancer and kidney cancer RNA sequencing count data and obtain biologically interpretable results. Our method represents the first effort in characterizing tumor heterogeneity using RNA sequencing count data that simultaneously removes the need of normalizing the counts, achieves statistical robustness, and obtains biologically/clinically meaningful insights. The R package BayCount implementing our model and algorithm is available for download. © Institute of Mathematical Statistics, 2018."
,10.1016/j.insmatheco.2018.06.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049729846&doi=10.1016%2fj.insmatheco.2018.06.010&partnerID=40&md5=e28d9c4784fffaa6e65a9a6005921975,"The presence of systematic risk in mortality forecasts, known as longevity risk, has called for the introduction of longevity instruments and their market development. Management of longevity risk has been an ongoing issue for insurance companies and pension funds who offer products with payout depending on the lifetime of policyholders. One of the major difficulties in pricing longevity instruments is the determination of a longevity risk-premium. This problem arises from the fact that the longevity market is illiquid and is considered to be incomplete. In this paper we provide an insight to the study of several pricing approaches for longevity instruments that have been proposed in the literature. To account for parameter uncertainty in mortality forecasts and longevity instruments pricing, our analysis hinges on a Bayesian state-space mortality model. The sampling-based Bayesian approach allows us to obtain a distribution of the longevity risk-premium, thus providing an alternative perspective in analyzing the pricing methods. We also discussed the advantages and disadvantages of the considered pricing approaches. © 2018 Elsevier B.V."
,10.1016/j.jneumeth.2018.04.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049610929&doi=10.1016%2fj.jneumeth.2018.04.006&partnerID=40&md5=6b8bb25efbc4e042f91d71cfd96a3ee0,"Background: The study of learning in populations of subjects can provide insights into the changes that occur in the brain with aging, drug intervention, and psychiatric disease. New method: We introduce a separable two-dimensional (2D) random field (RF) model for analyzing binary response data acquired during the learning of object-reward associations across multiple days. The method can quantify the variability of performance within a day and across days, and can capture abrupt changes in learning. Results: We apply the method to data from young and aged macaque monkeys performing a reversal-learning task. The method provides an estimate of performance within a day for each age group, and a learning rate across days for each monkey. We find that, as a group, the older monkeys require more trials to learn the object discriminations than do the young monkeys, and that the cognitive flexibility of the younger group is higher. We also use the model estimates of performance as features for clustering the monkeys into two groups. The clustering results in two groups that, for the most part, coincide with those formed by the age groups. Simulation studies suggest that clustering captures inter-individual differences in performance levels. Comparison with existing method(s): In comparison with generalized linear models, this method is better able to capture the inherent two-dimensional nature of the data and find between group differences. Conclusions: Applied to binary response data from groups of individuals performing multi-day behavioral experiments, the model discriminates between-group differences and identifies subgroups. © 2018 The Authors"
,10.1016/j.resconrec.2018.04.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046996755&doi=10.1016%2fj.resconrec.2018.04.026&partnerID=40&md5=9a91a607fce78ef49303e067371cac75,"Apart from the technical improvements, inducing the green citizen behavior is also a key way to reduce carbon emissions. Various studies have tried to identify the determinants of green behavior. Hitherto, there lacks a quantitative analysis directed to the effects of some factor on the outbreak of green behavior and the final adopted fraction. To fill this gap, this paper propose a Heterogeneous Green Behavior Spreading (HGBS) model to explore the impacts of negative information diffusion (about the green behavior) that effects on the spreading of green behavior. Simulations are performed on top of the two-layer multiplex networks, in which individuals are involved in two processes, the information diffusion in information layer and the green behavior spreading in physical contact layer. Based on the Microscopic Markov Chain Approach (MMCA) and the Monte Carlo (MC) simulations, we find the slight impact of information layer would make the green behavior harder to break out and reduce the adopted fraction. Moreover, the diversity of information diffusion ways makes it worse. It suggests that the control of negative information diffusion would be helpful in contributing to low-carbon city. Another effective way is to encourage individuals having more neighbours in real world to behave pro-environmentally since the adopted fraction is increased for small degree of the individuals in physical contact layer. It is essential to consider the heterogeneity in spreading activity if one wants to model the green behavior spreading. © 2018 Elsevier B.V."
,10.1002/wics.1438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051429298&doi=10.1002%2fwics.1438&partnerID=40&md5=43f55b175c1c968d9bac426ca857b47c,"In this paper we present an overview of the state of the art in Kalman filtering and dynamic Bayesian linear and nonlinear models. We present some of the basic results including the derivation of Kalman filtering equations as well as recent advances in Kalman filter models and their extensions including non-Gaussian state-space models. In so doing, we take a Bayesian perspective and discuss parameter learning in state-space models which typically involves Markov chain Monte Carlo and sequential Monte Carlo methods. We present particle filtering and Bayesian particle learning techniques for state space models and discuss recent advances. This article is categorized under: Applications of Computational Statistics > Signal and Image Processing and Coding Statistical Models > Bayesian Models Statistical Models > Time Series Models Statistical and Graphical Methods of Data Analysis > Markov Chain Monte Carlo (MCMC). © 2018 Wiley Periodicals, Inc."
,10.1061/JPEODX.0000055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047721312&doi=10.1061%2fJPEODX.0000055&partnerID=40&md5=0867bad67019cb7351766f08ad61e67b,"The study applied the Markov chain (MC) model that uses a transition matrix to transmit the probability of monitored pavement markings being in one service life state then changing into another service life state over a time interval. The service life prediction by MC models were then compared with those from linear models, testing if there were any clear advantages of using one model over the other in terms of predicting longevity of the marking retroreflectivity. The retroreflectivity data were collected by monitoring the coefficient of dry retroreflective luminance for 2 years using a handheld retroreflectometer. Using the MC model, the study found that the pavement marking retroreflectivity (PMR) degradation follows an exponential curve trend whereby the degradation rates decrease as the time increases. Significant differences were found in the deterioration of the markings based on the colors (white or yellow) and line type (center, lane line, or edge line). White thermoplastic edge lines on two-lane roadways were found to have a better performance (low deterioration rates) compared with the same lines on four-lane highways. Based on the transition probability matrix (TPM), it was observed that retroreflectivity is in an excellent or good state for a short period of time (54% probability) but is in a fair or poor state for a longer time (92%probability), suggesting the trend has a higher degradation rate at the beginning and a lower rate near the failure state. Keeping the minimum failure states at 150 and 100 mcd=m2=lx for white and yellow markings, respectively, the service life of white markings was found to be approximately 4 years (49.5 months) and it was found to be about 2.4 years (29 months) for yellow markings. The MC model findings were compared with those obtained through linear regression, which showed that white thermoplastic pavement markings take approximately 3.5 years (42 months) to deteriorate to failure state level, while yellow thermoplastics take about 2.1 years (25 months). The study concluded that there is a clear difference between the prediction using MC models compared with linear models, withMCmodels being more cost effective in terms of maintenance and replacement scheduling due to a longer life prediction. © 2018 American Society of Civil Engineers."
1,10.1785/0220180074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052601419&doi=10.1785%2f0220180074&partnerID=40&md5=55240001ef9b49d039d24d3575072e9e,"The Albuquerque-Belen and Socorro basins reside in the central Rio Grande rift and partially overlie the midcrustal Socorro magma body (SMB), the inflation of which contributes to ongoing seismicity and localized uplift of the region. Rayleigh-wave dispersion measurements extracted from ambient noise cross correlation of seismic data collected from the ∼800 station Sevilleta seismic array in the southern Albuquerque and northern Socorro basins were inverted to obtain Rayleigh-wave phasevelocity maps at 3-7 s. Tomography results indicate large (±6%) lateral variations in Rayleigh-wave phase velocities at this period range. Rayleigh-wave velocities were inverted utilizing a nonlinear Monte Carlo Markov chain method to obtain a 3D S-wave velocity in the uppermost 10 km of the crust. At shallow depths (< 5 km), S-wave velocity models show lowvelocity (2:90-3:10 km=s) anomalies in the Rio Grande rift axis, mainly due to Cenozoic rift sediments, and relatively higher (3:20-3:40 km=s) velocities in the local mountain ranges could be due to older or volcanic rocks. At 8-10 kmdepth, this relationship is inverted, and higher S-wave velocities (3:7-3:85 km=s) are generally more present beneath the rift axis than beneath the surrounding ranges. We suggest that the elevated S-wave velocities (3:7-3:85 km=s) could be related to a large (30 km by 12 km) granitic pluton found below the sediment cover in Consortium for Continental Reflection Profiling (COCORP) reflection profiles in the Abo Pass survey (line 2A). The low S-wave velocities could be related to structural changes in moderate- to-high-grade metamorphism of midcrustal rocks and upper-crustal extension from ongoing uplift of the northern part of midcrustal SMB. Copyright © 2018 Geo Science World."
,10.1016/j.bspc.2018.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049988398&doi=10.1016%2fj.bspc.2018.07.001&partnerID=40&md5=393e2a704ddd6701cf8d9cb2221f54cc,"We present a quantitative study of the performance of two automatic methods for the early detection of ovarian cancer that can exploit longitudinal measurements of multiple biomarkers. The study is carried out for a subset of the data collected in the UK Collaborative Trial of Ovarian Cancer Screening (UKCTOCS). We use statistical analysis techniques, such as the area under the Receiver Operating Characteristic (ROC) curve, for evaluating the performance of two techniques that aim at the classification of subjects as either healthy or suffering from the disease using time-series of multiple biomarkers as inputs. The first method relies on a Bayesian hierarchical model that establishes connections within a set of clinically interpretable parameters. The second technique is a purely discriminative method that employs a recurrent neural network (RNN) for the binary classification of the inputs. For the available dataset, the performance of the two detection schemes is similar (the area under ROC curve is 0.98 for the combination of three biomarkers) and the Bayesian approach has the advantage that its outputs (parameters estimates and their uncertainty) can be further analysed by a clinical expert. © 2018"
,10.1007/s10559-018-0072-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053832410&doi=10.1007%2fs10559-018-0072-6&partnerID=40&md5=13b98dd8dcc29e23f25bb5a57c35056d,"The paper considers the model of a network with the nodes being one-server queueing systems. Non-stationary Poisson flows (traffic flows) are input flows to some queueing systems. A statistical simulation algorithm is proposed. It identifies weak points of the network and allows formulating a heuristic flow control algorithm that reduces the total waiting time. This algorithm is illustrated by an example of a flow network of 20 intersections. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
,10.1007/s00181-017-1278-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021754101&doi=10.1007%2fs00181-017-1278-6&partnerID=40&md5=6777bbb32c3412825e96c89db74a5651,"This paper proposes a latent dynamic factor model for high-dimensional realized covariance matrices of stock returns. The approach is based on the matrix logarithm and combines common latent factors driven by HAR processes and idiosyncratic autoregressive dynamics. The model accounts for positive definiteness of covariance matrices without imposing parametric restrictions. Simulated Bayesian parameter estimates are obtained using basic Markov chain Monte Carlo methods. An empirical application to 5-dimensional and 30-dimensional realized covariance matrices shows remarkably good forecasting results, in-sample and out-of-sample. © 2017, Springer-Verlag GmbH Germany."
,10.1016/j.cognition.2018.04.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048716477&doi=10.1016%2fj.cognition.2018.04.017&partnerID=40&md5=6fb86d26fb4d301cd5e6e9a889adff98,"Bayesian models of cognition assume that people compute probability distributions over hypotheses. However, the required computations are frequently intractable or prohibitively expensive. Since people often encounter many closely related distributions, selective reuse of computations (amortized inference) is a computationally efficient use of the brain's limited resources. We present three experiments that provide evidence for amortization in human probabilistic reasoning. When sequentially answering two related queries about natural scenes, participants’ responses to the second query systematically depend on the structure of the first query. This influence is sensitive to the content of the queries, only appearing when the queries are related. Using a cognitive load manipulation, we find evidence that people amortize summary statistics of previous inferences, rather than storing the entire distribution. These findings support the view that the brain trades off accuracy and computational cost, to make efficient use of its limited cognitive resources to approximate probabilistic inference. © 2018 Elsevier B.V."
,10.1109/TCST.2017.2723877,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028944370&doi=10.1109%2fTCST.2017.2723877&partnerID=40&md5=185a6be5de9488e9c6ef4617fe45fa16,"Wireless networked control systems (WNCSs) with control loops closed over a wireless network are prevailing these days. However, due to uncertainties such as random accessing delays and possible packet drops, the stability analysis for a WNCS is a challenging task. Most previous studies on the communication network analysis either relied on Monte Carlo simulation or followed the multistate Markov chain framework. In this paper, our main contribution is to propose a formal method-based stability analysis in which the communication system is modeled as a probabilistic timed automaton. The underlying communication protocol is analyzed through probabilistic model checking. In particular, the stability condition of the WNCS is expressed in the probabilistic temporal logic formula as the quality of service requirement, which can be checked, and the satisfaction of the specification is equivalent to the stability guarantee of the WNCS. We then study the impact of different media access control (MAC) parameters on the satisfaction of the specification. Furthermore, if the specification is not satisfied initially, we propose a systematic way to tune the MAC parameters or redesign the controller so that the specification can be met. This paper presents an attempt and a new angle to the communication and control system codesign problem. © 1993-2012 IEEE."
,10.1007/s13253-018-0327-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048015494&doi=10.1007%2fs13253-018-0327-8&partnerID=40&md5=1075efc026e91918abab4e6091aa327e,"The distinction between an overlap in species daily activity patterns and proximate co-occurrence of species for a location and time due to behavioral attraction or avoidance is critical when addressing the question of species co-occurrence. We use data from a dense grid of camera traps in a forest in central North Carolina to inform about proximate co-occurrence. Camera trigger times are recorded when animals pass in front of the camera’s field of vision. We view the data as a point pattern over time for each species and model the intensities driving these patterns. These species-specific intensities are modeled jointly in linear time to preserve the notion of co-occurrence. We show that a multivariate log-Gaussian Cox process incorporating both circular and linear time provides a preferred choice for modeling occurrence of forest mammals based on daily activity rhythms. Model inference is obtained under a hierarchical Bayesian framework with an efficient Markov chain Monte Carlo sampling algorithm. After model fitting, we account for imperfect detection of individuals by the camera traps by incorporating species-specific detection probabilities that adjust estimates of occurrence and co-occurrence. We obtain rich inference including assessment of the probability of presence of one species in a particular time interval given presence of another species in the same or adjacent interval, enabling probabilities of proximate co-occurrence. Our results describe the ecology and interactions of four common mammals within this suburban forest including their daily rhythms, responses to temperature and rainfall, and effects of the presence of predator species. Supplementary materials accompanying this paper appear online. © 2018, International Biometric Society."
1,10.1016/j.resuscitation.2018.05.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047089935&doi=10.1016%2fj.resuscitation.2018.05.005&partnerID=40&md5=bad679445d0635c98566392d0698f335,"Aim: To compare relative efficacy and safety of mechanical compression devices (AutoPulse and LUCAS) with manual compression in patients with cardiac arrest undergoing cardiopulmonary resuscitation (CPR). Methods: For this Bayesian network meta-analysis, seven randomized controlled trials (RCTs) were selected using PubMed/Medline, EMBASE, and CENTRAL (Inception- 31 October 2017). For all the outcomes, median estimate of odds ratio (OR) from the posterior distribution with corresponding 95% credible interval (Cr I) was calculated. Markov chain Monte Carlo (MCMC) modeling was used to estimate the relative ranking probability of each intervention based on surface under the cumulative ranking curve (SUCRA). Results: In analysis of 12, 908 patients with cardiac arrest [AutoPulse (2, 608 patients); LUCAS (3, 308 patients) and manual compression (6, 992 patients)], manual compression improved survival at 30 days or hospital discharge (OR, 1.40, 95% Cr I, 1.09–1.94), and neurological recovery (OR, 1.51, 95% Cr I, 1.06–2.39) compared to AutoPulse. There were no differences between LUCAS and AutoPulse with regards to survival to hospital admission, neurological recovery or return of spontaneous circulation (ROSC). Manual compression reduced the risk of pneumothorax (OR, 0.56, 95% Cr I, 0.33–0.97); while, both manual compression (OR, 0.15, 95% Cr I, 0.01–0.73) and LUCAS (OR, 0.07, 95% Cr I, 0.00–0.43) reduced the risk of hematoma formation compared to AutoPulse. Probability analysis ranked manual compression as the most effective treatment for improving survival at 30 days or hospital discharge (SUCRA, 84%). Conclusions: Manual compression is more effective than AutoPulse and comparable to LUCAS in improving survival at 30 days or hospital discharge and neurological recovery. Manual compression had lesser risk of pneumothorax or hematoma formation compared to AutoPulse. © 2018 Elsevier B.V."
,10.1016/j.chaos.2018.07.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049991308&doi=10.1016%2fj.chaos.2018.07.010&partnerID=40&md5=c789f19967c10c31e7936857dad28dff,"Substantial evidence supports that financial returns time series exhibit abnormal properties including leptokurtosis, volatility clustering as well as intermittent jumps and leverage effects between returns and volatility processes. This paper studies a heavy-tailed stochastic volatility (SV) model with jumps components and leverage effects, and the Student's-t distribution is employed to describe the error innovations (SVJLt). Since the existence of high-dimensionality of the latent variables and the special structure of Hessian matrix of the stochastic volatility density, we develop an efficient Markov chain Monte Carlo (MCMC) posterior simulator exploiting the adaptive importance sampling technique based on band and sparse matrix routine rather than the conventional Kalman filter to estimate the new model. And the precision sampler is exploited due to the band structure of the inverse covariance matrix of the state variables. The model comparisons of returns volatility are conducted utilizing the observed-data based deviance information criterion (DIC) and the cross-entropy (CE) based marginal likelihood estimation. The effectiveness of the proposed model and the methodology are illustrated with applications in stock returns volatility forecast. Through employing several loss functions for evaluation, the empirical studies suggest strong evidence in heavy tailed distributions, jumps features and leverage effects simultaneously. © 2018 Elsevier Ltd"
,10.1002/nau.23559,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045101075&doi=10.1002%2fnau.23559&partnerID=40&md5=68692dcdbabef07b01b04b041b1e575c,"Aims: To investigate the long-term cost-utility of the artificial urinary sphincter (AUS) compared with Transobturator Retroluminal Sling (AdVance) in the treatment of patients with severe post prostatectomy stress urinary incontinence (PPSUI) from a Canadian provincial health perspective. Methods: A Markov model with Monte Carlo simulation was developed with a cycle length of 1 year and time horizon up to 10 years to estimate the incremental cost per quality-adjusted life years (QALYs). Patients were assigned to treatment with either AUS or an AdVance sling. Transition probabilities, efficacy data, and utility indices were derived from published literature and expert opinion. Cost data were obtained from provincial health care system and hospital data in 2016-Canadian dollars. The primary outcome was cost per quality-adjusted life year. A standard discount rate of 1.5% was applied annually. Probabilistic and one way deterministic sensitivity analyses were performed. Results: AUS implantation had a 10-year mean total cost of $14 228 (SD ± 3,509) for 7.58 QALYs. AdVance sling had a mean total cost $18 938 (SD ± 12,435) for 6.43 QALYs. The incremental cost savings of AUS over 10-years was −$ 4710 with an added effectiveness of 1.15 QALYs. At a willingness to pay threshold of $50 000, AUS remained the most cost-effective option. A limitation of our analysis is the lack of direct long-term comparisons between both scenarios along with standard success definition. Conclusions: AUS implantation appears to be more economical treatment strategy for severe PPSUI compared with AdVance sling for a publicly funded health care system over a 5- and 10-year time horizon. © 2018 Wiley Periodicals, Inc."
,10.1016/j.neuroimage.2018.04.077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048510824&doi=10.1016%2fj.neuroimage.2018.04.077&partnerID=40&md5=2748bc81c499c51f8e38f6e4e2a5f02d,"A Bayesian model for sparse, hierarchical, inver-covariance estimation is presented, and applied to multi-subject functional connectivity estimation in the human brain. It enables simultaneous inference of the strength of connectivity between brain regions at both subject and population level, and is applicable to fMRI, MEG and EEG data. Two versions of the model can encourage sparse connectivity, either using continuous priors to suppress irrelevant connections, or using an explicit description of the network structure to estimate the connection probability between each pair of regions. A large evaluation of this model, and thirteen methods that represent the state of the art of inverse covariance modelling, is conducted using both simulated and resting-state functional imaging datasets. Our novel Bayesian approach has similar performance to the best extant alternative, Ng et al.'s Sparse Group Gaussian Graphical Model algorithm, which also is based on a hierarchical structure. Using data from the Human Connectome Project, we show that these hierarchical models are able to reduce the measurement error in MEG beta-band functional networks by 10%, producing concomitant increases in estimates of the genetic influence on functional connectivity. © 2018"
,10.1371/journal.pone.0204359,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053672610&doi=10.1371%2fjournal.pone.0204359&partnerID=40&md5=b55c4bb3d1bbac484d36bb0b3cf12702,"Coxsackievirus A2 (CV-A2) has emerged as an important etiological agent in the hand, foot, and mouth disease and herpangina pathogen spectrum because of its high global prevalence. In the present study, we investigated the evolutionary dynamics of CV-A2 circulating in China. We analyzed a total of 163 entire VP1 sequences of CV-A2, including 74 sequences generated from the present study and 89 sequences collected from the GenBank database. Phylogenetic analysis based on the entire VP1 nucleotide sequences confirmed the persistent circulation of the predominant genotype D in mainland of China since 2008. Cluster analysis grouped the sequences into two distinct clusters, clusters 1 and 2, with most grouped under cluster 2. After 2012, cluster 1 was gradually replaced by cluster 2. Results of Bayesian Markov chain Monte Carlo analysis suggested that multiple lineages of genotype D were transmitted in mainland of China at an estimated evolutionary rate of 6.32×10−3 substitutions per site per year, which is consistent with the global evolutionary rate of CV-A2 (5.82×10−3 substitutions per site per year). Continuous transmission and evolution of CV-A2 resulted in the genetic polymorphism. © 2018 Yang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1002/jmv.25220,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049781987&doi=10.1002%2fjmv.25220&partnerID=40&md5=277c2879260776c7469132cb0b44cf58,"Despite a significant decrease in acute hepatitis A in the last 2 decades in Italy, outbreaks were observed occurring mostly in southern Italy. In this study, Bayesian phylogenetic analysis was used to analyze the origin of these epidemics. With this aim, 5 different data sets of hepatitis A virus sequences were built to perform genotyping by the neighbor-joining method to estimate the evolutionary rates by using a Bayesian Markov chain Monte Carlo approach and to investigate the demographic history by independent Markov chain Monte Carlo runs enforcing both a strict and relaxed clock. The estimated mean value of the evolutionary rate, representing Ia and Ib strains, was 1.21 × 10−3 and 2.0 × 10−3 substitutions/site/year, respectively. The Bayesian maximum clade credibility tree of hepatitis A virus (HAV) Ia and Ib strains showed that Italian sequences mostly formed separate clusters. The root of the time for the most recent common ancestor (tMRCA) for HAV Ia and Ib strains dated back to 1981 and to 1988, respectively, showing in both cases different epidemic entrances. Phylodynamic analysis showed that genotype Ia increased in 1997, when the Apulia epidemic started, then suffered a bottleneck, probably consequent to vaccination and to the herd immunity, followed by a new increase in virus population in the years 2013-2014 consequent to the epidemic caused by the ingestion of mixed frozen berries. A similar trend without an evident bottleneck was observed also in the case of genotype Ib. In conclusion, the Bayesian phylogenetic analysis represents a good tool to measure the effectiveness of the public health plans used for HAV control. © 2018 Wiley Periodicals, Inc."
,10.1007/s40484-018-0149-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053228281&doi=10.1007%2fs40484-018-0149-2&partnerID=40&md5=c70045fb74caa4e83cdad51d52661984,"Background: The recently emerged technology of methylated RNA immunoprecipitation sequencing (MeRIP-seq) sheds light on the study of RNA epigenetics. This new bioinformatics question calls for effective and robust peaking calling algorithms to detect mRNA methylation sites from MeRIP-seq data. Methods: We propose a Bayesian hierarchical model to detect methylation sites from MeRIP-seq data. Our modeling approach includes several important characteristics. First, it models the zero-inflated and over-dispersed counts by deploying a zero-inflated negative binomial model. Second, it incorporates a hidden Markov model (HMM) to account for the spatial dependency of neighboring read enrichment. Third, our Bayesian inference allows the proposed model to borrow strength in parameter estimation, which greatly improves the model stability when dealing with MeRIP-seq data with a small number of replicates. We use Markov chain Monte Carlo (MCMC) algorithms to simultaneously infer the model parameters in a de novo fashion. The R Shiny demo is available at https://qiwei.shinyapps.io/BaySeqPeak and the R/C ++ code is available at https://github.com/liqiwei2000/BaySeqPeak. Results: In simulation studies, the proposed method outperformed the competing methods exomePeak and MeTPeak, especially when an excess of zeros were present in the data. In real MeRIP-seq data analysis, the proposed method identified methylation sites that were more consistent with biological knowledge, and had better spatial resolution compared to the other methods. Conclusions: In this study, we develop a Bayesian hierarchical model to identify methylation peaks in MeRIP-seq data. The proposed method has a competitive edge over existing methods in terms of accuracy, robustness and spatial resolution. [Figure not available: see fulltext.]. © 2018, Higher Education Press and Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1016/j.ijthermalsci.2018.03.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047393262&doi=10.1016%2fj.ijthermalsci.2018.03.009&partnerID=40&md5=f6a5a32cdc90b2270c6caf510e3c06c5,"In this work, a novel experimental technique is developed to estimate spatially varying heat transfer coefficients from a flat plate with flush mounted discrete heat sources, using Bayesian inference with temperature measurements from liquid crystal thermography (LCT) at an adiabatic surface of the plate without disturbing the fluid flow. Steady state, laminar forced convection experiments have been done on a flat Bakelite plate with three identical embedded discrete aluminium heat sources of dimensions 0.16 × 0.06 × 0.015 (l × w × t all in m). The variation of local convective heat transfer coefficient is obtained in the form of a Nusselt number correlation Nu=aReb(x/l)c. This correlation is first developed by limited numerical simulations for two dimensional conjugate convection. With this correlation, a computationally less complex problem of conjugate conduction in the flat plate also known as the forward model is repeatedly solved for various values of ‘a’ ‘b’ and ‘c’ to obtain the temperature distributions at select points on the adiabatic surface using COMSOL. A surrogate model obtained by Artificial Neural Networks (ANN) built upon the data from these simulations then replaces the forward model. This surrogate model is used to drive a Markov Chain Monte Carlo based Metropolis Hastings algorithm to generate the samples to the forward model to solve the inverse problem of getting ‘a’ ‘b’ and ‘c’ from temperature measurements at the adiabatic surface. Bayesian framework is then adopted to compare the experimental and the simulated temperatures to generate posteriors and the mean, maximum a posteriori and standard deviation of the parameters ‘a’ ‘b’ and ‘c’ are estimated. The effect of number of samples and the temperature points on the performance of the estimation process has been reported. Finally, with the retrieved values of ‘a’ ‘b’ and ‘c’ temperature distributions are obtained by solving the conduction problem and these are compared with those actually measured with TLC. © 2018 Elsevier Masson SAS"
1,10.1007/s00362-016-0810-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979985979&doi=10.1007%2fs00362-016-0810-7&partnerID=40&md5=9bcd6d9155e5cbcae84af675f77b0fd5,"A s-out-of-k : G system consists of k components functions if and only if at least s components functions. In this paper, we consider the s-out-of-k : G system when this system is exposed a common random stress and the underlying distributions belong to the family of inverse exponentiated distributions. The estimates of this sytem reliability are investigated by using classical and Bayesian approaches. The uniformly minimum variance unbiased and exact Bayes estimates of the reliability of system are obtained analytically when the common second parameter is known. The Bayes estimates for the reliability of system have been developed by using Lindley’s approximation and the Markov Chain Monte Carlo method due to the lack of explicit forms when the all parameters are unknown. The asymptotic confidence interval and coverage probabilities are derived based on the Fisher’s information matrix. The highest probability density credible interval is constructed by using the Markov Chain Monte Carlo method. The comparison of the derived estimates are carried out by using Monte Carlo simulations. Real data set is also analysed for an illustration of the findings. © 2016, Springer-Verlag Berlin Heidelberg."
1,10.1214/17-BA1077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047837602&doi=10.1214%2f17-BA1077&partnerID=40&md5=ed2b46bd060863adfcad4b308f2904ae,"We propose a new scheme for selecting pool states for the embedded Hidden Markov Model (HMM) Markov Chain Monte Carlo (MCMC) method. This new scheme allows the embedded HMM method to be used for efficient sampling in state space models where the state can be high-dimensional. Previously, embedded HMM methods were only applicable to low-dimensional state-space models. We demonstrate that using our proposed pool state selection scheme, an embedded HMM sampler can have similar performance to a well-tuned sampler that uses a combination of Particle Gibbs with Backward Sampling (PGBS) and Metropolis updates. The scaling to higher dimensions is made possible by selecting pool states locally near the current value of the state sequence. The proposed pool state selection scheme also allows each iteration of the embedded HMM sampler to take time linear in the number of the pool states, as opposed to quadratic as in the original embedded HMM sampler. © 2018 International Society for Bayesian Analysis."
,10.1111/jedm.12184,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052605958&doi=10.1111%2fjedm.12184&partnerID=40&md5=792399a24789947429d8ab2e7a6e743f,"Bayesian methods incorporate model parameter information prior to data collection. Eliciting information from content experts is an option, but has seen little implementation in Bayesian item response theory (IRT) modeling. This study aims to use ethical reasoning content experts to elicit prior information and incorporate this information into Markov Chain Monte Carlo (MCMC) estimation. A six-step elicitation approach is followed, with relevant details at each stage for two IRT items parameters: difficulty and guessing. Results indicate that using content experts is the preferred approach, rather than noninformative priors, for both parameter types. The use of a noninformative prior for small samples provided dramatically different results when compared to results from content expert–elicited priors. The WAMBS (When to worry and how to Avoid the Misuse of Bayesian Statistics) checklist is used to aid in comparisons. © 2018 by the National Council on Measurement in Education"
,10.1016/j.jkss.2018.03.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045297978&doi=10.1016%2fj.jkss.2018.03.003&partnerID=40&md5=bec688158aa1213f096c374336881f22,"In this paper, we propose a Bayesian variable selection method for linear regression models with high-order interactions. Our method automatically enforces the heredity constraint, that is, a higher order interaction term can exist in the model only if both of its parent terms are in the model. Based on the stochastic search variable selection George and McCulloch (1993), we propose a novel hierarchical prior that fully considers the heredity constraint and controls the degree of sparsity simultaneously. We develop a Markov chain Monte Carlo (MCMC) algorithm to explore the model space efficiently while accounting for the heredity constraint by modifying the shotgun stochastic search algorithm Hans et al. (2007). The performance of the new model is demonstrated through comparisons with other methods. Numerical studies on both real data analysis and simulations show that our new method tends to find relevant variable more effectively when higher order interaction terms are considered. © 2018 The Korean Statistical Society"
1,10.1111/sjos.12310,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045190421&doi=10.1111%2fsjos.12310&partnerID=40&md5=446ea324adf30135f7335d6c8de33807,"When Gaussian errors are inappropriate in a multivariate linear regression setting, it is often assumed that the errors are iid from a distribution that is a scale mixture of multivariate normals. Combining this robust regression model with a default prior on the unknown parameters results in a highly intractable posterior density. Fortunately, there is a simple data augmentation (DA) algorithm and a corresponding Haar PX-DA algorithm that can be used to explore this posterior. This paper provides conditions (on the mixing density) for geometric ergodicity of the Markov chains underlying these Markov chain Monte Carlo algorithms. Letting d denote the dimension of the response, the main result shows that the DA and Haar PX-DA Markov chains are geometrically ergodic whenever the mixing density is generalized inverse Gaussian, log-normal, inverted Gamma (with shape parameter larger than d/2) or Fréchet (with shape parameter larger than d/2). The results also apply to certain subsets of the Gamma, F and Weibull families. © 2018 Board of the Foundation of the Scandinavian Journal of Statistics"
2,10.1214/17-AOAS1116,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052688774&doi=10.1214%2f17-AOAS1116&partnerID=40&md5=93d163459b067973ac480b5fdbab764c,"Medical imaging studies have collected high-dimensional imaging data to identify imaging biomarkers for diagnosis, screening, and prognosis, among many others. These imaging data are often represented in the form of a multi-dimensional array, called a tensor. The aim of this paper is to develop a tensor partition regression modeling (TPRM) framework to establish a relationship between low-dimensional clinical outcomes (e.g., diagnosis) and high-dimensional tensor covariates. Our TPRM is a hierarchical model and efficiently integrates four components: (i) a partition model, (ii) a canonical polyadic decomposition model, (iii) a principal components model, and (iv) a generalized linear model with a sparse inducing normal mixture prior. This framework not only reduces ultra-high dimensionality to a manageable level, resulting in efficient estimation, but also optimizes prediction accuracy in the search for informative sub-tensors. Posterior computation proceeds via an efficient Markov chain Monte Carlo algorithm. Simulation shows that TPRM outperforms several other competing methods. We apply TPRM to predict disease status (Alzheimer versus control) by using structural magnetic resonance imaging data obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) study. © Institute of Mathematical Statistics, 2018."
,10.1140/epjc/s10052-018-6190-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052832337&doi=10.1140%2fepjc%2fs10052-018-6190-5&partnerID=40&md5=34d0ea2510a8f32aac3f8ae186fec8c1,"Among the various possibilities to probe the theory behind the recent accelerated expansion of the universe, the energy conditions (ECs) are of particular interest, since it is possible to confront and constrain different theories of gravity with observational data. In this context, we use the ECs to probe any alternative theory of gravity whose extra term acts as a cosmological constant. For this purpose, we apply a model-independent approach to reconstruct the recent expansion of the universe. Using Type Ia supernova, baryon acoustic oscillations and cosmic-chronometer data, we perform a Markov Chain Monte Carlo analysis to put constraints on the effective cosmological constant Ωeff0. In addition, we find out that about 30% of the posterior distribution is incompatible with a cosmological constant, showing that this method can potentially rule it out as a mechanism for the accelerated expansion. We also study the consequence of these constraints for two particular formulations of the massive gravity in a scenario where both theories mimic General Relativity with a cosmological constant. Using the Ωeff0 observational bounds along with the upper bounds on the graviton mass we obtain constraints on the parameter spaces of both theories. © 2018, The Author(s)."
,10.1029/2018JB015490,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053705985&doi=10.1029%2f2018JB015490&partnerID=40&md5=6e63ae3b8dc831e10d65cf0c8830bc4c,"We present a high-resolution shear wave velocity model of Greenland's lithosphere from regional and teleseismic Rayleigh waves recorded by the Greenland Ice Sheet Monitoring Network supplemented with observations from several temporary seismic deployments. To construct Rayleigh wave group velocity maps, we integrated signals from regional and teleseismic earthquakes with several years of ambient seismic noise and used the dispersion to constrain crustal and upper-mantle seismic shear wave velocity structure. Specifically, we used a Markov Chain Monte Carlo technique to estimate 3-D shear wave velocities beneath Greenland to a depth of 200 km. Our model reveals four prominent anomalies: a deep high-velocity feature extending from southwestern to northwestern Greenland that may be the signature of a thick cratonic keel, a corridor of relatively low upper-mantle velocity across central Greenland that could be associated with lithospheric modification from the passage of the Iceland plume beneath Greenland or interpreted as a tectonic boundary between cratonic blocks, an upper-crustal southwest-northeast trending boundary separating Greenland into two regions of contrasting tectonic and crustal properties, and a midcrustal low-velocity anomaly beneath northeastern Greenland. The nature of this midcrustal anomaly is of particular interest given that it underlies the onset of the Northeast Greenland Ice Stream and raises interesting questions regarding how deeper processes may impact the ice stream dynamics and the evolution of the Greenland Ice Sheet. ©2018. American Geophysical Union. All Rights Reserved."
,10.1016/j.jmva.2018.05.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047878733&doi=10.1016%2fj.jmva.2018.05.003&partnerID=40&md5=86498826d3ae5ad99e76658b901cf337,"Multidimensional item response theory (MIRT) models are quite useful to analyze datasets involving multiple skills or latent traits, which occur in many applications. However, most of the works consider the usual multivariate (symmetric) normal distribution to model the latent traits and do not deal with the multiple group framework. Also, in general, the works consider a limited number of model fit assessment tools and do not investigate the measurement instrument dimensionality in a detailed way. When the assumption of normality of the latent traits distributions does not hold, misleading results and conclusions can be obtained. Our goal is to propose a MIRT multiple group model with multivariate skew normal distributions under the centered parameterization to model the distribution of the latent traits of each group, presenting simple and feasible conditions for model identification. Such an approach is more flexible than the usual multivariate (symmetric) normal one. In addition, a full Bayesian approach for parameter estimation, structural selection (model comparison and determination of the dimensionality of the measurement instrument) and model fit assessment are developed through Markov Chain Monte Carlo algorithms. The proposed tools are illustrated through the analysis of a real dataset related to the first stage of the University of Campinas 2013 admission exam. © 2018 Elsevier Inc."
,10.1093/mnras/sty1377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051526000&doi=10.1093%2fmnras%2fsty1377&partnerID=40&md5=cd65908297e66f131b622c13c30654ec,"Weak gravitational lensing depends on the integrated mass along the line of sight. Baryons contribute to the mass distribution of galaxy clusters and the resulting mass estimates from lensing analysis. We use the cosmo-OWLS suite of hydrodynamic simulations to investigate the impact of baryonic processes on the bias and scatter of weak lensing mass estimates of clusters. These estimates are obtained by fitting NFW profiles to mock data using Markov Chain Monte Carlo techniques. In particular, we examine the difference in estimates between dark matter-only runs and those including various prescriptions for baryonic physics. We find no significant difference in themass bias when baryonic physics is included, though the overall mass estimates are suppressed when feedback from active galactic nucleus is included. For lowest-mass systems for which a reliable mass can be obtained (M200 ≈ 2 × 1014 M⊙), we find a bias of ≈ 10 per cent. The magnitude of the bias tends to decrease for higher mass clusters, consistent with no bias for the most massive clusters which have masses comparable to those found in the CLASH and HFF samples. For the lowest mass clusters, the mass bias is particularly sensitive to the fit radii and the limits placed on the concentration prior, rendering reliable mass estimates difficult. The scatter in mass estimates between the dark matter-only and the various baryonic runs is less than between different projections of individual clusters, highlighting the importance of triaxiality. © 2018 The Author(s). Published by Oxford University Press on behalf of The Royal Astronomical Society."
3,10.1016/j.jand.2018.05.019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051965255&doi=10.1016%2fj.jand.2018.05.019&partnerID=40&md5=d2d2140ff6a7eaee743e734e4a9df537,"Background: The Healthy Eating Index (HEI), a diet quality index that measures alignment with the Dietary Guidelines for Americans, was updated with the 2015-2020 Dietary Guidelines for Americans. Objective and design: To evaluate the psychometric properties of the HEI-2015, eight questions were examined: five relevant to construct validity, two related to reliability, and one to assess criterion validity. Data sources: Three data sources were used: exemplary menus (n=4), National Health and Nutrition Examination Survey 2011-2012 (N=7,935), and the National Institutes of Health-AARP (formally known as the American Association of Retired Persons) Diet and Health Study (N=422,928). Statistical analyses: Exemplary menus: Scores were calculated using the population ratio method. National Health and Nutrition Examination Survey 2011-2012: Means and standard errors were estimated using the Markov Chain Monte Carlo approach. Analyses were stratified to compare groups (with t tests and analysis of variance). Principal components analysis examined the number of dimensions. Pearson correlations were estimated between components, energy, and Cronbach's coefficient alpha. National Institutes of Health-AARP Diet and Health Study: Adjusted Cox proportional hazards models were used to examine scores and mortality outcomes. Results: For construct validity, the HEI-2015 yielded high scores for exemplary menus as four menus received high scores (87.8 to 100). The mean score for National Health and Nutrition Examination Survey was 56.6, and the first to 99th percentile were 32.6 to 81.2, respectively, supporting sufficient variation. Among smokers, the mean score was significantly lower than among nonsmokers (53.3 and 59.7, respectively) (P<0.01), demonstrating differentiation between groups. The correlation between diet quality and diet quantity was low (all <0.25) supporting these elements being independent. The components demonstrated multidimensionality when examined with a scree plot (at least four dimensions). For reliability, most of the intercorrelations among the components were low to moderate (0.01 to 0.49) with a few exceptions, and the standardized Cronbach's alpha was.67. For criterion validity, the highest vs the lowest quintile of HEI-2015 scores were associated with a 13% to 23% decreased risk of all-cause, cancer, and cardiovascular disease mortality. Conclusions: The results demonstrated evidence supportive of construct validity, reliability, and criterion validity. The HEI-2015 can be used to examine diet quality relative to the 2015-2020 Dietary Guidelines for Americans. © 2018 Academy of Nutrition and Dietetics"
,10.1140/epjc/s10052-018-6233-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054193991&doi=10.1140%2fepjc%2fs10052-018-6233-y&partnerID=40&md5=b96dd5e8812025cf658caca8ececc2d9,"Most dark energy models have the ΛCDM as their limit, and if future observations constrain our universe to be close to ΛCDM Bayesian arguments about the evidence and the fine-tuning will have to be employed to discriminate between the models. Assuming a baseline ΛCDM model we investigate a number of quintessence and phantom dark energy models, and we study how they would perform when compared to observational data, such as the expansion rate, the angular distance, and the growth rate measurements, from the upcoming Dark Energy Spectroscopic Instrument (DESI) survey. We sample posterior likelihood surfaces of these dark energy models with Monte Carlo Markov Chains while using central values consistent with the Planck ΛCDM universe and covariance matrices estimated with Fisher information matrix techniques. We find that for this setup the Bayes factor provides a substantial evidence in favor of the ΛCDM model over most of the alternatives. We also investigated how well the CPL parametrization approximates various scalar field dark energy models, and identified the location for each dark energy model in the CPL parameter space. © 2018, The Author(s)."
,10.3390/sym10090372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054357052&doi=10.3390%2fsym10090372&partnerID=40&md5=a4de248341dc1e3f007d4a5963b2c5fe,"Decaying Dark Energy models modify the background evolution of the most common observables, such as the Hubble function, the luminosity distance and the Cosmic Microwave Background temperature-redshift scaling relation. We use the most recent observationally-determined datasets, including Supernovae Type Ia and Gamma Ray Bursts data, along with H(z) and Cosmic Microwave Background temperature versus z data and the reduced Cosmic Microwave Background parameters, to improve the previous constraints on these models. We perform a Monte Carlo Markov Chain analysis to constrain the parameter space, on the basis of two distinct methods. In view of the first method, the Hubble constant and the matter density are left to vary freely. In this case, our results are compatible with previous analyses associated with decaying Dark Energy models, as well as with the most recent description of the cosmological background. In view of the second method, we set the Hubble constant and the matter density to their best fit values obtained by the Planck satellite, reducing the parameter space to two dimensions, and improving the existent constraints on the model's parameters. Our results suggest that the accelerated expansion of the Universe is well described by the cosmological constant, and we argue that forthcoming observations will play a determinant role to constrain/rule out decaying Dark Energy. © 2018 by the authors."
,10.1109/ACCESS.2018.2867744,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052797220&doi=10.1109%2fACCESS.2018.2867744&partnerID=40&md5=ddbf28e9a5500226a2021d7ff2eb072c,"Most existing approaches to heterogeneous redundancy allocation problem (RAP) are prone to getting trapped in local optimal modes during optimization, mainly due to the rugged combinatoric landscapes. Recently, optimization-by-sampling paradigm based on the stochastic approximation Monte Carlo (SAMC) sampling has shown superior performance in solving the heterogeneous RAP for multi-state systems (MSSs). However, one drawback of this method is that the global move of a Markov chain relying only on a uniform distribution is typically hard to hit the low-energy regions due to the uninformative proposal, leading to insufficient global exploration in sampling. To address the problem of where to sample for efficient optimization of heterogeneous RAP, we introduce a rejection-free Monte Carlo method to sample from the target distribution over the combinatorial space. Specifically, a model-based proposal learning algorithm is derived to guide the global exploration towards promising regions of the descrete state space. Experimental evaluations on a set of benchmark instances show the superiority of the proposed approach compared with the several state-of-the-arts in terms of the solution quality and computational efficiency. © 2013 IEEE."
,10.1109/SSP.2018.8450801,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053831839&doi=10.1109%2fSSP.2018.8450801&partnerID=40&md5=206542a704298c88124d7edb61a726a1,"Imaging technologies, such as coherent fibred bundle optical microscopy (FBOM), operate with irregularly-spaced sparse subsamples from their field of view. In this paper, we address the problem of data deconvolution for applications where the observed irregularly distributed samples are considered as a result of a convolution operator acting on original samples and corrupted by additive observation noise. We propose a hierarchical Bayesian model in which suitable prior distributions are assigned to the unknown model parameters, and compare two estimation strategies including Markov chain Monte Carlo (MCMC) and variational Bayes (VB), which are used to perform Bayesian inference using the posterior distribution. Simulations conducted on both synthetic and real datasets illustrate the benefits of the proposed methods in terms quality of deconvolution of the sparse samples. © 2018 IEEE."
,10.1109/SSP.2018.8450740,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053847768&doi=10.1109%2fSSP.2018.8450740&partnerID=40&md5=308c06f360154bb797dbaaded1f53dcc,"Motivated by challenges in Computational Statistics such as Penalized Maximum Likelihood inference in statistical models with intractable likelihoods, we analyze the convergence of a stochastic perturbation of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), when the stochastic approximation relies on a biased Monte Carlo estimation as it happens when the points are drawn from a Markov chain Monte Carlo (MCMC) sampler. We first motivate this general framework and then show a convergence result for the perturbed FISTA algorithm. We discuss the convergence rate of this algorithm and the computational cost of the Monte Carlo approximation to reach a given precision. Finally, through a numerical example, we explore new directions for a better understanding of these Proximal-Gradient based stochastic optimization algorithms. © 2018 IEEE."
,10.1109/SSP.2018.8450772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053838247&doi=10.1109%2fSSP.2018.8450772&partnerID=40&md5=a12f46755eeb3446dc5869c33f5a6a0d,"Sequential MCMC (SMCMC) methods are a useful alternative to particle filters for performing sequential inference in a Bayesian framework in nonlinear and non-Gaussian state-space models. The weight degeneracy phenomenon which impacts the performance of even the most advanced particle filters in higher dimensions is avoided. In this paper, we explore the applicability of the discrete bouncy particle sampler, which is based on constructing a guided random walk and performing delayed rejection, to perform more effective sampling within SMCMC. We perform numerical simulations to examine when the proposed method offers advantages compared to state-of-the-art SMCMC techniques. © 2018 IEEE."
,10.1109/SSP.2018.8450720,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053838430&doi=10.1109%2fSSP.2018.8450720&partnerID=40&md5=43dfd09a5241dfa38ef18c345bbe0362,"This paper considers the identification of point-like source of groundwater pollution. The ill-posed character of this problem has recently led to the introduction of a regularization approach that combines source parametrization, and penalization of undesirable solutions based on prior information about the source parameters, thereby ending up with a parametric Bayesian estimation framework. In this framework, a stochastic-type Markov Chain Monte Carlo (MCMC) method has been introduced as an approximate computation tool of the posterior mean estimate of both source parameters and variance of the (assumed homogeneous) observation noise. Being in the more general case of inhomogeneous noise, our main goal is to propose a deterministic-type computation method based on the variational Bayesian approach. Simulation results suggest that the proposed scheme can provide comparable estimation accuracy to MCMC while requiring less computational time. © 2018 IEEE."
,10.1109/SSP.2018.8450787,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053841848&doi=10.1109%2fSSP.2018.8450787&partnerID=40&md5=0aae6fee46060612d0054bbe0fddd80e,"We present a model for point processes with gamma distributed increments. We assume a piecewise constant latent process controlling shape and scale of the distribution. For the discrete number of states of the latent process we use a non-parametric assumption by utilizing a Chinese restaurant process (CRP). For the inference of such inhomogeneous gamma processes with an unbounded number of states we do Bayesian inference using Markov Chain Monte Carlo. Finally, we apply the inference algorithm to simulated point processes and to empirical spike train recordings, which inherently possess non-stationary and non-Poissonian behavior. © 2018 IEEE."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052489405&partnerID=40&md5=73a078f6c4319938ba4e649ab692fd0b,"In this paper, Burr-X distribution with Type-I hybrid censored data is considered. E-Bayesian estimation (expectation of the Bayesian estimate) and the corresponding maximum likelihood and Bayesian estimation methods are discussed for the distribution parameter and the reliability function. Bayesian and E-Bayesian estimates are derived by using LINEX and squared error loss (SEL) functions. By applying Markov chain Monte Carlo (MCMC) techniques Bayesian and E-Bayesian estimates are obtained. An illustrative examples of Type-I hybrid censored samples and real data set are presented. Finally, a comparison among the proposed estimation methods is conducted. © International Association of Engineers."
,10.1109/IWCMC.2018.8450271,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053935457&doi=10.1109%2fIWCMC.2018.8450271&partnerID=40&md5=61e45a1c9216bcb9de7feb6fca101381,This paper presents a decisive threshold termed as the buffer-threshold to control the selection probability of source-relay (SR) or relay-destination (RD) links for the buffer aided cooperative wireless networks. The weights of the links are reassigned using buffer-threshold and a link with the maximum weight is activated. The proposed scheme is termed as Buffer-Threshold based relay selection scheme (BTRS). The relations of the outage probability (OP) and the average delay are calculated by the Markov Modelling of the buffers. Theoretical results are analyzed for different cases of the buffer-threshold and validated by the Monte-carlo simulations. For the performance evaluation BTRS is compared with the max link relay selection (MLRS) scheme and the max weight relay selection (MWRS) scheme and outperforms its counterparts in terms of the average delay. © 2018 IEEE.
,10.1109/ACCESS.2018.2867687,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052625977&doi=10.1109%2fACCESS.2018.2867687&partnerID=40&md5=75c351e7c01c3475910e29a41cb8c451,"This paper proposes a Bayesian framework for localization of multiple sources in the event of accidental hazardous contaminant release. The framework assimilates sensor measurements of the contaminant concentration with the integrated multizone computational fluid dynamics (multizone-CFD)-based contaminant fate and transport model. To ensure online tractability, we build deep Gaussian process-based emulators approximating multizone-CFD model. To effectively represent the transient response of the multizone-CFD model, the deep Gaussian processes are extended to matrix-variate architecture by adopting Kronecker products to the output covariance for each GP layer. The resultant deep matrix-variate Gaussian process emulators are used to define the likelihood of the Bayesian framework, while Markov chain Monte Carlo approach is used to sample from the posterior distribution. The proposed method is evaluated for single and multiple contaminant sources localization tasks modeled by CONTAM simulator in a single-story building of 30 zones, demonstrating that proposed approach accurately perform inference on locations of contaminant sources. Moreover, the proposed model not only shows outstanding regression performance but speed up training. © 2013 IEEE."
,10.1088/1475-7516/2018/08/042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053125653&doi=10.1088%2f1475-7516%2f2018%2f08%2f042&partnerID=40&md5=19a28613c979535b6ed9195641459bea,"Taking the neutrino oscillation data into consideration, a dimensionless parameter Δ = (m3-m1)/(m3+m1) is adopted to parameterize the three neutrino mass eigenstates and the normal (positive Δ) or inverted (negative Δ) mass hierarchies in three typical cosmological models. Using the currently available cosmic observational data, several Markov Chain Monte Carlo chains are obtained with uniform priors on the free parameters at first. Applying importance sampling the results are compared with three new priors, i.e., logarithmic prior on |Δ|, linear and logarithmic priors on Σ mν. It turns out that the three new priors increase the upper limits of neutrino mass, but do not change the tendency towards different model's preference for different hierarchies, i.e., the normal hierarchy tends to be favored by ΛCDM and wCDM, which, however, disappears in the w0 waCDM model. In addition, the almost symmetrical contours in the w-Δ, w0-Δ, wa-Δ planes indicate that the normal and inverted hierarchy have strong degeneracy. Finally, we perform a Bayesian model comparison analysis, finding that flat linear prior on Δ and w0 waCDM are the most preferred prior and model, respectively. © 2018 IOP Publishing Ltd and Sissa Medialab."
,10.1088/1751-8121/aad6fa,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053133854&doi=10.1088%2f1751-8121%2faad6fa&partnerID=40&md5=c1b38a6c5912894365642ca9513ca333,"We investigate randomized benchmarking (RB) in a general setting with quantum gates that form a representation, not necessarily an irreducible one, of a finite group. We derive an estimate for the average fidelity, to which experimental data may then be calibrated. Furthermore, we establish that RB can be achieved by the sole implementation of quantum gates that generate the group as well as one additional arbitrary group element. In this case, we need to assume that the noise is close to being covariant. This yields a more practical approach to RB. Moreover, we show that RB is stable with respect to approximate Haar sampling for the sequences of gates. This opens up the possibility of using Markov chain Monte Carlo methods to obtain the random sequences of gates more efficiently. We demonstrate these results numerically using the well-studied example of the Clifford group as well as the group of monomial unitary matrices. For the latter, we focus on the subgroup with nonzero entries consisting of nth roots of unity, which contains T gates. © 2018 IOP Publishing Ltd."
,10.1109/CCECE.2018.8447816,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053628272&doi=10.1109%2fCCECE.2018.8447816&partnerID=40&md5=493ffd2e9f395584fd8fdab19d2d8f1f,"We propose a fully Bayesian learning approach using reversible jump Markov chain Monte Carlo (RJMCMC) for asymmetric Gaussian mixtures (AGM). Compared to classic Gaussian mixture model, AGM doesn't imply that target data is symmetric which brings flexibility and better fitting results. This paper also introduces a RJMCMC learning implementation based on Metropolis-Hastings (MH) within Gibbs sampling method. As an improvement of traditional sampling-based MCMC learning, RJMCMC has no assumption concerning the number of components and, therefore, the AGM model itself could be transferred between iterations. For better evaluating models with different mixture components numbers, the model selection is achieved by calculating integrated likelihood using Laplace approximation to figure out the best-fit components number. We selected both synthetic and challenging spam filtering dataset to show the merits of the proposed model. © 2018 IEEE."
,10.3390/e20090642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053691374&doi=10.3390%2fe20090642&partnerID=40&md5=559e42fa903a9d4a955a2424686fafea,"In this paper, we study the performance of Bayesian computational methods to estimate the parameters of a bivariate survival model based on the Ali-Mikhail-Haq copula with marginal distributions given byWeibull distributions. The estimation procedure was based on Monte Carlo Markov Chain (MCMC) algorithms. We present three version of the Metropolis-Hastings algorithm: Independent Metropolis-Hastings (IMH), RandomWalk Metropolis (RWM) and Metropolis-Hastings with a natural-candidate generating density (MH). Since the creation of a good candidate generating density in IMH and RWM may be difficult, we also describe how to update a parameter of interest using the slice sampling (SS) method. A simulation study was carried out to compare the performances of the IMH, RWM and SS. A comparison was made using the sample root mean square error as an indicator of performance. Results obtained from the simulations show that the SS algorithm is an effective alternative to the IMH and RWM methods when simulating values from the posterior distribution, especially for small sample sizes. We also applied these methods to a real data set. © 2018 by the authors."
,10.1109/SPAWC.2018.8445994,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053481823&doi=10.1109%2fSPAWC.2018.8445994&partnerID=40&md5=9e8e837e9620aabedee30e3c95cf72e4,"The identification of useful temporal dependence structure in discrete time series data is an important component of algorithms applied to many tasks in statistical inference and machine learning, and used in a wide variety of problems across the spectrum of biological studies. Most of the early statistical approaches were ineffective in practice, because the amount of data required for reliable modelling grew exponentially with memory length. On the other hand, many of the more modern methodological approaches that make use of more flexible and parsimonious models result in algorithms that do not scale well and are computationally ineffective for larger data sets. In this paper we describe a class of novel methodological tools for effective Bayesian inference for general discrete time series, motivated primarily by questions regarding data originating from studies in genetics and neuroscience. Our starting point is the development of a rich class of Bayesian hierarchical models for variable-memory Markov chains. The particular prior structure we adopt makes it possible to design effective, linear-time algorithms that can compute most of the important features of the relevant posterior and predictive distributions without resorting to Markov chain Monte Carlo simulation. The origin of some of these algorithms can be traced to the family of Context Tree Weighting (CTW) algorithms developed for data compression since the mid-1990s. We have used the resulting methodological tools in numerous application-specific tasks (including prediction, segmentation, classification, anomaly detection, entropy estimation, and causality testing) on data from different areas of application. The results obtained compare quite favourably with those obtained using earlier approaches, such as Probabilistic Suffix Trees (PST), Variable-Length Markov Chains (VLMC), and the class of Markov Transition Distributions (MTD). © 2018 IEEE."
,10.1109/ICCSEC.2017.8446846,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053889436&doi=10.1109%2fICCSEC.2017.8446846&partnerID=40&md5=5d31675102b8e80b3242a53bb46ecdea,"In this paper, a particle filter is designed to deal with a class of nonlinear systems where the measurements undergo multi-step random delay due to the limited transmission capability of data link. First, a discrete-time variable governed by a Markov chain is defined to describe the measurement random delay. Considering the target maneuver, the multi-model method is adopted where another discrete-time variable is introduced to model the system as a jump Markov system. Second, the two newly defined variables are used to augment the original state vector, and then a hybrid system containing two kinds of discrete-time components is obtained. Finally, the hybrid system is estimated in the particle filtering framework and the simulation results show the proposed filter can effectively deal with the systems where the target maneuver and measurement delay occur simultaneously. © 2017 IEEE."
,10.1063/1.5030531,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049309835&doi=10.1063%2f1.5030531&partnerID=40&md5=c57b3369ab6d9274b2787c3d51dff654,"A configurational sampling algorithm based on nested layerings of Markov chains (Layered Nested Markov Chain Monte Carlo or L-NMCMC) is presented for simulations of systems characterized by rugged free energy landscapes. The layerings are generated using a set of auxiliary potential energy surfaces. The implementation of the method is demonstrated in the context of a rugged, two-dimensional potential energy surface. The versatility of the algorithm is next demonstrated on a simple, many-body system, namely, a canonical Lennard-Jones fluid in the liquid state. In that example, different layering schemes and auxiliary potentials are used, including variable cutoff distances and excluded-volume tempering. In addition to calculating a variety of properties of the system, it is also shown that L-NMCMC, when combined with a free-energy perturbation formalism, provides a straightforward means to construct approximate free-energy surfaces at no additional computational cost using the sampling distributions of each auxiliary Markov chain. The proposed L-NMCMC scheme is general in that it could be complementary to any number of methods that rely on sampling from a target distribution or methods that exploit a hierarchy of time scales and/or length scales through decomposition of the potential energy. © 2018 Author(s)."
,10.1063/1.5027001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051503769&doi=10.1063%2f1.5027001&partnerID=40&md5=e65ac7071dc6a9a1f231096f1b2bc884,"Markov State Model (MSM) has become a popular approach to study the conformational dynamics of complex biological systems in recent years. Built upon a large number of short molecular dynamics simulation trajectories, MSM is able to predict the long time scale dynamics of complex systems. However, to achieve Markovianity, an MSM often contains hundreds or thousands of states (microstates), hindering human interpretation of the underlying system mechanism. One way to reduce the number of states is to lump kinetically similar states together and thus coarse-grain the microstates into macrostates. In this work, we introduce a probabilistic lumping algorithm, the Gibbs lumping algorithm, to assign a probability to any given kinetic lumping using the Bayesian inference. In our algorithm, the transitions among kinetically distinct macrostates are modeled by Poisson processes, which will well reflect the separation of time scales in the underlying free energy landscape of biomolecules. Furthermore, to facilitate the search for the optimal kinetic lumping (i.e., the lumped model with the highest probability), a Gibbs sampling algorithm is introduced. To demonstrate the power of our new method, we apply it to three systems: a 2D potential, alanine dipeptide, and a WW protein domain. In comparison with six other popular lumping algorithms, we show that our method can persistently produce the lumped macrostate model with the highest probability as well as the largest metastability. We anticipate that our Gibbs lumping algorithm holds great promise to be widely applied to investigate conformational changes in biological macromolecules. © 2018 Author(s)."
,10.1109/ISEMA.2018.8442313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053493575&doi=10.1109%2fISEMA.2018.8442313&partnerID=40&md5=4547fe1dab8057d15046dfb55d88e07b,"Non-contact measurement of heterogeneous moisture content is constrained by inconsistencies in the probing signal. As the first step in a new approach using a non-diffracting probing signal, we derive a robust Markov Chain Monte Carlo formulation to determine the antenna patch signals to generate a microwave Bessel beam. We show solutions that provide a set of robust driving signals for a well collimated beam with high SNR over a region of 0.5-3 m, easily sufficient for proximal sensing of moisture content. © 2018 IEEE."
,10.1109/TVCG.2018.2866436,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052653479&doi=10.1109%2fTVCG.2018.2866436&partnerID=40&md5=73543f481087dee17f6476c26255458c,"The arrangement of objects into a layout can be challenging for non-experts, as is affirmed by the existence of interior design professionals. Recent research into the automation of this task has yielded methods that can synthesize layouts of objects respecting aesthetic and functional constraints that are non-linear and competing. These methods usually adopt a stochastic optimization scheme, which samples from different layout configurations, a process that is slow and inefficient. We introduce an physics-motivated, continuous layout synthesis technique, which results in a significant gain in speed and is readily scalable. We demonstrate our method on a variety of examples and show that it achieves results similar to conventional layout synthesis based on Markov chain Monte Carlo (McMC) state-search, but is faster by at least an order of magnitude and can handle layouts of unprecedented size as well as tightly-packed layouts that can overwhelm McMC. IEEE"
,10.3847/1538-4357/aad236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052396012&doi=10.3847%2f1538-4357%2faad236&partnerID=40&md5=f874d56b365efccf23dff1fe4331a619,"We investigate the dark matter density profile of the massive elliptical galaxy, NGC 1407, by constructing spherically symmetric Jeans models of its field star and globular cluster systems. Two major challenges in such models are the degeneracy between the stellar mass and the dark matter halo profiles, and the degeneracy between the orbital anisotropy of the tracer population and the total mass causing the observed motions. We address the first issue by using new measurements of the mass-to-light ratio profile from stellar population constraints that include a radially varying initial mass function. To mitigate the mass-anisotropy degeneracy, we make use of multiple kinematic tracers, including two subpopulations of globular clusters in addition to the galaxys field stars. We create a hierarchical Bayesian model that addresses several often-neglected systematic uncertainties, such as the statistical weight given to various data sets and the adopted distance. After sampling the posterior probability distribution with a Markov chain Monte Carlo method, we find evidence for a central cusp with a log slope of γ = 1.0-0.4 +0.2 (stat) -0.5 0.3 (sys), with the quantified systematic uncertainty dominated by choice of anisotropy profile. This is lower than expected for dark matter halos that have undergone adiabatic contraction, supporting inferences from gravitational lensing that some process has suppressed the steepening of halos in massive galaxies. We also confirm radially biased orbits for the metal-rich globular clusters and tangentially biased orbits for the metal-poor globular clusters, which remains a puzzling finding for an accretion-dominated halo. © 2018. The American Astronomical Society. All rights reserved."
,10.1080/08912963.2017.1336620,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020696626&doi=10.1080%2f08912963.2017.1336620&partnerID=40&md5=c8cb812a3dfdb7324498769d38ca3cbd,"Faunal skeletal profiles from archaeological assemblages have been long analysed regarding differential transport of carcasses to infer hunting preferences, human mobility, or even dietary stress. However, the existence of several possible accumulating agents, together with the effect of bone attrition, is known to introduce a potential bias, thus hindering the possibilities of meaningful concussions. In order to overcome this problem, several methods were proposed during the late 90s and early 2000’s, although a consensus was not reached, mainly because the different approaches were based on a certain initial hypothesis that significantly affected the output. Building on that previous experience, a new methodological framework is proposed and compared here. Moving from rather deterministic techniques, a Bayesian alternative approach based on a Monte Carlo Markov Chain sampling is presented and applied to several ethnographic and Pleistocene key sites. This new method makes use of the available information to constrain the possible degrees of attrition and carcass processing strategies, leading to easily comparable results. © 2017, © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1109/PMAPS.2018.8440277,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053155645&doi=10.1109%2fPMAPS.2018.8440277&partnerID=40&md5=beedfd3ba8241c72dcfcf9a3e6145fe0,"In recent years, studies on Natural Inflow Energy (NIE) synthetic scenario simulations have resulted in new methodological proposals. Such developments often assume Gaussianity in the residues; thus, making it possible to transform the data into a parametric distribution. It was noticed that, in most real cases in the Brazilian Electric Sector, the noise cannot be treated thus, since it presents intrinsically skewed tail behaviors that are challenging for the National Interconnected System's operational planning to reproduce. Thus, this work proposes a nonparametric approach to simulate and sample the NIE-series residues using the Markov chain Monte Carlo technique and the Kernel Density Estimation; hence, it is possible to simulate synthetic NIE scenarios. The presented results show that the proposed methodology is a good alternative to the current model. © 2018 IEEE."
,10.1109/PMAPS.2018.8440564,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053122548&doi=10.1109%2fPMAPS.2018.8440564&partnerID=40&md5=ea3dcfd10ba485107882ddb6e75cc4f6,"A great deal of literature examines economic dispatch from the perspective of a grid operator, assuming full knowledge of generating unit costs and constraints, network topology, line capacities, etc. But others without access to the complete, data-intensive model may also wish to predict unit dispatch under various scenarios. This paper develops a Bayesian approach to predicting future economic dispatch, which relies only on historical dispatch observations, as well as general assumptions of operating costs. The approach uses a Markov-chain Monte Carlo method to create an ensemble of network-free models, which capture fundamental properties of economic dispatch (like merit order and marginal generator behavior) under particular 'regimes' of grid operation (a particular range of load, set of congested lines, set of committed units, etc.). It then uses Bayesian averaging over many of these simple models to create an ensemble model, which approximates economic dispatch under general operating conditions. A case study using data from New York is used to verify the ensemble model. © 2018 IEEE."
,10.1109/PMAPS.2018.8440239,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053110910&doi=10.1109%2fPMAPS.2018.8440239&partnerID=40&md5=7f2d0d1e4e70b0b2e599f9aab0810932,"This paper presents an algorithm that employs a Sequential Monte Carlo Simulation (SMCS), to estimate operational states of components connected to a grid. Then, by the use of an Accelerated Quantum Particle Swarm Optimization (AQPSO), the algorithm determines the optimum size and location of Static Var Compensators (SVCs). The approach maximizes the level of reliability of the smart grid, which is subject to voltage regulation. The specific contribution of the paper is that it presents the impact of the integration of SVC over the system reliability that leads to a comprehensive composite system adequacy evaluation for a smart grid environment. © 2018 IEEE."
,10.1109/TSP.2018.2847660,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048639281&doi=10.1109%2fTSP.2018.2847660&partnerID=40&md5=b9faf3b8c74831635bfbd7343a10adbd,"Missing values can be an impediment to designing and applying classifiers. Missing values are common in biomedical studies due to various reasons, including missing tests or complex profiling technologies for different omics measurements in modern biomedicine. Many procedures have been proposed to impute values that are missing. This paper considers missing feature values in the context of optimal Bayesian classification, which selects a classifier that minimizes the expected error with respect to the posterior distribution governing an uncertainty class of feature-label distributions. The missing-value problem fits neatly into the overall framework of optimal Bayesian classification by marginalizing out the missing-value process from the feature-label distribution, and then updating the prior distribution of class-conditional parameters to posterior distributions using new observations. Generally, an optimal Bayesian classifier is defined via the effective class-conditional densities, which are averages of the parameterized feature-label distributions in the uncertainty class relative to the posterior distribution. Hence, once the posterior distribution incorporating the missing value process is found, the optimal Bayesian classifier pertaining to the features with missing values can be derived from the corresponding effective class-conditional densities. This paper presents the general theory, derives a closed-form decision rule for the optimal Bayesian classifier in a Gaussian model with independent features, and utilizes Hamiltonian Monte Carlo for the Gaussian model with arbitrary covariance matrices. Superior performance is demonstrated when compared to linear discriminant analysis, quadratic discriminant analysis, and support vector machines in conjunction with Gibbs sampling imputation using synthetic and real-world omics data. © 1991-2012 IEEE."
4,10.1016/j.nucengdes.2018.06.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048431095&doi=10.1016%2fj.nucengdes.2018.06.003&partnerID=40&md5=45745f7a17d54ade332589af2680937b,"Inverse Uncertainty Quantification (UQ) is a process to quantify the uncertainties in random input parameters while achieving consistency between code simulations and physical observations. In this paper, we performed inverse UQ using an improved modular Bayesian approach based on Gaussian Process (GP) for TRACE physical model parameters using the BWR Full-size Fine-Mesh Bundle Tests (BFBT) benchmark steady-state void fraction data. The model discrepancy is described with a GP emulator. Numerical tests have demonstrated that such treatment of model discrepancy can avoid over-fitting. Furthermore, we constructed a fast-running and accurate GP emulator to replace TRACE full model during Markov Chain Monte Carlo (MCMC) sampling. The computational cost was demonstrated to be reduced by several orders of magnitude. A sequential approach was also developed for efficient test source allocation (TSA) for inverse UQ and validation. This sequential TSA methodology first selects experimental tests for validation that has a full coverage of the test domain to avoid extrapolation of model discrepancy term when evaluated at input setting of tests for inverse UQ. Then it selects tests that tend to reside in the unfilled zones of the test domain for inverse UQ, so that one can extract the most information for posterior probability distributions of calibration parameters using only a relatively small number of tests. This research addresses the “lack of input uncertainty information” issue for TRACE physical input parameters, which was usually ignored or described using expert opinion or user self-assessment in previous work. The resulting posterior probability distributions of TRACE parameters can be used in future uncertainty, sensitivity and validation studies of TRACE code for nuclear reactor system design and safety analysis. © 2018 Elsevier B.V."
,10.1103/PhysRevD.98.043008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052642162&doi=10.1103%2fPhysRevD.98.043008&partnerID=40&md5=19df0ff8a1a4d9c8afa4123850179f13,"In the mHz gravitational-wave band, galactic ultracompact binaries (UCBs) are continuous sources emitting at near-constant frequency. The signals from many of these galactic binaries will be sufficiently strong to be detectable by the Laser Interferometer Space Antenna (LISA) after ∼O(1 week) of observing. In addition to their astrophysical value, these UCBs can be used to monitor the data quality of the observatory. This paper demonstrates the capabilities of galactic UCBs to be used as calibration sources for LISA by demanding signal coherence between adjacent week-long data segments separated by a gap in time of a priori unknown duration. A parameter for the gap duration is added to the UCB waveform model and used in a Markov-chain Monte Carlo algorithm simultaneously fitting for the astrophysical source parameters. Results from measurements of several UCBs are combined to produce a joint posterior on the gap duration. The measurement accuracy's dependence on how much is known about the UCBs through prior observing, and seasonal variations due to the LISA orbital motion, is quantified. The duration of data gaps in a two-week segment of data can be constrained to within ∼0.2 s using O(10) UCBs after one month of observing. The timing accuracy from UCBs improves to 0.1 s after 1 year of mission operations. These results are robust to within a factor of ∼2 when taking into account seasonal variations. © 2018 us. Published by the American Physical Society."
,10.1063/1.5029566,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051468466&doi=10.1063%2f1.5029566&partnerID=40&md5=1cebb97e13b72a6f2b86fd30f2501b88,"The pruned-enriched Rosenbluth method (PERM) is a popular and powerful Monte-Carlo technique for sampling flexible chain polymers of substantial length. In its original form, however, the method cannot be applied in Markov-chain Monte-Carlo schemes, which has rendered PERM unsuited for systems that consist of many chains. The current work builds on the configurational-bias Monte-Carlo (CBMC) method. The growth of a large set of trial configurations in each move is governed by simultaneous pruning and enrichment events, which tend to replace configurations with a low statistical weight by clones of stronger configurations. In simulations of dense brushes of flexible chains, a gain in efficiency of at least three orders of magnitude is observed with respect to CBMC and one order of magnitude with respect to recoil-growth approaches. Moreover, meaningful statistics can be collected from all trial configurations through the so-called “waste-recycling” Monte Carlo scheme. © 2018 Author(s)."
,10.1063/1.5036638,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051441946&doi=10.1063%2f1.5036638&partnerID=40&md5=4aac423736d03d1a4c3a1f2b07d44420,"We apply the irreversible event-chain Monte Carlo (ECMC) algorithm to the simulation of dense all-atom systems with long-range Coulomb interactions. ECMC is event-driven and exactly samples the Boltzmann distribution. It neither uses time-step approximations nor spatial cutoffs on the range of the interaction potentials. Most importantly, it need not evaluate the total Coulomb potential and thus circumvents the major computational bottleneck of traditional approaches. It only requires the derivatives of the two-particle Coulomb potential, for which we discuss mutually consistent choices. ECMC breaks up the total interaction potential into factors. For particle systems made up of neutral dipolar molecules, we demonstrate the superior performance of dipole-dipole factors that do not decompose the Coulomb potential beyond the two-molecule level. We demonstrate that these long-range factors can nevertheless lead to local lifting schemes, where subsequently moved particles are mostly close to each other. For the simple point-charge water model with flexible molecules (SPC/Fw), which combines the long-ranged intermolecular Coulomb potential with hydrogen-oxygen bond-length vibrations, a flexible hydrogen-oxygen-hydrogen bond angle, and Lennard-Jones oxygen-oxygen potentials, we break up the potential into factors containing between two and six particles. For this all-atom liquid-water model, we demonstrate that the computational complexity of ECMC scales very well with the system size. This is achieved in a pure particle-particle framework, without the interpolating mesh required for the efficient implementation of other modern Coulomb algorithms. Finally, we discuss prospects and challenges for ECMC and outline several future applications. © 2018 Author(s)."
,10.1109/TITS.2018.2852493,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051645543&doi=10.1109%2fTITS.2018.2852493&partnerID=40&md5=2da811460e143d9a994336c64eb3f179,"In the context of public transport modeling and simulation, we address the problem of mismatch between simulated transit trips and observed ones. We point to the weakness of the current travel demand modeling process; the trips it generates are overly optimistic and do not reflect the real passenger choices. To explain the deviation of simulated trips from the observed trips, we introduce the notion of mini-activities the travelers do during the trips. We propose to mine the smart card data and identify characteristics that help detect the mini activities. We develop a technique to integrate them in the generated trips and learn such an integration from two available sources, the trip history and trip planner recommendations. For an input travel demand, we build a Markov chain over the trip collection and apply the Monte Carlo Markov Chain algorithm to integrate mini activities in such a way that the trip characteristics converge to the target distributions. We test our method on the trip data set collected in Nancy, France. The evaluation results demonstrate a very important reduction of the trip generation error, and a good capacity to cope with new simulation scenarios. IEEE"
,10.1080/00949655.2018.1462813,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045629427&doi=10.1080%2f00949655.2018.1462813&partnerID=40&md5=1948a185006668e13fe04dfee0046264,"The existing studies on spatial dynamic panel data model (SDPDM) mainly focus on the normality assumption of response variables and random effects. This assumption may be inappropriate in some applications. This paper proposes a new SDPDM by assuming that response variables and random effects follow the multivariate skew-normal distribution. A Markov chain Monte Carlo algorithm is developed to evaluate Bayesian estimates of unknown parameters and random effects in skew-normal SDPDM by combining the Gibbs sampler and the Metropolis–Hastings algorithm. A Bayesian local influence analysis method is developed to simultaneously assess the effect of minor perturbations to the data, priors and sampling distributions. Simulation studies are conducted to investigate the finite-sample performance of the proposed methodologies. An example is illustrated by the proposed methodologies. © 2018 Informa UK Limited, trading as Taylor & Francis Group."
1,10.1108/IJLM-02-2017-0047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048364490&doi=10.1108%2fIJLM-02-2017-0047&partnerID=40&md5=7260aef16ed21ae621f597454bc1372a,"Purpose: This study proposes the use of cumulative prospect theory (CPT) to predict over- and under-estimation of risks and the counteractive adjustment in a cold chain context. In particular, the purpose of this paper is to address the importance of the socio-demographic characteristics of an individual in influencing risk attitude and the analysis of measurable risk probability. Design/methodology/approach: This study uses CPT as the basis to develop a decision analysis model in which the two functions of value editing and probability weighting are nonlinear to adequately determine the flexible risk attitudes of individuals, as well as their prospects with numerous outcomes and different probabilities. An experiment was conducted to obtain empirical predictions, and an efficient Markov Chain Monte Carlo algorithm was applied to overcome the nonlinearity and dimensionality in the process of parameter estimation. Findings: The respondents overweigh the minor cold chain risks with small probabilities and behave in a risk-averse manner, while underweighting major events with larger ones, thereby leading to risk-seeking behavior. Judgment distortion regarding probability was observed under risk decision with a low probability and a high impact. Moreover, the findings indicate that factors, such as gender, job familiarity and confidentiality significantly influence the risk attitudes and subjective probability weighting of the respondents. Research limitations/implications: The findings fit the framework of CPT and extend this theory to deal with human risk attitudes and subjective bias in cold chains. In particular, this study enhances the literature by providing an analysis of cold chain risk from both the human decision-making and managerial perspectives. Moreover, this research determined the importance of the socio-demographic characteristics of an individual to explain the variability in risk attitudes and responses. Practical implications: Managers must consider the issues of flexible risk attitude and subjective judgment when making choices for risk mitigation strategies. Given the focus on counteractive adjustment for over- and under-estimated risk, firms could evaluate cold chain risk more accurately, and thereby enhance their resilience to risky events while reducing the variability of their performance. Originality/value: The current study is the first to materialize the phenomena of over- and under-estimation of cold chain risks, as well as to emphasize the different characteristics for loss aversion and judgment distortion at the individual level. © 2018, Emerald Publishing Limited."
,10.1080/00949655.2018.1458310,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045029793&doi=10.1080%2f00949655.2018.1458310&partnerID=40&md5=aa35aeb9c4295f5827f6db633d7205ad,"In this paper, we consider Marshall–Olkin extended exponential (MOEE) distribution which is capable of modelling various shapes of failure rates and aging criteria. The purpose of this paper is three fold. First, we derive the maximum likelihood estimators of the unknown parameters and the observed the Fisher information matrix from progressively type-II censored data. Next, the Bayes estimates are evaluated by applying Lindley’s approximation method and Markov Chain Monte Carlo method under the squared error loss function. We have performed a simulation study in order to compare the proposed Bayes estimators with the maximum likelihood estimators. We also compute 95% asymptotic confidence interval and symmetric credible interval along with the coverage probability. Third, we consider one-sample and two-sample prediction problems based on the observed sample and provide appropriate predictive intervals under classical as well as Bayesian framework. Finally, we analyse a real data set to illustrate the results derived. © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.5194/amt-11-4627-2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051359687&doi=10.5194%2famt-11-4627-2018&partnerID=40&md5=ebf06c07909a78fbb27f98113f07ac90,"A neural-network-based method, quantile regression neural networks (QRNNs), is proposed as a novel approach to estimating the a posteriori distribution of Bayesian remote sensing retrievals. The advantage of QRNNs over conventional neural network retrievals is that they learn to predict not only a single retrieval value but also the associated, case-specific uncertainties. In this study, the retrieval performance of QRNNs is characterized and compared to that of other state-of-the-art retrieval methods. A synthetic retrieval scenario is presented and used as a validation case for the application of QRNNs to Bayesian retrieval problems. The QRNN retrieval performance is evaluated against Markov chain Monte Carlo simulation and another Bayesian method based on Monte Carlo integration over a retrieval database. The scenario is also used to investigate how different hyperparameter configurations and training set sizes affect the retrieval performance. In the second part of the study, QRNNs are applied to the retrieval of cloud top pressure from observations by the Moderate Resolution Imaging Spectroradiometer (MODIS). It is shown that QRNNs are not only capable of achieving similar accuracy to standard neural network retrievals but also provide statistically consistent uncertainty estimates for non-Gaussian retrieval errors. The results presented in this work show that QRNNs are able to combine the flexibility and computational efficiency of the machine learning approach with the theoretically sound handling of uncertainties of the Bayesian framework. Together with this article, a Python implementation of QRNNs is released through a public repository to make the method available to the scientific community. © 2018 Author(s)."
,10.1109/TNNLS.2018.2855699,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051401712&doi=10.1109%2fTNNLS.2018.2855699&partnerID=40&md5=2889450a8e5335ab88799501226c8e28,"Learning a hidden Markov model (HMM) is typically based on the computation of a likelihood which is intractable due to a summation over all possible combinations of states and mixture components. This estimation is often tackled by a maximization strategy, which is known as the Baum-Welch algorithm. However, some drawbacks of this approach have led to the consideration of Bayesian methods that add a prior over the parameters in order to work with the posterior probability and the marginal likelihood. These approaches can lead to good models but to the cost of extremely long computations (e.g., Markov Chain Monte Carlo). More recently, variational Bayesian frameworks have been proposed as a Bayesian alternative that keeps the computation tractable and the approximation tight. It relies on the introduction of a prior over the parameters to be learned and on an approximation of the true posterior distribution. After proving good standing in the case of finite mixture models and discrete and Gaussian HMMs, we propose here to derive the equations of the variational learning of the Dirichlet mixture-based HMM, and to extend it to the generalized Dirichlet. The latter case presents several properties that make the estimation more accurate. We prove the validity of this approach within the context of unusual event detection in public areas using the University of California San Diego data sets. HMMs are trained over normal video sequences using the typical Baum-Welch approach versus the variational one. The variational learning leads to more accurate models for the detection and localization of anomaly, and the general HMM approach is shown to be versatile enough to handle the detection of various synthetically generated tampering events. IEEE"
,10.1080/00223131.2018.1445564,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043691744&doi=10.1080%2f00223131.2018.1445564&partnerID=40&md5=9afd27062a1683529fcd82fc524f1c3b,"In probabilistic risk assessment (PRA), an event tree (ET) methodology is widely used to quantify accident scenarios which result in core damage and fission products release. However, the current approach using the ET methodology is not applicable to evaluate dynamic characteristics of accident progression, when the accident progression is time-dependent and headings in the ET have inter-dependency between events. Thus, a dynamic approach of accident scenario quantification is necessary to evaluate more realistic PRA. This research addressed this need by developing a dynamic scenario quantification method for the level 2 PRA by coupling of Continuous Markov chain and Monte Carlo (CMMC) method and a plant thermal–hydraulic analysis code for a sodium-cooled fast reactor (SFR). The CMMC method is applied to protected loss of heat sink (PLOHS) accident of the SFR to analyze dynamic scenario quantifications. The coupling method requires heavy computational cost and it makes difficult to quantify the whole accident scenarios by comparing the results from existing plant state analysis codes. Thus, a meta-analysis coupling method is proposed to obtain dynamic scenario quantifications with reasonable computational cost. Also, a categorizing method is used to depict analytical results in a transparent manner. © 2018, © 2018 Atomic Energy Society of Japan. All rights reserved."
,10.1088/1757-899X/392/6/062107,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052389620&doi=10.1088%2f1757-899X%2f392%2f6%2f062107&partnerID=40&md5=6e1f172984b893bd0f136f94f4940a6c,"The calculation of flood quantiles and its uncertainty estimation are important subjects of hydraulic engineering planning and water resources management. In this study, the Bayesian theory is used to implement frequency analysis and uncertainty assessment of annual maximum flood series, the Generalized Extreme Value (GEV) distribution is considered as the flood frequency distribution line type, and the Markov chain Monte Carlo (MCMC) method based on Metropolis-Hastings algorithm is used to evaluate the GEV distribution parameters, then the posterior distributions of flood flow quantiles are used to calculate the point estimations and interval estimations of flood design values under different return periods. The results show that the fitting effect of the Bayesian MCMC method is the same as the maximum likelihood estimation (MLE), but the Bayesian MCMC more superior when the uncertainties were considered. Compared with the traditional methods of flood frequency analysis, the proposed Bayesian MCMC method provides not only the design flood estimated values, but also the confidence intervals of the estimated values. In addition, the lengths between upper confidence limits and estimated values are greater than the lower confidence limits and estimated values, this asymmetry is more realistic than the traditional methods such as the delta method, thus improve the reliability of flood frequency analysis. © Published under licence by IOP Publishing Ltd."
,10.1109/TITS.2018.2852726,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051049472&doi=10.1109%2fTITS.2018.2852726&partnerID=40&md5=9d76fe9fd4d295c78b0b10bcb779ddcb,"Public transport planners can predict passenger loads and levels of service by applying the prior knowledge about the transit network and using transit assignment models. The individual travel history data available from automated fare collection (AFC) systems bring the opportunity of understanding the individual's travel behavior, which is necessary to develop a transit assignment model. By combining the prior knowledge about the transit network with the AFC data, a transit assignment model can be calibrated. This paper proposes a Bayesian hierarchical model to estimate attributes of travel time components and to calibrate a transit assignment model. In this model, route choices are represented by a multinomial logit model, and its coefficients are estimated via a Markov chain Monte Carlo method. The proposed model is specified in two ways, and in order to consider travel time variability, it is assumed that travel time on links follows a gamma distribution. In the first specification, route choice variables and parameters are the same for all transit modes of bus, train, and ferry. In the second specification, mode-specific route choice variables and parameters are defined. In order to assess the model fitness, the root-mean-square error (RMSE) between each posterior estimate and the actual observation is computed. The lowest &#x0025;RMSE belongs to the third-model specification (at 15&#x0025;), which indicates its high predictive power. IEEE"
,10.1016/j.media.2018.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047641088&doi=10.1016%2fj.media.2018.05.007&partnerID=40&md5=9219cf229d482bbee975a51c58fb21ce,"Model personalization requires the estimation of patient-specific tissue properties in the form of model parameters from indirect and sparse measurement data. Moreover, a low-dimensional representation of the parameter space is needed, which often has a limited ability to reveal the underlying tissue heterogeneity. As a result, significant uncertainty can be associated with the estimated values of the model parameters which, if left unquantified, will lead to unknown variability in model outputs that will hinder their reliable clinical adoption. Probabilistic estimation of model parameters, however, remains an unresolved challenge. Direct Markov Chain Monte Carlo (MCMC) sampling of the posterior distribution function (pdf) of the parameters is infeasible because it involves repeated evaluations of the computationally expensive simulation model. To accelerate this inference, one popular approach is to construct a computationally efficient surrogate and sample from this approximation. However, by sampling from an approximation, efficiency is gained at the expense of sampling accuracy. In this paper, we address this issue by integrating surrogate modeling of the posterior pdf into accelerating the Metropolis-Hastings (MH) sampling of the exact posterior pdf. It is achieved by two main components: (1) construction of a Gaussian process (GP) surrogate of the exact posterior pdf by actively selecting training points that allow for a good global approximation accuracy with a focus on the regions of high posterior probability; and (2) use of the GP surrogate to improve the proposal distribution in MH sampling, in order to improve the acceptance rate. The presented framework is evaluated in its estimation of the local tissue excitability of a cardiac electrophysiological model in both synthetic data experiments and real data experiments. In addition, the obtained posterior distributions of model parameters are interpreted in relation to the factors contributing to parameter uncertainty, including different low-dimensional representations of the parameter space, parameter non-identifiability, and parameter correlations. © 2018 Elsevier B.V."
,10.3150/16-BEJ914,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041896923&doi=10.3150%2f16-BEJ914&partnerID=40&md5=c30aa9fe91fe6030097fbc5e05feb869,Markov chain Monte Carlo (MCMC) algorithms are used to estimate features of interest of a distribution. The Monte Carlo error in estimation has an asymptotic normal distribution whose multivariate nature has so far been ignored in the MCMC community. We present a class of multivariate spectral variance estimators for the asymptotic covariance matrix in the Markov chain central limit theorem and provide conditions for strong consistency. We examine the finite sample properties of the multivariate spectral variance estimators and its eigenvalues in the context of a vector autoregressive process of order 1. © 2018 ISI/BS.
,10.1007/s00024-018-1870-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051455477&doi=10.1007%2fs00024-018-1870-5&partnerID=40&md5=cc3d58310f57efb9cb97f43d3848f9bf,"We present the first inversion of magnetotelluric (MT) data using a Hamiltonian Monte Carlo algorithm. The inversion of MT data is an underdetermined problem which leads to an ensemble of feasible models for a given dataset. A standard approach in MT inversion is to perform a deterministic search for the single solution which is maximally smooth for a given data-fit threshold. An alternative approach is to use Markov Chain Monte Carlo (MCMC) methods, which have been used in MT inversion to explore the entire solution space and produce a suite of likely models. This approach has the advantage of assigning confidence to resistivity models, leading to better geological interpretations. Recent advances in MCMC techniques include the No-U-Turns Sampler (NUTS), an efficient and rapidly converging method which is based on Hamiltonian Monte Carlo. We have implemented a 1D MT inversion which uses the NUTS algorithm. Our model includes a fixed number of layers of variable thickness and resistivity, as well as probabilistic smoothing constraints which allow sharp and smooth transitions. We present the results of a synthetic study and show the accuracy of the technique, as well as the fast convergence, independence of starting models, and sampling efficiency. Finally, we test our technique on MT data collected from a site in Boulia, Queensland, Australia to show its utility in geological interpretation and ability to provide probabilistic estimates of features such as depth to basement. © 2018, Springer International Publishing AG, part of Springer Nature."
,10.1007/s00477-018-1571-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048805419&doi=10.1007%2fs00477-018-1571-8&partnerID=40&md5=599bd336377d8e09c276050a2d34d2d2,"In this study, we focus on a hydrogeological inverse problem specifically targeting monitoring soil moisture variations using tomographic ground penetrating radar (GPR) travel time data. Technical challenges exist in the inversion of GPR tomographic data for handling non-uniqueness, nonlinearity and high-dimensionality of unknowns. We have developed a new method for estimating soil moisture fields from crosshole GPR data. It uses a pilot-point method to provide a low-dimensional representation of the relative dielectric permittivity field of the soil, which is the primary object of inference: the field can be converted to soil moisture using a petrophysical model. We integrate a multi-chain Markov chain Monte Carlo (MCMC)–Bayesian inversion framework with the pilot point concept, a curved-ray GPR travel time model, and a sequential Gaussian simulation algorithm, for estimating the dielectric permittivity at pilot point locations distributed within the tomogram, as well as the corresponding geostatistical parameters (i.e., spatial correlation range). We infer the dielectric permittivity as a probability density function, thus capturing the uncertainty in the inference. The multi-chain MCMC enables addressing high-dimensional inverse problems as required in the inversion setup. The method is scalable in terms of number of chains and processors, and is useful for computationally demanding Bayesian model calibration in scientific and engineering problems. The proposed inversion approach can successfully approximate the posterior density distributions of the pilot points, and capture the true values. The computational efficiency, accuracy, and convergence behaviors of the inversion approach were also systematically evaluated, by comparing the inversion results obtained with different levels of noises in the observations, increased observational data, as well as increased number of pilot points. © 2018, The Author(s)."
,10.1016/j.petrol.2018.04.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045686572&doi=10.1016%2fj.petrol.2018.04.029&partnerID=40&md5=2c7f0be35c85f2f7516d8982be24af54,"History matching is a crucial step for reservoir simulation and for decision-making process in field development under uncertainty. The history-matching task is well known to be technically and computationally challenging. Several algorithms have been studied for more than a decade to assist history matching. One of the algorithms used is Markov chain Monte Carlo (MCMC), which is capable of providing accurate posterior probability density (PPD) of the history-matched realizations. While several researchers have applied and studied MCMC for assisted history matching (AHM) in many conventional reservoirs, only few studies have been performed on unconventional reservoirs. Since the difference in physics of the two reservoir types are important, it is worthwhile investigating the performance of AHM in unconventional reservoirs. For this purpose, we apply an AHM workflow using proxy-based MCMC on a shale oil well in Vaca Muerta formation to demonstrate application of the workflow and highlight the lessons learnt. The direct MCMC is also performed on the same field case to compare accuracy and efficiency of the first method. In this study, design of experiment (DOE) is used for selecting the most influential uncertain parameters before performing either of the two MCMC methods. It is found that the direct MCMC cannot find enough solutions to construct the statistically meaningful PPD in an efficient manner. By contrast, the proxy-based MCMC is less computationally demanding than the direct MCMC and efficient enough to construct the PPD. The tested workflow was then used to probabilistically forecast the cumulative oil and water production as well as the oil recovery factor for the Vaca Muerta well. © 2018 Elsevier B.V."
,10.4230/LIPIcs.APPROX-RANDOM.2018.36,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052436490&doi=10.4230%2fLIPIcs.APPROX-RANDOM.2018.36&partnerID=40&md5=d9af2f50420dce1ad57f1fc9dde239b6,"We consider the well-studied problem of uniformly sampling (bipartite) graphs with a given degree sequence, or equivalently, the uniform sampling of binary matrices with fixed row and column sums. In particular, we focus on Markov Chain Monte Carlo (MCMC) approaches, which proceed by making small changes that preserve the degree sequence to a given graph. Such Markov chains converge to the uniform distribution, but the challenge is to show that they do so quickly, i.e., that they are rapidly mixing. The standard example of this Markov chain approach for sampling bipartite graphs is the switch algorithm, that proceeds by locally switching two edges while preserving the degree sequence. The Curveball algorithm is a variation on this approach in which essentially multiple switches (trades) are performed simultaneously, with the goal of speeding up switch-based algorithms. Even though the Curveball algorithm is expected to mix faster than switch-based algorithms for many degree sequences, nothing is currently known about its mixing time. On the other hand, the switch algorithm has been proven to be rapidly mixing for several classes of degree sequences. In this work we present the first results regarding the mixing time of the Curveball algorithm. We give a theoretical comparison between the switch and Curveball algorithms in terms of their underlying Markov chains. As our main result, we show that the Curveball chain is rapidly mixing whenever a switch-based chain is rapidly mixing. We do this using a novel state space graph decomposition of the switch chain into Johnson graphs. This decomposition is of independent interest. © 2018 Aditya Bhaskara and Srivatsan Kumar."
,10.1109/TVT.2018.2822943,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051649424&doi=10.1109%2fTVT.2018.2822943&partnerID=40&md5=7672989592bc0600dd9ee7bd66c70435,"In this paper, the low-complexity soft-decision detectors of the orthogonal frequency division multiplexing with index modulation (OFDM-IM) in both single- and two-level coded modulation systems are studied. In the single-level coded OFDM-IM, the bitwise Markov chain Monte Carlo (b-MCMC) detector can overcome the drawback of the conventional symbolwise MCMC detector that the signal vectors obtained by Gibbs sampling are trapped in the activation pattern (AP) of the initial vector. By integrating with the randomized step further to alleviate the stalling problem, the randomized b-MCMC detector is proposed. In the two-level coded OFDM-IM, the framework of iterative multistage decoding (IMSD) with two component detectors for AP and symbol information, respectively, is employed. To reduce the complexity of IMSD, the efficient processing of the multiplicative interference in the component detectors is proposed. Finally, the effectiveness of the proposed detectors is verified by computer simulations. © 1967-2012 IEEE."
,10.1145/3186327,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053426775&doi=10.1145%2f3186327&partnerID=40&md5=a6fa11dd07be269012ec2e6f78690df8,"This article addresses statistical output analysis of transient simulations in the parallel computing environment with fixed computing time. Using parallel computing, most commonly used unbiased estimators based on the output sequence compromise. To rectify this issue, this article proposes an estimation procedure in the Bayesian framework. The proposed procedure is particularly useful when the computing time depends on the output value in each simulation replication. The effectiveness of our method is demonstrated through studies on queuing simulation and control chart simulation. © 2018 ACM."
,10.4230/LIPIcs.APPROX-RANDOM.2018.57,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052449379&doi=10.4230%2fLIPIcs.APPROX-RANDOM.2018.57&partnerID=40&md5=6c5d8ada57056bf57fa73e1c76212b26,"We consider the problem of sampling a proper k-coloring of a graph of maximal degree Δ uniformly at random. We describe a new Markov chain for sampling colorings, and show that it mixes rapidly on graphs of logarithmically bounded pathwidth if κ ≥ (1 + ϵ)Δ, for any ϵ > 0, using a hybrid paths argument. © 2018 Aditya Bhaskara and Srivatsan Kumar."
1,10.3150/16-BEJ911,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041908437&doi=10.3150%2f16-BEJ911&partnerID=40&md5=40e4a2f4cf0852ff7b1e8bed52f53b85,"We provide a general methodology for unbiased estimation for intractable stochastic models. We consider situations where the target distribution can be written as an appropriate limit of distributions, and where conventional approaches require truncation of such a representation leading to a systematic bias. For example, the target distribution might be representable as the L2-limit of a basis expansion in a suitable Hilbert space; or alternatively the distribution of interest might be representable as the weak limit of a sequence of random variables, as in MCMC. Our main motivation comes from infinite-dimensional models which can be parameterised in terms of a series expansion of basis functions (such as that given by a Karhunen-Loeve expansion). We introduce and analyse schemes for direct unbiased estimation along such an expansion. However, a substantial component of our paper is devoted to the study of MCMC schemes which, due to their infinite dimensionality, cannot be directly implemented, but which can be effectively estimated unbiasedly. For all our methods we give theory to justify the numerical stability for robust Monte Carlo implementation, and in some cases we illustrate using simulations. Interestingly the computational efficiency of our methods is usually comparable to simpler methods which are biased. Crucial to the effectiveness of our proposed methodology is the construction of appropriate couplings, many of which resonate strongly with the Monte Carlo constructions used in the coupling from the past algorithm. © 2018 ISI/BS."
,10.1016/j.csda.2018.02.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044112773&doi=10.1016%2fj.csda.2018.02.009&partnerID=40&md5=7434355e1e0ef0c394608ae14b61f50d,"Likelihood-free methods, such as approximate Bayesian computation, are powerful tools for practical inference problems with intractable likelihood functions. Markov chain Monte Carlo and sequential Monte Carlo variants of approximate Bayesian computation can be effective techniques for sampling posterior distributions in an approximate Bayesian computation setting. However, without careful consideration of convergence criteria and selection of proposal kernels, such methods can lead to very biased inference or computationally inefficient sampling. In contrast, rejection sampling for approximate Bayesian computation, despite being computationally intensive, results in independent, identically distributed samples from the approximated posterior. An alternative method is proposed for the acceleration of likelihood-free Bayesian inference that applies multilevel Monte Carlo variance reduction techniques directly to rejection sampling. The resulting method retains the accuracy advantages of rejection sampling while significantly improving the computational efficiency. © 2018 Elsevier B.V."
,10.1016/j.scitotenv.2018.02.302,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043333827&doi=10.1016%2fj.scitotenv.2018.02.302&partnerID=40&md5=eb222a8d2078722f09981e45031f3c5c,"Spatial modelling of environmental data commonly only considers spatial variability as the single source of uncertainty. In reality however, the measurement errors should also be accounted for. In recent years, infrared spectroscopy has been shown to offer low cost, yet invaluable information needed for digital soil mapping at meaningful spatial scales for land management. However, spectrally inferred soil carbon data are known to be less accurate compared to laboratory analysed measurements. This study establishes a methodology to filter out the measurement error variability by incorporating the measurement error variance in the spatial covariance structure of the model. The study was carried out in the Lower Hunter Valley, New South Wales, Australia where a combination of laboratory measured, and vis-NIR and MIR inferred topsoil and subsoil soil carbon data are available. We investigated the applicability of residual maximum likelihood (REML) and Markov Chain Monte Carlo (MCMC) simulation methods to generate parameters of the Matérn covariance function directly from the data in the presence of measurement error. The results revealed that the measurement error can be effectively filtered-out through the proposed technique. When the measurement error was filtered from the data, the prediction variance almost halved, which ultimately yielded a greater certainty in spatial predictions of soil carbon. Further, the MCMC technique was successfully used to define the posterior distribution of measurement error. This is an important outcome, as the MCMC technique can be used to estimate the measurement error if it is not explicitly quantified. Although this study dealt with soil carbon data, this method is amenable for filtering the measurement error of any kind of continuous spatial environmental data. © 2018 Elsevier B.V."
,10.1007/s00158-018-1911-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042094124&doi=10.1007%2fs00158-018-1911-9&partnerID=40&md5=20ae92005f9e1383b32885d74a1cd69e,"For structural reliability analysis with time-consuming performance functions, an innovative design of experiment (DoE) strategy of the Kriging model is proposed, which is named as the stepwise accuracy-improvement strategy. The epistemic randomness of the performance value at any point provided by the Kriging model is used to derive an accuracy measure of the Kriging model. The basic idea of the proposed strategy is to enhance the accuracy of the Kriging model with the best next point that has the largest improvement with regard to the accuracy measure. An optimization problem is developed to define the best next point. The objective function is the expectation that quantifies how much an untried point could enhance the accuracy of the Kriging model. Markov chain Monte Carlo sampling and Gauss–Hermite quadrature are employed to make several approximations to solve the optimization problem and get the best next point. A structural reliability analysis method is also constructed based on the proposed strategy and the accuracy measure employed. Several examples are studied. The results validate the advantages of the proposed DoE strategy. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1016/j.ress.2018.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045273076&doi=10.1016%2fj.ress.2018.04.001&partnerID=40&md5=2bf9f67ad24a2dd8a644123b0519d0ba,"The hydraulic reliability and sensitivity analysis of large scale water distribution systems in presence of uncertainty is considered in this work. The assessment of the network reliability and sensitivity is performed by an efficient Markov chain Monte Carlo method, namely Subset simulation. Prescribed nodal heads of storage tanks, nodal demands and pipe roughness coefficients are modeled as uncertain parameters and described in a probabilistic manner. Failure is assumed to occur when the minimum nodal head in the network is lower than a minimum allowable value. The efficiency of the proposed method is demonstrated with the analysis of a real water distribution network consisting of a large number of nodes and pipes (of the order of thousands). The corresponding reliability problem represents a high dimensional problem. The approach gives an important insight into the performance, reliability and sensitivity of a class of complex utility networks. © 2018 Elsevier Ltd"
,10.1016/j.ast.2018.05.050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048742215&doi=10.1016%2fj.ast.2018.05.050&partnerID=40&md5=0642b09382ae7c1cfef7157cbe76562b,"The failure-probability-based global sensitivity measure can detect the effect of input variables on the structural failure probability, which can provide useful information in reliability-based design. In this paper, a new efficient simulation method is proposed to estimate the failure-probability-based global sensitivity measure. The proposed method is based on the Bayes’ theorem and importance sampling Markov chain simulation. The Bayes’ theorem is used to provide a single-loop simulation method and the importance sampling Markov chain simulation is used to further reduce the computational cost. Compared to the traditional double-loop Monte Carlo simulation method, the proposed method requires only a single set of samples to estimate the failure-probability-based global sensitivity measure and its computational cost does not depend on the dimensionality of input variables. Finally, one numerical example and two engineering examples are presented to illustrate the accuracy and efficiency of the proposed method. © 2018 Elsevier Masson SAS"
,10.3150/17-BEJ932,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041895827&doi=10.3150%2f17-BEJ932&partnerID=40&md5=11dda07b0e1d23a47b5e7d861a3f6f12,"This paper defines an approximation scheme for a solution of the Poisson equation of a geometrically ergodic Metropolis-Hastings chain . The scheme is based on the idea of weak approximation and gives rise to a natural sequence of control variates for the ergodic average Sk(F) = (1/k)k i=1 F(i), where F is the force function in the Poisson equation. The main results show that the sequence of the asymptotic variances (in the CLTs for the control-variate estimators) converges to zero and give a rate of this convergence. Numerical examples in the case of a double-well potential are discussed. © 2018 ISI/BS."
,10.1016/j.compgeo.2018.04.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045747493&doi=10.1016%2fj.compgeo.2018.04.006&partnerID=40&md5=01b1e7333818da9ed63c05b862d863cf,"Of late, various constitutive models have been proposed in the literature for the purpose of capturing the various complex physical mechanisms governing the creep behavior of soft soil. However, the more complex the model, the greater the number of associated uncertain parameters it has, and the less robustness it is. In this study, the Bayesian model class selection approach is applied to select the most plausible/suitable model describing the creep behavior of soft soil using laboratory measurements. In total, one elastic plastic (EP) model and eight elastic viscoplastic (EVP) models are investigated. To assess the performance of the different models in the prediction of creep behavior of soft soils, Bayesian model class selection is respectively performed using the oedometer test data from the intact samples of Vanttila clay and reconstituted samples of Hong Kong Marine Clay collected from the literature. All unknown model parameters are identified simultaneously by adopting the transitional Markov Chain Monte Carlo (TMCMC) method, and their uncertainty is quantified through the obtained posterior probability density functions (PDFs). The result shows that the proposed method is an excellent candidate for identifying the most plausible model and its associated parameters for different kinds of soft soils. The approach also provides uncertainty evaluation of the model prediction based on the given data. © 2018"
,10.1007/s11356-018-2409-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047956574&doi=10.1007%2fs11356-018-2409-0&partnerID=40&md5=e4c561a8e2e1d076bc9732a7b2234585,"Drought is the main abiotic stress seriously influencing wheat production. Information about the inheritance of drought tolerance is necessary to determine the most appropriate strategy to develop tolerant cultivars and populations. In this study, generation means analysis to identify the genetic effects controlling grain yield inheritance in water deficit and normal conditions was considered as a model selection problem in a Bayesian framework. Stochastic search variable selection (SSVS) was applied to identify the most important genetic effects and the best fitted models using different generations obtained from two crosses applying two water regimes in two growing seasons. The SSVS is used to evaluate the effect of each variable on the dependent variable via posterior variable inclusion probabilities. The model with the highest posterior probability is selected as the best model. In this study, the grain yield was controlled by the main effects (additive and non-additive effects) and epistatic. The results demonstrate that breeding methods such as recurrent selection and subsequent pedigree method and hybrid production can be useful to improve grain yield. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1002/env.2460,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026457410&doi=10.1002%2fenv.2460&partnerID=40&md5=10ef42f49c67e0e8c190dfd814e29ada,"The future behavior of the West Antarctic Ice Sheet (WAIS) may have a major impact on future climate. For instance, ice sheet melt may contribute significantly to global sea-level rise. Understanding the current state of the WAIS is therefore of great interest. The WAIS is drained by fast-flowing glaciers, which are major contributors to ice loss. Hence, understanding the stability and dynamics of glaciers is critical for predicting the future of the ice sheet. Glacier dynamics are driven by the interplay between the topography, temperature, and basal conditions beneath the ice. A glacier dynamics model describes the interactions between these processes. We develop a hierarchical Bayesian model that integrates multiple ice-sheet surface data sets with a glacier dynamics model. Our approach allows us to (a) infer important parameters describing the glacier dynamics, (b) learn about ice sheet thickness, and (c) account for errors in the observations and the model. Because we have relatively dense and accurate ice thickness data from the Thwaites Glacier in West Antarctica, we use these data to validate the proposed approach. The long-term goal of this work is to have a general model that may be used to study multiple glaciers in the Antarctic. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1016/j.csda.2018.03.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044919583&doi=10.1016%2fj.csda.2018.03.007&partnerID=40&md5=49a172b065c5684c445200c78f53d566,"Recent advances on overfitting Bayesian mixture models provide a solid and straightforward approach for inferring the underlying number of clusters and model parameters in heterogeneous datasets. The applicability of such a framework in clustering correlated high dimensional data is demonstrated. For this purpose an overfitting mixture of factor analyzers is introduced, assuming that the number of factors is fixed. A Markov chain Monte Carlo (MCMC) sampler combined with a prior parallel tempering scheme is used to estimate the posterior distribution of model parameters. The optimal number of factors is estimated using information criteria. Identifiability issues related to the label switching problem are dealt by post-processing the simulated MCMC sample by relabeling algorithms. The method is benchmarked against state-of-the-art software for maximum likelihood estimation of mixtures of factor analyzers using an extensive simulation study. Finally, the applicability of the method is illustrated in publicly available data. © 2018 Elsevier B.V."
,10.1111/rssc.12259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050252151&doi=10.1111%2frssc.12259&partnerID=40&md5=3ea9d8d3577ff90a8162bfe1db63f846,"Many data sets, especially from surveys, are made available to users with weights. Where the derivation of such weights is known, this information can often be incorporated in the user's substantive model (model of interest). When the derivation is unknown, the established procedure is to carry out a weighted analysis. However, with non-trivial proportions of missing data this is inefficient and may be biased when data are not missing at random. Bayesian approaches provide a natural approach for the imputation of missing data, but it is unclear how to handle the weights. We propose a weighted bootstrap Markov chain Monte Carlo algorithm for estimation and inference. A simulation study shows that it has good inferential properties. We illustrate its utility with an analysis of data from the Millennium Cohort Study. © 2018 Royal Statistical Society"
,10.1214/17-AAP1365,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052710300&doi=10.1214%2f17-AAP1365&partnerID=40&md5=45bf1684e049084390a4ec6a3231a231,"We introduce a Markov chain for sampling from the uniform distribution on a Riemannian manifold M, which we call the geodesic walk. We prove that the mixing time of this walk on any manifold with positive sectional curvature Cx (u, v) bounded both above and below by 0 &lt; m2 ≤ Cx (u, v) ≤ M2 &lt; ∞ is O∗ (M m2 2 ). In particular, this bound on the mixing time does not depend explicitly on the dimension of the manifold. In the special case that M is the boundary of a convex body, we give an explicit and computationally tractable algorithm for approximating the exact geodesic walk. As a consequence, we obtain an algorithm for sampling uniformly from the surface of a convex body that has running time bounded solely in terms of the curvature of the body. © Institute of Mathematical Statistics, 2018."
,10.1214/17-AAP1358,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052684601&doi=10.1214%2f17-AAP1358&partnerID=40&md5=4cfd12752c33e9870a85a42bfd087e88,"We show that the class of L2 functions for which ergodic averages of a reversible Markov chain have finite asymptotic variance is determined by the class of L2 functions for which ergodic averages of its associated jump chain have finite asymptotic variance. This allows us to characterize completely which ergodic averages have finite asymptotic variance when the Markov chain is an independence sampler. From a practical perspective, the most important result identifies a simple sufficient condition for all ergodic averages of L2 functions of the primary variable in a pseudo-marginal Markov chain to have finite asymptotic variance. © Institute of Mathematical Statistics, 2018."
,10.3390/e20080569,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052203710&doi=10.3390%2fe20080569&partnerID=40&md5=eed5ca09b01341c81711d26ccbab13f1,"Parameter estimation is one of the key technologies for system identification. The Bayesian parameter estimation algorithms are very important for identifying stochastic systems. In this paper, a random finite set based algorithm is proposed to overcome the disadvantages of the existing Bayesian parameter estimation algorithms. It can estimate the unknown parameters of the stochastic system which consists of a varying number of constituent elements by using the measurements disturbed by false detections, missed detections and noises. The models used for parameter estimation are constructed by using random finite set. Based on the proposed system model and measurement model, the key principles and formula derivation of the proposed algorithm are detailed. Then, the implementation of the algorithm is presented by using sequential Monte Carlo based Probability Hypothesis Density (PHD) filter and simulated tempering based importance sampling. Finally, the experiments of systematic errors estimation of multiple sensors are provided to prove the main advantages of the proposed algorithm. The sensitivity analysis is carried out to further study the mechanism of the algorithm. The experimental results verify the superiority of the proposed algorithm. © 2018 by the authors."
3,10.1029/2017JB015418,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051217570&doi=10.1029%2f2017JB015418&partnerID=40&md5=7c13cc6218251400a9ee4fb6e847c717,"This paper shows that imaging the interior of solid bodies with fully nonlinear physics can be highly beneficial compared to imaging with the equivalent linearized tomographic methods and that this is true for a variety of different types of physics. Including full nonlinearity provides interpretable uncertainties and far greater depth of image penetration into unknown targets such as the Earth's subsurface. We use an adaptively parameterized Monte Carlo method to invert electrical resistivity data for the conductivity structure of the Earth and demonstrate the method on two field data sets. Key results include the observation of directly interpretable uncertainty loops which define possible geometrical variations in the edges of isolated anomalies, hence quantifying the spatial resolution of these boundaries. These topologies of uncertainties are similar to those observed when performing fully nonlinear seismic traveltime tomography. This shows that loop-like uncertainty topologies are expected in the solutions to a wide variety of tomographic problems, using a variety of data types and hence laws of physics (here the Laplace equation; in previous work the Eikonal or ray equations). We also show that the depth to which we can construct a tomographic image using electrical data is extended by up to a factor of 8 using nonlinear methods compared to linearized inversion using common standard linearized programs. These advantages come at the cost of significantly increased computation. All of these results are illustrated on both synthetic and real data examples. ©2018. American Geophysical Union. All Rights Reserved."
,10.1177/0962280216682284,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049881536&doi=10.1177%2f0962280216682284&partnerID=40&md5=430737b9632ee128488a6728708c66ad,"Population-based cancer screening is often asked but hardly addressed by a question: “How many rounds of screening are required before identifying a cancer of interest staying in the pre-clinical detectable phase (PCDP)?” and also a similar one related to the number of screens required for stopping screening for the low risk group. It can be answered by using longitudinal follow-up data on repeated rounds of screen, namely periodic screen, but such kind of data are rather complicated and fraught with intractable statistical properties including correlated multistate outcomes, unobserved and incomplete (censoring or truncation) information, and imperfect measurements. We therefore developed a negative-binomial-family-based discrete-time stochastic process, taking sensitivity and specificity into account, to accommodate these thorny issues. The estimation of parameters was implemented with Bayesian Markov Chain Monte Carlo method. We demonstrated how to apply this proposed negative-binomial-family-based model to the empirical data similar to the Finnish breast cancer screening program. © 2016, The Author(s) 2016."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052928611&partnerID=40&md5=48e5e17754ea9290edc04ab6adc95226,"Modeling continuous-time physiological processes that manifest a patient’s evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike existing models, the HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning the HASMM parameters from the EHR data is achieved via a novel forward-filtering backward-sampling Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients’ clinical states in the reverse-time direction while conditioning on the future states. Real-time inferences are drawn via a forward-filtering algorithm that operates on a virtually constructed discrete-time embedded Markov chain that mirrors the patient’s continuous-time state trajectory. We demonstrate the prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center. In particular, we show that using HASMMs, a patient’s clinical deterioration can be predicted 8-9 hours prior to intensive care unit admission, with a 22% AUC gain compared to the Rothman index, which is the state-of-the-art critical care risk scoring technology. © 2018 Ahmed M. Alaa and Mihaela van der Schaar."
,10.1145/3161569,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053402202&doi=10.1145%2f3161569&partnerID=40&md5=f08fbaf92c8b886f06ad53cc45b6a991,"We introduce Path-ZVA: An efficient simulation technique for estimating the probability of reaching a rare goal state before a regeneration state in a (discrete-time) Markov chain. Standard Monte Carlo simulation techniques do not work well for rare events, so we use importance sampling; i.e., we change the probability measure governing the Markov chain such that transitions ""towards"" the goal state become more likely. To do this, we need an idea of distance to the goal state, so some level of knowledge of the Markov chain is required. In this article, we use graph analysis to obtain this knowledge. In particular, we focus on knowledge of the shortest paths (in terms of ""rare"" transitions) to the goal state. We show that only a subset of the (possibly huge) state space needs to be considered. This is effective when the high dependability of the system is primarily due to high component reliability, but less so when it is due to high redundancies. For several models, we compare our results to well-known importance sampling methods from the literature and demonstrate the large potential gains of our method. © 2018 ACM."
,10.1007/s00181-017-1409-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040019823&doi=10.1007%2fs00181-017-1409-0&partnerID=40&md5=7915ebb47b11ffc12f0f7b093f043b42,"This paper proposes a generalized spatial panel-data probit model with spatial autocorrelation of the dependent variable, the time-invariant individual shocks, and the remainder disturbances. It proposes its estimation with a Bayesian Markov chain Monte Carlo procedure. Simulation results show that the proposed estimation method performs well in small- to medium-sized samples. This method is then applied to the analysis of export-market participation of 1451 Chinese firms between 2002 and 2006 in the prefecture-level city of Wenzhou in the province of Zhejiang. Empirical results show that two of the three forms of the hypothesized spatial autocorrelation are significant, namely the spatial lag for the dependent variable and the time-invariant firm-specific shocks, but not the time-variant shocks. Ignoring any of these significant spatial effects would lead to misspecification. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1016/j.pnucene.2018.04.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046168452&doi=10.1016%2fj.pnucene.2018.04.015&partnerID=40&md5=601413cb4cb72271d480ab680ffe8606,"MCNPX is a general purpose Monte Carlo radiation transport code designed to track many particle types over broad ranges of energies and it has the potential to deal with Accelerator Driven System (ADS) problems. The neutronics design and analysis of ADS using MCNPX code is significantly complex mainly on constructing the three dimensional geometry model, especially when there is an additional spallation target coupled with the subcritical reactor constructed by abundant nested repeated structures in several levels. This modeling process has long been recognized as a time consuming, tedious and error-prone task, which is hard to master for novice users. Therefore, it is imperative to build a code system that can translate CAD models of ADS to the native language of MCNPX code. In this context of demand, a code system named CAD-PSMC (FreeCAD based parsing script for MCNPX code) has been developed to solve the ADS modeling conversion problems. In the framework of this code, hierarchical tree-based basic geometry classes and Boolean&Affine operations have been established with a mapping relationship to MCNPX code. Additionally, ray-casting technology and Markov chain based iteration method have been proposed to solve the problem of spline surfaces in complex geometries. Finally, the applicability and accuracy of CAD-PSMC code have been demonstrated by comparing with various reference models and numerical calculation results. © 2018 Elsevier Ltd"
1,10.5588/ijtld.17.0869,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049746027&doi=10.5588%2fijtld.17.0869&partnerID=40&md5=c9ef228bacfbbecad2b30e4e05fe5b94,"B A C K G R O U N D: Adverse drug reactions (ADRs) are common during standard, long-course treatment for multidrug-resistant and rifampicin-resistant tuberculosis (MDR-/RR-TB). In particular, second-line injectables (SLIs) are associated with permanent hearing loss, acute renal injury and electrolyte imbalance. We adapted an established Markov model for ambulatory treatment to estimate the impact of the toxicity profile on the incremental cost-effectiveness ratio (ICER) for a proposed MDR-/RR-TB regimen replacing the SLI with bedaquiline (BDQ). M E T H O D S: Treatment effectiveness was evaluated in disability-adjusted life-years (DALYs). Clinical outcomes and ingredient costs from a provider perspective were derived from the South African public-sector treatment program or extracted from the literature. Costs and effectiveness were discounted at 3% per year over 10 years. R E S U L T S: A BDQ-based MDR-/RR-TB regimen compared with the SLI regimen had a mean ICER of US$516 per DALY averted using the standard Markov model. Costs for both regimens increased and effectiveness decreased for the SLI regimen once adjusted for toxicity. The resulting ICER for the BDQ-based regimen was cost saving (US$96/patient) and more effective (0.96 DALYs averted) after adjusting for ADRs. C O N C L U S I O N: Decision-analysis models of treatment for MDR-/RR-TB, including new drug regimens, should consider the costs of managing ADRs and their sequelae. © 2018 The Union."
1,10.1016/j.csda.2018.02.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044477732&doi=10.1016%2fj.csda.2018.02.007&partnerID=40&md5=f3e54f179f0dee5609e060fbb5f3163a,"Bayesian methods for flexible time-to-event models usually rely on the theory of Markov chain Monte Carlo (MCMC) to sample from posterior distributions and perform statistical inference. These techniques are often plagued by several potential issues such as high posterior correlation between parameters, slow chain convergence and foremost a strong computational cost. A novel methodology is proposed to overcome the inconvenient facets intrinsic to MCMC sampling with the major advantage that posterior distributions of latent variables can rapidly be approximated with a high level of accuracy. This can be achieved by exploiting the synergy between Laplace's method for posterior approximations and P-splines, a flexible tool for nonparametric modeling. The methodology is developed in the class of cure survival models, a useful extension of standard time-to-event models where it is assumed that an unknown proportion of unidentified (cured) units will never experience the monitored event. An attractive feature of this new approach is that point estimators and credible intervals can be straightforwardly constructed even for complex functionals of latent model variables. The properties of the proposed methodology are evaluated using simulations and illustrated on two real datasets. The fast computational speed and accurate results suggest that the combination of P-splines and Laplace approximations can be considered as a serious competitor of MCMC to make inference in semi-parametric models, as illustrated on survival models with a cure fraction. © 2018 Elsevier B.V."
,10.1007/s11769-018-0975-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049948815&doi=10.1007%2fs11769-018-0975-1&partnerID=40&md5=2ab8e857658621c42881d265ac07d4ea,"In this article, we propose a novel, multilevel, dynamic factor model, to determine endogenously clustered regions for the investigation of regional clustering and synchronization of provincial business fluctuations in China. The parameter identification and model estimation was conducted using the Markov Chain Monte Carlo method. We then conducted an empirical study of the provincial business fluctuations in China (31 Chinese provinces are considered except Hong Kong, Macau, and Taiwan due to the data unavailability), which were sampled from January 2000 to December 2015. Our results indicated that these provinces could be clustered into four regions: leading, coincident, lagging, and overshooting. In comparison with traditional geographical divisions, this novel clustering into four regions enabled the regional business cycle synchronization to be more accurately captured. Within the four regional clusters it was possible to identify substantial heterogeneities among regional business cycle fluctuations, especially during the periods of the 2008 financial crisis and the ‘four-trillion economic stimulus plan’. © 2018, Science Press, Northeast Institute of Geography and Agricultural Ecology, CAS and Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1016/j.taap.2018.05.033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048476414&doi=10.1016%2fj.taap.2018.05.033&partnerID=40&md5=bcd189c8f6023060f13c7edd73a7b1ca,"Background: Perchloroethylene (perc) induced target organ toxicity has been associated with tissue-specific metabolic pathways. Previous physiologically-based pharmacokinetic (PBPK) modeling of perc accurately predicted oxidative metabolites but suggested the need to better characterize glutathione (GSH) conjugation as well as toxicokinetic uncertainty and variability. Objectives: We updated the previously published “harmonized” perc PBPK model in mice to better characterize GSH conjugation metabolism as well as the uncertainty and variability of perc toxicokinetics. Methods: The updated PBPK model includes expanded models for perc and its oxidative metabolite trichloroacetic acid (TCA), and physiologically-based sub-models for conjugative metabolites. Previously compiled mouse kinetic data in B6C3F1 and Swiss-Webster mice were augmented to include data from a recent study in male C57BL/6J mice that measured perc and metabolites in serum and multiple tissues. Hierarchical Bayesian population analysis using Markov chain Monte Carlo was conducted to characterize uncertainty and inter-strain variability in perc metabolism. Results: The updated model fit the data as well or better than the previously published “harmonized” PBPK model. Tissue dosimetry for both oxidative and conjugative metabolites was successfully predicted across the three strains of mice, with estimated residuals errors of 2-fold for majority of data. Inter-strain variability across three strains was evident for oxidative metabolism; GSH conjugation data were only available for one strain. Conclusions: This updated PBPK model fills a critical data gap in quantitative risk assessment by predicting the internal dosimetry of perc and its oxidative and GSH conjugation metabolites and lays the groundwork for future studies to better characterize toxicokinetic variability. © 2018 Elsevier Inc."
,10.1007/s10450-018-9958-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050796380&doi=10.1007%2fs10450-018-9958-x&partnerID=40&md5=eeb81a34c2dec015b4c01d215435a461,"This paper reports the results of an international interlaboratory study led by the National Institute of Standards and Technology (NIST) on the measurement of high-pressure surface excess carbon dioxide adsorption isotherms on NIST Reference Material RM 8852 (ammonium ZSM-5 zeolite), at 293.15 K (20 °C) from 1 kPa up to 4.5 MPa. Eleven laboratories participated in this exercise and, for the first time, high-pressure adsorption reference data are reported using a reference material. An empirical reference equation nex=d(1+exp[(-ln(P)+a)/b])c, [nex-surface excess uptake (mmol/g), P-equilibrium pressure (MPa), a = −6.22, b = 1.97, c = 4.73, and d = 3.87] along with the 95% uncertainty interval (Uk = 2 = 0.075 mmol/g) were determined for the reference isotherm using a Bayesian, Markov Chain Monte Carlo method. Together, this zeolitic reference material and the associated adsorption data provide a means for laboratories to test and validate high-pressure adsorption equipment and measurements. Recommendations are provided for measuring reliable high-pressure adsorption isotherms using this material, including activation procedures, data processing methods to determine surface excess uptake, and the appropriate equation of state to be used. © 2018, The Author(s)."
,10.1016/j.jhydrol.2018.06.043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049305594&doi=10.1016%2fj.jhydrol.2018.06.043&partnerID=40&md5=e7fe7becb7b0df3a015d92fcd1ed7438,"Statistical calibration of flow and transport models in unsaturated porous media is often carried out with Markov Chain Monte Carlo (MCMC) methods. However, the practicality of these methods is limited by their computational requirement, particularly when large prior intervals are assigned to the model parameters. In this work, a new operational strategy is investigated to alleviate the computational burden of MCMC samplers using results from a preliminary calibration performed with the First-Order Approximation (FOA) method. With the new strategy, the posterior distribution is approximated using a high-order Polynomial Chaos Expansion (PCE) surrogate model constructed over reduced parameter ranges. The latter are obtained from the 99.9 FOA confidence intervals. Two challenging test cases are investigated to assess efficiency and accuracy of the new strategy. The first test case considers estimation of flow and pesticide transport parameters from a synthetic infiltration experiment. The second test case deals with the assessment of unsaturated hydraulic soil parameters from a real-word laboratory drainage experiment. The results of the proposed strategy are compared to those of FOA, of the standard MCMC method and of an improved MCMC method in which the sampler is preconditioned with draws from the FOA posterior distribution. For both test cases, the new strategy provides accurate mean estimated parameter values and uncertainty regions and is much more efficient than the other MCMC methods. It is up to 50 times more efficient than the standard MCMC method. © 2018 Elsevier B.V."
,10.1007/s11239-018-1694-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048586866&doi=10.1007%2fs11239-018-1694-2&partnerID=40&md5=78ba45ce705d26579044c041c09b1d95,[No abstract available]
,10.1093/GJI/GGY163,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052529694&doi=10.1093%2fGJI%2fGGY163&partnerID=40&md5=8858b87d389a0b4eb17cd14d1e07f819,"We introduce a new Bayesian inversion method that estimates the spatial distribution of geological facies from attributes of seismic data, by showing how the usual probabilistic inverse problem can be solved using an optimization framework while still providing full probabilistic results. Our mathematical model consists of seismic attributes as observed data, which are assumed to have been generated by the geological facies. The method infers the post-inversion (posterior) probability density of the facies plus some other unknown model parameters, from both the seismic attributes and geological prior information. Most previous research in this domain is based on the localized likelihoods assumption, whereby the seismic attributes at a location are assumed to depend on the facies only at that location. Such an assumption is unrealistic because of imperfect seismic data acquisition and processing, and fundamental limitations of seismic imaging methods. In this paper, we relax this assumption: we allow probabilistic dependence between seismic attributes at a location and the facies in any neighbourhood of that location through a spatial filter. We term such likelihoods quasilocalized. Exact Bayesian inference is impractical because it requires normalization of the posterior distribution which is intractable for large models and must be approximated. Stochastic sampling (e.g. by using Markov chain Monte Carlo) is the most commonly used approximate inference method but it is computationally expensive and detection of its convergence is often subjective and unreliable.We use the variational Bayes method which is a more efficient alternative that offers reliable detection of convergence. It achieves this by replacing the intractable posterior distribution by a tractable approximation. Inference can then be performed using the approximate distribution in an optimization framework, thus circumventing the need for sampling, while still providing probabilistic results. We show in a noisy synthetic example that the new method recovered the coefficients of the spatial filter with reasonable accuracy, and recovered the correct facies distribution. We also show that our method is robust against weak prior information and non-localized likelihoods, and that it outperforms previous methods which require likelihoods to be localized. Our method is computationally efficient, and is expected to be applicable to 3-D models of realistic size on modern computers without incurring any significant computational limitations. © The Authors 2017."
,10.1016/j.jeconom.2018.01.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047229242&doi=10.1016%2fj.jeconom.2018.01.009&partnerID=40&md5=e7bfc3a46c7bb2527cb57457eaba20d8,"To draw inference on serial extremal dependence within heavy-tailed Markov chains, Drees et al., (2015) proposed nonparametric estimators of the spectral tail process. The methodology can be extended to the more general setting of a stationary, regularly varying time series. The large-sample distribution of the estimators is derived via empirical process theory for cluster functionals. The finite-sample performance of these estimators is evaluated via Monte Carlo simulations. Moreover, two different bootstrap schemes are employed which yield confidence intervals for the pre-asymptotic spectral tail process: the stationary bootstrap and the multiplier block bootstrap. The estimators are applied to stock price data to study the persistence of positive and negative shocks. © 2018"
,10.1016/j.electstud.2018.05.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047256789&doi=10.1016%2fj.electstud.2018.05.002&partnerID=40&md5=31cd4b52237dd27bb212f00053f66739,"Poll-based methods have been used to forecast elections in various countries and settings. A common factor of these models is the inclusion of a time-dependent variable to model the evolution of public support for a party or cause over time. Typically this variable is modeled as a random walk. This is problematic as the underlying assumptions of a random walk, which are very restrictive, don't hold empirically. We propose a more flexible alternative where the time-dependent variable is modeled using ideas from time series analysis. This approach is embedded in a generic model which allows for the inclusion of various covariates. We rely on Bayesian estimation techniques. Using Markov chain Monte Carlo algorithms, credible intervals can be obtained, and it is straightforward to include the uncertainty of the estimated parameters in forecasts. An application of the model to the German federal election of 2013 indicates the benefits of the new approach. © 2018 Elsevier Ltd"
,10.1016/j.spl.2018.02.059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045920064&doi=10.1016%2fj.spl.2018.02.059&partnerID=40&md5=9214ae04434538e3627294bda850a1d9,"This paper concerns with a generalized regime-switching GARCH model to capture dynamic behavior of volatility in financial market. Four-state Markov chain regime-switching is adopted with white noise, stationary, integrated and explosive states. We consider time-dependent transition probabilities of the Markov chain and derive time-dependent probability of each state under the assumption of conditional normality on the noise of the GARCH model. Multi-step ahead volatility is formulated and cumulative impulse response function, which is a measure of persistence in volatility, is discussed. A Monte-Carlo experiment shows the dynamics of the volatilities and time-dependent probabilities as well as the behaviors of the cumulative impulse response functions. © 2018 Elsevier B.V."
,10.1007/s40300-018-0141-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050038469&doi=10.1007%2fs40300-018-0141-7&partnerID=40&md5=cf519fb04f19c485d42ee2c28bc05ba9,"This paper presents a Bayesian approach, using differential evolution Markov chain method, to estimate the parameters of the failure time distribution and its percentiles based on grouped and non-grouped degradation data. The observed failure times are modeled by linear degradation path model with random degradation rates follow log-logistic distribution. Two Monte Carlo simulation studies are conducted. The first one is devoted to assess the performance of the proposed method with respect to the mean squared error (MSE) for different values of the scale and shape parameters of the degradation model using small, moderate and large sample sizes. The proposed method performs better when applied to non-grouped data compared with grouped data. The second simulation study is conducted to compare the proposed log-logistic model with a Weibull degradation model. More importantly, the log-logistic model outperforms the Weibull model. The proposed methods are demonstrated by modeling real life times of laser devices. © 2018, Sapienza Università di Roma."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052970495&partnerID=40&md5=a062dfe8b127bd750d71e5e4a9001f5b,"Graphical models with change-points are computationally challenging to fit, particularly in cases where the number of observation points and the number of nodes in the graph are large. Focusing on Gaussian graphical models, we introduce an approximate majorize-minimize (MM) algorithm that can be useful for computing change-points in large graphical models. The proposed algorithm is an order of magnitude faster than a brute force search. Under some regularity conditions on the data generating process, we show that with high probability, the algorithm converges to a value that is within statistical error of the true change-point. A fast implementation of the algorithm using Markov Chain Monte Carlo is also introduced. The performances of the proposed algorithms are evaluated on synthetic data sets and the algorithm is also used to analyze structural changes in the S&P 500 over the period 2000-2016. © 2018 Leland Bybee and Yves Atchadé."
1,10.3847/1538-4357/aaccfa,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051560186&doi=10.3847%2f1538-4357%2faaccfa&partnerID=40&md5=2be1da393007b3d9d7118d88967c80e0,"This paper uses the multi-epoch astrometry from the Wide-field Infrared Survey Explorer (WISE) to demonstrate a method to measure proper motions and trigonometric parallaxes with precisions of ∼4 mas yr-1 and ∼7 mas, respectively, for low-mass stars and brown dwarfs. This method relies on WISE single exposures (Level 1b frames) and a Markov Chain Monte Carlo method. The limitations of Gaia in observing very low-mass stars and brown dwarfs are discussed, and it is shown that WISE will be able to measure astrometry past the 95% completeness limit and magnitude limit of Gaia (L, T, and Y dwarfs fainter than G ≈ 19 and G = 21, respectively). This method is applied to WISE data of 20 nearby (≲17 pc) dwarfs with spectral types between M6-Y2 and previously measured trigonometric parallaxes. Also provided are WISE astrometric measurements for 23 additional low-mass dwarfs with spectral types between M6-T7 and estimated photometric distances &lt;17 pc. Only nine of these objects contain parallaxes within Gaia Data Release 2. © 2018. The American Astronomical Society."
,10.1002/env.2478,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031098823&doi=10.1002%2fenv.2478&partnerID=40&md5=01283d3abfb037b1b72dbeed81c67e2e,"Environmental health and disease mapping studies are often concerned with the evaluation of the combined effect of various sociodemographic and behavioral factors, and environmental exposures, on time-to-event outcomes of interest, such as death of individuals, organisms, or plants. In such studies, estimation of the hazard function is often of interest. In addition to the known explanatory variables, the hazard function may be subject to spatial/geographical variations, such that proximally located regions may experience hazards similar to those of regions that are distantly located. A popular approach for handling this type of spatially correlated time-to-event data is the Cox proportional hazards regression model with spatial frailties. However, the proportional hazards assumption poses a major practical challenge, as it entails that the effects of the various explanatory variables remain constant over time. This assumption is often unrealistic, for instance, in studies with long follow-ups where the effects of some exposures on the hazard may vary drastically over time. Our goal in this paper is to offer a flexible semiparametric additive hazards model with spatial frailties. Our proposed model allows both the frailties and the regression coefficients to be time varying, thus relaxing the proportionality assumption. Our estimation framework is Bayesian, powered by carefully tailored posterior sampling strategies via Markov chain Monte Carlo techniques. We apply the model to a data set on prostate cancer survival from the U.S. state of Louisiana to illustrate its advantages. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.1007/s11069-018-3291-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044580749&doi=10.1007%2fs11069-018-3291-2&partnerID=40&md5=39b176a03e26eef4b7a2289ed03a42b2,"In the context of climate change, it is essential to quantify the uncertainty for effective design and risk management practices. In the present study, we have accessed the climate model and flood return level uncertainties over a river basin. Six high-resolution global climate models (GCMs) with two Representative Concentration Pathways (RCPs) are used to project the future climate change impact on streamflow of Wainganga River basin. Uncertainty associated with the use of high-resolution multiple GCM is treated with reliability ensemble average (REA) followed by bias correction. The bias-corrected weighted outputs are used as input to variable infiltration capacity (VIC) model, a physically based hydrological model. Calibration and validation are carried out for the hydrological model, and the parameters of VIC are fixed through trial-and-error method. The uncertainty in flood return level associated with the future projected flows is dealt with the Bayesian analysis and modelled through Markov Chain Monte Carlo (MCMC) simulation technique using Metropolis–Hastings algorithm with the non-informative prior distribution. The study provides a robust framework, which will help in effective decision-making and adaptation strategies over the river basin. © 2018, Springer Science+Business Media B.V., part of Springer Nature."
,10.1029/2018JE005574,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053378606&doi=10.1029%2f2018JE005574&partnerID=40&md5=97220ba36889ce2ce3f387ef1f06b502,"Mars has lost a large fraction of its water to space, with the H component of this loss thought to occur mainly as a result of thermal (Jeans) escape from the upper atmosphere. Constraints on H loss have historically been made using hydrogen Lyman alpha (121.6 nm) light scattered in the planet's extended upper atmosphere or corona. Here we employ observations from the Mars Atmosphere and Volatile Evolution (MAVEN) mission's Imaging Ultraviolet Spectrograph (IUVS) to constrain H escape in December 2014 and August 2016, when MAVEN observed the dayside corona at low latitude. To obtain adequate fits and address systematic sources of uncertainty including instrument calibration, we fit in exobase number density and escape rate instead of density and temperature, employing Markov Chain Monte Carlo techniques. This produces better model fits to data than most previous analyses. When we assume a single population of H atoms, we obtain H temperatures inconsistent with expected trends and a shape mismatch between observed and modeled profiles, similar to previous studies. Introducing either a second population of H (at a distinct temperature and density) or adding deuterium to the corona allows for essentially perfect fits. Despite this model ambiguity, derived loss rates for both periods are within a factor of four, 3.3–8.8×108cm−2/s in December 2014 (Ls∼250) and 0.6–2.3×108cm−2/s in August 2016 (Ls∼200). These rates are similar to those found in prior studies and confirm the known seasonal trend—doing so while incorporating the substantial uncertainty in absolute calibration insufficiently explored by previous studies. ©2018. American Geophysical Union. All Rights Reserved."
,10.1371/journal.pone.0201892,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052837063&doi=10.1371%2fjournal.pone.0201892&partnerID=40&md5=f067a8ec3ad677efcb4acdd10b469fd3,"Measurements of energy balance components (energy intake, energy expenditure, changes in energy stores) are often plagued with measurement error. Doubly-labeled water can measure energy intake (EI) with negligible error, but is expensive and cumbersome. An alternative approach that is gaining popularity is to use the energy balance principle, by measuring energy expenditure (EE) and change in energy stores (ES) and then back-calculate EI. Gold standard methods for EE and ES exist and are known to give accurate measurements, albeit at a high cost. We propose a joint statistical model to assess the measurement error in cheaper, non-intrusive measures of EE and ES. We let the unknown true EE and ES for individuals be latent variables, and model them using a bivariate distribution. We try both a bivariate Normal as well as a Dirichlet Process Mixture Model, and compare the results via simulation. Our approach, is the first to account for the dependencies that exist in individuals’ daily EE and ES. We employ semiparametric regression with free knot splines for measurements with error, and linear components for error free covariates. We adopt a Bayesian approach to estimation and inference and use Reversible Jump Markov Chain Monte Carlo to generate draws from the posterior distribution. Based on the semipar-ameteric regression, we develop a calibration equation that adjusts a cheaper, less reliable estimate, closer to the true value. Along with this calibrated value, our method also gives credible intervals to assess uncertainty. A simulation study shows our calibration helps produce a more accurate estimate. Our approach compares favorably in terms of prediction to other commonly used models. This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication."
2,10.1214/17-AOS1597,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048035074&doi=10.1214%2f17-AOS1597&partnerID=40&md5=b57509473af27c881c0a8a16074e5bd8,"Consider a Bayesian situation in which we observe Y ∼ pθ, where θ ∈ , and we have a family {νh, h ∈ H} of potential prior distributions on . Let g be a real-valued function of θ, and let Ig(h) be the posterior expectation of g(θ) when the prior is νh. We are interested in two problems: (i) selecting a particular value of h, and (ii) estimating the family of posterior expectations {Ig(h), h ∈ H}. Let my(h) be the marginal likelihood of the hyperparameter h: my(h) = pθ(y)νh(dθ). The empirical Bayes estimate of h is, by definition, the value of h that maximizes my(h). It turns out that it is typically possible to use Markov chain Monte Carlo to form point estimates for my(h) and Ig(h) for each individual h in a continuum, and also confidence intervals for my(h) and Ig(h) that are valid pointwise. However, we are interested in forming estimates, with confidence statements, of the entire families of integrals {my(h), h ∈ H} and {Ig(h), h ∈ H}: we need estimates of the first family in order to carry out empirical Bayes inference, and we need estimates of the second family in order to do Bayesian sensitivity analysis. We establish strong consistency and functional central limit theorems for estimates of these families by using tools from empirical process theory. We give two applications, one to latent Dirichlet allocation, which is used in topic modeling, and the other is to a model for Bayesian variable selection in linear regression. © Institute of Mathematical Statistics, 2018"
,10.1371/journal.pone.0201209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053513036&doi=10.1371%2fjournal.pone.0201209&partnerID=40&md5=061a49fa6b3dd64062c456b19c1154d7,"Japanese Encephalitis (JE) is the most important cause of human encephalitis throughout Asia and the Pacific. Although JE is a vector-borne disease, it has been demonstrated experimentally that transmission between pigs can occur through direct contact. Whether pig-to-pig transmission plays a role in the natural epidemiological cycle of JE remains unknown. To assess whether direct transmission between pigs may occur under field conditions, we built two mathematical models of JE transmission incorporating vector-borne transmission alone or a combination of vector-borne and direct transmission. We used Markov Chain Monte Carlo (MCMC) techniques to estimate the parameters of the models. We fitted the models to (i) two serological datasets collected longitudinally from two pig cohorts (C1 and C2) during two periods of four months on a farm on the outskirts of Phnom-Penh, Cambodia and to (ii) a cross-sectional (CS) serological survey dataset collected from 505 swine coming from eight different provinces of Cambodia. In both cases, the model incorporating both vector-borne and direct transmission better explained the data. We computed the value of the basic reproduction number R0 (2.93 for C1, 2.66 for C2 and 2.27 for CS), as well as the vector-borne reproduction number Rpv and the direct transmission reproduction number Rpp. We then determined the contribution of direct transmission on R0 (11.90% for C1, 11.62% for C2 and 7.51% for CS). According to our results, the existence of pig-to-pig transmission is consistent with our swine serological data. Thus, direct transmission may contribute to the epidemiological cycle of JE in Cambodia. These results need to be confirmed in other eco-climatic settings, in particular in temperate areas where pig-to-pig transmission may facilitate the persistence of JE virus (JEV) during cold seasons when there are no or few mosquitoes. © 2018 Diallo et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
2,10.3997/1873-0604.2017065,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053739015&doi=10.3997%2f1873-0604.2017065&partnerID=40&md5=0aaf166c7b6cf341c810f7934dc5c4c3,"The induced polarization phenomenon, both in time domain and frequency domain, is often parameterised using the empirical Cole-Cole model. To improve the resolution of model parameters and to decrease the parameter correlations in the inversion process of induced polarization data, we suggest here three re-parameterisations of the Cole-Cole model, namely the maximum phase angle Cole-Cole model, the maximum imaginary conductivity Cole-Cole model, and the minimum imaginary resistivity Cole-Cole model. The maximum phase angle Cole-Cole model uses the maximum phase φmax and the inverse of the phase peak frequency, τφ, instead of the intrinsic charge-ability m0 and the time constant adopted in the classic Cole-Cole model. The maximum imaginary conductivity Cole-Cole model uses the maximum imaginary conductivity σmax ″ instead of m0 and the time constant τσ of the Cole-Cole model in its conductivity form. The minimum imaginary resistivity Cole-Cole model uses the minimum imaginary resistivity ρmin ″ instead of m0 and the time constant τρ of the Cole-Cole model in its resistivity form. The effects of the three re-parameterisations have been tested on synthetic timedomain and frequency-domain data using a Markov chain Monte Carlo inversion method, which allows for easy quantification of parameter uncertainty, and on field data using 2D gradient-based inversion. In comparison with the classic Cole-Cole model, it was found that for all the three re-parameterisations, the model parameters are less correlated with each other and, consequently, better resolved for both time-domain and frequency-domain data. The increase in model resolution is particularly significant for models that are poorly resolved using the classic Cole-Cole parameterisation, for instance, for low values of the frequency exponent or with low signal-to-noise ratio. In general, this leads to a significantly deeper depth of investigation for the φmax, σmax ″, and ρmin ″ parameters, when compared with the classic m0 parameter, which is shown with a field example. We believe that the use of reparameterisations for inverting field data will contribute to narrow the gap between induced polarization theory, laboratory findings, and field applications. © 2018 EAGE Publishing BV. All rights reserved."
,10.1371/journal.pone.0201872,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052061614&doi=10.1371%2fjournal.pone.0201872&partnerID=40&md5=f0548f78f574a6ad868e23343a1ee7fe,"In this paper, we propose a novel object tracking algorithm by using high-dimensional particle filter and combined features. Firstly, the refined two-dimensional principal component analysis and the tendency are combined to represent an object. Secondly, we present a framework using high-order Monte Carlo Markov Chain which considers more information and performs more discriminative and efficient on moving objects than the traditional first-order particle filtering. Finally, an advanced sequential importance resampling is applied to estimate the posterior density and obtains the high-quality particles. To further gain the better samples, K-means clustering is used to select more typical particles, which reduces the computational cost. Both qualitative and quantitative evaluations on challenging image sequences demonstrate that the performance of our proposed algorithm is superior to the state-of-the-art methods. © 2018 Liu et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1140/epjc/s10052-018-6101-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051739518&doi=10.1140%2fepjc%2fs10052-018-6101-9&partnerID=40&md5=6270e083f06048d3581f1630e0597d03,"An approximate dual representation for non-Abelian lattice gauge theories in terms of a new set of dynamical variables, the plaquette occupation numbers (PONs) that are natural numbers, is discussed. They are the expansion indices of the local series of the expansion of the Boltzmann factors for every plaquette of the Yang–Mills action. After studying the constraints due to gauge symmetry, the SU(2) gauge theory is solved using Monte Carlo simulations. For a PONs configuration the weight factor is given by Haar-measure integrals over all links whose integrands are products of powers of plaquettes. Herein, updates are limited to changes of the PON at a plaquette or all PONs on a coordinate plane. The Markov chain transition probabilities are computed employing truncated maximal trees and the Metropolis algorithm. The algorithm performance is investigated with different types of updates for the plaquette mean value over a large range of βs. Using a 12 4 lattice very good agreement with a conventional heath bath algorithm is found for the strong and weak coupling limits. Deviations from the latter being below 0.1% for 2.5 &lt; β&lt; 3. The mass of the lightest JPC= 0 + + glueball is evaluated and reproduces the results found in the literature. © 2018, The Author(s)."
,10.1109/COGSIMA.2018.8423984,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051469526&doi=10.1109%2fCOGSIMA.2018.8423984&partnerID=40&md5=3808e008e779fe834817b8ed74715ba7,"The distributed detection problem with consideration of correlated sensor observations is an NP-hard problem. In this paper, a heuristic Markov Chain Monte Carlo algorithm, which consists of methods of slice sampling and simulated annealing, is investigated to solve this problem. Based on the criterion of minimizing the probability of error, sub-optimal solutions including fusion rules and sensor decisions are acquired. The performance of this algorithm is studied with analysis of experimental results. © 2018 IEEE."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051542690&partnerID=40&md5=aaa97121b0a00d4b0f5059fde0bfb4f2,"The proceedings contain 29 papers. The topics discussed include: automating behaviour tree generation for simulating troop movements; situations, information, and evidence; situation granularity; automatic identification of maritime incidents from unstructured articles; adaptive situational leadership framework; high clutter, close range, Wi-Fi imaging and probabilistic, learning classifier; empirically identified gaps in a situation awareness model for human-machine coordination; a probabilistic time reversal theorem; a method to identify relevant information sufficient to answer situation dependent queries; situations in simulations: an initial appraisal; virtual leadership in complex multiorganizational research and development programs; modelling complex system-of-systems for creating situation awareness; the observer effect; ACT-R modeling of intelligence analysts' perception of information to improve situational understanding; artificial swarms find social optima; tactical decision support for UAV deployment in MUM-T helicopter missions; distributed detection of correlated signal using Markov chain Monte Carlo; cognitive support to promote shared mental models during safety-critical situations in cardiac surgery; and interactive decision support: a framework to improve diagnostic processes of cancerous diseases using Bayesian networks."
,10.1016/j.ecoenv.2018.03.044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044612933&doi=10.1016%2fj.ecoenv.2018.03.044&partnerID=40&md5=20a39538f6f3de35b14b86cd95244b70,"Here we developed an analytical means of estimating population-level effects of endocrine disruptors on Daphnia magna. Our approach was based on the fact that the endocrine-disrupting juvenile hormone analogs induce the production of male neonates if they are exposed to the analogs during a particular period in their prenatal development; the method also assumed that the abnormal production of male neonates in the sake of production of female neonates reduces population growth. We constructed a linear toxicodynamics model to elucidate the period in which D. magna neonates are sensitive to exposure to the analog and also the probability of an individual neonate changing sex under specific exposure concentrations. The proposed model was applied to D. magna reproduction test data obtained under time-varying exposure to pyriproxyfen to derive the maximum-likelihood estimates and the posterior distributions of the model parameters. To quantitatively assess the ecological risk at the population level, we conducted a population dynamics simulation under two time-varying exposure scenarios (i.e., constant or pulsed exposure) by using an age-structured population model. When the change in sex ratio was based on the time-weighted average concentration during the period of sensitivity, change in sex ratio caused approximately equivalent population-level effects as did reproductive inhibition (i.e., reduction in the total number of neonates per female parent) regardless of the exposure scenario. In contrast, when change in sex ratio was based on maximum concentration during the sensitive period, change in sex ratio caused only half the population-level effects as did reproductive inhibition under constant exposure, whereas it caused a much larger population-level effect than did reproductive inhibition under pulsed exposure. © 2018 Elsevier Inc."
,10.1080/02664763.2017.1391754,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032347757&doi=10.1080%2f02664763.2017.1391754&partnerID=40&md5=ce35f80bc5025486d6999c7f8043f31f,"The choice of the model framework in a regression setting depends on the nature of the data. The focus of this study is on changepoint data, exhibiting three phases: incoming and outgoing, both of which are linear, joined by a curved transition. Bent-cable regression is an appealing statistical tool to characterize such trajectories, quantifying the nature of the transition between the two linear phases by modeling the transition as a quadratic phase with unknown width. We demonstrate that a quadratic function may not be appropriate to adequately describe many changepoint data. We then propose a generalization of the bent-cable model by relaxing the assumption of the quadratic bend. The properties of the generalized model are discussed and a Bayesian approach for inference is proposed. The generalized model is demonstrated with applications to three data sets taken from environmental science and economics. We also consider a comparison among the quadratic bent-cable, generalized bent-cable and piecewise linear models in terms of goodness of fit in analyzing both real-world and simulated data. This study suggests that the proposed generalization of the bent-cable model can be valuable in adequately describing changepoint data that exhibit either an abrupt or gradual transition over time. © 2017, © 2017 Informa UK Limited, trading as Taylor & Francis Group."
1,10.5194/gmd-11-3027-2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050933372&doi=10.5194%2fgmd-11-3027-2018&partnerID=40&md5=989db67c3561d233273c16efe4ff4d1e,"Soil organic carbon (SOC) has a significant effect on carbon emissions and climate change. However, the current SOC prediction accuracy of most models is very low. Most evaluation studies indicate that the prediction error mainly comes from parameter uncertainties, which can be improved by parameter calibration. Data assimilation techniques have been successfully employed for the parameter calibration of SOC models. However, data assimilation algorithms, such as the sampling-based Bayesian Markov chain Monte Carlo (MCMC), generally have high computation costs and are not appropriate for complex global land models. This study proposes a new parameter calibration method based on surrogate optimization techniques to improve the prediction accuracy of SOC. Experiments on three types of soil carbon cycle models, including the Community Land Model with the Carnegie-Ames-Stanford Approach biogeochemistry submodel (CLM-CASA') and two microbial models show that the surrogate-based optimization method is effective and efficient in terms of both accuracy and cost. Compared to predictions using the tuned parameter values through Bayesian MCMC, the root mean squared errors (RMSEs) between the predictions using the calibrated parameter values with surrogate-base optimization and the observations could be reduced by up to 12% for different SOC models. Meanwhile, the corresponding computational cost is lower than other global optimization algorithms. © 2018 Author(s)."
,10.1080/02664763.2017.1401049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034269172&doi=10.1080%2f02664763.2017.1401049&partnerID=40&md5=14492004de7a71eead0b160a9e294f6e,"We present a Bayesian approach to the problem of estimating density matrices in quantum state tomography. A general framework is presented based on a suitable mathematical formulation, where a study of the convergence of the Monte Carlo Markov Chain algorithm is given, including a comparison with other estimation methods, such as maximum likelihood estimation and linear inversion. This analysis indicates that our approach not only recovers the underlying parameters quite properly, but also produces physically acceptable punctual and interval estimates. A prior sensitive study was conducted indicating that when useful prior information is available and incorporated, more accurate results are obtained. This general framework, which is based on a reparameterization of the model, allows an easier choice of the prior and proposal distributions for the Metropolis–Hastings algorithm. © 2017, © 2017 Informa UK Limited, trading as Taylor &amp; Francis Group."
,10.1515/jtse-2017-0011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050800927&doi=10.1515%2fjtse-2017-0011&partnerID=40&md5=1559f6dd179739fd351181c5bfba9151,"Numerical standard error (NSE) is an estimate of the standard deviation of a simulation result if the simulation experiment were to be repeated many times. We review standard methods for computing NSE and perform a Monte Carlo experiments to compare their performance in the case of high/extreme autocorrelation. In particular, we propose an application to risk management where we assess the precision of the value-at-risk measure when the underlying risk model is estimated by simulation-based methods. Overall, heteroscedasticity and autocorrelation estimators with prewhitening perform best in the presence of large/extreme autocorrelation. © 2018 Walter de Gruyter GmbH, Berlin/Boston 2018."
,10.1109/TSG.2018.2860783,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050760353&doi=10.1109%2fTSG.2018.2860783&partnerID=40&md5=f37473da59c73aa27e70eeb4c3417d91,"Fast charging stations are critical infrastructures to enable high penetration of plug-in electric vehicles (PEVs) into future distribution networks. They need to be carefully planned to ensure meeting charging demand as well as economic benefits. Accurate estimation of PEV charging demand is the prerequisite of such planning, but a non-trivial task. This paper addresses the sizing (number of chargers and waiting spaces) problem of fast charging stations and presents an optimal planning solution based on an explicit temporal-SoC characterization of PEV fast charging demand. The characteristics of PEV charging demand are derived through the vehicle travel behavior analysis using available statistics. The PEV dynamics in charging stations is modelled with a Markov chain and queuing theory. As a result, the optimal number of chargers and waiting spaces in fast charging stations can be jointly determined so as to maximize the expected operator profits, considering profit of charging service, penalty of waiting and rejection, as well as maintenance cost of idle facilities. The proposed solution is validated through a case study with mathematical justifications and numerical results from simulation. IEEE"
,10.1093/mnras/sty1079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048472826&doi=10.1093%2fmnras%2fsty1079&partnerID=40&md5=931afe090d48d7e074f88b805a8ff882,"Extreme-mass-ratio-inspiral observations from future space-based gravitational-wave detectors such as LISA will enable strong-field tests of general relativity with unprecedented precision, but at prohibitive computational cost if existing statistical techniques are used. In one such test that is currently employed for LIGO black hole binary mergers, generic deviations from relativity are represented by N deformation parameters in a generalized waveform model; the Bayesian evidence for each of its 2N combinatorial submodels is then combined into a posterior odds ratio for modified gravity over relativity in a null-hypothesis test. We adapt and apply this test to a generalized model for extreme-mass-ratio inspirals constructed on deformed black hole spacetimes, and focus our investigation on how computational efficiency can be increased through an evidence-free method of model selection. This method is akin to the algorithm known as product-space Markov chain Monte Carlo, but uses nested sampling and improved error estimates from a rethreading technique. We perform benchmarking and robustness checks for the method, and find order-of-magnitude computational gains over regular nested sampling in the case of synthetic data generated from the null model. © 2018 The Author(s) Published by Oxford University Press on behalf of the Royal Astronomical Society."
,10.1109/VTCSpring.2018.8417572,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050967387&doi=10.1109%2fVTCSpring.2018.8417572&partnerID=40&md5=6118f60a14ed5aefcb8b73a62a6e4a88,"To meet the goal of tenfold increase in spectral efficiency, interference cancellation and multiuser detection are expected to be important tasks of fifth-generation (5G) radio access systems. Both tasks can be realized by joint detection algorithms. However, joint detection algorithms such as maximum likelihood (ML) detection have a high computational complexity. Previous works have shown that joint detectors based on Markov chain Monte Carlo (MCMC) methods can achieve similar results compared to ML detection with a large reduction in the computational complexity for systems with a large number of streams or users. The purpose of this work is to present a MIMO joint detector based on MCMC methods and evaluate it within the constraints of LTE Advanced (LTE-A), namely, using at most 8 transmit antennas and 64-QAM modulation. The evaluation is done separately from the channel decoder. Moreover, the complexity of the presented algorithm is compared to the one of an ML detector. The results show that the proposed MIMO detector offers a similar detection error rate compared to an ML detector. Furthermore, it was observed that the complexity reduction is significant for systems with more than six transmit antennas. © 2018 IEEE."
,10.1109/VTCSpring.2018.8417493,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050983442&doi=10.1109%2fVTCSpring.2018.8417493&partnerID=40&md5=c6ab97e032e357aac4d1ee9dc4a60bfd,"Load modulated arrays (LMAs) is getting recent research attention as an attractive multiantenna transmission architecture in wireless communications. LMAs use a single power amplifier to drive the entire transmit antenna array and implement the multidimensional signaling constellation in the analog domain. In this paper, we consider LMAs in a multiuser setting on the uplink. In this setting, multiple user terminals, each using an LMA (e.g., with 2 or 4 antenna elements), communicate with a base station (BS) with multiple (tens to hundreds) receive antennas. For this system, we consider the problem of low-complexity signal detection at the BS receiver. Specifically, we propose a Markov Chain Monte Carlo (MCMC) sampling based detection algorithm. We evaluate the bit error rate performance of the algorithm via numerical simulations. Simulation results show that the proposed detection achieves very good performance, while also scaling well in complexity. © 2018 IEEE."
,10.1002/sim.7649,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044430956&doi=10.1002%2fsim.7649&partnerID=40&md5=06ee383a9c61bbde4db6c5c6fb916305,"Hierarchical models are extensively used in pharmacokinetics and longitudinal studies. When the estimation is performed from a Bayesian approach, model comparison is often based on the deviance information criterion (DIC). In hierarchical models with latent variables, there are several versions of this statistic: the conditional DIC (cDIC) that incorporates the latent variables in the focus of the analysis and the marginalized DIC (mDIC) that integrates them out. Regardless of the asymptotic and coherency difficulties of cDIC, this alternative is usually used in Markov chain Monte Carlo (MCMC) methods for hierarchical models because of practical convenience. The mDIC criterion is more appropriate in most cases but requires integration of the likelihood, which is computationally demanding and not implemented in Bayesian software. Therefore, we consider a method to compute mDIC by generating replicate samples of the latent variables that need to be integrated out. This alternative can be easily conducted from the MCMC output of Bayesian packages and is widely applicable to hierarchical models in general. Additionally, we propose some approximations in order to reduce the computational complexity for large-sample situations. The method is illustrated with simulated data sets and 2 medical studies, evidencing that cDIC may be misleading whilst mDIC appears pertinent. Copyright © 2018 John Wiley & Sons, Ltd."
,10.3390/s18072363,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050639620&doi=10.3390%2fs18072363&partnerID=40&md5=58c6abd80b7c926e3c96f5fd7d2c66e3,"Multi-object tracking (MOT), especially by using a moving monocular camera, is a very challenging task in the field of visual object tracking. To tackle this problem, the traditional tracking-by-detection-based method is heavily dependent on detection results. Occlusion and mis-detections will often lead to tracklets or drifting. In this paper, the tasks of MOT and camera motion estimation are formulated as finding a maximum a posteriori (MAP) solution of joint probability and synchronously solved in a unified framework. To improve performance, we incorporate the three-dimensional (3D) relative-motion model into a sequential Bayesian framework to track multiple objects and the camera’s ego-motion estimation. A 3D relative-motion model that describes spatial relations among objects is exploited for predicting object states robustly and recovering objects when occlusion and mis-detections occur. Reversible jump Markov chain Monte Carlo (RJMCMC) particle filtering is applied to solve the posteriori estimation problem. Both quantitative and qualitative experiments with benchmark datasets and video collected on campus were conducted, which confirms that the proposed method is outperformed in many evaluation metrics. © 2018 by the authors. Licensee MDPI, Basel, Switzerland."
,10.15961/j.jsuese.201700538,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053727990&doi=10.15961%2fj.jsuese.201700538&partnerID=40&md5=f905006d73b27e0ca7906afc9d01a42c,"Rerouting is an important strategy to deal with bad weather, military activities, traffic flow control and other emergency situations in the process of flight operation. An improved rerouting algorithm based on mining features of historical radar data is proposed. Firstly, rerouting constraints are established by taking the economy, security, route features, and the impact of traffic flow and capacity into consideration. A rough process is made to select the optimal rerouting zone and possible key-point sequences which meet those constraints for rerouting. The routes during historical flight operations are analyzed by the positions of historical flights. Combining with the flights whose plan path contain the given route, a utilization representation is proposed to select the preferred key point. Then, the optimal key-point sequence is obtained by analyzing its utilization from historical radar data. Taking the velocity vector as the modeling object, the machine learning algorithm is used to mine the flight pattern of the route segment. Gaussian Mixture Model is applied to model the distribution of velocity, and Markov chain Monte Carlo is used to predict the velocity sequence during rerouting. Finally, the whole rerouting path is planned by kinematics equation with constant acceleration rules. The application of a large-scale traffic management system demonstrates the high practicability of the proposed algorithm with considering the actual operating conditions of flight. The proposed approach can also predict the trajectory of rerouting flights, which provides the data support for the air traffic flow management in the rerouting path areas. Therefore, the rerouting issue of flight can be solved reasonably and efficiently. © 2018, Advanced Engineering Sciences. All right reserved."
,10.1109/VTCSpring.2018.8417682,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050973984&doi=10.1109%2fVTCSpring.2018.8417682&partnerID=40&md5=2f421b6ff1b74c8951a49d761819aef7,"This paper focuses on the handover probability as a result of the random rotation of a user equipment (UE) in hybrid light-fidelity (LiFi) and radio frequency (RF) networks. A received signal strength indicator (RSSI)-based handover algorithm is considered in this study. Using RSSI as the user association rule does not guarantee that the randomly-oriented UE is always associated with the nearest access point (AP). In some cases, depending on the orientation of the UE, the received signal powers from the LiFi APs are very weak and unreliable. Therefore, a vertical handover from the LiFi AP to the RF AP is required to maintain the user quality of service. Hence, it is essential to study the handover probability due to the change of orientation. A theoretical analysis of the handover probability based on a Markov chain model is provided. The analytical results are confirmed by the Monte Carlo simulation. The effects of some parameters, such as the threshold, hysteresis level and the trade-off between the frequency and the delay of handover are presented in this paper. © 2018 IEEE."
,10.3847/1538-4357/aacc6c,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050742089&doi=10.3847%2f1538-4357%2faacc6c&partnerID=40&md5=db4c5d12306128196b019f67084db549,"We present a near-infrared K-band R ≃ 1500 Keck spectrum of S68N, a Class 0 protostar in the Serpens molecular cloud. The spectrum shows a very red continuum, CO absorption bands, weak or nonexistent atomic metal absorptions, and H2 emission lines. The near-IR H2 emission is consistent with excitation in shocks or by X-rays but not by UV radiation. We model the absorption component as a stellar photosphere plus circumstellar continuum emission with wavelength-dependent extinction. A Markov Chain Monte Carlo analysis shows that the most likely model parameters are consistent with a low-temperature, low-gravity photosphere with significant extinction and no more than modest continuum veiling. Its T eff ≃ 3260 K effective temperature is similar to that of older, more evolved pre-main-sequence stars, but its surface gravity log g ≃ 2.4 cm s-2 is approximately 1 dex lower. This implies that the radius of this protostar is a factor of ∼3 larger than that of 106 year old T Tauri stars. Its low veiling is consistent with a circumstellar disk having intrinsic near-IR emission that is less than or equal to that of more evolved Class I protostars. Along with the high extinction, this suggests that most of the circumstellar material is in a cold envelope, as expected for a Class 0 protostar. This is the first known detection and analysis of a Class 0 protostar absorption spectrum. © 2018. The American Astronomical Society."
,10.3889/oamjms.2018.296,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051417765&doi=10.3889%2foamjms.2018.296&partnerID=40&md5=fe97da51a35bfad63f86829ea3564d0e,"AIM: Verification bias is one of the major problems encountered in diagnostic accuracy studies. It occurs when a standard test performed on a non-representative subsample of subjects which have undergone the diagnostic test. In this study we extend a Bayesian model to correct this bias. METHODS: The study population is patients that have undergone at least two repeated failed IVF/ICSI (in vitro fertilization/intra cytoplasmic sperm injection) cycles. Patients were screened using ultrasonography and those with polyps were recommended for hysteroscopy. A Bayesian modeling was applied on mechanism of missing data using an informative prior on disease prevalence. The parameters of the model were estimated through Markov Chain Monte Carlo methods. RESULTS: A total of 238 patients were screened, 47 of which had polyps. Those with polyps were strongly recommended to undergo hysteroscopy, 47/47 decide to have a hysteroscopy and in 37/47 polyps confirmed. None of the 191 patients with no polyps detected in ultrasonography underwent a hysteroscopy. A model using Bayesian approach was applied with informative prior on polyp prevalence. False and true negatives were estimated in the Bayesian framework. The false negative was obtained 14 and 177 true negatives were obtained, so sensitivity and specificity was estimated easily after estimating the missing data. Sensitivity and specificity were equal to 74% and 94% respectively. CONCLUSION: Bayesian analyses with informative prior seem to be powerful tools in the simulation of experimental space. © 2018 Abdollah Hajivandi, Hamid Reza Ghafarian Shirazi, Seyed Hassan Saadat, Mohammad Chehrazi."
,10.3847/1538-4357/aacaf1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050765614&doi=10.3847%2f1538-4357%2faacaf1&partnerID=40&md5=7085f30a177b796d2fe39c07231aea6e,"In this second paper in a series studying galaxy-galaxy lensing signals using Sloan Digital Sky Survey Data Release 7 (SDSS DR7), we present our measurement and modeling of the lensing signals around groups of galaxies. We divide the groups into four halo mass bins and measure the signals around four different halo-center tracers: brightest central galaxies (BCGs), luminosity-weighted centers, number-weighted centers, and X-ray peak positions. For groups cross-identified in both X-ray and SDSS DR7, we further split the groups into low and high X-ray emission subsamples, both of which are assigned to two halo-center tracers, BCGs and X-ray peak positions. The galaxy-galaxy lensing signals show that BCGs, among the four candidates, are the best halo-center tracers. We model the lensing signals using a combination of four contributions: the off-center NFW host halo profile, subhalo contribution, stellar contribution, and projected two-halo term. We sample the posterior of five parameters, i.e., the halo mass, concentration, off-centering distance, subhalo mass, and fraction of subhalos, via a Monte Carlo Markov Chain (MCMC) package using the galaxy-galaxy lensing signals. After taking into account the sampling effects (e.g., Eddington bias), we found that the best-fit halo masses obtained from lensing signals are quite consistent with those obtained in the group catalog based on an abundance matching method, except in the lowest mass bin. © 2018. The American Astronomical Society. All rights reserved."
,10.1109/DSN.2018.00040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051075476&doi=10.1109%2fDSN.2018.00040&partnerID=40&md5=230ec7c7d995e30d0ed6abb3f469eec6,"In real-world systems, rare events often characterize critical situations like the probability that a system fails within some time bound and they are used to model some potentially harmful scenarios in dependability of safety-critical systems. Probabilistic Model Checking has been used to verify dependability properties in various types of systems but is limited by the state space explosion problem. An alternative is the recourse to Statistical Model Checking (SMC) that relies on Monte Carlo simulations and provides estimates within predefined error and confidence bounds. However, rare properties require a large number of simulations before occurring at least once. To tackle the problem, Importance Sampling, a rare event simulation technique, has been proposed in SMC for different types of probabilistic systems. Importance Sampling requires the full knowledge of probabilistic measure of the system, e.g. Markov chains. In practice, however, we often have models with some uncertainty, e.g., Interval Markov Chains. In this work, we propose a method to apply importance sampling to Interval Markov Chains. We show promising results in applying our method to multiple case studies. © 2018 IEEE."
,10.1109/TAC.2018.2857760,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050400972&doi=10.1109%2fTAC.2018.2857760&partnerID=40&md5=70936457d42fd67a0391dc841a76db75,"In this work we consider using multiple noisy binary sensors to track a target that moves as a Markov Chain in a finite discrete environment, with symmetric probability of false alarm and missed detection. We study two policies. Firstly, we show that the greedy policy, whereby m sensors are placed at the m most-likely target locations, is one-step optimal in that it maximizes the expected maximum a posteriori (MAP) estimate. Secondly, we show that a policy in which the m sensors are placed in the second through <formula><tex>$(m+1)^{st}$</tex></formula> most likely target locations achieves equal or slightly worse expected MAP performance, but leads to significantly decreased variance on the MAP estimate. The result is proven for m = 1, and Monte Carlo simulations give evidence for <formula><tex>$m &#x003E; 1$</tex></formula>. Both policies are closed-loop, index-based active sensing strategies that are computationally trivial to implement. Our approach focuses on one-step optimality because of the apparent intractability of computing an optimal policy via dynamic programming in belief space. However, Monte Carlo simulations suggest that both policies perform well over arbitrary horizons. IEEE"
,10.3389/fgene.2018.00254,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050266890&doi=10.3389%2ffgene.2018.00254&partnerID=40&md5=38e75c7170e3efe738a3a2c9bb1d4103,"Large-scale tumor genome sequencing projects have revealed a complex landscape of genomic mutations in multiple cancer types. A major goal of these projects is to characterize somatic mutations and discover cancer drivers, thereby providing important clues to uncover diagnostic or therapeutic targets for clinical treatment. However, distinguishing only a few somatic mutations from the majority of passenger mutations is still a major challenge facing the biological community. Fortunately, combining other functional features with mutations to predict cancer driver genes is an effective approach to solve the above problem. Protein lysine modifications are an important functional feature that regulates the development of cancer. Therefore, in this work, we have systematically analyzed somatic mutations on seven protein lysine modifications and identified several important drivers that are responsible for tumorigenesis. From published literature, we first collected more than 100,000 lysine modification sites for analysis. Another 1 million non-synonymous single nucleotide variants (SNVs) were then downloaded from TCGA and mapped to our collected lysine modification sites. To identify driver proteins that significantly altered lysine modifications, we further developed a hierarchical Bayesian model and applied the Markov Chain Monte Carlo (MCMC) method for testing. Strikingly, the coding sequences of 473 proteins were found to carry a higher mutation rate in lysine modification sites compared to other background regions. Hypergeometric tests also revealed that these gene products were enriched in known cancer drivers. Functional analysis suggested that mutations within the lysine modification regions possessed higher evolutionary conservation and deleteriousness. Furthermore, pathway enrichment showed that mutations on lysine modification sites mainly affected cancer related processes, such as cell cycle and RNA transport. Moreover, clinical studies also suggested that the driver proteins were significantly associated with patient survival, implying an opportunity to use lysine modifications as molecular markers in cancer diagnosis or treatment. By searching within protein-protein interaction networks using a random walk with restart (RWR) algorithm, we further identified a series of potential treatment agents and therapeutic targets for cancer related to lysine modifications. Collectively, this study reveals the functional importance of lysine modifications in cancer development and may benefit the discovery of novel mechanisms for cancer treatment. © 2018 Chen, Miao, Liu, Zeng, Gao, Peng, Hu, Li, Zheng, Xue, Zuo, Xie and Ren."
,10.1109/ICSTW.2018.00052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050985329&doi=10.1109%2fICSTW.2018.00052&partnerID=40&md5=922749fa99a4c598fbfc7bd2b4db940a,"Temporal logic falsification is a promising approach to model-based testing of cyber-physical systems. It starts off with a formalized system requirement specified as a Metric Temporal Logic (MTL) property. Subsequently, test input signals are generated in order to stimulate the system and produce an output signal. Finally, output signals of the system under test are compared to those prescribed by the property to falsify the property by means of a counterexample. To find such a counterexample, Markov Chain Monte-Carlo (MCMC) methods are used to construct an optimization problem to steer the test input generations to those input areas that maximize the probability of falsifying the property. In this paper, we identify two practical issues in the above-mentioned falsification process. Firstly, a fixed time domain of the input-signal space is assumed in this process, which restricts the frequency content of the (generated) input signals. Secondly, the existing process allows for input selection steered by the distribution of a single input variable. We address these issues, by firstly, considering multiple time domains for input-signal space. Subsequently, an input-signal-space optimization problem is formally defined and implemented in S-TaLiRo+, an extension of S-TaLiRo (an existing implementation for solving the MTL falsification problem). Secondly, we propose a decoupled scheme that considers the distribution of each input variable independently. The applicability of the proposed solutions are experimentally evaluated on well-known benchmark problems. © 2018 IEEE."
,10.1016/j.ejor.2018.01.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042451249&doi=10.1016%2fj.ejor.2018.01.016&partnerID=40&md5=b5ae966cb3164a03957f2325298cef81,"This paper presents a novel model of measuring technical inefficiency based on the notion that higher efficiency requires a certain cost. First, we apply the “rational inefficiency hypothesis” of Bogetoft and Hougaard (2003) but we fail to find that it rationalizes our data set of large U.S banks with multiple inputs and outputs. In consequence, we adopt a novel model of profit maximization which explicitly incorporates the cost of technical inefficiency. The cost of inefficiency is treated as unknown and is parametrized as a function of inputs, outputs and decision-making-unit specific fixed effects. More importantly, by showing the model to be equivalent to one in which inefficiency is an arbitrary function of inputs, outputs and the inefficiency cost, we are able to determine optimal directions in the input-output space that would reduce inefficiency. Bayesian techniques organized around Markov Chain Monte Carlo are used to perform the computations and provide statistical inferences as well as useful policy measures to reduce inefficiencies in the U.S banking sector through an examination of different realistic scenarios. © 2018 Elsevier B.V."
,10.1080/00036846.2018.1430338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041537436&doi=10.1080%2f00036846.2018.1430338&partnerID=40&md5=1f9fabe0290e4a3c662da6708b59712c,"Based on the general time-varying parameter vector autoregressive model and data mining technology, this study proposes a new extension mixed innovation time-varying parameter stochastic volatility vector autoregressive model and investigates time-varying characteristics and efficiencies of different shock effects on China’s monetary policy towards inflation and GDP. Using sample monthly data for 1979–2014, we utilize typical time points to illustrate the mechanisms between different economic variables via the Markov Chain Monte Carlo method and impulse response function. The empirical results show that the monetary transmission mechanism in China can be effective in the real economy, but with delay and efficiency leakage. The average delay and maximum efficiency can be measured through the MI model, which can capture accurate information of economic variables, effectively improving the precision of macroeconomic regulation and control. Meanwhile, the difference between the impacts of different channels is obvious; while the impact of interest rates is not significant, the impact of stock market is significant. The action mechanism between GDP and the inflation rate undergoes a gradual structural change, evidently displaying time-varying characteristics and a gradually weakening impact over time. © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1103/PhysRevD.98.023510,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051133881&doi=10.1103%2fPhysRevD.98.023510&partnerID=40&md5=67951e3648f108d80bd260894f68ef52,"Studies of dark energy at advanced gravitational-wave (GW) interferometers normally focus on the dark energy equation of state wDE(z). However, modified gravity theories that predict a nontrivial dark energy equation of state generically also predict deviations from general relativity in the propagation of GWs across cosmological distances, even in theories where the speed of gravity is equal to c. We find that, in generic modified gravity models, the effect of modified GW propagation dominates over that of wDE(z), making modified GW propagation a crucial observable for dark energy studies with standard sirens. We present a convenient parametrization of the effect in terms of two parameters (Ξ0,n), analogue to the (w0,wa) parametrization of the dark energy equation of state, and we give a limit from the LIGO/Virgo measurement of H0 with the neutron star binary GW170817. We then perform a Markov chain Monte Carlo analysis to estimate the sensitivity of the Einstein Telescope (ET) to the cosmological parameters, including (Ξ0,n), both using only standard sirens, and combining them with other cosmological data sets. In particular, the Hubble parameter can be measured with an accuracy better than 1% already using only standard sirens while, when combining ET with current CMB+BAO+SNe data, Ξ0 can be measured to 0.8%. We discuss the predictions for modified GW propagation of a specific nonlocal modification of gravity, recently developed by our group, and we show that they are within the reach of ET. Modified GW propagation also affects the GW transfer function, and therefore the tensor contribution to the ISW effect. © 2018 American Physical Society."
,10.1142/S0219455419400121,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049777430&doi=10.1142%2fS0219455419400121&partnerID=40&md5=e2d2e2ee72cff3e74b70ab4bee7a986e,"This paper reports the step-by-step procedures of a full-scale ambient vibration test and the corresponding modal identification and Bayesian structural model updating of a coupled building. The building is characterized as a combination of a main part and a complementary part connected together by corridors in between. Compared with the main part, the volume of the complementary part is much smaller. Therefore, the influence on the dynamic properties of the complementary part from its counterpart is expected. To capture the dynamic properties of the coupled building, a 21-setup ambient vibration test was designed to cover all the degrees of freedom (DOFs) of interest. The modal parameters of each setup were identified following the frequency domain decomposition (FDD) method and the partial mode shapes from different setups were assembled following a least-squares method. To determine the stiffness of the linkage between the two parts, the coupled building was simulated with two linked shear buildings and updated utilizing the Markov chain Monte Carlo (MCMC)-based Bayesian model updating method. The identified modal parameters revealed interesting features about the coupled effects between the main part and complementary part and were discussed in detail. The good match between the model-predicted and identified modal parameters verified the validity of proposed shear building model. This study provides valuable experience in the area of structural model updating and structural health monitoring. © 2019 World Scientific Publishing Company"
,10.1080/02664763.2018.1495701,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049802210&doi=10.1080%2f02664763.2018.1495701&partnerID=40&md5=aee277e4762711438c01d6a71447e787,"The modeling and analysis of lifetime data in which the main endpoints are the times when an event of interest occurs is of great interest in medical studies. In these studies, it is common that two or more lifetimes associated with the same unit such as the times to deterioration levels or the times to reaction to a treatment in pairs of organs like lungs, kidneys, eyes or ears. In medical applications, it is also possible that a cure rate is present and needed to be modeled with lifetime data with long-term survivors. This paper presented a comparative study under a Bayesian approach among some existing continuous and discrete bivariate distributions such as the bivariate exponential distributions and the bivariate geometric distributions in presence of cure rate, censored data and covariates. In presence of lifetimes related to cured patients, it is assumed standard mixture cure rate models in the data analysis. The posterior summaries of interest are obtained using Markov Chain Monte Carlo methods. To illustrate the proposed methodology two real medical data sets are considered. © 2018 Informa UK Limited, trading as Taylor & Francis Group"
,10.1088/1475-7516/2018/07/025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051699540&doi=10.1088%2f1475-7516%2f2018%2f07%2f025&partnerID=40&md5=cf30734ac58bcb8de2bcea89a0abdf75,"A flux of extra-terrestrial neutrinos at energies ≥1015 eV has the potential to serve as a cosmological probe of the high-energy universe as well as tests of fundamental particle interactions. Cosmogenic neutrinos, produced from the interactions of ultra-high energy cosmic rays (UHECRs) with cosmic photon backgrounds, have been regarded as a guaranteed flux. However, the expected neutrino flux depends on the composition of UHECRs at the highest energies; heavier nuclei result in lower neutrino fluxes compared to lighter nuclei and protons. The objective of this study is to estimate the range of cosmogenic neutrino spectra consistent with recent cosmic-ray spectral and compositional data using a fully inferential Bayesian approach. The study assumes a range of source distributions consistent with astrophysical sources, the flux and composition of cosmic rays, and detector systematic uncertainties. The technique applied to this study is the use of an affine-invariant Markov Chain Monte Carlo, which is an effective Bayesian inference tool for characterizing multi-dimensional parameter spaces and their correlations. © 2018 IOP Publishing Ltd and Sissa Medialab."
,10.1007/s10237-018-1049-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049809513&doi=10.1007%2fs10237-018-1049-0&partnerID=40&md5=f0736152e31d966eb8fe5a39d314e42d,"We propose a reduced ODE model for the mechanical activation of cardiac myofilaments, which is based on explicit spatial representation of nearest-neighbour interactions. Our model is derived from the cooperative Markov Chain model of Washio et al. (Cell Mol Bioeng 5(1):113–126, 2012), under the assumption of conditional independence of specific sets of events. This physically motivated assumption allows to drastically reduce the number of degrees of freedom, thus resulting in a significantly large computational saving. Indeed, the original Markov Chain model involves a huge number of degrees of freedom (order of (Formula presented.)) and is solved by means of the Monte Carlo method, which notoriously reaches statistical convergence in a slow fashion. With our reduced model, instead, numerical simulations can be carried out by solving a system of ODEs, reducing the computational time by more than 10, 000 times. Moreover, the reduced model is accurate with respect to the original Markov Chain model. We show that the reduced model is capable of reproducing physiological steady-state force–calcium and force–length relationships with the observed asymmetry in apparent cooperativity near the calcium level producing half activation. Finally, we also report good qualitative and quantitative agreement with experimental measurements under dynamic conditions. © 2018 Springer-Verlag GmbH Germany, part of Springer Nature"
,10.1021/acs.iecr.8b00293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048734247&doi=10.1021%2facs.iecr.8b00293&partnerID=40&md5=246d948f24ccea42d8e57460f2b8d0f6,"An Approximate Bayesian Expectation Maximization (ABEM) methodology and a Laplace Approximation Bayesian (LAB) methodology are developed for estimating parameters in nonlinear stochastic differential equation (SDE) models of chemical processes. These new methodologies are more powerful than previous maximum-likelihood methodologies for SDEs because they enable modelers to account for prior information about unknown parameters and initial conditions. The ABEM methodology is suitable for situations in which the modeler can assume that measurement noise variances are well-known, whereas LAB includes measurement noise variances among the parameters that require estimation. Both techniques estimate the magnitude of stochastic terms included in the differential equations to account for model mismatch and unknown process disturbances. The proposed ABEM and LAB methodologies are illustrated using a nonlinear continuous stirred tank reactor (CSTR) case study, with simulated data sets generated using a variety of scenarios. The ABEM and LAB objective functions used in the case study result in improved estimates of model parameters and noise parameters compared to previous maximum-likelihood objective functions, especially in situations for which data available for parameter estimation are sparse. Because the proposed ABEM and LAB methodologies rely on B-spline basis functions rather than Markov Chain Monte Carlo techniques, they are straightforward to implement using available optimizers and modeling software and require only modest computational effort. © Copyright 2018 American Chemical Society."
1,10.1080/01621459.2018.1448827,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049997626&doi=10.1080%2f01621459.2018.1448827&partnerID=40&md5=8cf998a5b92a31a63994ee925200e964,"We propose subsampling Markov chain Monte Carlo (MCMC), an MCMC framework where the likelihood function for n observations is estimated from a random subset of m observations. We introduce a highly efficient unbiased estimator of the log-likelihood based on control variates, such that the computing cost is much smaller than that of the full log-likelihood in standard MCMC. The likelihood estimate is bias-corrected and used in two dependent pseudo-marginal algorithms to sample from a perturbed posterior, for which we derive the asymptotic error with respect to n and m, respectively. We propose a practical estimator of the error and show that the error is negligible even for a very small m in our applications. We demonstrate that subsampling MCMC is substantially more efficient than standard MCMC in terms of sampling efficiency for a given computational budget, and that it outperforms other subsampling methods for MCMC proposed in the literature. Supplementary materials for this article are available online. © 2018 The Authors. Published with License by Taylor and Francis"
,10.1016/j.ecolmodel.2018.03.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046140758&doi=10.1016%2fj.ecolmodel.2018.03.013&partnerID=40&md5=2152eca4a60306f293856702129cde97,"Process-based ecosystem models are increasingly used to estimate the carbon and water exchanges between ecosystems and the atmosphere. These models inevitably suffer from deficiencies and uncertainties, which should be thoroughly examined to better understand the processes governing the ecosystem dynamics. In this paper, we systematically explored the uncertainties in model predictions of Changbaishan (CBS) broad-leaved Korean pine mixed forest using the SImplified PhotosyNthesis and Evapo-Transpiration (SIPNET) model and eddy flux and meteorological data from 2004 to 2009. We first screened out 21 key parameters from 42 model parameters using Morris global sensitivity analysis method, and then estimated their probability distributions through Markov Chain Monte Carlo technique. Two optimization set-ups, i.e. using observed net ecosystem exchange of CO2 (NEE) only and using observed NEE and evapotranspiration (ET) simultaneously, were conducted to detect the different constraints of different observations on model parameters. Four parameters were well constrained using observed NEE only, including photosynthesis and respiration related parameters. While seven parameters were well constrained using measured NEE and ET simultaneously, four of which were water related parameters. Obviously, more information can be derived from the simultaneous optimization, since there was additional process information in water flux observation. The modeled ET of the NEE and ET optimization set-up had a much better fit to measured values than the NEE only optimization set-up (R2 = 0.70 vs. R2 = 0.30), although the modeled NEE from the two set-ups had a good fit to the observations (R2 = 0.85 vs. R2 = 0.83). This implied that assimilating carbon and water fluxes simultaneously can improve the parameterization and overall performance of the model. Then, we quantified the uncertainties in model predictions using Monte Carlo simulation, and trace them to specific parameter and parameter interactions through Sobol’ variance decomposition method. The uncertainties of five outputs of interest in CBS site, NEE, gross primary productivity (GPP), ecosystem respiration (RE), ET and transpiration (T), were 50.82%, 22.35%, 21.25%, 9.98% and 19.54%, respectively. The uncertainty in predicted NEE was much larger since NEE is a small difference between two large fluxes, i.e. GPP and RE. The maximum net CO2 assimilation rate (Amax) and carbon content of leaves (SLW) were classified as highly sensitive parameters for all outputs of interest in CBS site, contributing more than 70% of the uncertainties in all outputs except NEE. The importance of these two parameters holds for one subtropical evergreen coniferous plantation and one subtropical evergreen broad-leaved forest, too. Therefore, these two parameters and their underlying processes should be a focus of future model research, plant trait data collection and field measurement, at least for the sites in this study. This can help connect the model simulation research and field data collection, making them mutually informative. © 2018 Elsevier B.V."
,10.1021/acs.jctc.7b01245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047606645&doi=10.1021%2facs.jctc.7b01245&partnerID=40&md5=d23d564475932a5336cc2f1b81237a6a,"Knowledge of the structure and dynamics of biomolecules is essential for elucidating the underlying mechanisms of biological processes. Given the stochastic nature of many biological processes, like protein unfolding, it is almost impossible that two independent simulations will generate the exact same sequence of events, which makes direct analysis of simulations difficult. Statistical models like Markov chains, transition networks, etc. help in shedding some light on the mechanistic nature of such processes by predicting long-time dynamics of these systems from short simulations. However, such methods fall short in analyzing trajectories with partial or no temporal information, for example, replica exchange molecular dynamics or Monte Carlo simulations. In this work, we propose a probabilistic algorithm, borrowing concepts from graph theory and machine learning, to extract reactive pathways from molecular trajectories in the absence of temporal data. A suitable vector representation was chosen to represent each frame in the macromolecular trajectory (as a series of interaction and conformational energies), and dimensionality reduction was performed using principal component analysis (PCA). The trajectory was then clustered using a density-based clustering algorithm, where each cluster represents a metastable state on the potential energy surface (PES) of the biomolecule under study. A graph was created with these clusters as nodes with the edges learned using an iterative expectation maximization algorithm. The most reactive path is conceived as the widest path along this graph. We have tested our method on RNA hairpin unfolding trajectory in aqueous urea solution. Our method makes the understanding of the mechanism of unfolding in the RNA hairpin molecule more tractable. As this method does not rely on temporal data, it can be used to analyze trajectories from Monte Carlo sampling techniques and replica exchange molecular dynamics (REMD). Copyright © 2018 American Chemical Society."
,10.1080/01621459.2018.1423984,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049785007&doi=10.1080%2f01621459.2018.1423984&partnerID=40&md5=8465d50664c7836f07553f9b237c4f5d,"Malware is computer software that has either been designed or modified with malicious intent. Hundreds of thousands of new malware threats appear on the internet each day. This is made possible through reuse of known exploits in computer systems that have not been fully eradicated; existing pieces of malware can be trivially modified and combined to create new malware, which is unknown to anti-virus programs. Finding new software with similarities to known malware is therefore an important goal in cyber-security. A dynamic instruction trace of a piece of software is the sequence of machine language instructions it generates when executed. Statistical analysis of a dynamic instruction trace can help reverse engineers infer the purpose and origin of the software that generated it. Instruction traces have been successfully modeled as simple Markov chains, but empirically there are change points in the structure of the traces, with recurring regimes of transition patterns. Here, reversible jump Markov chain Monte Carlo for change point detection is extended to incorporate regime-switching, allowing regimes to be inferred from malware instruction traces. A similarity measure for malware programs based on regime matching is then used to infer the originating families, leading to compelling performance results. © 2018 American Statistical Association"
,10.1080/01621459.2018.1458618,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049786049&doi=10.1080%2f01621459.2018.1458618&partnerID=40&md5=b92e26b0d00c9df12f96ae54af30aa01,"A fundamental problem in network analysis is clustering the nodes into groups which share a similar connectivity pattern. Existing algorithms for community detection assume the knowledge of the number of clusters or estimate it a priori using various selection criteria and subsequently estimate the community structure. Ignoring the uncertainty in the first stage may lead to erroneous clustering, particularly when the community structure is vague. We instead propose a coherent probabilistic framework for simultaneous estimation of the number of communities and the community structure, adapting recently developed Bayesian nonparametric techniques to network models. An efficient Markov chain Monte Carlo (MCMC) algorithm is proposed which obviates the need to perform reversible jump MCMC on the number of clusters. The methodology is shown to outperform recently developed community detection algorithms in a variety of synthetic data examples and in benchmark real-datasets. Using an appropriate metric on the space of all configurations, we develop nonasymptotic Bayes risk bounds even when the number of clusters is unknown. Enroute, we develop concentration properties of nonlinear functions of Bernoulli random variables, which may be of independent interest in analysis of related models. Supplementary materials for this article are available online. © 2018 American Statistical Association"
,10.1109/ACCESS.2018.2853998,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049692319&doi=10.1109%2fACCESS.2018.2853998&partnerID=40&md5=c5f4ea8a8ff734772fb94cc4557d124f,"This article develops a sequential Bayesian learning method to estimate the parameters and recover the state variables for generalized autoregressive conditional heteroscedasticity (GARCH) models, which are commonly used in the financial time-series analysis. This simulation-based method combines particle-filtering technology with a Markov chain Monte Carlo algorithm when the model is non-linear and the number of observed variables is relatively sparse. We compare the performance of the sequential Bayesian learning approach with the numerical maximum likelihood estimation (NMLE) in estimating models based on SP 500 return rates. Our research concludes that the sequential parameter learning approach performs more robustly and accurately than the NMLE, by taking into account the uncertainty of the model. We also carry out simulation studies to confirm that the sequential Bayesian learning method is extremely reliable for GARCH models. © 2013 IEEE."
,10.1007/s00477-018-1580-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049586094&doi=10.1007%2fs00477-018-1580-7&partnerID=40&md5=c938031c06217758f3af14bfb6c921d0,"The benchmark dose (BMD) approach for the exposure limit in the risk assessment of cancer and non-cancer endpoints is well established; it is often based on dose–response modeling of the most critical or the most sensitive outcome. However, neither the most critical endpoint nor the most sensitive endpoint may necessarily be representative of the overall toxic effects. To have a whole picture, it is preferable to express responses for different endpoints with equivalent severity levels and integrate them into one analysis framework. In this paper, we derive BMD in the case of multivariate ordered categorical responses such as none, mild, adverse, and severe based on structural equation models (SEMs). First, for each of the ordered categorical responses, we obtain a latent continuous variable based on fictitious cutoffs of a standard normal distribution. Second, we use SEMs to integrate the multiple continuous variables into a single latent continuous variable and derive the corresponding BMD. We employed a Bayesian statistical approach using Markov chain Monte Carlo simulations to obtain the parameter estimates of the latent variables, SEMs, and the corresponding BMD. We illustrate the proposed procedure by simulation studies and analysis of an experimental study of acrylamide exposure in mice with multivariate endpoints of different severity levels. © 2018 Springer-Verlag GmbH Germany, part of Springer Nature"
,10.3390/w10070900,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049660376&doi=10.3390%2fw10070900&partnerID=40&md5=a99e68d613c88bfacc60729c009ae4ca,"In recent years, several Bayesian Markov chain Monte Carlo (MCMC) methods have been proposed in extreme value analysis (EVA) for assessing the flood risk in a certain location. In this study, the Hamiltonian Monte Carlo (HMC) method was employed to obtain the approximations to the posterior marginal distribution of the Generalized Extreme Value (GEV) model by using annual maximum discharges in two major river basins in Bangladesh. As a comparison, the well-known Metropolis-Hasting (MH) algorithm was also applied, but did not converge well and yielded skewness values opposite those of HMC and the statistical characteristic of the data sets. The discharge records of the Ganges and Brahmaputra rivers in Bangladesh for the past 42 years were analyzed. To estimate flood risk, a return level with 95% confidence intervals (CI) has also been calculated. Results show that the shape parameter of each station was greater than zero, which describes the heavy-tailed Fréchet cases of the GEV distributions. One station, Bahadurabad in the Brahmaputra river basin, estimated 141,387 m3·s-1 with a 95% CI range of [112,636, 170,138] for the 100-year return level, and the 1000-year return level was 195,018 m3·s-1 with a 95% CI of [122,493, 267,544]. The other station, Hardinge Bridge at the Ganges basin, estimated 124,134 m3·s-1 with a 95% CI of [108,726, 139,543] for the 100-year return level, and the 1000-year return level was 170,537 m3·s-1 with a 95% CI of [133,784, 207,289]. As Bangladesh is a flood-prone country, the approach of Bayesian with HMC in EVA can help policy-makers to plan initiatives that could result in preventing damage to both lives and assets. © 2018 by the authors."
,10.1080/01621459.2017.1415908,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049655648&doi=10.1080%2f01621459.2017.1415908&partnerID=40&md5=c2e2a5c9277daa5eb85f9358a9860694,"This article introduces a nonparametric approach to multivariate time-varying power spectrum analysis. The procedure adaptively partitions a time series into an unknown number of approximately stationary segments, where some spectral components may remain unchanged across segments, allowing components to evolve differently over time. Local spectra within segments are fit through Whittle likelihood-based penalized spline models of modified Cholesky components, which provide flexible nonparametric estimates that preserve positive definite structures of spectral matrices. The approach is formulated in a Bayesian framework, in which the number and location of partitions are random, and relies on reversible jump Markov chain and Hamiltonian Monte Carlo methods that can adapt to the unknown number of segments and parameters. By averaging over the distribution of partitions, the approach can approximate both abrupt and slowly varying changes in spectral matrices. Empirical performance is evaluated in simulation studies and illustrated through analyses of electroencephalography during sleep and of the El Niño-Southern Oscillation. Supplementary materials for this article are available online. © 2018 American Statistical Association"
,10.1080/02664763.2018.1492527,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049597530&doi=10.1080%2f02664763.2018.1492527&partnerID=40&md5=d35b538b53c27ff4e3a54a9890811a71,"In this work, we assume that the sequence recording whether or not an ozone exceedance of an environmental threshold has occurred in a given day is ruled by a non-homogeneous Markov chain of order one. In order to account for the possible presence of cycles in the empirical transition probabilities, a parametric form incorporating seasonal components is considered. Results show that even though some covariates (namely, relative humidity and temperature) are not included explicitly in the model, their influence is captured in the behavior of the transition probabilities. Parameters are estimated using the Bayesian point of view via Markov chain Monte Carlo algorithms. The model is applied to ozone data obtained from the monitoring network of Mexico City, Mexico. An analysis of how the methodology could be used as an aid in the decision-making is also given. © 2018 Informa UK Limited, trading as Taylor & Francis Group"
1,10.1142/S021945541940011X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049348700&doi=10.1142%2fS021945541940011X&partnerID=40&md5=d7e91453cb609e6b5c3297fe7bb62542,"This paper presents a probabilistic damage identification methodology tailor-made for periodically-supported structures with finite-length. The free wave motion of a general periodically-supported structure with a single disorder is analyzed through the characteristic receptance approach, and the corresponding frequency characteristic equation is developed. In addition, a concept of nondimensional frequency is introduced, and the sensitivity matrix of the nondimensional frequencies with respect to changes in stiffness of periodic cells is obtained by solving the frequency characteristic equation and utilizing the sensitivity analysis technique. Following the sensitivity-based identification equation with nondimensional frequency information, the probabilistic methodology for identifying the damage occurring in the periodically-supported structures is developed by implementing the Bayesian approach and the Markov chain Monte Carlo (MCMC) simulation with the Metropolis–Hasting sampling algorithm. The validity of the proposed methodology is demonstrated by both numerical simulations for a periodically-supported flanged pipeline example and experimental case studies conducted for a multi-span aluminum beam model endowed with bolted connections in the laboratory. © 2019 World Scientific Publishing Company"
,10.1088/1757-899X/383/1/012018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050468093&doi=10.1088%2f1757-899X%2f383%2f1%2f012018&partnerID=40&md5=505ae44d8edaa1d189fef653516f17ac,"This study proposes an online to offline (O2O) learning framework, ArgiZero, based on three components: a generative adversarial network, an online auction market, and offline simulated agents (digital twins: buyers, farmers, natures, and markets). The generative time series and digital twins are massively generated in a manner of Monte Carlo but with extremely efficient algorithms. The goal of the generator is to produce time series that are statistically indistinguishable with the records from auction market. The goal of the discriminator is to develop a triangulation method based on semi-modeless assimilation to separate generated from actual time series. Most farmers believe agriculture is impossible to be planned because uncertainty, crowding, crisis, and risk cannot be foreseen and accommodated. This AgriZero framework alleviates the challenges through the techniques of Bayesian deep learning and data assimilation as well as the mega power of GPU computation. Thanks to Bayesian hierarchical estimation, which is akin to deep learning but more sophisticate and longer history. We are able to estimate human behaviour in the agents of buyers and farmers, natural disaster in the agents of natures, and price fluctuations in the agents of markets. The framework has been validated by a large amount of records in vegetable auctions of Taiwan and USA. The hierarchical Bayesian estimation and Monte Carlo Markov Chain particle filters used in hidden Markov model are appreciated during the massive construction of the most probable digital twins. The feature space mapping in wavelet time series, Bayesian deep learning in recurrent neural network, kernel induced Hamiltonian dynamics ABC, and hybrid SDE-kernel based forecasting for time series analysis embodies the particle generator in the GAN structure. We also apply time series clustering, RNN, bagging and boosting, and semi-modeless assimilation to assist the performance of the triangulated discriminator. © Published under licence by IOP Publishing Ltd."
1,10.1080/13647830.2017.1370557,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047045189&doi=10.1080%2f13647830.2017.1370557&partnerID=40&md5=aa771dda5b9d585b016cc2b318fa86c5,"This investigation tackles the probabilistic parameter estimation problem involving the Arrhenius parameters for the rate coefficient of the chain branching reaction H + O2 → OH + O. This is achieved in a Bayesian inference framework that uses indirect data from the literature in the form of summary statistics by approximating the maximum entropy solution with the aid of approximate bayesian computation. The summary statistics include nominal values and uncertainty factors of the rate coefficient, obtained from shock-tube experiments performed at various initial temperatures. The Bayesian framework allows for the incorporation of uncertainty in the rate coefficient of a secondary reaction, namely OH + H2 → H2O + H, resulting in a consistent joint probability density on Arrhenius parameters for the two rate coefficients. It also allows for uncertainty quantification in numerical ignition predictions while conforming with the published summary statistics. The method relies on probabilistic reconstruction of the unreported data, OH concentration profiles from shock-tube experiments, along with the unknown Arrhenius parameters. The data inference is performed using a Markov chain Monte Carlo sampling procedure that relies on an efficient adaptive quadrature in estimating relevant integrals needed for data likelihood evaluations. For further efficiency gains, local Padé–Legendre approximants are used as surrogates for the time histories of OH concentration, alleviating the need for 0-D auto-ignition simulations. The reconstructed realisations of the missing data are used to provide a consensus joint posterior probability density on the unknown Arrhenius parameters via probabilistic pooling. Uncertainty quantification analysis is performed for stoichiometric hydrogen–air auto-ignition computations to explore the impact of uncertain parameter correlations on a range of quantities of interest. © 2017, This work was authored as part of the Contributor's official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 USC. 105, no copyright protection is available for such works under US Law."
,10.1088/1742-6596/1047/1/012014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050476726&doi=10.1088%2f1742-6596%2f1047%2f1%2f012014&partnerID=40&md5=852d9c6e48e0f1fb8a90dba5ee2ba135,"Estimating properties using maximum likelihood (MLE) gives point values and a rough idea of the uncertainty in the parameters if their distributions are approximately normal. More accurate results require a Monte Carlo approach, often using the Markov Chain Monte Carlo (MCMC) method, which for complex models is unrealistic because of the computational expense. The Variational Bayes approach gives results comparable to MCMC with only a few evaluation of the model. When the model is imprecise or expensive to evaluate, Gaussian processes provide one means of analyzing the noise to determine the uncertainties. © Published under licence by IOP Publishing Ltd."
,10.1080/10543406.2017.1372768,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030570997&doi=10.1080%2f10543406.2017.1372768&partnerID=40&md5=5c70dd99ee4cce95c0aa822de25db432,"Receiver operating characteristic (ROC) curve is a well-established analysis method to evaluate biomarker’s discrimination accuracy for binary outcomes. When the endpoint of interest is time to event outcome such as time to cancer recurrence, a biomarker’s time-varying discriminatory performance is often assessed by time-dependent ROC analysis. In practice, biomarkers are often imprecisely measured due to the limitation of assay sensitivity. The values below the limit of detection are not detectable. Ignorance of such data characteristic may lead to inaccurate estimation of marker’s potential discriminatory power. The objective of this article is to extend time-dependent ROC method to censored biomarker data by using parameter estimates from the Cox regression model that accommodates censored biomarker measurements. In the simulation study, the proposed methods are shown to outperform the simple substitution method that has been conventionally adopted for handling censored data. Application data are also given to illustrate our methods. © 2018, © 2018 Taylor &amp; Francis."
,10.1080/02664763.2017.1386771,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031781964&doi=10.1080%2f02664763.2017.1386771&partnerID=40&md5=8f502f674eda65cbfba5f9b2915fe003,"A spatial hidden Markov model (SHMM) is introduced to analyse the distribution of a species on an atlas, taking into account that false observations and false non-detections of the species can occur during the survey, blurring the true map of presence and absence of the species. The reconstruction of the true map is tackled as the restoration of a degraded pixel image, where the true map is an autologistic model, hidden behind the observed map, whose normalizing constant is efficiently computed by simulating an auxiliary map. The distribution of the species is explained under the Bayesian paradigm and Markov chain Monte Carlo (MCMC) algorithms are developed. We are interested in the spatial distribution of the bird species Greywing Francolin in the south of Africa. Many climatic and land-use explanatory variables are also available: they are included in the SHMM and a subset of them is selected by the mutation operators within the MCMC algorithm. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/09637486.2017.1402868,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034810057&doi=10.1080%2f09637486.2017.1402868&partnerID=40&md5=b28a1845f24f53a3197555a9f108810b,"Aim: The meta-analysis was conducted to estimate of the cardiovascular benefits of indiscriminate supplementation of omega-3 capsules. The results, expressed in terms of quality adjusted life years (QALY) intuitively understood by the general public, can be the basis for the (personal) decision on whether to take omega-3 supplements. Methods: The results of meta-analysis of eight double-blind, placebo-controlled clinical trials are expressed in terms of QALY, using the Markov model and Monte Carlo simulations. Results: Omega-3 supplementation results in a 8% decrease of the risk of cardiac death, unless the patients are treated by statins. Results indicate that omega-3 supplementation may prolong QALY by about a month. Old people gain less, whereas DM-2 patients and people with history of CV events gain more. Discussion: Our analysis yielded an algorithm for estimating benefit from omega-3 supplementation, based on the age and the individual risk of CV events of the patient. © 2017, © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/07350015.2018.1469998,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049803072&doi=10.1080%2f07350015.2018.1469998&partnerID=40&md5=4eaa542dd629ad8beee901fb1b4b20ce,"This article extends the literature on copulas with discrete or continuous marginals to the case where some of the marginals are a mixture of discrete and continuous components. We do so by carefully defining the likelihood as the density of the observations with respect to a mixed measure. The treatment is quite general, although we focus on mixtures of Gaussian and Archimedean copulas. The inference is Bayesian with the estimation carried out by Markov chain Monte Carlo. We illustrate the methodology and algorithms by applying them to estimate a multivariate income dynamics model. Supplementary materials for this article are available online. &#xa9; 2018 American Statistical Association"
,10.1080/10589759.2018.1449841,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044526731&doi=10.1080%2f10589759.2018.1449841&partnerID=40&md5=9d0d85bbc97396adf02746c0158a0856,"In current evaluation of ancient stone monuments, 2-D ultrasonic tomography is widely used. The detection result, however, has deviation in the location and the shape since the ultrasonic velocity of voids and cracks area change much more drastic than the healthy parts. Base on the homogeneous material assumption, a contour node representation is therefore proposed fitting the best defect shape and location. With the contour node representation, a quick ray tracing method is proposed. This inverse problem is solved under the Bayesian framework in which we use Markov chain Monte Carlo to search the global optimal result. Through applying more prior knowledge to the model and the quick ray tracing model, the computational cost of the inverse process is highly reduced. Compared to the ultrasonic tomography result, the proposed method has more accurate location and shape in 2-D inspection. Finally, the laboratory experiment also tests the ability of the proposed method. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1109/MECBME.2018.8402432,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050036964&doi=10.1109%2fMECBME.2018.8402432&partnerID=40&md5=3860121426861aa7a52b04ddf0549750,This paper presents a sparse Bayesian regularization technique for image restoration in parallel magnetic resonance imaging (pMRI). This technique is based on a hierarchical Bayesian model that solves the inverse problem of pMRI reconstruction by promoting sparsity using a Bernoulli-Laplace mixture prior. A Markov Chain Monte Carlo (MCMC) sampling technique is used to numerically approximate the target posterior. Our model allows handling complex-valued data. Promising results obtained on synthetic data demonstrate the performance of the proposed sparse Bayesian restoration model to provide accurate estimation of the target images. © 2018 IEEE.
,10.1080/01621459.2018.1448824,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054626424&doi=10.1080%2f01621459.2018.1448824&partnerID=40&md5=1fc233efa72e231831a888717213d089,"Models with intractable normalizing functions arise frequently in statistics. Common examples of such models include exponential random graph models for social networks and Markov point processes for ecology and disease modeling. Inference for these models is complicated because the normalizing functions of their probability distributions include the parameters of interest. In Bayesian analysis, they result in so-called doubly intractable posterior distributions which pose significant computational challenges. Several Monte Carlo methods have emerged in recent years to address Bayesian inference for such models. We provide a framework for understanding the algorithms, and elucidate connections among them. Through multiple simulated and real data examples, we compare and contrast the computational and statistical efficiency of these algorithms and discuss their theoretical bases. Our study provides practical recommendations for practitioners along with directions for future research for Markov chain Monte Carlo (MCMC) methodologists. Supplementary materials for this article are available online. © 2018, © 2018 American Statistical Association."
,10.1080/01621459.2017.1409122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049121069&doi=10.1080%2f01621459.2017.1409122&partnerID=40&md5=4e3cd9f2dcfe5b3e6b7bea89aadcad4a,"Recent advances in high-throughput biotechnologies have provided an unprecedented opportunity for biomarker discovery, which, from a statistical point of view, can be cast as a variable selection problem. This problem is challenging due to the high-dimensional and nonlinear nature of omics data and, in general, it suffers three difficulties: (i) an unknown functional form of the nonlinear system, (ii) variable selection consistency, and (iii) high-demanding computation. To circumvent the first difficulty, we employ a feed-forward neural network to approximate the unknown nonlinear function motivated by its universal approximation ability. To circumvent the second difficulty, we conduct structure selection for the neural network, which induces variable selection, by choosing appropriate prior distributions that lead to the consistency of variable selection. To circumvent the third difficulty, we implement the population stochastic approximation Monte Carlo algorithm, a parallel adaptive Markov Chain Monte Carlo algorithm, on the OpenMP platform that provides a linear speedup for the simulation with the number of cores of the computer. The numerical results indicate that the proposed method can work very well for identification of relevant variables for high-dimensional nonlinear systems. The proposed method is successfully applied to identification of the genes that are associated with anticancer drug sensitivities based on the data collected in the cancer cell line encyclopedia study. Supplementary materials for this article are available online. © 2018, © 2018 American Statistical Association."
,10.1080/10618600.2017.1395343,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048132059&doi=10.1080%2f10618600.2017.1395343&partnerID=40&md5=22be4b3ffd0c6de0cde4183f227f2f31,"The normal and the t distribution are classical tools for building random effects regression models where both can be used for the specification of either the conditional response distribution or the random effects distribution. However, the underlying assumption of symmetry can be questionable in many applications. We, therefore, propose regression models where the skew-normal and skew-t distribution are considered for both the response and the random effects specification and embed these models in the framework of distributional regression such that regression predictors can be specified for all distributional parameters. The distributional regression framework also allows us to consider multivariate versions of the skew-normal and the skew-t distribution. For Bayesian inference, we adapt iteratively weighted least-square proposals within Markov chain Monte Carlo simulations such that they can also facilitate the inclusion of nonnormal random effects specifications. Model choice is based on the Watanabe–Akaike information criterion, in particular, to differentiate between skew and nonskew distributional specifications in a number of simulation studies. Finally, to illustrate their practical applicability, the developed models are applied to a study on cholesterol levels originating from the Framingham Heart Study and a dataset from the Demographic and Health Surveys on undernutrition among children in Nigeria. Supplementary material for this article is available online. © 2018, © 2018 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
,10.1007/s10955-018-2103-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049568453&doi=10.1007%2fs10955-018-2103-0&partnerID=40&md5=5170229183eb062216c6697df59cf9c5,"We study models of weighted exponential random graphs in the large network limit. These models have recently been proposed to model weighted network data arising from a host of applications including socio-econometric data such as migration flows and neuroscience. Analogous to fundamental results derived for standard (unweighted) exponential random graph models in the work of Chatterjee and Diaconis, we derive limiting results for the structure of these models as the number of nodes goes to infinity. Our results are applicable for a wide variety of base measures including measures with unbounded support. We also derive sufficient conditions for continuity of functionals in the specification of the model including conditions on nodal covariates. Finally we include a number of open problems to spur further understanding of this model especially in the context of applications. © 2018 Springer Science+Business Media, LLC, part of Springer Nature"
1,10.1080/10618600.2017.1415911,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047312427&doi=10.1080%2f10618600.2017.1415911&partnerID=40&md5=a891fe8109a1c433e4aee29bb5192362,"Although the Metropolis algorithm is simple to implement, it often has difficulties exploring multimodal distributions. We propose the repelling–attracting Metropolis (RAM) algorithm that maintains the simple-to-implement nature of the Metropolis algorithm, but is more likely to jump between modes. The RAM algorithm is a Metropolis-Hastings algorithm with a proposal that consists of a downhill move in density that aims to make local modes repelling, followed by an uphill move in density that aims to make local modes attracting. The downhill move is achieved via a reciprocal Metropolis ratio so that the algorithm prefers downward movement. The uphill move does the opposite using the standard Metropolis ratio which prefers upward movement. This down-up movement in density increases the probability of a proposed move to a different mode. Because the acceptance probability of the proposal involves a ratio of intractable integrals, we introduce an auxiliary variable which creates a term in the acceptance probability that cancels with the intractable ratio. Using several examples, we demonstrate the potential for the RAM algorithm to explore a multimodal distribution more efficiently than a Metropolis algorithm and with less tuning than is commonly required by tempering-based methods. Supplementary materials are available online. © 2018, © 2018 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
,10.1080/10485252.2018.1470241,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046892481&doi=10.1080%2f10485252.2018.1470241&partnerID=40&md5=4aeaa13770e64660066d949b2b8f0f98,"In the last 20 years, a lot of achievements have been made in the study of posterior contraction rates of nonparametric Bayesian methods, and plenty of them involve sieve priors, but mainly for specific models or sieves. We provide a posterior contraction theorem for general parametric sieve priors. The theorem has weaker and simpler conditions compared with the existing results, and indicates that the sieve prior is rate adaptive. We apply the general theorem to density estimations and nonparametric regression with jumps. We also provided a reversible jump MCMC (Markov Chain Monte Carlo) algorithm for the sieve prior. © 2018, © American Statistical Association and Taylor & Francis 2018."
1,10.1080/15598608.2018.1431575,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042232719&doi=10.1080%2f15598608.2018.1431575&partnerID=40&md5=fa325eac4befa8653cfa5edfdcf0190e,"This article deals with the problem of estimating parameters of the modified Weibull distribution (MWD) using a progressively type-II censored sample under the constant-stress partially accelerated life test model. The maximum likelihood, Bayes, and parametric bootstrap methods are obtained as point estimations for the distribution parameters and the acceleration factor. Furthermore, the approximate confidence intervals (ACIs), bootstrap confidence intervals, and credible intervals of the estimators have been obtained. The results of Bayes estimators are computed under the squared error loss (SEL) function using the Markov-chain Monte Carlo (MCMC) method. Gibbs sampling within the Metropolis–Hasting algorithm is applied to generate MCMC samples from the posterior density functions. Analysis of a simulated data set has been presented for illustrative purposes. Finally, a Monte Carlo simulation study is carried out to investigate the precision of the Bayes estimates with maximum likelihood estimates and two bootstrap estimates, also to compare the performance of different corresponding confidence intervals considered. © 2018, © 2018 Grace Scientific Publishing, LLC."
3,10.1080/10618600.2017.1407325,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053593442&doi=10.1080%2f10618600.2017.1407325&partnerID=40&md5=41725e69f4aecafe6dff3b5ce2f3f229,"Bayesian analysis provides a convenient setting for the estimation of complex generalized additive regression models (GAMs). Since computational power has tremendously increased in the past decade, it is now possible to tackle complicated inferential problems, for example, with Markov chain Monte Carlo simulation, on virtually any modern computer. This is one of the reasons why Bayesian methods have become increasingly popular, leading to a number of highly specialized and optimized estimation engines and with attention shifting from conditional mean models to probabilistic distributional models capturing location, scale, shape (and other aspects) of the response distribution. To embed many different approaches suggested in literature and software, a unified modeling architecture for distributional GAMs is established that exploits distributions, estimation techniques (posterior mode or posterior mean), and model terms (fixed, random, smooth, spatial,…). It is shown that within this framework implementing algorithms for complex regression problems, as well as the integration of already existing software, is relatively straightforward. The usefulness is emphasized with two complex and computationally demanding application case studies: a large daily precipitation climatology, as well as a Cox model for continuous time with space-time interactions. Supplementary material for this article is available online. © 2018, © 2018 The Author(s). Published with license by Taylor & Francis Group, LLC. © 2018, © Nikolaus Umlauf, Nadja Klein, and Achim Zeileis."
,10.1080/01621459.2017.1379402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049145270&doi=10.1080%2f01621459.2017.1379402&partnerID=40&md5=fa9826d97967319c528a5efa10fb3843,"Many researchers in biology and medicine have focused on trying to understand biological rhythms and their potential impact on disease. A common biological rhythm is circadian, where the cycle repeats itself every 24 hours. However, a disturbance of the circadian pattern may be indicative of future disease. In this article, we develop new statistical methodology for assessing the degree of disturbance or irregularity in a circadian pattern for count sequences that are observed over time in a population of individuals. We develop a latent variable Poisson modeling approach with both circadian and stochastic short-term trend (autoregressive latent process) components that allow for individual variation in the degree of each component. A parameterization is proposed for modeling covariate dependence on the proportion of these two model components across individuals. In addition, we incorporate covariate dependence in the overall mean, the magnitude of the trend, and the phase-shift of the circadian pattern. Innovative Markov chain Monte Carlo sampling is used to carry out Bayesian posterior computation. Several variations of the proposed models are considered and compared using the deviance information criterion. We illustrate this methodology with longitudinal physical activity count data measured in a longitudinal cohort of adolescents. © 2018, In the Public Domain."
,10.5194/hess-22-3561-2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049389989&doi=10.5194%2fhess-22-3561-2018&partnerID=40&md5=2d22e64d7edcf1f0cabd7c1fcd6dea90,"Fluid flow in a charged porous medium generates electric potentials called streaming potential (SP). The SP signal is related to both hydraulic and electrical properties of the soil. In this work, global sensitivity analysis (GSA) and parameter estimation procedures are performed to assess the influence of hydraulic and geophysical parameters on the SP signals and to investigate the identifiability of these parameters from SP measurements. Both procedures are applied to a synthetic column experiment involving a falling head infiltration phase followed by a drainage phase. GSA is used through variance-based sensitivity indices, calculated using sparse polynomial chaos expansion (PCE). To allow high PCE orders, we use an efficient sparse PCE algorithm which selects the best sparse PCE from a given data set using the Kashyap information criterion (KIC). Parameter identifiability is performed using two approaches: the Bayesian approach based on the Markov chain Monte Carlo (MCMC) method and the first-order approximation (FOA) approach based on the Levenberg-Marquardt algorithm. The comparison between both approaches allows us to check whether FOA can provide a reliable estimation of parameters and associated uncertainties for the highly nonlinear hydrogeophysical problem investigated. GSA results show that in short time periods, the saturated hydraulic conductivity (Ks) and the voltage coupling coefficient at saturation (Csat) are the most influential parameters, whereas in long time periods, the residual water content (θs), the Mualem-van Genuchten parameter (n) and the Archie saturation exponent (na) become influential, with strong interactions between them. The Mualem-van Genuchten parameter (α) has a very weak influence on the SP signals during the whole experiment. Results of parameter estimation show that although the studied problem is highly nonlinear, when several SP data collected at different altitudes inside the column are used to calibrate the model, all hydraulic (Ks, θs,α,n) and geophysical parameters (na,Csat) can be reasonably estimated from the SP measurements. Further, in this case, the FOA approach provides accurate estimations of both mean parameter values and uncertainty regions. Conversely, when the number of SP measurements used for the calibration is strongly reduced, the FOA approach yields accurate mean parameter values (in agreement with MCMC results) but inaccurate and even unphysical confidence intervals for parameters with large uncertainty regions. © 2018 Author(s)."
,10.1190/geo2016-0594.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048483218&doi=10.1190%2fgeo2016-0594.1&partnerID=40&md5=08e288b0773610ebe2aeba60fcd6d52e,"We applied a transdimensional stochastic inversion algorithm, reversible jump Markov chain Monte Carlo (rjMCMC), to angle-stack seismic inversion for characterization of reservoir acoustic and shear impedance with uncertainty quantification. The rjMCMC is able to infer the number of parameters for the model as well as the parameter values. In our case, the number of parameters depends on the number of model layers for a given data set. We also use this method in uncertainty quantification because a transdimensional sampling helps prevent underparameterization or strong overparameterization. An ensemble of models with proper parameterization can improve parameter estimation and uncertainty quantification. Our new results in uncertainty analysis indicate that (1) the uncertainty in seismic inversion, including uncertainty in earth properties and their locations, is related to the discontinuity of property across an interface, and (2) there is a trade-off between property uncertainty and location uncertainty. A stronger discontinuity will induce more property uncertainty but less location uncertainty at the discontinuity interface. Therefore, we further use the inversion uncertainty as a novel seismic attribute to assist in delineation of subsurface discontinuity interfaces and quantify the magnitude of the discontinuities, which further facilitates quantitative interpretation and stratigraphic interpretation. © 2018 Society of Exploration Geophysicists."
,10.1016/j.petrol.2018.03.062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044755051&doi=10.1016%2fj.petrol.2018.03.062&partnerID=40&md5=b3efbc2d267eac271862024056c95854,"We present a probabilistic approach for integrating multiple data types into subsurface flow models. Our approach is based on a Bayesian framework whereby we exhaustively sample the multi-dimensional posterior distribution to define a Pareto front which represents the trade-off between multiple objectives during history matching. These objectives can be matching of water-cut, GOR, BHP and time-lapse seismic data. For field applications, these objectives do not necessarily move in tandem because of measurement errors and also interpretative nature of the seismic data. Our proposed method is built on a Differential Evolution Markov Chain Monte Carlo (DEMC) algorithm in which multiple Markov Chains are run in parallel. First, a dominance relationship is established amongst multiple models. This is followed by construction of the posterior distribution based on a hypervolume measure. A unique aspect of our method is in the parameter proposal generation which is based on a random walk on two arbitrarily selected chains. This promotes effective mixing of the chains resulting in improved convergence. We illustrate the algorithm using a nine-spot waterflood model whereby we use water-cut and bottomhole flowing pressure data to calibrate the permeability field. The permeability field is re-parameterized using a previously proposed Grid Connectivity Transform (GCT) which is a model order reduction technique defined based only on the decomposition of the grid Laplacian. The compression power of GCT allows us to reconstruct the permeability field with few parameters, thus significantly improving the computational efficiency of the McMC approach. Next, we applied the method to the Brugge benchmark case involving 10 water injectors and 20 producers. For both cases, the algorithm provides an ensemble of models all constrained to the history data and defines a probabilistic Pareto front in the objective space. Several experimental runs were conducted to compare the effectiveness of the algorithm with Non-Dominated Sorting Genetic Algorithms (NSGA-II). Higher hypervolume was constantly measured using our algorithm which indicates that more optimal solutions were sampled. Our method provides a novel approach for subsurface model calibration and uncertainty quantification using McMC in which the communication between parallel Markov chains enhances adequate mixing. This significantly improves the convergence without loss in sampling quality. © 2018 Elsevier B.V."
2,10.1016/j.ijepes.2018.01.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044635599&doi=10.1016%2fj.ijepes.2018.01.008&partnerID=40&md5=efc0722348eab6ade191d15ff17c7560,"As the penetration of electric vehicles (EVs) increases, their patterns of use need to be well understood for future system planning and operating purposes. Using high resolution data, accurate driving patterns were generated by a Markov Chain Monte Carlo (MCMC) simulation. The simulated driving patterns were then used to undertake an uncertainty analysis on the network impact due to EV charging. Case studies of workplace and domestic uncontrolled charging are investigated. A 99% confidence interval is adopted to represent the associated uncertainty on the following grid operational metrics: network voltage profile and line thermal performance. In the home charging example, the impact of EVs on the network is compared for weekday and weekend cases under different EV penetration levels. © 2018 Elsevier Ltd"
1,10.1177/0962280217747054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047953488&doi=10.1177%2f0962280217747054&partnerID=40&md5=6c1cdb8fa20cc476729ebe2af056bb20,"Simple mechanistic epidemic models are widely used for forecasting and parameter estimation of infectious diseases based on noisy case reporting data. Despite the widespread application of models to emerging infectious diseases, we know little about the comparative performance of standard computational-statistical frameworks in these contexts. Here we build a simple stochastic, discrete-time, discrete-state epidemic model with both process and observation error and use it to characterize the effectiveness of different flavours of Bayesian Markov chain Monte Carlo (MCMC) techniques. We use fits to simulated data, where parameters (and future behaviour) are known, to explore the limitations of different platforms and quantify parameter estimation accuracy, forecasting accuracy, and computational efficiency across combinations of modeling decisions (e.g. discrete vs. continuous latent states, levels of stochasticity) and computational platforms (JAGS, NIMBLE, Stan). © 2017, © The Author(s) 2017."
,10.1029/2018WR022658,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050928412&doi=10.1029%2f2018WR022658&partnerID=40&md5=80ae85e4dade4e8759eda64ef8e66cdb,"Markov chain Monte Carlo (MCMC) simulation methods are widely used to assess parametric uncertainties of hydrologic models conditioned on measurements of observable state variables. However, when the model is CPU-intensive and high dimensional, the computational cost of MCMC simulation will be prohibitive. In this situation, a CPU-efficient while less accurate low-fidelity model (e.g., a numerical model with a coarser discretization or a data-driven surrogate) is usually adopted. Nowadays, multifidelity simulation methods that can take advantage of both the efficiency of the low-fidelity model and the accuracy of the high-fidelity model are gaining popularity. In the MCMC simulation, as the posterior distribution of the unknown model parameters is the region of interest, it is wise to distribute most of the computational budget (i.e., the high-fidelity model evaluations) therein. Based on this idea, in this paper we propose an adaptive multifidelity MCMC algorithm for efficient inverse modeling of hydrologic systems. In this method, we evaluate the high-fidelity model mainly in the posterior region through iteratively running MCMC based on a Gaussian process system that is adaptively constructed with multifidelity simulation. The error of the Gaussian process system is rigorously considered in the MCMC simulation and gradually reduced to a negligible level in the posterior region. Thus, the proposed method can obtain an accurate estimate of the posterior distribution with a small number of the high-fidelity model evaluations. The performance of the proposed method is demonstrated by three numerical case studies in inverse modeling of hydrologic systems. ©2018. American Geophysical Union. All Rights Reserved."
,10.5004/dwt.2018.22381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054851138&doi=10.5004%2fdwt.2018.22381&partnerID=40&md5=a2680318edfbbd4482f293460cb95173,"The soil water characteristic curve (SWCC) is an important property for unsaturated soils and is essential to unsaturated soil engineering analysis. There is significant uncertainty of SWCC obtained by experiment due to the complicated unmodelled influencing factors on SWCC. In this paper, regarding the fitting parameters in Fredlund and Xing (FX) model, Van Genuchten (VG) model, and Gardner model as the random vectors, the uncertainty of SWCC fitting parameters is evaluated using the Bayesian framework. This framework is demonstrated using sandy experimental data with 1,030 records in UNSODA. The posterior distributions of fitting parameters are obtained by the Markov chain Monte Carlo simulation. Different levels of confidence intervals of fitting parameters for FX, VG and Gardner models are obtained intuitively by proposed Bayesian framework. It is found that the confidence interval of the VG model is narrowest, and its uncertainty is the lowest. Different levels of confidence intervals of SWCC with VG model are applied in the one-dimensional vertical soil water filtration. The results demonstrated that the uncertainty in SWCC had significant effects on soil water infiltration. © 2018, Desalination Publications. All rights reserved."
,10.1016/j.forsciint.2018.04.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046170124&doi=10.1016%2fj.forsciint.2018.04.003&partnerID=40&md5=7cb9039f2ca15c2ebcb7571780264076,Closed Circuit TV (CCTV) systems often record vehicle motion prior to incidents. From the footage an estimate of the average speed of the vehicle between two frames can be calculated. In a forensic investigation not only an estimate of the average speed is needed but also an estimation of the measurement error. In earlier papers an approach was explained how to estimate the average speed and the corresponding uncertainty in terms of a confidence interval. In practice confidence intervals are often wrongly interpreted as being probability intervals. In this paper we show how to use the Markov Chain Monte Carlo approach to derive probability intervals instead of confidence intervals. We show the robustness of the Markov Chain Monte Carlo approach and the numerical differences between both approaches. In casework the difference between confidence intervals and probability intervals turns out to be very limited. As a consequence the impact of confusion between confidence and probability intervals can also be expected to be limited. © 2018 Elsevier B.V.
,10.3847/1538-4365/aaca30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051164086&doi=10.3847%2f1538-4365%2faaca30&partnerID=40&md5=8c57d0fcd882b37c18ef12d09e364256,"By employing Monte Carlo random sampling, traditional binary population synthesis (BPS) offers a substantial improvement in efficiency over brute force, grid-based studies. Even so, BPS models typically require a large number of simulation realizations, a computationally expensive endeavor, to generate statistically robust results. Recent advances in statistical methods have led us to revisit the traditional approach to BPS. In this work we describe our publicly available code dart-board, which combines rapid binary evolution codes, typically used in traditional BPS, with modern Markov chain Monte Carlo methods. dart-board takes a novel approach that treats the initial binary parameters and the supernova kick vector as model parameters. This formulation has several advantages, including the ability to model either populations of systems or individual binaries, the natural inclusion of observational uncertainties, and the flexible addition of new constraints, which are problematic to include using traditional BPS. After testing our code with mock systems, we demonstrate the flexibility of dart-board by applying it to three examples: (i) a generic population of high-mass X-ray binaries (HMXBs); (ii) the population of HMXBs in the Large Magellanic Cloud (LMC), in which the spatially resolved star formation history is used as a prior; and (iii) one particular HMXB in the LMC, Swift J0513.4-6547, in which we include observations of the system's component masses and orbital period. Although this work focuses on HMXBs, dart-board can be applied to a variety of stellar binaries, including the recent detections by gravitational wave observatories of merging compact object binaries. © 2018. The American Astronomical Society. All rights reserved.."
1,10.1016/j.jmva.2018.03.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045195363&doi=10.1016%2fj.jmva.2018.03.012&partnerID=40&md5=a13cf46ad5e0179b35788e73fde47730,"Let π denote the intractable posterior density that results when the likelihood from a multivariate linear regression model with errors from a scale mixture of normals is combined with the standard non-informative prior. There is a simple data augmentation algorithm (based on latent data from the mixing density) that can be used to explore π. Let h and d denote the mixing density and the dimension of the regression model, respectively. Hobert et al. (2018) have recently shown that, if h converges to 0 at the origin at an appropriate rate, and ∫0 ∞ud∕2h(u)du&lt;∞ then the Markov chains underlying the data augmentation (DA) algorithm and an alternative Haar parameter expanded DA (PX-DA) algorithm are both geometrically ergodic. Their results are established using probabilistic techniques based on drift and minorization conditions. In this paper, spectral analytic techniques are used to establish that something much stronger than geometric ergodicity often holds. In particular, it is shown that, under simple conditions on h, the Markov operators defined by the DA and Haar PX-DA Markov chains are trace-class, i.e., compact with summable eigenvalues. Many standard mixing densities satisfy the conditions developed in this paper. Indeed, the new results imply that the DA and Haar PX-DA Markov operators are trace-class whenever the mixing density is generalized inverse Gaussian, log-normal, Fréchet (with shape parameter larger than d∕2), or inverted Gamma (with shape parameter larger than d∕2). © 2018 Elsevier Inc."
,10.1029/2017WR021176,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050936752&doi=10.1029%2f2017WR021176&partnerID=40&md5=c28f45ce8a557569e00e3fc7ac275293,"Dam breach models are commonly used to predict outflow hydrographs of potentially failing dams and are key ingredients for evaluating flood risk. In this paper a new dam breach modeling framework is introduced that shall improve the reliability of hydrograph predictions of homogeneous earthen embankment dams. Striving for a small number of parameters, the simplified physics-based model describes the processes of failing embankment dams by breach enlargement, driven by progressive surface erosion. Therein the erosion rate of dam material is modeled by empirical sediment transport formulations. Embedding the model into a Bayesian multilevel framework allows for quantitative analysis of different categories of uncertainties. To this end, data available in literature of observed peak discharge and final breach width of historical dam failures were used to perform model inversion by applying Markov chain Monte Carlo simulation. Prior knowledge is mainly based on noninformative distribution functions. The resulting posterior distribution shows that the main source of uncertainty is a correlated subset of parameters, consisting of the residual error term and the epistemic term quantifying the breach erosion rate. The prediction intervals of peak discharge and final breach width are congruent with values known from literature. To finally predict the outflow hydrograph for real case applications, an alternative residual model was formulated that assumes perfect data and a perfect model. The fully probabilistic fashion of hydrograph prediction has the potential to improve the adequate risk management of downstream flooding. ©2018. American Geophysical Union. All Rights Reserved."
,10.1029/2017WR022185,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050334686&doi=10.1029%2f2017WR022185&partnerID=40&md5=479935f9ac6ca08f37743015663b7348,"We develop a Bayesian model to predict the maximum thickness of seasonally frozen ground (MTSFG) using historical air temperature and precipitation observations. We use the Stefan solution and meteorological data from 11 stations to estimate the MTSFG changes from 1961 to 2016 in the Yellow River source region of northwestern China. We employ an antecedent precipitation index model to estimate changes in the liquid soil water content. The marginal posterior probability distributions of the antecedent precipitation index parameters are estimated using Markov chain Monte Carlo sampling methods. We compare the results of our stochastic method with those obtained from the traditional deterministic method and find that they are consistent in general. The stochastic approach is effective for estimating the historical changes in the frozen ground depth (root-mean-square errors = 0.13–0.35 m), and it provides more information on model uncertainty regarding soil moisture variations. Additionally, simulation shows that the MTSFG has decreased by 0.31 cm per year over the last 56 years on the northeastern Qinghai-Tibet Plateau. This decrease in frost depth accelerated in the 1990s and 2000s. Considering the lack of data on seasonally frozen soil monitoring, the Bayesian method provides a pragmatic approach to statistically model frozen ground changes using available meteorological data. ©2018. American Geophysical Union. All Rights Reserved."
,10.1109/TNNLS.2017.2688499,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022088337&doi=10.1109%2fTNNLS.2017.2688499&partnerID=40&md5=020bb10f7333c1289339806995d58232,"Deep generative models (DGMs), which are often organized in a hierarchical manner, provide a principled framework of capturing the underlying causal factors of data. Recent work on DGMs focussed on the development of efficient and scalable variational inference methods that learn a single model under some mean-field or parameterization assumptions. However, little work has been done on extending Markov chain Monte Carlo (MCMC) methods to Bayesian DGMs, which enjoy many advantages compared with variational methods. We present doubly stochastic gradient MCMC, a simple and generic method for (approximate) Bayesian inference of DGMs in a collapsed continuous parameter space. At each MCMC sampling step, the algorithm randomly draws a mini-batch of data samples to estimate the gradient of log-posterior and further estimates the intractable expectation over hidden variables via a neural adaptive importance sampler, where the proposal distribution is parameterized by a deep neural network and learnt jointly along with the sampling process. We demonstrate the effectiveness of learning various DGMs on a wide range of tasks, including density estimation, data generation, and missing data imputation. Our method outperforms many state-of-the-art competitors. © 2012 IEEE."
,10.1098/rsif.2018.0318,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051432422&doi=10.1098%2frsif.2018.0318&partnerID=40&md5=026990aefe4c8efa471a3b784b3626f8,"As systems approaches to the development of biological models become more mature, attention is increasingly focusing on the problem of inferring parameter values within those models from experimental data. However, particularly for nonlinear models, it is not obvious, either from inspection of the model or from the experimental data, that the inverse problem of parameter fitting will have a unique solution, or even a non-unique solution that constrains the parameters to lie within a plausible physiological range. Where parameters cannot be constrained they are termed 'unidentifiable'. We focus on gaining insight into the causes of unidentifiability using inference-based methods, and compare a recently developed measure-theoretic approach to inverse sensitivity analysis to the popular Markov chain Monte Carlo and approximate Bayesian computation techniques for Bayesian inference. All three approaches map the uncertainty in quantities of interest in the output space to the probability of sets of parameters in the input space. The geometry of these sets demonstrates how unidentifiability can be caused by parameter compensation and provides an intuitive approach t inference-based experimental design. © 2018 The Author(s) Published by the Royal Society. All rights reserved."
1,10.1016/j.trc.2018.05.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047600880&doi=10.1016%2fj.trc.2018.05.017&partnerID=40&md5=2a3e9e9a94d543dce0c008a4bb3d001d,"Driving behavior in general is considered a leading cause of intersection related traffic crashes. However, due to unavailability of real-world driving data, intersection safety performance evaluations are largely reactive where state-of-the-art methods are applied to analyze historical crash data. In this regard, the emerging connected vehicles technology provides a promising opportunity for investigating intersection safety more from a proactive perspective. Driving volatility captures the extent of variations in instantaneous driving decisions when a vehicle is being driven. This study develops a fundamental understanding of microscopic driving volatility and how it relates to unsafe outcomes at intersections. Using high resolution driving data from a real-world connected vehicle testbed, Safety Pilot Model Deployment, in Ann Arbor, Michigan, a methodology is presented to quantify driving volatility at 116 intersections by analyzing more than 230 million real-world Basic Safety Messages. For proactive intersection safety evaluation, the large-scale connected vehicle data is then linked to detailed intersection data containing crashes, traffic exposure, and other geometric features. By using vehicular speed, acceleration/deceleration, and vehicular jerk based eight different volatility measures, descriptive analysis is performed to spot differences between driving volatility at signalized and un-signalized intersections. Then, in-depth statistical analysis is conducted separately for all intersections (signalized and un-signalized) and signalized intersections only. Importantly, not all factors that may influence crash frequency can be observed in the data. If unobserved factors could be included in a model, then correlations between driving volatility and crash frequency can change, e.g., the relationship can become statistically insignificant. Given the important methodological concerns of unobserved heterogeneity and potential omitted variable bias, hierarchical fixed- and random-parameter Poisson and Poisson log-normal models are estimated. Full Bayesian estimation via Markov Chain Monte Carlo (MCMC) based Gibbs sampling is performed, providing more efficient results. For all intersections, after controlling for traffic exposure, geometrics, and unobserved factors, a one-percent increase in intersection-level volatility calculated through two standard deviations threshold for acceleration/deceleration, passing level volatility captured through coefficient of variation of speed, and mean absolute deviance of vehicular jerk results in a 1.25%, 0.25%, and 0.35% increase in crash frequencies respectively. However, the relationships between intersection-specific volatility and crash frequencies are different for signalized intersections. Several of the exogenous factors are found to be normally distributed random parameters, suggesting that the effects of such variables vary across different intersections. The implications of the findings for proactive safety management are discussed. © 2018"
,10.1177/0954405416673109,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053472090&doi=10.1177%2f0954405416673109&partnerID=40&md5=78d5fdd8a449202a481985b643e6ea02,"The hazard rate curve of the numerical control machine tool is a bathtub curve. The change point between the early failure period and the random failure period of the curve is difficult to obtain with a small data sample; thus, a Bayesian method is proposed. A method to build the prior distributions of the Weibull parameters is developed, which integrates the multi-source prior information of the target numerical control machine tool and the reference numerical control machine tool. The Markov chain Monte Carlo method is adopted to calculate the estimators of the Weibull parameters corresponding to each failure, which solves the problem of the absence of an analytical solution. The total working time of the numerical control machine tool when the estimator of the shape parameter is equal to 1 is estimated by taking the estimator of the shape parameter as the function of time. As a result, the change point and the early failure period are obtained. Comparison result shows that the result obtained through an existing change point solving method with a large dataset is close to the result generated through the proposed method with a small dataset. The change point and the early failure period obtained with the proposed method can be used to guide the early failure test and to design a rational maintenance strategy, which are of vital engineering significance. © IMechE 2016."
1,10.1016/j.jngse.2018.04.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047250044&doi=10.1016%2fj.jngse.2018.04.031&partnerID=40&md5=db1172499bc2128d6ed160e750bd7aa6,"Petrophysical parameters are of great importance in the evaluation and characterization for reservoirs, especially for the unconventional reservoirs with complex properties. The geophysical inversion is an efficient and economic method to obtain the petrophysical parameters. In this paper, Bayesian inversion method is presented to predict petrophysical model with conventional well logs. Statistical analysis results of accepted Markov chain Monte Carlo (MCMC) samples are used to study the uncertainty of forecasted parameters, since the MCMC is a powerful approach to obtain adequate samples obeying the posterior distribution of Bayesian inversion. The proposed method is applied to reservoirs of the Xiashihezi Formation which are typical tight sandstone layers in the Ordos Basin. Model prediction and corresponding uncertainty analysis are presented in detail at a specific depth. The interactive effects of multiple petrophysical parameters are investigated by correlation coefficients. Then, the accuracy and reliability of predicted model is validated by both forward log responses and core data of the whole depth interval. According to the results and discussions, it can be concluded that: (1) a reasonable prior information of model parameters will simplify the inversion problem, which provides much conveniences of statistical analysis of the MCMC samples; (2) the weak correlation between each two petrophysical parameters indicates that it is reasonable and feasible to disregard dependence of parameters; (3) synthetic logs calculated by predicted model are in good agreement with observed well logs, which implies the precision and credibility of Bayesian inversion; (4) the predicted porosity, permeability and minerals content are consistent with core data, verifying the effectiveness and reliability of proposed method and inversion results; (5) it is an advantage of Bayesian inversion to locate the most probable reservoirs with the extreme value. © 2018 Elsevier B.V."
,10.1002/eco.1957,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044731353&doi=10.1002%2feco.1957&partnerID=40&md5=2daa0d5536cc415319ce434db4fb74f9,"Evapotranspiration (ET) and net ecosystem exchange (NEE) are driven by both high and slow frequency scalar fluxes. Quantifying the variation of these two processes at different timescales remains a challenge. Bridging this knowledge gap is crucial in order to improve insights of the impact of biotic and abiotic factors modulating these fluxes as well as for accurate estimation of gross primary productivity (GPP) and ecosystem respiration (Re). This issue was addressed using a model–data fusion approach within a Bayesian framework by running the model against ET and NEE observations at three different time steps: subdaily (30 min), daily (1 day), and intermediate (7 days). The model was tested against eddy covariance data collected for a 2-month period (June and July) from a sagebrush-steppe ecosystem in the United States. The 95% credible interval (CI) of fast processes such as transpiration and photosynthesis reduced by more than 90% compared with its a priori range when model was run at 30-min time step. The reduction in CI of the same parameters varied between 30% and 70% when the model was run at 1- or 7-day time step. The 95% CI of slow process such as root respiration reduced by 89% and 73% when model was run at 7-day and 30-min time step, respectively. We found strong confidence in predicting ET and NEE at subdaily timescale, whereas uncertainty increased with increase in temporal resolution. GPP and Re varied strongly as the system transitioned from a traditionally wet (June) to a dry (July) month. Copyright © 2018 John Wiley & Sons, Ltd."
,10.1093/sysbio/syy008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050917016&doi=10.1093%2fsysbio%2fsyy008&partnerID=40&md5=e2d10c3706d5160b3f61fe99d51c4a0f,"Bayesian phylogenetic inference relies on the use of Markov chain Monte Carlo (MCMC) to provide numerical approximations of high-dimensional integrals and estimate posterior probabilities. However, MCMC performs poorly when posteriors are very rugged (i.e., regions of high posterior density are separated by regions of low posterior density). One technique that has become popular for improving numerical estimates from MCMC when distributions are rugged is Metropolis coupling (MC3). InMC3, additional chains are employed to sample flattened transformations of the posterior and improve mixing. Here, we highlight several underappreciated behaviors of MC3. Notably, estimated posterior probabilities may be incorrect but appear to converge, when individual chains do not mixwell, despite different chains sampling trees from all relevant areas in tree space. Counter intuitively, such behavior can be more difficult to diagnose with increased numbers of chains. We illustrate these surprising behaviors of MC3 using a simple, non-phylogenetic example and phylogenetic examples involving both constrained and unconstrained analyses. To detect and mitigate the effects of these behaviors, we recommend increasing the number of independent analyses and varying the temperature of the hottest chain in current versions of Bayesian phylogenetic software. Convergence diagnostics based on the behavior of the hottest chain may also help detect these behaviors and could form a useful addition to future software releases. © The Author(s) 2018."
,10.1145/3182392,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052597871&doi=10.1145%2f3182392&partnerID=40&md5=55ce0fcdfb2f0041f7d0c9b5aeb1a260,"Counting subgraphs is a fundamental analysis task for online social networks (OSNs). Given the sheer size and restricted access of OSN, efficient computation of subgraph counts is highly challenging. Although a number of algorithms have been proposed to estimate the relative counts of subgraphs in OSNs with restricted access, there are only few works which try to solve a more general problem, i.e., counting subgraph frequencies. In this article, we propose an efficient random walk-based framework to estimate the subgraph counts. Our framework generates samples by leveraging consecutive steps of the random walk as well as by observing neighbors of visited nodes. Using the importance sampling technique, we derive unbiased estimators of the subgraph counts. To make better use of the degree information of visited nodes, we also design improved estimators, which increases the accuracy of the estimation with no additional cost. We conduct extensive experimental evaluation on real-world OSNs to confirm our theoretical claims. The experiment results show that our estimators are unbiased, accurate, efficient, and better than the state-of-the-art algorithms. For the Weibo graph with more than 58 million nodes, our method produces estimate of triangle count with an error less than 5% using only 20,000 sampled nodes. Detailed comparison with the state-of-the-art methods demonstrates that our algorithm is 2-10 times more accurate. © 2018 ACM."
,10.1016/j.csda.2018.01.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042913559&doi=10.1016%2fj.csda.2018.01.017&partnerID=40&md5=ff907ac217d738e889e00dda704a04ed,"Cox regression is one of the most commonly used methods in the analysis of interval-censored failure time data. In many practical studies, the covariate effects on the failure time may not be constant over time. Time-varying coefficients are therefore of great interest due to their flexibility in capturing the temporal covariate effects. To analyze spatially correlated interval-censored time-to-event data with time-varying covariate effects, a Bayesian approach with dynamic Cox regression model is proposed. The coefficient is estimated as a piecewise constant function and the number of jump points estimated from the data. A conditional autoregressive distribution is employed to model the spatial dependency. The posterior summaries are obtained via an efficient reversible jump Markov chain Monte Carlo algorithm. The properties of our method are illustrated by simulation studies as well as an application to smoking cessation data in southeast Minnesota. © 2018 Elsevier B.V."
,10.1007/s11042-017-5195-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030089456&doi=10.1007%2fs11042-017-5195-7&partnerID=40&md5=ba94d169d0edbb75d0ff67e63c713b65,"This paper addresses the problem of estimating a camera motion from a non-calibrated monocular camera. Compared to existing methods that rely on restrictive assumptions, we propose a method which can estimate camera motion with much less restrictions by adopting new example-based techniques compensating the lack of information. Specifically, we estimate the focal length of the camera by referring to visually similar training images with which focal lengths are associated. For one step camera estimation, we refer to stationary points (landmark points) whose depths are estimated based on RGB-D candidates. In addition to landmark points, moving objects can be also used as an information source to estimate the camera motion. Therefore, our method simultaneously estimates the camera motion for a video, and the 3D trajectories of objects in this video by using Reversible Jump Markov Chain Monte Carlo (RJ-MCMC) particle filtering. Our method is evaluated on challenging datasets demonstrating its effectiveness and efficiency. © 2017, Springer Science+Business Media, LLC."
,10.1121/1.5044423,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050114517&doi=10.1121%2f1.5044423&partnerID=40&md5=170c809b67a59e9537b2f0fbc5c0675c,"The purpose of this paper is to present a method for the ultrasonic characterization of air-saturated porous media, by solving the inverse problem using only the reflected waves from the first interface to infer the porosity, the tortuosity, and the viscous and thermal characteristic lengths. The solution of the inverse problem relies on the use of different reflected pressure signals obtained under multiple obliquely incident waves, in the time domain. In this paper, the authors propose to solve the inverse problem numerically with a first level Bayesian inference method, summarizing the authors' knowledge on the inferred parameters in the form of posterior probability densities, exploring these densities using a Markov-Chain Monte-Carlo approach. Despite their low sensitivity to the reflection coefficient, it is still possible to extract the knowledge of the viscous and thermal characteristic lengths, allowing the simultaneous determination of all the physical parameters involved in the expression of the reflection operator. To further constrain the problem and guide the inference, the knowledge of a particular incident angle is used at one's advantage in order to more precisely define the thermal length, by effectively yielding a statistical relationship between tortuosity and characteristic length ratio. © 2018 Acoustical Society of America."
2,10.1002/pst.1851,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049791472&doi=10.1002%2fpst.1851&partnerID=40&md5=8777fa873f5f14676a73e6946e507bbd,"With the recent advancement in many therapeutic areas, quest for better and enhanced treatment options is ever increasing. While the “efficacy” metric plays the most important role in this development, emphasis on other important clinical factors such as less intensive side effects, lower toxicity, ease of delivery, and other less debilitating factors may result in the selection of treatment options, which may not beat current established treatment option in terms efficacy, yet prove to be desirable for subgroups of patients. The resultant clinical trial by means of which one establishes such slightly less efficacious treatment is known as noninferiority (NI) trial. Noninferiority trials often involve an active established comparator arm, along with a placebo and an experimental treatment arm, resulting into a 3-arm trial. Most of the past developments in a 3-arm NI trial consider defining a prespecified fraction of unknown effect size of reference drug, i.e., without directly specifying a fixed NI margin. However, in some recent developments, more direct approach is being considered with prespecified fixed margin, albeit in the frequentist setup. In this article, we consider Bayesian implementation of such trial when primary outcome of interest is binary. Bayesian paradigm is important, as it provides a path to integrate historical trials and current trial information via sequential learning. We use several approximation-based and 2 exact fully Bayesian methods to evaluate the feasibility of the proposed approach. Finally, a clinical trial example is reanalyzed to demonstrate the benefit of the proposed approach. Copyright © 2018 John Wiley & Sons, Ltd."
,10.1016/j.compedu.2018.04.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045216062&doi=10.1016%2fj.compedu.2018.04.002&partnerID=40&md5=35149cb6a99689e65fadcd8ad371bb7f,"This paper assesses how the integration of ICT in education has affected the mathematics test scores for Italian students measured by the Programme for International Student Assessment 2012 data. The problem of endogeneity that affects survey data in this area, is addressed by applying the Bayesian Additive Regression Trees (BART) methodology as in Cabras & Tena Horrillo (2016). The BART methodology needs a prior and likelihood functions using the Markov Chain Monte Carlo (MCMC) algorithm to obtain the posterior distribution. Controlling for socio-economic, demographic and school factors, the predicted posterior distribution implies an increase, on average, of 16 points in the test scores. The result indicates that the use of ICT at school has a positive and strong impact on mathematic test scores. © 2018"
,10.1007/s11222-017-9764-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023207113&doi=10.1007%2fs11222-017-9764-4&partnerID=40&md5=5944003a4a12bfaa55da4fc8e201a948,"Approximate Bayesian computation (ABC) methods permit approximate inference for intractable likelihoods when it is possible to simulate from the model. However, they perform poorly for high-dimensional data and in practice must usually be used in conjunction with dimension reduction methods, resulting in a loss of accuracy which is hard to quantify or control. We propose a new ABC method for high-dimensional data based on rare event methods which we refer to as RE-ABC. This uses a latent variable representation of the model. For a given parameter value, we estimate the probability of the rare event that the latent variables correspond to data roughly consistent with the observations. This is performed using sequential Monte Carlo and slice sampling to systematically search the space of latent variables. In contrast, standard ABC can be viewed as using a more naive Monte Carlo estimate. We use our rare event probability estimator as a likelihood estimate within the pseudo-marginal Metropolis–Hastings algorithm for parameter inference. We provide asymptotics showing that RE-ABC has a lower computational cost for high-dimensional data than standard ABC methods. We also illustrate our approach empirically, on a Gaussian distribution and an application in infectious disease modelling. © 2017, The Author(s)."
,10.1007/s11222-017-9763-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022186020&doi=10.1007%2fs11222-017-9763-5&partnerID=40&md5=d4f1356e05bf47413c3729bed9b0b4f3,"Riemann manifold Hamiltonian Monte Carlo (RMHMC) has the potential to produce high-quality Markov chain Monte Carlo output even for very challenging target distributions. To this end, a symmetric positive definite scaling matrix for RMHMC is proposed. The scaling matrix is obtained by applying a modified Cholesky factorization to the potentially indefinite negative Hessian of the target log-density. The methodology is able to exploit the sparsity of the Hessian, stemming from conditional independence modeling assumptions, and thus admit fast implementation of RMHMC even for high-dimensional target distributions. Moreover, the methodology can exploit log-concave conditional target densities, often encountered in Bayesian hierarchical models, for faster sampling and more straightforward tuning. The proposed methodology is compared to alternatives for some challenging targets and is illustrated by applying a state-space model to real data. © 2017, Springer Science+Business Media, LLC."
,10.1007/s00168-017-0859-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040054841&doi=10.1007%2fs00168-017-0859-9&partnerID=40&md5=1627194379bba174f0cac0c2e66b13c2,"Spatial effects have been recognized to play an important role in transitional dynamics of regional incomes. Detection and evaluation of both spatial heterogeneity and spatial dependence in discrete Markov chain models, which have been widely applied to the study of regional income distribution dynamics and convergence, are vital, but under-explored issues. Indeed, in this spatiotemporal setting, spatial effects can take much more complex forms than that in a pure cross-sectional setting. In this paper, we address two test frameworks. The first is a conditional spatial Markov chains test framework, which can be used to detect spatial heterogeneity and temporally lagged spatial dependence; the second is a joint spatial Markov chains test framework, which tests for contemporaneous spatial dependence. A series of Monte Carlo experiments are designed to examine size, power and robustness properties of these tests for a range of sample sizes (spatial × temporal dimensions), for different levels of discretization granularity and for different number of regimes. Results indicate that all tests display good size property except when sample size is fairly small. All tests for spatial dependence are similar in almost all aspects—size, power and robustness. Conditional spatial Markov tests for spatial heterogeneity have highest power for detecting spatial heterogeneity. Granularity of discretization has a major impact on the size properties of the tests when sample size is fairly small. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1002/asmb.2258,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021160713&doi=10.1002%2fasmb.2258&partnerID=40&md5=944fef376d0b651cf6e6a90f294b3909,"In this work, we investigate sequential Bayesian estimation for inference of stochastic volatility with variance-gamma (SVVG) jumps in returns. We develop an estimation algorithm that combines the sequential learning auxiliary particle filter with the particle learning filter. Simulation evidence and empirical estimation results indicate that this approach is able to filter latent variances, identify latent jumps in returns, and provide sequential learning about the static parameters of SVVG. We demonstrate comparative performance of the sequential algorithm and off-line Markov Chain Monte Carlo in synthetic and real data applications. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1016/j.jval.2017.10.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035360231&doi=10.1016%2fj.jval.2017.10.021&partnerID=40&md5=cc8df1c25c1caa47547a93df73210dee,"Objectives: To conduct a cost-effectiveness analysis of two planning strategies of the second-generation direct-acting antiviral interferon-free regimens for the treatment of chronic hepatitis C virus infection. Methods: A lifetime multicohort model comprised 8125 real-life patients enrolled in the PITER (Italian platform for the study of viral hepatitis) registry, implemented by the ISS (Istituto Superiore di Sanità). Two treatment planning strategies were compared: 1) policy 1—treat all patients regardless of the stage of fibrosis (F0–F4) with second-generation direct-acting antivirals and 2) policy 2—treat patients at F3/F4 stage and those who are prioritized by the scientific guidelines first, and the remaining patients when they reach the F3 stage. Clinical outcomes and costs were evaluated by using a lifetime horizon Markov model and adopting the third-party payer perspective. Health outcomes were expressed in terms of quality-adjusted life-years (QALYs). A sensitivity analysis was run to explore first- and second-order uncertainty and heterogeneity. An expected value of perfect information analysis was also conducted. Results: Policy 1 exhibits an incremental cost-effectiveness ratio of €8,775/QALY gained and remains less than €30,000/QALY in 94% of realizations produced by the Monte-Carlo simulation. Such a proportion increases to 97% when adopting a threshold of €40,000/QALY gained. Conclusions: Moving from the urgency criterion to evidence-based escalating strategies when prioritizing the access to new anti–hepatitis C virus treatments is a good investment in health, whose affordability should be explored through context-specific budget impact analyses. © 2018 International Society for Pharmacoeconomics and Outcomes Research (ISPOR)"
,10.1016/j.jclinepi.2018.03.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044948118&doi=10.1016%2fj.jclinepi.2018.03.005&partnerID=40&md5=0a5e636484ba98300a09acbec3554d55,"Objectives: Network meta-analyses (NMA) have extensively been used to compare the effectiveness of multiple interventions for health care policy and decision-making. However, methods for evaluating the performance of multiple diagnostic tests are less established. In a decision-making context, we are often interested in comparing and ranking the performance of multiple diagnostic tests, at varying levels of test thresholds, in one simultaneous analysis. Study Design and Setting: Motivated by an example of cognitive impairment diagnosis following stroke, we synthesized data from 13 studies assessing the efficiency of two diagnostic tests: Mini-Mental State Examination (MMSE) and Montreal Cognitive Assessment (MoCA), at two test thresholds: MMSE <25/30 and <27/30, and MoCA <22/30 and <26/30. Using Markov chain Monte Carlo (MCMC) methods, we fitted a bivariate network meta-analysis model incorporating constraints on increasing test threshold, and accounting for the correlations between multiple test accuracy measures from the same study. Results: We developed and successfully fitted a model comparing multiple tests/threshold combinations while imposing threshold constraints. Using this model, we found that MoCA at threshold <26/30 appeared to have the best true positive rate, whereas MMSE at threshold <25/30 appeared to have the best true negative rate. Conclusion: The combined analysis of multiple tests at multiple thresholds allowed for more rigorous comparisons between competing diagnostics tests for decision making. © 2018 The Authors"
,10.1093/gji/ggy146,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047840156&doi=10.1093%2fgji%2fggy146&partnerID=40&md5=63bae0c79c254ae0cb5d0985b6da077f,"Despite surface displacements observed by geodesy are linear combinations of slip at faults in an elastic medium, determining the spatial distribution of fault slip remains a ill-posed inverse problem. A widely used approach to circumvent the illness of the inversion is to add regularization constraints in terms of smoothing and/or damping so that the linear system becomes invertible. However, the choice of regularization parameters is often arbitrary, and sometimes leads to significantly different results. Furthermore, the resolution analysis is usually empirical and cannot be made independently of the regularization. The stochastic approach of inverse problems provides a rigorous framework where the a priori information about the searched parameters is combined with the observations in order to derive posterior probabilities of the unkown parameters. Here, I investigate an approach where the prior probability density function (pdf) is a multivariate Gaussian function, with single truncation to impose positivity of slip or double truncation to impose positivity and upper bounds on slip for interseismic modelling. I show that the joint posterior pdf is similar to the linear untruncated Gaussian case and can be expressed as a truncated multivariate normal (TMVN) distribution. The TMVN form can then be used to obtain semi-analytical formulae for the single, 2-D or n-D marginal pdf. The semi-analytical formula involves the product of a Gaussian by an integral term that can be evaluated using recent developments in TMVN probabilities calculations. Posterior mean and covariance can also be efficiently derived. I show that the maximum posterior (MAP) can be obtained using a non-negative least-squares algorithm for the single truncated case or using the bounded-variable least-squares algorithm for the double truncated case. I show that the case of independent uniform priors can be approximated using TMVN. The numerical equivalence to Bayesian inversions using Monte Carlo Markov chain (MCMC) sampling is shown for a synthetic example and a real case for interseismic modelling in Central Peru. The TMVN method overcomes several limitations of the Bayesian approach using MCMC sampling. First, the need of computer power is largely reduced. Second, unlike Bayesian MCMC-based approach, marginal pdf, mean, variance or covariance are obtained independently one from each other. Third, the probability and cumulative density functions can be obtained with any density of points. Finally, determining the MAP is extremely fast. © The Author(s) 2018. Published by Oxford University Press on behalf of The Royal Astronomical Society."
,10.1145/3186586,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052573954&doi=10.1145%2f3186586&partnerID=40&md5=9c36ec309ef95ce4d087f0441d0db8f7,"Counting graphlets is a well-studied problem in graph mining and social network analysis. Recently, several papers explored very simple and natural algorithms based on Monte Carlo sampling of Markov Chains (MC), and reported encouraging results. We show, perhaps surprisingly, that such algorithms are outperformed by color coding (CC) [2], a sophisticated algorithmic technique that we extend to the case of graphlet sampling and for which we prove strong statistical guarantees. Our computational experiments on graphs with millions of nodes show CC to be more accurate than MC; furthermore, we formally show that the mixing time of the MC approach is too high in general, even when the input graph has high conductance. All this comes at a price however. While MC is very efficient in terms of space, CC's memory requirements become demanding when the size of the input graph and that of the graphlets grow. And yet, our experiments show that CC can push the limits of the state-of-the-art, both in terms of the size of the input graph and of that of the graphlets. © 2018 ACM."
,10.3969/j.issn.1000-7229.2018.07.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054599093&doi=10.3969%2fj.issn.1000-7229.2018.07.016&partnerID=40&md5=bd0d7e578bb36721841a781aa2da3033,"In view of the uncertainty of electric vehicle charging behavior, models of electric vehicle travel and battery electricity change based on trip chain theory are established, and an analysis method for charging behavior of electric vehicles by introducing the Markov decision processes(MDP) is proposed. The method takes the user's charging behavior as a Markov decision set, constructs a state transfer matrix according to the transfer probability between various regions. The user satisfaction index is set up as the reward function of the decision process. The optimal charging decision of the electric vehicle users at every decision point is obtained by solving the finite stage total reward criterion. The example is simulated by extracting characteristic data of electric vehicles, results show the time and space distribution of electric vehicle charging load. Compared with the traditional Monte Carlo method, the proposed MDP method can simulate user charging behavior more accurately and reflect the temporal and spatial distribution characteristics of charging demand. At the same time, different charging behavior of electric vehicles in different areas and different parking hours is analyzed, which can provide support for the planning and construction of electric vehicle charging piles. © 2018 State Power Economic Research Institute. All rights reserved."
1,10.1029/2018GC007585,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051075759&doi=10.1029%2f2018GC007585&partnerID=40&md5=fa3cfb739a961d59eab5744f9e517e07,"New satellite missions (e.g., the European Space Agency's Sentinel-1 constellation), advances in data downlinking, and rapid product generation now provide us with the ability to access space-geodetic data within hours of their acquisition. To truly take advantage of this opportunity, we need to be able to interpret geodetic data in a prompt and robust manner. Here we present a Bayesian approach for the inversion of multiple geodetic data sets that allows a rapid characterization of posterior probability density functions (PDFs) of source model parameters. The inversion algorithm efficiently samples posterior PDFs through a Markov chain Monte Carlo method, incorporating the Metropolis-Hastings algorithm, with automatic step size selection. We apply our approach to synthetic geodetic data simulating deformation of magmatic origin and demonstrate its ability to retrieve known source parameters. We also apply the inversion algorithm to interferometric synthetic aperture radar data measuring co-seismic displacements for a thrust-faulting earthquake (2015 Mw 6.4 Pishan earthquake, China) and retrieve optimal source parameters and associated uncertainties. Given its robustness and rapidity in estimating deformation source parameters and uncertainties, our Bayesian framework is capable of taking advantage of real-time geodetic measurements. Thus, our approach can be applied to geodetic data to study magmatic, tectonic, and other geophysical processes, especially in rapid-response operational settings (e.g., volcano observatories). Our algorithm is fully implemented in a MATLAB®-based software package (Geodetic Bayesian Inversion Software) that we make freely available to the scientific community. ©2018. The Authors."
1,10.1051/0004-6361/201732547,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051949735&doi=10.1051%2f0004-6361%2f201732547&partnerID=40&md5=4eb2b0c56ecd50cc9dd25475ef1929c2,"Galaxies follow a tight radial acceleration relation (RAR): the acceleration observed at every radius correlates with that expected from the distribution of baryons. We use the Markov chain Monte Carlo method to fit the mean RAR to 175 individual galaxies in the SPARC database, marginalizing over stellar mass-to-light ratio (γ ∗), galaxy distance, and disk inclination. Acceptable fits with astrophysically reasonable parameters are found for the vast majority of galaxies. The residuals around these fits have an rms scatter of only 0.057 dex (~13%). This is in agreement with the predictions of modified Newtonian dynamics (MOND). We further consider a generalized version of the RAR that, unlike MOND, permits galaxy-to-galaxy variation in the critical acceleration scale. The fits are not improved with this additional freedom: there is no credible indication of variation in the critical acceleration scale. The data are consistent with the action of a single effective force law. The apparent universality of the acceleration scale and the small residual scatter are key to understanding galaxies. © 2018 ESO."
1,10.1371/journal.pcbi.1006202,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050992207&doi=10.1371%2fjournal.pcbi.1006202&partnerID=40&md5=39e182e9247768081db272327f4ddadb,"In the event of a new infectious disease outbreak, mathematical and simulation models are commonly used to inform policy by evaluating which control strategies will minimize the impact of the epidemic. In the early stages of such outbreaks, substantial parameter uncertainty may limit the ability of models to provide accurate predictions, and policymakers do not have the luxury of waiting for data to alleviate this state of uncertainty. For policymakers, however, it is the selection of the optimal control intervention in the face of uncertainty, rather than accuracy of model predictions, that is the measure of success that counts. We simulate the process of real-time decision-making by fitting an epidemic model to observed, spatially-explicit, infection data at weekly intervals throughout two historical outbreaks of foot-and-mouth disease, UK in 2001 and Miyazaki, Japan in 2010, and compare forward simulations of the impact of switching to an alternative control intervention at the time point in question. These are compared to policy recommendations generated in hindsight using data from the entire outbreak, thereby comparing the best we could have done at the time with the best we could have done in retrospect. Our results show that the control policy that would have been chosen using all the data is also identified from an early stage in an outbreak using only the available data, despite high variability in projections of epidemic size. Critically, we find that it is an improved understanding of the locations of infected farms, rather than improved estimates of transmission parameters, that drives improved prediction of the relative performance of control interventions. However, the ability to estimate undetected infectious premises is a function of uncertainty in the transmission parameters. Here, we demonstrate the need for both real-time model fitting and generating projections to evaluate alternative control interventions throughout an outbreak. Our results highlight the use of using models at outbreak onset to inform policy and the importance of state-dependent interventions that adapt in response to additional information throughout an outbreak. © 2018, Public Library of Science. All rights reserved."
,10.29220/CSAM.2018.25.4.355,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054033525&doi=10.29220%2fCSAM.2018.25.4.355&partnerID=40&md5=3ae05abf407c32b6bb7c4aa12d78e9fd,"The two parameter negative exponential distribution has many practical applications in queuing theory such as the service times of agents in system, the time it takes before your next telephone call, the time until a radioactive practical decays, the distance between mutations on a DNA strand, and the extreme values of annual snowfall or rainfall; consequently, has many applications in reliability systems. This paper considers an estimation problem of stress-strength model with two parameter negative parameter exponential distribution. We introduce a maximum penalized likelihood method, Bayes estimator using Lindley approximation to estimate stress-strength model and compare the proposed estimators with regular maximum likelihood estimator for complete data. We also introduce a maximum penalized likelihood method, Bayes estimator using a Markov chain Mote Carlo technique for incomplete data. A Monte Carlo simulation study is performed to compare stress-strength model estimates. Real data is used as a practical application of the proposed model. © 2018 The Korean Statistical Society, and Korean International Statistical Society."
,10.1007/s00601-018-1384-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046291102&doi=10.1007%2fs00601-018-1384-9&partnerID=40&md5=5d0120d488ec5a6a67c8d8c635af6632,"A method is presented, which ensures that different polarization observables describing one reaction channel are consistent with each other. Using the connection of the observables to the same underlying reaction amplitudes, a constrained estimate of the observables is carried out using a Markov Chain Monte Carlo method. Initial results indicate that the new estimates are guaranteed to be physical, and will remove the need for artificial fudge factors when these processed data are used in subsequent fits. © 2018, The Author(s)."
1,10.1007/s11222-017-9774-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028543383&doi=10.1007%2fs11222-017-9774-2&partnerID=40&md5=15aff8bdc73f59f0db24394f644d8ac5,"Label switching is a well-known and fundamental problem in Bayesian estimation of finite mixture models. It arises when exploring complex posterior distributions by Markov Chain Monte Carlo (MCMC) algorithms, because the likelihood of the model is invariant to the relabelling of mixture components. If the MCMC sampler randomly switches labels, then it is unsuitable for exploring the posterior distributions for component-related parameters. In this paper, a new procedure based on the post-MCMC relabelling of the chains is proposed. The main idea of the method is to perform a clustering technique on the similarity matrix, obtained through the MCMC sample, whose elements are the probabilities that any two units in the observed sample are drawn from the same component. Although it cannot be generalized to any situation, it may be handy in many applications because of its simplicity and very low computational burden. © 2017, Springer Science+Business Media, LLC."
,10.18576/amis/120413,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053404661&doi=10.18576%2famis%2f120413&partnerID=40&md5=fe556e85885b8f9e658d414a6a615f27,"This article considers the problem of estimating the unknown parameters of the compound Rayleigh distribution with progressive first-failure censoring scheme during step-stress partially accelerated life tests (ALT). Progressive first-failure censoring and accelerated life testing are performed to decrease the duration of testing and to lower test expenses. The maximum likelihood estimators (MLEs) and Bayes estimates (BEs) for the distribution parameters and acceleration factor are obtained. The optimal time for stress change is determined. Furthermore, the approximate, bootstrap and credible confidence intervals (CIs) of the parameters are derived. Methods of Markov chain Monte Carlo (MCMC) are used to obtain the Bayes estimates. Finally, the accuracy of the MLEs and BEs for the model parameters is investigated through simulation studies. © 2018 NSP."
,10.1142/S0218271818440078,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045930412&doi=10.1142%2fS0218271818440078&partnerID=40&md5=3072d9a07e735aa1908015e301eea2ce,"The discovery of very-high-energy gamma-ray emission from Flat Spectrum Radio Quasars (FSRQs) by ground-based Cherenkov telescopes (HESS, MAGIC, VERITAS) provides a new view of blazar emission processes. The available data from multiwavelength observations of FSRQs, allow us to constrain the size (possibly also location) of the emitting region, magnetic field, electron energy distribution, etc., which are crucial for the understanding of the jet properties. We investigate the origin of emission from FSRQs (PKS 1510-089, PKS 1222+216 and 3C 279) by modeling the broadband spectral energy distribution in their quiescent and flaring states, using estimation of the parameter space that describes the underlying particle distribution responsible for the emission through the Markov Chain Monte Carlo (MCMC) technique. © 2018 World Scientific Publishing Company."
,10.1007/s11222-017-9770-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026771351&doi=10.1007%2fs11222-017-9770-6&partnerID=40&md5=f91b033290896e2e9641f0bb054828b3,"A statistical model assuming a preferential attachment network, which is generated by adding nodes sequentially according to a few simple rules, usually describes real-life networks better than a model assuming, for example, a Bernoulli random graph, in which any two nodes have the same probability of being connected, does. Therefore, to study the propagation of “infection” across a social network, we propose a network epidemic model by combining a stochastic epidemic model and a preferential attachment model. A simulation study based on the subsequent Markov Chain Monte Carlo algorithm reveals an identifiability issue with the model parameters. Finally, the network epidemic model is applied to a set of online commissioning data. © 2017, The Author(s)."
,10.1002/asna.201813491,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053525313&doi=10.1002%2fasna.201813491&partnerID=40&md5=a023688c347cd278478528644890d93d,"Photometric observation is one of the most important means to understand the fundamental properties of asteroids. As more and more photometric data are obtained, expanding effectively the number of samples on the spin states of asteroids has become an important research topic. To obtain a preliminary solution of the spin state without statistical discrepancy by the photometric observations derived from a handful of apparitions, a moderate-complexity shape model consisting of eight adjacent octants with different semiaxes, namely cellinoid ellipsoid, can be adopted. Using this model and employing a Markov chain Monte Carlo (MCMC) method, the spin states of four asteroids are discussed based on the existing photometric data. © 2018 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim"
,10.15446/rce.v41n2.69621,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050977672&doi=10.15446%2frce.v41n2.69621&partnerID=40&md5=65788f57cfb111c72694d9ddc965cde5,"In this paper we study the reliability of a multicomponent stress-strength model assuming that the components follow power Lindley model. The maximum likelihood estimate of the reliability parameter and its asymptotic confidence interval are obtained. Applying the parametric Bootstrap technique, interval estimation of the reliability is presented. Also, the Bayes estimate and highest posterior density credible interval of the reliability parameter are derived using suitable priors on the parameters. Because there is no closed form for the Bayes estimate, we use the Markov Chain Monte Carlo method to obtain approximate Bayes estimate of the reliability. To evaluate the performances of different procedures, simulation studies are conducted and an example of real data sets is provided. © 2018, Universidad Nacional de Colombia. All rights reserved."
2,10.1007/s11222-017-9773-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028765412&doi=10.1007%2fs11222-017-9773-3&partnerID=40&md5=4797d850d23af43745291276de877620,"Synthetic likelihood is an attractive approach to likelihood-free inference when an approximately Gaussian summary statistic for the data, informative for inference about the parameters, is available. The synthetic likelihood method derives an approximate likelihood function from a plug-in normal density estimate for the summary statistic, with plug-in mean and covariance matrix obtained by Monte Carlo simulation from the model. In this article, we develop alternatives to Markov chain Monte Carlo implementations of Bayesian synthetic likelihoods with reduced computational overheads. Our approach uses stochastic gradient variational inference methods for posterior approximation in the synthetic likelihood context, employing unbiased estimates of the log likelihood. We compare the new method with a related likelihood-free variational inference technique in the literature, while at the same time improving the implementation of that approach in a number of ways. These new algorithms are feasible to implement in situations which are challenging for conventional approximate Bayesian computation methods, in terms of the dimensionality of the parameter and summary statistic. © 2017, Springer Science+Business Media, LLC."
,10.1590/0001-3765201820171040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054562013&doi=10.1590%2f0001-3765201820171040&partnerID=40&md5=b00e7888ed6e5cde8f025c63f671ba19,"This paper takes into account the estimation for the unknown parameters of the Gompertz distribution from the frequentist and Bayesian view points by using both objective and subjective prior distributions. We first derive non-informative priors using formal rules, such as Jefreys prior and maximal data information prior (MDIP), based on Fisher information and entropy, respectively. We also propose a prior distribution that incorporate the expert’s knowledge about the issue under study. In this regard, we assume two independent gamma distributions for the parameters of the Gompertz distribution and it is employed for an elicitation process based on the predictive prior distribution by using Laplace approximation for integrals. We suppose that an expert can summarize his/her knowledge about the reliability of an item through statements of percentiles. We also present a set of priors proposed by Singpurwala assuming a truncated normal prior distribution for the median of distribution and a gamma prior for the scale parameter. Next, we investigate the effects of these priors in the posterior estimates of the parameters of the Gompertz distribution. The Bayes estimates are computed using Markov Chain Monte Carlo (MCMC) algorithm. An extensive numerical simulation is carried out to evaluate the performance of the maximum likelihood estimates and Bayes estimates based on bias, mean-squared error and coverage probabilities. Finally, a real data set have been analyzed for illustrative purposes. © 2018, Academia Brasileira de Ciencias. All rights reserved."
,10.3847/1538-4357/aac77d,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050117249&doi=10.3847%2f1538-4357%2faac77d&partnerID=40&md5=946e45f8e51ac5aa7fb7dae863e38b96,"The spectral type of Mrk 1018 changed from Type 1.9 to 1 and returned back to 1.9 over a period of 40 years. We have investigated physical mechanisms responsible for the spectral change in Mrk 1018 by analyzing archival spectral and imaging data. Two kinematically distinct broad-line components, blueshifted and redshifted components, are found from spectral decomposition. The velocity offset curve of the broad line as a function of time shows a characteristic pattern. An oscillating recoiled supermassive black hole (rSMBH) scenario is proposed to explain the observed velocity offset in broad emission lines. A Bayesian Markov chain Monte Carlo simulation is performed to derive the best-fit orbital parameters; we find that the rSMBH has a highly eccentric orbit with a period of ∼29 years. The active galactic nucleus (AGN) activity traced by a variation of broad Hβ emission lines is found to increase and decrease rapidly at the start and end of the cycle and peaks twice at the start and near the end of the cycle. Extinction at the start and end of the cycle (when its spectral type is Type 1.9) is found to increase due to an increased covering factor. Perturbations of the accretion disk caused by pericentric passage can reasonably explain the AGN activity and spectral change in Mrk 1018. Currently, the spectral type of Mrk 1018 is Type 1.9, and we do not know if it will repeat a similar pattern of spectral change in the future, but, if it does, then spectral type will turn to Type 1 around the mid-2020s. © 2018. The American Astronomical Society. All rights reserved."
,10.1177/1060028018786954,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049899625&doi=10.1177%2f1060028018786954&partnerID=40&md5=18ac7e1045098a1ad9bba7ae27eac813,"Background: The comparative effects of droxidopa and midodrine on standing systolic blood pressure (sSBP) and risk of supine hypertension in patients with neurogenic orthostatic hypotension (NOH) are unknown. Objective: To perform a Bayesian mixed-treatment comparison meta-analysis of droxidopa and midodrine in the treatment of NOH. Methods: The PubMed, CENTRAL, and EMBASE databases were searched up to November 16, 2016. Study selection consisted of randomized trials comparing droxidopa or midodrine with placebo and reporting on changes in sSBP and supine hypertension events. Data were pooled to perform a comparison among interventions in a Bayesian fixed-effects model using vague priors and Markov chain Monte Carlo simulation with Gibbs sampling, calculating pooled mean changes in sSBP and risk ratios (RRs) for supine hypertension with associated 95% credible intervals (CrIs). Results: Six studies (4 administering droxidopa and 2 administering midodrine) enrolling a total of 783 patients were included for analysis. The mean change from baseline in sSBP was significantly greater for both drugs when compared with placebo (droxidopa 6.2 mm Hg [95% CrI = 2.4-10] and midodrine 17 mm Hg [95% CrI = 11.4-23]). Comparative analysis revealed a significant credible difference between droxidopa and midodrine. The RR for supine hypertension was significantly greater for midodrine, but not droxidopa, when compared with placebo (droxidopa RR = 1.4 [95% CrI = 0.7-2.7] and midodrine RR = 5.1 [95% CrI = 1.6-24]). Conclusion and Relevance: In patients with NOH, both droxidopa and midodrine significantly increase sSBP, the latter to a greater extent. However, midodrine, but not droxidopa, significantly increases risk of supine hypertension. © 2018, The Author(s) 2018."
3,10.1002/uog.19073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047825136&doi=10.1002%2fuog.19073&partnerID=40&md5=03d7f53cffe300f3d424aa09dceda3d9,"Objective: To develop fetal and neonatal population weight charts. The rationale was that, while reference ranges of estimated fetal weight (EFW) are representative of the whole population, the traditional approach of deriving birth-weight (BW) charts is misleading, because a large proportion of babies born preterm arise from pathological pregnancy. We propose that the reference population for BW charts, as in the case of EFW charts, should comprise all babies at a given gestational age, including those still in utero. Methods: Two sources of data were used for this study. For both, the inclusion criteria were singleton pregnancy, dating by fetal crown–rump length at 11 + 0 to 13 + 6 weeks' gestation, availability of ultrasonographic measurements of fetal head circumference (HC), abdominal circumference (AC) and femur length (FL) and live birth of phenotypically normal neonate. Dataset 1 comprised a sample of 5163 paired measurements of EFW and BW; ultrasound examinations were carried out at 22–43 weeks' gestation and birth occurred within 2 days of the ultrasound examination. EFW was derived from the HC, AC and FL measurements using the formula reported by Hadlock et al. in 1985. Dataset 2 comprised a sample of 95 579 pregnancies with EFW obtained by routine ultrasonographic fetal biometry at 20 + 0 to 23 + 6 weeks' gestation (n = 45 034), 31 + 0 to 33 + 6 weeks (n = 19 224) or 35 + 0 to 36 + 6 weeks (n = 31 321); for the purpose of this study we included data for only one of the three visits per pregnancy. In the development of reference ranges of EFW and BW according to gestational age, the following assumptions were made: first, that EFW and BW have a common median, dependent on gestational age; and second, that deviations from the median occur in both EFW and BW and these deviations are correlated with different levels of spread for EFW and BW, dependent on gestational age. We adopted a Bayesian approach to inference, combining information from the two datasets using Markov Chain Monte-Carlo sampling. The fitted model assumed that the mean log transformed measurements of EFW and BW are related to gestational age according to a cubic equation and that deviations about the mean follow a bivariate Gaussian distribution. Results: In the case of EFW in Dataset 2, there was a good distribution of values &lt; 3rd, &lt; 5th, &lt; 10th, &gt; 90th, &gt; 95th and &gt; 97th percentiles of the reference range of EFW according to gestational age throughout the gestational age range of 20 + 0 to 36 + 6 weeks. In the case of BW, there was a good distribution of values only for the cases delivered &gt; 39 weeks' gestation. For preterm births, particularly at 27–36 weeks, BW was below the 3rd, 5th and 10th percentiles in a very high proportion of cases, particularly in cases of iatrogenic birth. The incidence of small-for-gestational-age fetuses and neonates in the respective EFW and BW charts was higher in women of black than those of white racial origin. Conclusion: We established a BW chart for all babies at a given gestational age, including those still in utero, thereby overcoming the problem of underestimation of growth restriction in preterm birth. BW and EFW charts have a common median but differ in the levels of spread from the median. Copyright © 2018 ISUOG. Published by John Wiley &amp; Sons Ltd. Copyright © 2018 ISUOG. Published by John Wiley & Sons Ltd."
,10.1093/mnras/sty796,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047157879&doi=10.1093%2fmnras%2fsty796&partnerID=40&md5=378cc20870d9c1f780944b4f769e2e93,"We extend 21CMMC, a Monte Carlo Markov Chain sampler of 3D reionization simulations, to perform parameter estimation directly on 3D light-cones of the cosmic 21 cm signal. This brings theoretical analysis closer to the tomographic 21 cm observations achievable with next generation interferometers like the Hydrogen Epoch of Reionization Array and the Square Kilometre Array. Parameter recovery can therefore account for modes that evolve with redshift/ frequency. Additionally, simulated data can be more easily corrupted to resemble real data. Using the light-cone version of 21CMMC, we quantify the biases in the recovered astrophysical parameters if we use the 21 cm power spectrum from the co-evolution approximation to fit a 3D light-cone mock observation. While ignoring the light-cone effect under most assumptions will not significantly bias the recovered astrophysical parameters, it can lead to an underestimation of the associated uncertainty. However, significant biases (~few - 10σ) can occur if the 21 cm signal evolves rapidly (i.e. the epochs of reionization and heating overlap significantly), and (i) foreground removal is very efficient, allowing large physical scales (k ≲ 0.1Mpc-1) to be used in the analysis or (ii) theoretical modelling is accurate to within ~10 per cent in the power spectrum amplitude. © 2018 The Author(s) Published by Oxford University Press on behalf of the Royal Astronomical Society."
1,10.1103/PhysRevLett.120.262702,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049370396&doi=10.1103%2fPhysRevLett.120.262702&partnerID=40&md5=2a3172d8286677225ba2d27402db3d5d,"The Canadian Penning Trap mass spectrometer at the Californium Rare Isotope Breeder Upgrade (CARIBU) facility was used to measure the masses of eight neutron-rich isotopes of Nd and Sm. These measurements are the first to push into the region of nuclear masses relevant to the formation of the rare-earth abundance peak at A∼165 by the rapid neutron-capture process. We compare our results with theoretical predictions obtained from ""reverse engineering"" the mass surface that best reproduces the observed solar abundances in this region through a Markov chain Monte Carlo technique. Our measured masses are consistent with the reverse-engineering predictions for a neutron star merger wind scenario. © 2018 American Physical Society."
1,10.1088/1361-6420/aaca8f,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051219538&doi=10.1088%2f1361-6420%2faaca8f&partnerID=40&md5=00f6b72f572b8c372ff485e1d36bee96,"Traditional imaging algorithms within the ultrasonic non-destructive testing community typically assume that the material being inspected is primarily homogeneous, with heterogeneities only at sub-wavelength scales. When the medium is of a more generally heterogeneous nature, this assumption can contribute to the poor detection, sizing and characterisation of defects. Prior knowledge of the varying wave speeds within the component would allow more accurate imaging of defects, leading to better decisions about how to treat the damaged component. This work endeavours to reconstruct the inhomogeneous wave speed maps of random media from simulated ultrasonic phased array data. This is achieved via application of the reversible-jump Markov chain Monte Carlo method: a sampling-based approach within a Bayesian framework. The inverted maps are used in conjunction with an imaging algorithm to correct for deviations in the wave speed, and the reconstructed flaw images are then used to quantitatively assess the success of this methodology. Using full matrix capture data arising from a finite element simulation of a phased array inspection of a heterogeneous component, a six-fold improvement in flaw location is achieved by taking into account the reconstructed wave speed map which exploits almost no a priori knowledge of the material's internal structure. Receiver operating characteristic curves are then calculated to demonstrate the enhanced probability of detection achieved when the material speed map is accounted for. © 2018 IOP Publishing Ltd."
,10.1080/12269328.2018.1486738,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049122053&doi=10.1080%2f12269328.2018.1486738&partnerID=40&md5=d10ebeb8b5ad83407f5de532d365538c,"Different sizes of pore spaces play different roles in fluid flow properties of rocks. The advantages of image analysis were employed to study the effect of different sizes of pore spaces, including micropores (10–50 µm), mesopores (50–100 µm) and macropores (>100 µm), on their contribution in the permeability of carbonates. First, all of the original binarized photomicrographs were subdivided into three images each contains one class of pores. Then, the three-dimensional digital models of the images were reconstructed using Markov Chain Monte Carlo method. Then, the models were integrated using a superposition operation. Finally, the lattice Boltzmann method was used to estimate the permeability of each model. The results show: (1) higher percentage of macropores provides higher permeability, while growth of micropores percentage has insignificant effect on flow properties due to lack of pores connectivity; (2) In spite of negligible permeability of micropores lonely, they can improve the carbonate rock permeability in cooperation with the meso- and macropores; (3) the estimated permeability from the superposed digital models were closer to the reality compared with the not-processed models, which were reconstructed without pore differentiation. © 2018, Taylor & Francis. All rights reserved."
,10.13465/j.cnki.jvs.2018.12.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053667315&doi=10.13465%2fj.cnki.jvs.2018.12.024&partnerID=40&md5=8a0f268d310c1cfc73a86628c9fc3a94,"Within the framework of probability and information theory, this paper presents a methodology for finite-element (FE) model-class selection for selecting suitable modeling parameters to update FE models based on Bayesian evidence inference and Markov chain Monte Carlo (MCMC) method. The amount of information needed to be extracted from the measurement data is explicitly quantified during the procedure of FE model updating by introducing the concept of information divergence. This is then employed for penalizing the complexity of FE parameterization with a trade-off between the complexity of a given FE model class and that of its corresponding information-theoretic interpretation, in order to obtain a relatively simple FE parameterization scheme for keeping similar model-data matching and avoid the over-fitting problem arisen from excessive modeling parameters efficiently. The validity of the presented methodology was verified through both numerical simulation and experimental verification carried out for a two-story bolt-connected steel frame. © 2018, Editorial Office of Journal of Vibration and Shock. All right reserved."
,10.1088/1361-6420/aac9b3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049741759&doi=10.1088%2f1361-6420%2faac9b3&partnerID=40&md5=7c2f5de83e1e5cb3867650322d7a6529,"Majorization-minimization (MM) is a standard iterative optimization technique which consists in minimizing a sequence of convex surrogate functionals. MM approaches have been particularly successful to tackle inverse problems and statistical machine learning problems where the regularization term is a sparsity-promoting concave function. However, due to non-convexity, the solution found by MM depends on its initialization. Uniform initialization is the most natural and often employed strategy as it boils down to penalizing all coefficients equally in the first MM iteration. Yet, this arbitrary choice can lead to unsatisfactory results in severely under-determined inverse problems such as source imaging with magneto- and electro-encephalography (M/EEG). The framework of hierarchical Bayesian modeling (HBM) is an alternative approach to encode sparsity. This work shows that for certain hierarchical models, a simple alternating scheme to compute fully Bayesian maximum a posteriori (MAP) estimates leads to the exact same sequence of updates as a standard MM strategy (see the adaptive lasso). With this parallel outlined, we show how to improve upon these MM techniques by probing the multimodal posterior density using Markov Chain Monte-Carlo (MCMC) techniques. Firstly, we show that these samples can provide well-informed initializations that help MM schemes to reach better local minima. Secondly, we demonstrate how it can reveal the different modes of the posterior distribution in order to explore and quantify the inherent uncertainty and ambiguity of such ill-posed inference procedure. In the context of M/EEG, each mode corresponds to a plausible configuration of neural sources, which is crucial for data interpretation, especially in clinical contexts. Results on both simulations and real datasets show how the number or the type of sensors affect the uncertainties on the estimates. © 2018 IOP Publishing Ltd."
,10.1063/1.5041544,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049783110&doi=10.1063%2f1.5041544&partnerID=40&md5=2107ab0e51368d89ba6df402527643d0,"To deal with the variation and correlation structure of accident data along with recognized covariate effects, we develop a spatio-temporal model for multivariate accident count data. Based on the multivariate Poisson lognormal model, we introduce linear combinations of random impulses to capture spatial correlation. For temporal effects, the lagged observations are added to this model. Model estimation is carried out using Markov Chain Monte Carlo methods. Simulated data sets are used in assessing the performance of this model. An advantage of this new model is that it not only copes with three sources of variations; time, space and multivariate data variations, but also provides information on time and space dependency. The model is also capable of providing an improvement in the accuracy of count data modelling. © 2018 Author(s)."
,10.1088/1742-5468/aac915,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049672218&doi=10.1088%2f1742-5468%2faac915&partnerID=40&md5=3c98a2d735f3b8fb6248bde10a18f27e,"We describe a simple method of umbrella trajectory sampling for Markov chains. The method allows the estimation of large-deviation rate functions, for path-extensive dynamic observables, for an arbitrary number of models within a certain family. The general relationship between probability distributions of dynamic observables of members of this family is an extended fluctuation relation. When the dynamic observable is chosen to be entropy production, members of this family include the forward Markov chain and its time reverse, whose probability distributions are related by the expected simple fluctuation relation. © 2018 IOP Publishing Ltd and SISSA Medialab srl."
,10.1109/ACCESS.2018.2849213,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049139955&doi=10.1109%2fACCESS.2018.2849213&partnerID=40&md5=d313d481b7b4b2efaa5a50333d0a30f3,"Graphical models provide an effective way to reveal complicated associations in data and especially to learn the structures among large numbers of variables with respect to few observations in a high-dimensional space. In this paper, a novel graphical algorithm that integrates the dynamic time warping (DTW)-D measure into the birth-death Markov Chain Monte Carlo (BDMCMC) methodology (DTWD-BDMCMC) is proposed for modeling the intrinsic correlations buried in data. The DTW-D, which is the ratio of DTW over the Euclidean distance (ED), is targeted to calibrate the warping observation sequences. The approach of the BDMCMC is a Bayesian framework used for structure learning in sparse graphical models. In detail, a modified DTW-D distance matrix is first developed to construct a weighted covariance instead of the traditional covariance calculated with the ED. We then build on Bayesian Gaussian models with the weighted covariance with the aim to be robust against problems of sequence distortion. Moreover, the weighted covariance is used as limited prior information to facilitate an initial graphical structure, on which we finally employ the BDMCMC for the determination of the reconstructed Gaussian graphical model. This initialization is beneficial to improve the convergence of the BDMCMC sampling. We implement our method on broad simulated data to test its ability to deal with different kinds of graphical structures. This paper demonstrates the effectiveness of the proposed method in comparison with its rivals, as it is competitively applied to Gaussian graphical models and copula Gaussian graphical models. In addition, we apply our method to explore real-network attacks and genetic expression data. © 2013 IEEE."
,10.1109/SBSE.2018.8395774,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050186972&doi=10.1109%2fSBSE.2018.8395774&partnerID=40&md5=f0d36ed6332f6eb180f4b09f5ed2f1a5,"This paper shows an application of the Markov chain Monte Carlo method (MCMC) for the generation of synthetic wind speed time series. Some variations within the method were discussed, such as how to divide the states of the transition matrix, how to convert states into speeds and how to include seasonality in the series. This article also discusses the best metrics to evaluate the performance of the time series, where the statistical parameters, probability distribution and autocorrelation function were analyzed. © 2018 IEEE."
,10.17713/ajs.v47i3.752,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050478492&doi=10.17713%2fajs.v47i3.752&partnerID=40&md5=4a17bb243244bb73ca12e7ab9821c04e,"This article presents the procedures of parameter estimation based on Type-II progressively hybrid censored fuzzy lifetime data. Classical as well as the Bayesian procedures for the estimation of unknown model parameters has been developed. For this purpose we have considered the problem of point estimation of the parameter of Rayleigh distribution. The estimators obtained here are Maximum likelihood (ML) estimator, Method of moments (MM) estimator, Computational approach (CA) estimator and Bayes estimator. Highest posterior density (HPD) credible intervals of the unknown parameter are obtained by using Markov Chain Monte Carlo (MCMC) technique. For numerical illustration, a real data set has been considered. © 2018, Austrian Statistical Society. All rights reserved."
1,10.1093/MNRAS/STY558,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049963826&doi=10.1093%2fMNRAS%2fSTY558&partnerID=40&md5=e9aab43ebf04d8904cc8ac93eab34ac9,"We present SPIDERMAN (Secondary eclipse and Phase curve Integrator for 2D tempERature MAppiNg), a fast code for calculating exoplanet phase curves and secondary eclipses with arbitrary surface brightness distributions in two dimensions. Using a geometrical algorithm, the code solves exactly the area of sections of the disc of the planet that are occulted by the star. The code is written in C with a user-friendly Python interface, and is optimized to run quickly, with no loss in numerical precision. Approximately 1000 models can be generated per second in typical use, making Markov Chain Monte Carlo analyses practicable. The modular nature of the code allows easy comparison of the effect of multiple different brightness distributions for the data set. As a test case, we apply the code to archival data on the phase curve of WASP-43b using a physically motivated analytical model for the two-dimensional brightness map. The model provides a good fit to the data; however, it overpredicts the temperature of the nightside. We speculate that this could be due to the presence of clouds on the nightside of the planet, or additional reflected light from the dayside. When testing a simple cloud model, we find that the best-fitting model has a geometric albedo of 0.32 ± 0.02 and does not require a hot nightside. We also test for variation of the map parameters as a function of wavelength and find no statistically significant correlations. SPIDERMAN is available for download at https://github.com/tomlouden/spiderman. © 2017 The Authors."
,10.1103/PhysRevC.97.064612,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049003285&doi=10.1103%2fPhysRevC.97.064612&partnerID=40&md5=9ba07d3b3422decd56918ab480269edb,"Background: Being able to rigorously quantify the uncertainties in reaction models is crucial to moving this field forward. Even though Bayesian methods are becoming increasingly popular in nuclear theory, they have yet to be implemented and applied in reaction theory problems. Purpose: The purpose of this work is to investigate, using Bayesian methods, the uncertainties in the optical potentials generated from fits to elastic-scattering data and the subsequent uncertainties in the transfer predictions. We also study the differences in two reaction models where the parameters are constrained in a similar manner, as well as the impact of reducing the experimental error bars on the data used to constrain the parameters. Method: We use Bayes' theorem combined with a Markov chain Monte Carlo to determine posterior distributions for the parameters of the optical model, constrained by neutron-, proton-, and/or deuteron-target elastic scattering. These potentials are then used to predict transfer cross sections within the adiabatic wave approximation or the distorted-wave Born approximation. Results: We study a number of reactions involving deuteron projectiles with energies in the range 10-25 MeV/nucleon on targets with mass A=48-208. The case of Ca48(d,p)Ca49 transfer to the ground state is described in detail. A comparative study of the effect of the size of experimental errors is also performed. Five transfer reactions are studied, and their results are compiled in order to systematically identify trends. Conclusions: Uncertainties in transfer cross sections can vary significantly (25-100 %) depending on the reaction. While these uncertainties are reduced when smaller experimental error bars are used to constrain the potentials, this reduction is not trivially related to the error reduction. We also find smaller uncertainties when using the adiabatic formulation than when using the distorted-wave Born approximation. © 2018 American Physical Society."
,10.3901/JME.2018.12.115,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052104866&doi=10.3901%2fJME.2018.12.115&partnerID=40&md5=5944ce7f3614eae331921d676d64bfbe,"A novel remaining useful life(RUL) estimation method is proposed based on the data-driven method and Bayesian theory for the remaining useful life estimation of complex mechanical systems. Firstly, the condition monitoring data of same or similar systems are fused to form the Health Index indicating the degradation degree of systems and the state model by the data-driven method. Then, a Bayesian model of the state model parameters is built on Bayesian theory. With the on-line condition monitoring data of system to be estimated and the Bayesian model, the model parameters are updated by Markov Chain Monte Carlo (MCMC) and the RUL of system is estimated. At last, a turbofan engine case is used to show the effectiveness of the present method. © 2018 Journal of Mechanical Engineering."
,10.1136/bjophthalmol-2018-312198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048873917&doi=10.1136%2fbjophthalmol-2018-312198&partnerID=40&md5=241005c2ca2a17b44b0ebd98d4747e77,"Background/aim: To clarify the nature of the relationship between novel oral anticoagulants (NOACs) and traditional anticoagulation in respect to intraocular bleeding. Methods: A comprehensive literature search up to October 2017 yielded 12 randomised controlled trials. Bayesian Markov chain Monte Carlo analysis was employed to investigate the relationship across multiple trials with varying NOACs. Random effects (informative priors) ORs were applied for the risk of intraocular bleeding due to various treatment measures. Mantel-Haenszel pairwise analyses were also performed. A total of 102 617 participants from 12 different randomised controlled trials. 11 746 received apixaban, 16 074 received dabigatran, 18 132 received edoxaban, 11 893 received rivaroxaban and 44 764 received warfarin. Results: Edoxaban was significantly associated with a reduced risk of intraocular bleeding in comparison to warfarin (OR 0.59; 95% CI 0.34 to 0.98). All other findings were non-significant; however, apixaban was the only NOAC to trend with an increased event rate against warfarin. The Bayesian Markov chain Monte Carlo modelling indicated that edoxaban had the greatest chance of producing the lowest rate of bleeding (surface under the cumulative ranking curve 0.8642). Pooled pairwise analysis supported the network analysis results favouring edoxaban against warfarin (OR 0.59; 95% CI 0.39 to 0.90; p=0.02) as well as subgroup analysis of low-dose edoxaban versus warfarin (OR 0.43; 95% CI 0.24 to 0.78). Conclusion: The analysis suggests that edoxaban may be the paramount agent in reducing intraocular bleeding rates. Given a paucity of reporting data for this rare event, future research and confirmation is strongly recommended. © Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2018. All rights reserved."
,10.1088/1757-899X/371/1/012020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049789787&doi=10.1088%2f1757-899X%2f371%2f1%2f012020&partnerID=40&md5=0c0c932d3acabd30cbb7658355664324,"Compared with conventional concrete products, pervious concrete usually features with high water permeability rate and low compressive strength due to the lack of fine aggregates. Thus the determination of optimal mix design of ingredients has been recognized as an effective mechanism to achieve the trade-off between compressive strength and permeability rate. In this paper, we proposed a Markov Chain Monte Carlo based approach to approximate the optimal mix design of pervious concrete to achieve a relatively high compressive strength while maintaining desired permeability rate. It is proved that the proposed approach effectively converges to the optimal solutions and the convergence rate and accuracy rely on a control parameter used in the proposed algorithm. A number of simulations are carried out and the results show that the proposed system converges to the optimal solutions quickly and the derived optimal mix design. © Published under licence by IOP Publishing Ltd."
,10.1109/ICCCC.2018.8390433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050083081&doi=10.1109%2fICCCC.2018.8390433&partnerID=40&md5=a560e7801969b1860f6b3469d36492fa,"This paper proposes an ensemble Kalman filter implementation for non-linear data assimilation. As in any ensemble based method, the moments of background error distributions are approximated by means of an ensemble of model realizations. The precision background covariance is estimated via a modified Cholesky decomposition in order to decrease the impact of sampling errors. Once all hyper-parameters are estimated, samples from the posterior distribution are estimated via a Markov-Chain-Monte-Carlo (MCMC) method. The MCMC implementation is enhanced by means of linear approximations of the observation operator. Posterior ensembles are then built by using a series of rank-one updates over prior Cholesky factors. Experimental tests are carried out by using the Lorenz 96 model. The numerical results evidence that, as the degree of the observational operator increases, the accuracy of the proposed filter is not affected and even more, for full observational networks, posterior errors are much lower than those of backgrounds, in some cases, by several order of magnitudes. © 2018 IEEE."
,10.1080/03610926.2017.1346805,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031495115&doi=10.1080%2f03610926.2017.1346805&partnerID=40&md5=21c7d75d2922e765e133d4a13912de6d,"The properties of high-dimensional Bingham distributions have been studied by Kume and Walker (2014). Fallaize and Kypraios (2016) propose the Bayesian inference for the Bingham distribution and they use developments in Bayesian computation for distributions with doubly intractable normalizing constants (Møller et al. 2006; Murray, Ghahramani, and MacKay 2006). However, they rely heavily on two Metropolis updates that they need to tune. In this article, we propose instead a model selection with the marginal likelihood. © 2018 Taylor & Francis Group, LLC."
,10.1109/TMI.2018.2848104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048890082&doi=10.1109%2fTMI.2018.2848104&partnerID=40&md5=5039f1493a77c3a6399d869110cee2e9,"In computed tomography, there is a trade-off between the quality of the reconstructed image and the radiation dose received by the patient. In order to find an appropriate compromise between the image quality of the reconstructed images and the radiation dose, it is important to have reliable methods for evaluating the quality of the reconstructed images. A successful family of methods for the assessment of image quality is task-based image quality assessment, which often involves the use of model observers, and which assesses the quality of the image reconstruction by deriving a figure of merit. Here, we present a Bayesian framework that can be used in task-based image quality assessment. Our framework is applicable to binary classification problems with normally distributed observations, and we make the additional assumption that the covariance matrix is the same in both image classes. We choose a particular non-informative prior for the parameters of our model, which allows us to derive an expression for the Bayes factor for the binary classification problem which to the best of our knowledge is novel. We introduce a novel model observer based on this Bayes factor. Further, we have developed a methodology for estimating the posterior distribution of the figure of merit for this type of classification problem. Compared to classical statistical approaches, our Bayesian approach has the advantage that it provides a full characterization of the uncertainty of the figure of merit. Our choice of prior allows us to design a simple Monte Carlo algorithm to efficiently sample the posterior of the figure of merit of the ideal observer, in contrast to common Bayesian procedures which rely on computationally expensive Markov chain Monte Carlo sampling. We have shown that for training samples of sufficient size, our estimated credible intervals for the figure of merit have coverage probabilities close to their credibility, so that our approach can reasonably be used within a classical statistical framework as well. IEEE"
,10.1109/JIOT.2018.2847697,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048614410&doi=10.1109%2fJIOT.2018.2847697&partnerID=40&md5=c4a671f8252e4ce648b9a0998fbdaf29,"We consider the problem of blind calibration of a sensor network where the sensor gains and offsets are estimated from noisy observations of unknown signals. This is in general a non-identifiable problem, unless restrictive assumptions on the signal subspace or sensor observations are imposed. We show that if each signal observed by the sensors follows a known dynamic model with additive noise, then the sensor gains and offsets are identifiable. We propose a dynamic Bayesian nonparametric model to infer the sensors&#x2019; gains and offsets. Our model allows different sensor clusters to observe different unknown signals, without knowing the sensor clusters a priori. We develop an offline algorithm using block Gibbs sampling and a linearized forward filtering backward sampling method that estimates the sensor clusters, gains and offsets jointly. Furthermore, for practical implementation, we also propose an online inference algorithm based on particle filtering and local Markov chain Monte Carlo. Simulations using a synthetic dataset, and experiments on two real datasets suggest that our proposed methods perform better than several other blind calibration methods, including a sparse Bayesian learning approach, and methods that first cluster the sensor observations and then estimate the gains and offsets. IEEE"
,10.1016/j.rse.2018.04.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045119533&doi=10.1016%2fj.rse.2018.04.006&partnerID=40&md5=79114fdef9522981ad8195e816ec129a,"Surface air temperature (SAT) is a critical metric that is used to assess regional warming and cooling patterns, and maximum and minimum SATs are required to evaluate the model predictions of climate extremes. Since station SAT data are irregularly distributed, land surface temperature (LST) values derived from Moderate Resolution Imaging Spectroradiometer (MODIS) data are used to estimate regional SAT by using linear regression methods. The deviations between SAT and LST are largely dependent on space and time, which hampers the estimation of linear regression, especially for the maximum SAT. To obtain accurate regional SAT estimates, a three-stage hierarchical Bayesian (HB) model is proposed that incorporates the MODIS LSTs as model covariates and specifies the deviations with structured dependence of MODIS LST fields. Sampling of model parameters and estimation of SAT values are implemented under the Bayesian paradigm using a Markov Chain Monte Carlo algorithm. Sensitivity analyses involving various model configurations and running processes are discussed to help build a robust HB model. The model's performance is evaluated using station measurements that are not used in the modeling process, with RMSEs of 2.15 K (0.75%) and 1.97 K (0.73%) for monthly maximum and minimum SATs, respectively. The evaluation indicates that HB modeling is an effective method to estimate SAT from MODIS LST. The verified HB model with the covariate inputs of both MODIS daytime and nighttime LSTs is used to reproduce monthly maximum and minimum SATs that are spatially continuous over the Qinghai province in Northwestern China for 2003–2011. From the comparison between MODIS LST and HB-estimated SAT, it is found that the spatial structure and warming patterns of LST and SAT show significant distinctions, implying that they cannot be substituted for one another when assessing the regional warming trends. The spatial heterogeneity of HB model estimation is able to provide thorough insights into regional SAT status changes that could otherwise be biased by station deployment. © 2018 Elsevier Inc."
,10.1109/TAES.2018.2848360,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048643810&doi=10.1109%2fTAES.2018.2848360&partnerID=40&md5=ef204c82844f14bbe50c72b62b6efc61,"The problem considered is that of estimating unambiguously migrating targets observed with a wideband radar. We extend a previously described sparse Bayesian algorithm to the presence of diffuse clutter and off-grid targets. A hybrid- Gibbs sampler is formulated to jointly estimate the sparse target amplitude vector, the grid mismatch and the (assumed) autoregressive noise. Results on synthetic and fully experimental data show that targets can be actually unambiguously estimated even if located in blind speeds IEEE"
,10.1002/sim.7627,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042534937&doi=10.1002%2fsim.7627&partnerID=40&md5=049d1585ab7ca7e50904a5f62994905b,"Ecological momentary assessment studies usually produce intensively measured longitudinal data with large numbers of observations per unit, and research interest is often centered around understanding the changes in variation of people's thoughts, emotions and behaviors. Hedeker et al developed a 2-level mixed effects location scale model that allows observed covariates as well as unobserved variables to influence both the mean and the within-subjects variance, for a 2-level data structure where observations are nested within subjects. In some ecological momentary assessment studies, subjects are measured at multiple waves, and within each wave, subjects are measured over time. Li and Hedeker extended the original 2-level model to a 3-level data structure where observations are nested within days and days are then nested within subjects, by including a random location and scale intercept at the intermediate wave level. However, the 3-level random intercept model assumes constant response change rate for both the mean and variance. To account for changes in variance across waves, as well as clustering attributable to waves, we propose a more comprehensive location scale model that allows subject heterogeneity at baseline as well as across different waves, for a 3-level data structure where observations are nested within waves and waves are then further nested within subjects. The model parameters are estimated using Markov chain Monte Carlo methods. We provide details on the Bayesian estimation approach and demonstrate how the Stan statistical software can be used to sample from the desired distributions and achieve consistent estimates. The proposed model is validated via a series of simulation studies. Data from an adolescent smoking study are analyzed to demonstrate this approach. The analyses clearly favor the proposed model and show significant subject heterogeneity at baseline as well as change over time, for both mood mean and variance. The proposed 3-level location scale model can be widely applied to areas of research where the interest lies in the consistency in addition to the mean level of the responses. Copyright © 2018 John Wiley & Sons, Ltd."
,10.1109/TSP.2018.2824286,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045207689&doi=10.1109%2fTSP.2018.2824286&partnerID=40&md5=7f48699ae1aba9a3efcaa10e17dcbc5f,"In this paper, we develop a Bayesian evidence maximization framework to solve the sparse non-negative least squares (S-NNLS) problem. We introduce a family of probability densities referred to as the rectified Gaussian scale mixture (R-GSM) to model the sparsity enforcing prior distribution for the solution. The R-GSM prior encompasses a variety of heavy-tailed densities such as the rectified Laplacian and rectified Student's t-distributions with a proper choice of the mixing density. We utilize the hierarchical representation induced by the R-GSM prior and develop an evidence maximization framework based on the expectation-maximization (EM) algorithm. Using the EM based method, we estimate the hyper-parameters and obtain a point estimate for the solution. We refer to the proposed method as rectified sparse Bayesian learning (R-SBL). We provide four R-SBL variants that offer a range of options for computational complexity and the quality of the E-step computation. These methods include the Markov chain Monte Carlo EM, linear minimum mean-square-error estimation, approximate message passing, and a diagonal approximation. Using numerical experiments, we show that the proposed R-SBL method outperforms existing S-NNLS solvers in terms of both signal and support recovery performance, and is also very robust against the structure of the design matrix. © 1991-2012 IEEE."
1,10.1103/PhysRevD.97.123525,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049535947&doi=10.1103%2fPhysRevD.97.123525&partnerID=40&md5=8be597254fc62f0fd8a92bd592d50ba9,"In this paper, we investigate a new phenomenological parametrization for unified dark matter and dark energy based on the polynomial expansion of the barotropic equation of state parameter w. Our parametrization provides a well-behaving evolution of w for both small and big redshifts as well as in the far future. The dark fluid described by our parametrization behaves for big redshifts like dark matter (DM). Therefore, one can parametrize dark energy and dark matter using a single dark fluid, like in the case of the Chaplygin gas. Within this parametrization, we consider two models: one with a dark energy (DE) barotropic parameter fixed to be -1 and the second one, where w≠-1 is chosen to match the best fit to the data. We study the main cosmological properties of these models at the expansion and perturbation levels. Based on the Markov chain Monte Carlo method with the currently available cosmic observational data sets, we constrain these models to determine the cosmological parameters at the level of the background and clustering of matter. We consider the interaction between dark matter and dark energy which directly affects the evolution of matter and its clustering. Our model appears to be perfectly consistent with the ΛCDM model, while providing unification of DE and DM. © 2018 American Physical Society."
1,10.1080/07350015.2018.1451336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048792866&doi=10.1080%2f07350015.2018.1451336&partnerID=40&md5=a7207e96380251c8b080792ea1dd022e,"We introduce a class of large Bayesian vector autoregressions (BVARs) that allows for non-Gaussian, heteroscedastic, and serially dependent innovations. To make estimation computationally tractable, we exploit a certain Kronecker structure of the likelihood implied by this class of models. We propose a unified approach for estimating these models using Markov chain Monte Carlo (MCMC) methods. In an application that involves 20 macroeconomic variables, we find that these BVARs with more flexible covariance structures outperform the standard variant with independent, homoscedastic Gaussian innovations in both in-sample model-fit and out-of-sample forecast performance. © 2018 American Statistical Association"
,10.3758/s13428-018-1069-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048368853&doi=10.3758%2fs13428-018-1069-9&partnerID=40&md5=fafbbfd9866efdbc03c330e25a388b42,"The Bayesian literature has shown that the Hamiltonian Monte Carlo (HMC) algorithm is powerful and efficient for statistical model estimation, especially for complicated models. Stan, a software program built upon HMC, has been introduced as a means of psychometric modeling estimation. However, there are no systemic guidelines for implementing Stan with the log-linear cognitive diagnosis model (LCDM), which is the saturated version of many cognitive diagnostic model (CDM) variants. This article bridges the gap between Stan application and Bayesian LCDM estimation: Both the modeling procedures and Stan code are demonstrated in detail, such that this strategy can be extended to other CDMs straightforwardly. © 2018 Psychonomic Society, Inc."
,10.1080/00207721.2018.1464607,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046484234&doi=10.1080%2f00207721.2018.1464607&partnerID=40&md5=609ef2f7f1f9c2fe4db325dc2a8682e1,"This paper presents a type of heavy-tailed market microstructure models with the scale mixtures of normal distributions (MM-SMN), which include two specific sub-classes, viz. the slash and the Student-t distributions. Under a Bayesian perspective, the Markov Chain Monte Carlo (MCMC) method is constructed to estimate all the parameters and latent variables in the proposed MM-SMN models. Two evaluating indices, namely the deviance information criterion (DIC) and the test of white noise hypothesis on the standardised residual, are used to compare the MM-SMN models with the classic normal market microstructure (MM-N) model and the stochastic volatility models with the scale mixtures of normal distributions (SV-SMN). Empirical studies on daily stock return data show that the MM-SMN models can accommodate possible outliers in the observed returns by use of the mixing latent variable. These results also indicate that the heavy-tailed MM-SMN models have better model fitting than the MM-N model, and the market microstructure model with slash distribution (MM-s) has the best model fitting. Finally, the two evaluating indices indicate that the market microstructure models with three different distributions are superior to the corresponding stochastic volatility models. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1145/3192366.3192409,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049555316&doi=10.1145%2f3192366.3192409&partnerID=40&md5=e2736ac4e4558ad606c85b5a50df8398,"We introduce inference metaprogramming for probabilistic programming languages, including new language constructs, a formalism, and the first demonstration of effectiveness in practice. Instead of relying on rigid black-box inference algorithms hard-coded into the language implementation as in previous probabilistic programming languages, inference metaprogramming enables developers to 1) dynamically decompose inference problems into subproblems, 2) apply inference tactics to subproblems, 3) alternate between incorporating new data and performing inference over existing data, and 4) explore multiple execution traces of the probabilistic program at once. Implemented tactics include gradient-based optimization, Markov chain Monte Carlo, variational inference, and sequental Monte Carlo techniques. Inference metaprogramming enables the concise expression of probabilistic models and inference algorithms across diverse fields, such as computer vision, data science, and robotics, within a single probabilistic programming language. © 2018 Copyright held by the owner/author(s)."
1,10.1016/j.jclepro.2018.03.173,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045223687&doi=10.1016%2fj.jclepro.2018.03.173&partnerID=40&md5=a0d299a184caa13b41c9a92565488b2c,"Solar profile and component outage create considerable impact on solar generation. This paper proposes a Markov chain model that incorporates both factors into solar generation. This model considers the solar farm as a generating unit with multiple generation states, where each state is characterized by generating photovoltaic arrays, ambient temperature, and solar radiation. The probability, frequency, and transition rate between generation states are obtained from collected solar profile samples and component reliability parameters. Three generation performance indices that describe the solar generation from the perspective of energy, time, and frequency are defined and estimated from the proposed model. The accuracy and efficiency of the proposed model is verified by a sequential Monte Carlo simulation approach using collected solar profile samples from six distinctive sites in North Dakota, USA. Influence of component reliability parameters, seasonal solar profile pattern, and photovoltaic module type on solar generation is investigated in details. © 2018 Elsevier Ltd"
2,10.3847/1538-4357/aac2bc,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049202831&doi=10.3847%2f1538-4357%2faac2bc&partnerID=40&md5=48a583e719e2e8685939fe66d58fad6a,"The shape of the damping profile of kink oscillations in coronal loops has recently allowed the transverse density profile of the loop to be estimated. This requires accurate measurement of the damping profile that can distinguish the Gaussian and exponential damping regimes, otherwise there are more unknowns than observables. Forward modeling of the transverse intensity profile may also be used to estimate the width of the inhomogeneous layer of a loop, providing an independent estimate of one of these unknowns. We analyze an oscillating loop for which the seismological determination of the transverse structure is inconclusive except when supplemented by additional spatial information from the transverse intensity profile. Our temporal analysis describes the motion of a coronal loop as a kink oscillation damped by resonant absorption, and our spatial analysis is based on forward modeling the transverse EUV intensity profile of the loop under the isothermal and optically thin approximations. We use Bayesian analysis and Markov chain Monte Carlo sampling to apply our spatial and temporal models both individually and simultaneously to our data and compare the results with numerical simulations. Combining the two methods allows both the inhomogeneous layer width and density contrast to be calculated, which is not possible for the same data when each method is applied individually. We demonstrate that the assumption of an exponential damping profile leads to a significantly larger error in the inferred density contrast ratio compared with a Gaussian damping profile. © 2018. The American Astronomical Society. All rights reserved."
1,10.3847/1538-4357/aac2d9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049172012&doi=10.3847%2f1538-4357%2faac2d9&partnerID=40&md5=d13aaa3bfafed1829f736413f34ce83b,"The first gravitational-wave event from the merger of a binary neutron star system (GW170817) was detected recently. The associated short gamma-ray burst (GRB 170817A) has a low isotropic luminosity (∼1047 erg s-1) and a peak energy E p ∼ 145 keV during the initial main emission between -0.3 and 0.4 s. The origin of this short GRB is still under debate, but a plausible interpretation is that it is due to the off-axis emission from a structured jet. We consider two possibilities. First, since the best-fit spectral model for the main pulse of GRB 170817A is a cutoff power law with a hard low-energy photon index (α = -0.62-0.54 +0.49), we consider an off-axis photosphere model. We develop a theory of photosphere emission in a structured jet and find that such a model can reproduce a low-energy photon index that is softer than a blackbody through enhancing high-latitude emission. The model can naturally account for the observed spectrum. The best-fit Lorentz factor along the line of sight is ∼20, which demands that there is a significant delay between the merger and jet launching. Alternatively, we consider that the emission is produced via synchrotron radiation in an optically thin region in an expanding jet with decreasing magnetic fields. This model does not require a delay of jet launching but demands a larger bulk Lorentz factor along the line of sight. We perform Markov Chain Monte Carlo fitting to the data within the framework of both models and obtain good fitting results in both cases. © 2018. The American Astronomical Society. All rights reserved."
,10.1007/s11222-018-9817-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048308224&doi=10.1007%2fs11222-018-9817-3&partnerID=40&md5=db4829daa46fa5dd78869d541253f4e2,"This paper introduces a framework for speeding up Bayesian inference conducted in presence of large datasets. We design a Markov chain whose transition kernel uses an unknown fraction of fixed size of the available data that is randomly refreshed throughout the algorithm. Inspired by the Approximate Bayesian Computation literature, the subsampling process is guided by the fidelity to the observed data, as measured by summary statistics. The resulting algorithm, Informed Sub-Sampling MCMC, is a generic and flexible approach which, contrary to existing scalable methodologies, preserves the simplicity of the Metropolis–Hastings algorithm. Even though exactness is lost, i.e the chain distribution approximates the posterior, we study and quantify theoretically this bias and show on a diverse set of examples that it yields excellent performances when the computational budget is limited. If available and cheap to compute, we show that setting the summary statistics as the maximum likelihood estimator is supported by theoretical arguments. © 2018 Springer Science+Business Media, LLC, part of Springer Nature"
,10.1109/ICACI.2018.8377503,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049806071&doi=10.1109%2fICACI.2018.8377503&partnerID=40&md5=291eb3b51b6a951e7ecb82febc3721a7,"As a high availability product, Electronic Flight Instrument System (EFIS) has very complicated redundancy structures to fulfill high safety integrity requirement. This paper presents a comprehensive study on the availability analysis for EFIS by using Dynamic Fault Tree (DFT) approach based on Markov chain with modularization method. The static fault sub-tree is solved by Binary Decision Diagram (BDD) and the dynamic fault sub-tree is solved by Markov chain. A novel Markov chain expression is utilized to avoid state explosion of dynamic fault sub-tree. Besides, Minimal Cut Sequence Set (MCSS) are generated. At last, Monte Carlo simulation is carried out to verify the theoretical results. © 2018 IEEE."
,10.1007/s10742-018-0184-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048075544&doi=10.1007%2fs10742-018-0184-5&partnerID=40&md5=e7c56eefa69e7037bd70dcd6c6b714c5,"In this paper, we address the problem of accounting for informative missing in the context of ecological momentary assessment studies (sometimes referred to as intensive longitudinal studies), where each study unit gets measured intensively over time and intermittent missing is usually present. We present a shared parameter modeling approach that links the primary longitudinal outcome with potentially informative missingness by a common set of random effects that summarize a subjects’ specific traits in terms of their mean (location) and variability (scale). The primary outcome, conditional on the random effects, are allowed to exhibit heterogeneity in terms of both the mean and within subject variance. Unlike previous methods which largely rely on numerical integration or approximation, we estimate the model by a full Bayesian approach using Markov Chain Monte Carlo. An adolescent mood study example is illustrated together with a series of simulation studies. Results in comparison to more conventional approaches suggest that accounting for the common but unobserved random subject mean and variance effects, shared between the primary outcome and missingness models, can significantly improve the model fit, and also provide the benefit of understanding how missingness can affect the inference for the primary outcome. © 2018 Springer Science+Business Media, LLC, part of Springer Nature"
1,10.1109/PDP2018.2018.00114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048752835&doi=10.1109%2fPDP2018.2018.00114&partnerID=40&md5=eaae836146aea0c0390ddd9f608a8656,"Models of biological systems often have many unknown parameters that must be determined in order for model behavior to match experimental observations. Commonly-used methods for parameter estimation that return point estimates of the best-fit parameters are insufficient when models are high dimensional and under-constrained. As a result, Bayesian methods, which treat model parameters as random variables and attempt to estimate their probability distributions given data, have become popular in systems biology. Bayesian parameter estimation often relies on Markov Chain Monte Carlo (MCMC) methods to sample model parameter distributions, but the slow convergence of MCMC sampling can be a major bottleneck. One approach to improving performance is parallel tempering (PT), a physics-based method that uses swapping between multiple Markov chains run in parallel at different temperatures to accelerate sampling. The temperature of a Markov chain determines the probability of accepting an unfavorable move, so swapping with higher temperatures chains enables the sampling chain to escape from local minima. In this work we compared the MCMC performance of PT and the commonly-used Metropolis-Hastings (MH) algorithm on six biological models of varying complexity. We found that for simpler models PT accelerated convergence and sampling, and that for more complex models, PT often converged in cases MH became trapped in non-optimal local minima. We also developed a freely-available MATLAB package for Bayesian parameter estimation called PTEMPEST (http://github.com/RuleWorld/ptempest), which is closely integrated with the popular BioNetGen software for rule-based modeling of biological systems. © 2018 IEEE."
,10.1016/j.applthermaleng.2018.04.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045260782&doi=10.1016%2fj.applthermaleng.2018.04.028&partnerID=40&md5=4500d726433847bee18f92b26d8009fc,"In this study, a shell-and-tube heat exchanger (STHX) design based on seven continuous independent design variables is proposed. Delayed Rejection Adaptive Metropolis hasting (DRAM) was utilized as a powerful tool in the Markov chain Monte Carlo (MCMC) sampling method. This Reverse Sampling (RS) method was used to find the probability distribution of design variables of the shell and tube heat exchanger. Thanks to this probability distribution, an uncertainty analysis was also performed to find the quality of these variables. In addition, a decision-making strategy based on confidence intervals of design variables and on the Total Annual Cost (TAC) provides the final selection of design variables. Results indicated high accuracies for the estimation of design variables which leads to marginally improved performance compared to commonly used optimization methods. In order to verify the capability of the proposed method, a case of study is also presented, it shows that a significant cost reduction is feasible with respect to multi-objective and single-objective optimization methods. Furthermore, the selected variables have good quality (in terms of probability distribution) and a lower TAC was also achieved. Results show that the costs of the proposed design are lower than those obtained from optimization method reported in previous studies. The algorithm was also used to determine the impact of using probability values for the design variables rather than single values to obtain the best heat transfer area and pumping power. In particular, a reduction of the TAC up to 3.5% was achieved in the case considered. © 2018"
1,10.1016/j.matdes.2018.03.037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044127902&doi=10.1016%2fj.matdes.2018.03.037&partnerID=40&md5=5131efea444e2ad3926a23524a2edab9,"Instrumented indentation enables rapid characterization of mechanical behavior in small material volumes. The heterogeneous deformation fields beneath the indenter however make it difficult to infer the intrinsic constitutive properties (e.g., Young's modulus, yield strength). This inverse problem is addressed in the literature using optimization techniques that are generally unable to yield robust values for the properties of interest and cannot quantify property uncertainty. Furthermore, current approaches tend to exhibit very high sensitivity to the error definitions and the optimization techniques employed. In order to overcome these difficulties, we propose an alternate approach that involves two main steps: (i) Development of a Gaussian Process (or kriging) surrogate model using finite element models of spherical indentation, and (ii) inverse solution using a Bayesian framework and Markov Chain Monte Carlo sampling. These approaches are demonstrated using selected case studies. © 2018 Elsevier Ltd"
,10.3389/fgene.2018.00195,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048285917&doi=10.3389%2ffgene.2018.00195&partnerID=40&md5=db70e50771043c122a872b57988a8750,"A widely used method for prediction of complex traits in animal and plant breeding is ""genomic best linear unbiased prediction"" (GBLUP). In a quantitative genetics setting, BLUP is a linear regression of phenotypes on a pedigree or on a genomic relationship matrix, depending on the type of input information available. Normality of the distributions of random effects and of model residuals is not required for BLUP but a Gaussian assumption is made implicitly. A potential downside is that Gaussian linear regressions are sensitive to outliers, genetic or environmental in origin. We present simple (relative to a fully Bayesian analysis) to implement robust alternatives to BLUP using a linear model with residual t or Laplace distributions instead of a Gaussian one, and evaluate the methods with milk yield records on Italian Brown Swiss cattle, grain yield data in inbred wheat lines, and using three traits measured on accessions of Arabidopsis thaliana. The methods do not use Markov chain Monte Carlo sampling and model hyper-parameters, viewed here as regularization ""knobs,"" are tuned via some cross-validation. Uncertainty of predictions are evaluated by employing bootstrapping or by random reconstruction of training and testing sets. It was found (e.g., test-day milk yield in cows, flowering time and FRIGIDA expression in Arabidopsis) that the best predictions were often those obtained with the robust methods. The results obtained are encouraging and stimulate further investigation and generalization. © 2018 Gianola, Cecchinato, Naya and Schön."
,10.7498/aps.67.20172639,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053027245&doi=10.7498%2faps.67.20172639&partnerID=40&md5=d4a18ad2d64a3e5317637703d432f1df,"Laser micro-Doppler (MD) effect is capable of obtaining obvious modulation in weak vibration detection. It helps to estimate target micro-motion parameters with high precision, which may extend the application field of MD to subtle identification and recognition. In laser detection, the multiple scattering points in the field of view will generate the singlechannel multi-component (SCMC) signal. Moreover, the micro-Doppler features of each component will be overlapped in the time-frequency domain because of the similar micro-motion parameters. The overlapped SCMC signal makes the estimation of the MD parameters a very difficult problem, and there has been no good method so far. In this paper, a separate parameter estimator based on the maximum likelihood framework is proposed to deal with this underdetermined problem. First, the detailed period scanning method is presented to improve the estimation accuracy of micro-motion frequency from the singular value ratio (SVR) spectrum. Further, the amplitude ratio information of each component is extracted from the SVR spectrum. Then, the closed-form expressions of the maximum likelihood estimation (MLE) for the remaining micro-motion parameters are derived, where the mean likelihood estimation is used to approximate to the performance of MLE. The high nonlinearity and multi-peak distribution shape of the likelihood function (LF) in laser MD signal will lead to incorrect estimation result. To this end, a new LF based on the energy spectrum characteristics is designed. The new LF acts as a smoothing filter to the probability density function, through which the ideal PDF distribution form that has only one smooth peak is obtained. With this modification, the requirements for the initialization are reduced and the robustness in low SNR situation is increased. The Markov chain Monte Carlo sampling is employed to implement the MLE. The Gibbs method is chosen to solve the multi-dimensional parametric problems, and the detailed process is listed. In the end, the simulation results prove the feasibility and high efficiency of the proposed method. The accuracy of parameter estimation reaches the Cramer-Rao boundary. The inverse Radon transform is used as a comparison with the experiment, and the results show the precise estimation advantage of the presented method. © 2018 Chinese Physical Society."
,10.1002/asia.201800074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047431068&doi=10.1002%2fasia.201800074&partnerID=40&md5=a7b48e01ba9a72a4fd3c9e5e933fe954,"A CoII coordination polymer built from a mixed azide and zwitterionic pyridinium ions and its temperature-dependent magnetic properties are described. We used the Markov chain Monte Carlo (MCMC) method to fit the data, and found the following results: (1) there are strong correlations between the model parameters; (2) the data at above 28 K are well fitted by the magnetism model. © 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim"
,10.1109/TPAMI.2018.2832641,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048196937&doi=10.1109%2fTPAMI.2018.2832641&partnerID=40&md5=0bb70a0bd428836afa5c32a3568320c1,"Latent Dirichlet Allocation (LDA) is a topic model widely used in natural language processing and machine learning. Most approaches to training the model rely on iterative algorithms, which makes it difficult to run LDA on big corpora that are best analyzed in parallel and distributed computational environments. Indeed, current approaches to parallel inference either don&#x0027;t converge to the correct posterior or require storage of large dense matrices in memory. We present a novel sampler that overcomes both problems, and we show that this sampler is faster, both empirically and theoretically, than previous Gibbs samplers for LDA. We do so by employing a novel P&#x00F3;lya-urn-based approximation in the sparse partially collapsed sampler for LDA. We prove that the approximation error vanishes with data size, making our algorithm asymptotically exact, a property of importance for large-scale topic models. In addition, we show, via an explicit example, that -- contrary to popular belief in the topic modeling literature -- partially collapsed samplers can be more efficient than fully collapsed samplers. We conclude by comparing the performance of our algorithm with that of other approaches on well-known corpora. IEEE"
,10.1080/03610926.2017.1342839,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031095042&doi=10.1080%2f03610926.2017.1342839&partnerID=40&md5=f652e0609fbf0a751cb9f6f544723daa,"The purpose of this paper is to develop a Bayesian analysis for the zero-inflated hyper-Poisson model. Markov chain Monte Carlo methods are used to develop a Bayesian procedure for the model and the Bayes estimators are compared by simulation with the maximum-likelihood estimators. Regression modeling and model selection are also discussed and case deletion influence diagnostics are developed for the joint posterior distribution based on the functional Bregman divergence, which includes ψ-divergence and several others, divergence measures, such as the Itakura–Saito, Kullback–Leibler, and χ2 divergence measures. Performance of our approach is illustrated in artificial, real apple cultivation experiment data, related to apple cultivation. © 2018 Taylor & Francis Group, LLC."
1,10.1080/14697688.2017.1383627,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041355624&doi=10.1080%2f14697688.2017.1383627&partnerID=40&md5=bf75afae2bff0dd1a10bff6e34655b58,"Bond rating Transition Probability Matrices (TPMs) are built over a one-year time-frame and for many practical purposes, like the assessment of risk in portfolios or the computation of banking Capital Requirements (e.g. the new IFRS 9 regulation), one needs to compute the TPM and probabilities of default over a smaller time interval. In the context of continuous time Markov chains (CTMC) several deterministic and statistical algorithms have been proposed to estimate the generator matrix. We focus on the Expectation-Maximization (EM) algorithm by Bladt and Sorensen. [J. R. Stat. Soc. Ser. B (Stat. Method.), 2005, 67, 395–410] for a CTMC with an absorbing state for such estimation. This work’s contribution is threefold. Firstly, we provide directly computable closed form expressions for quantities appearing in the EM algorithm and associated information matrix, allowing to easy approximation of confidence intervals. Previously, these quantities had to be estimated numerically and considerable computational speedups have been gained. Secondly, we prove convergence to a single set of parameters under very weak conditions (for the TPM problem). Finally, we provide a numerical benchmark of our results against other known algorithms, in particular, on several problems related to credit risk. The EM algorithm we propose, padded with the new formulas (and error criteria), outperforms other known algorithms in several metrics, in particular, with much less overestimation of probabilities of default in higher ratings than other statistical algorithms. © 2017 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group."
,10.1007/s10851-017-0783-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038395952&doi=10.1007%2fs10851-017-0783-8&partnerID=40&md5=ecc0c68a3b67b6ff857293db19c85947,"We present a novel tracking system that adaptively selects a shape of the posterior over time, where the selection is efficiently performed by the uncertainty calibrated Markov Chain Monte Carlo (UCMCMC) sampler. In conventional trackers, the posterior is typically described by a single prior distribution. On the other hand, our tracker allows the posterior to have multiple prior distributions, namely normal and Student’s t distribution, and to choose an appropriate distribution according to the tracking environment. The optimal distribution is determined by the UCMCMC sampler in the process of reducing the uncertainty of the shape. Experimental results demonstrate that the proposed multi-shape posterior helps improve the tracking performance in terms of accuracy. © 2017, Springer Science+Business Media, LLC, part of Springer Nature."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050003839&partnerID=40&md5=b78ce8c30f786d753b735a4eef8d59cd,"We proposes a new Bayesian MCMC algorithm for dynamic stochastic copula models with dependence parameters as unobserved state variables and presents the performance of the proposed MCMC algorithm through simulations. Our MCMC algorithm draws the state variables with an acceptancerejection Metropolis-Hastings algorithm using the candidate generating probability density function obtained by approximating the probability density function of the observed variables to the normal distribution of the dependence parameter. As an empirical example, we analyzed the stochastic copula models for the KOSPI index and the HSCE index (Hang Seng China enterprise index) returns from January 3, 2003 to December 30, 2014 using the proposed algorithm. The Bayesian inference and model comparison results of the stochastic copula models of Gaussian copula, Student t-copula, Clayton copula, Frank copula, rotated Gumbel copula, and Plackett copula showed that Student t-copula model could be selected as the best model. These model comparisons results imply that even though Gaussian stochastic copula model can capture ‘near asymptotic dependence’, there may exist extreme tail dependence that can not be captured by the Gaussian stochastic copula model. © 2018, Korean Econometric Society. All rights reserved."
,10.1371/journal.pone.0199123,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048824088&doi=10.1371%2fjournal.pone.0199123&partnerID=40&md5=5a876fa50fea2c6aef73cada40612eca,"Oceanographic field programs often use δ15N biogeochemical measurements and in situ rate measurements to investigate nitrogen cycling and planktonic ecosystem structure. However, integrative modeling approaches capable of synthesizing these distinct measurement types are lacking. We develop a novel approach for incorporating δ15N isotopic data into existing Markov Chain Monte Carlo (MCMC) random walk methods for solving linear inverse ecosystem models. We test the ability of this approach to recover food web indices (nitrate uptake, nitrogen fixation, zooplankton trophic level, and secondary production) derived from forward models simulating the planktonic ecosystems of the California Current and Amazon River Plume. We show that the MCMC with δ15N approach typically does a better job of recovering ecosystem structure than the standard MCMC or L2 minimum norm (L2MN) approaches, and also outperforms an L2MN with δ15N approach. Furthermore, we find that the MCMC with δ15N approach is robust to the removal of input equations and hence is well suited to typical pelagic ecosystem studies for which the system is usually vastly under-constrained. Our approach is easily extendable for use with δ13C isotopic measurements or variable carbon:nitrogen stoichiometry. © 2018 Stukel et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1016/j.apgeochem.2017.11.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034991410&doi=10.1016%2fj.apgeochem.2017.11.004&partnerID=40&md5=7afde64dc22fb86d16c9129916f7e7fd,"The BDB-1 deep inclined borehole was drilled at the Mont Terri rock laboratory (Switzerland) and enabled to acquire relevant data on porewater composition through the Opalinus Clay (OPA) and its bounding formations. Petrophysical measurements were carried out and included water content, water accessible porosity and grain density determination. Conservative anion profiles were obtained by aqueous leaching and out diffusion experiments performed on drillcore samples, and revealed to be consistent with previous studies carried out at the rock laboratory level. Diffusive properties were also investigated using three experimental setups: cubic out diffusion, radial diffusion and through diffusion. These transport parameters were used as a priori values in a Bayesian inversion using a Markov Chain Monte Carlo method to interpret the chloride profile in the Opalinus Clay. Based on a Peclet number analysis using the transport parameters formerly acquired, a purely diffusive scenario enabled specifying the paleohydrogeological evolution of the Mont Terri site from the folding and thrusting of the Jura Mountains to present time and transport parameters. © 2017"
,10.3969/j.issn.0258-2724.2018.03.019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052860017&doi=10.3969%2fj.issn.0258-2724.2018.03.019&partnerID=40&md5=7fcaedf696040d6419b533134686969e,"Multiple Model Analysis was applied to study the groundwater modelling uncertainties caused by the deviation of model structure and heterogeneity in aquifer media. According to different natural conditions, two hydrogeological conceptual models were established. Using a large number of model parameter data, obtained through hydrogeological tests, as a priori information and based on the two conceptual models, a series of seepage field models was constructed using the Adaptive Metropolis-Markov Chain Monte Carlo method that acceptance condition was adjusted. Uncertainties of modelling output data are analysed based on corrected Akaike's Information Criteron. Research indicates that the ergodicity and convergence of sample parameters will not be affected by changes in acceptance conditions. The model output data include the following effects: ""same results with different parameters"" and ""same results with different models"". Although these effects exist, the model structure is closer to the objective of improving the probability of obtaining a high precision model. The proportion of the primary conceptual model, with a variance between 1 and 2, is 65%. When the model with Delta values greater than 10 is excluded, the top 10 models are retained and the cumulative a posterior probability is 0.996. The proportion of the second conceptual model, with a variance between 1 and 2, is 46%. When the model with Delta values greater than 10 is excluded, the top 21 models are retained. The cumulative posterior probability of the top 10 models is only 0.884. © 2018, Editorial Department of Journal of Southwest Jiaotong University. All right reserved."
1,10.1029/2017MS001044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051298065&doi=10.1029%2f2017MS001044&partnerID=40&md5=8220c43a5b71d7685faeb4d4c634d45d,"While boreal lowland bogs have been extensively studied using the eddy-covariance (EC) technique, less knowledge exists on mountainous peatlands. Hence, half-hourly CO2 fluxes of an ombrotrophic peat bog in the Harz Mountains, Germany, were measured with the EC technique during a growing season with exceptionally dry weather spells. A common biophysical process model for net ecosystem exchange was used to describe measured CO2 fluxes and to fill data gaps. Model parameters and uncertainties were estimated by robust inverse modelling in a Bayesian framework using a population-based Markov Chain Monte Carlo sampler. The focus of this study was on the correct statistical description of error, i.e. the differences between the measured and simulated carbon fluxes, and the influence of distributional assumptions on parameter estimates, cumulative carbon fluxes, and uncertainties. We tested the Gaussian, Laplace, and Student's t distribution as error models. The t-distribution was identified as best error model by the deviance information criterion. Its use led to markedly different parameter estimates, a reduction of parameter uncertainty by about 40%, and, most importantly, to a 5% higher estimated cumulative CO2 uptake as compared to the commonly assumed Gaussian error distribution. As open-path measurement systems have larger measurement error at high humidity, the standard deviation of the error was modeled as a function of measured vapor pressure deficit. Overall, this paper demonstrates the importance of critically assessing the influence of distributional assumptions on estimated model parameters and cumulative carbon fluxes between the land surface and the atmosphere. © 2018. The Authors."
1,10.1016/j.advwatres.2018.04.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046169672&doi=10.1016%2fj.advwatres.2018.04.006&partnerID=40&md5=0d9d4ee37d402b6f3a16a7d61753934e,"Significant Input uncertainty is a major source of error in watershed water quality (WWQ) modeling. It remains challenging to address the input uncertainty in a rigorous Bayesian framework. This study develops the Bayesian Analysis of Input and Parametric Uncertainties (BAIPU), an approach for the joint analysis of input and parametric uncertainties through a tight coupling of Markov Chain Monte Carlo (MCMC) analysis and Bayesian Model Averaging (BMA). The formal likelihood function for this approach is derived considering a lag-1 autocorrelated, heteroscedastic, and Skew Exponential Power (SEP) distributed error model. A series of numerical experiments were performed based on a synthetic nitrate pollution case and on a real study case in the Newport Bay Watershed, California. The Soil and Water Assessment Tool (SWAT) and Differential Evolution Adaptive Metropolis (DREAM(ZS)) were used as the representative WWQ model and MCMC algorithm, respectively. The major findings include the following: (1) the BAIPU can be implemented and used to appropriately identify the uncertain parameters and characterize the predictive uncertainty; (2) the compensation effect between the input and parametric uncertainties can seriously mislead the modeling based management decisions, if the input uncertainty is not explicitly accounted for; (3) the BAIPU accounts for the interaction between the input and parametric uncertainties and therefore provides more accurate calibration and uncertainty results than a sequential analysis of the uncertainties; and (4) the BAIPU quantifies the credibility of different input assumptions on a statistical basis and can be implemented as an effective inverse modeling approach to the joint inference of parameters and inputs. © 2018 Elsevier Ltd"
2,10.1109/TR.2017.2778804,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040599059&doi=10.1109%2fTR.2017.2778804&partnerID=40&md5=58a6668b82b0e7bf5cd172ba4f03475d,"Traditional quantitative risk assessment methods (e.g., event tree analysis) are static in nature, i.e., the risk indexes are assessed before operation, which prevents capturing time-dependent variations as the components and systems operate, age, fail, are repaired and changed. To address this issue, we develop a dynamic risk assessment (DRA) method that allows online estimation of risk indexes using data collected during operation. Two types of data are considered: statistical failure data, which refer to the counts of accidents or near misses from similar systems and condition-monitoring data, which come from online monitoring the degradation of the target system of interest. For this, a hierarchical Bayesian model is developed to compute the reliability of the safety barriers and a Bayesian updating algorithm, which integrates particle filtering (PF) with Markov Chain Monte Carlo, is developed to update the reliability evaluations based on both the statistical and condition-monitoring data. The updated safety barriers reliabilities, are, then, used in an event tree (ET) for consequence analysis and the risk indexes are updated accordingly. A case study on a high-flow safety system is conducted to demonstrate the developed methods. A comparison to the DRA method which only uses statistical failure data shows that by introducing condition-monitoring data on the system degradation process, it is possible to capture the system-specific characteristics, and, therefore, provide a more complete and accurate description of the risk of the target system. © 2018 IEEE."
2,10.1214/17-BA1060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044400724&doi=10.1214%2f17-BA1060&partnerID=40&md5=f1b980e15658ec16b27f157bba5583f2,"Traditionally, the field of computational Bayesian statistics has been divided into two main subfields: variational methods and Markov chain Monte Carlo (MCMC). In recent years, however, several methods have been proposed based on combining variational Bayesian inference and MCMC simulation in order to improve their overall accuracy and computational efficiency. This marriage of fast evaluation and flexible approximation provides a promising means of designing scalable Bayesian inference methods. In this paper, we explore the possibility of incorporating variational approximation into a state-of-the-art MCMC method, Hamiltonian Monte Carlo (HMC), to reduce the required expensive computation involved in the sampling procedure, which is the bottleneck for many applications of HMC in big data problems. To this end, we exploit the regularity in parameter space to construct a free-form approximation of the target distribution by a fast and flexible surrogate function using an optimized additive model of proper random basis, which can also be viewed as a single-hidden layer feedforward neural network. The surrogate function provides sufficiently accurate approximation while allowing for fast computation in the sampling procedure, resulting in an efficient approximate Bayesian inference algorithm. We demonstrate the advantages of our proposed method using both synthetic and real data problems. © 2018 International Society for Bayesian Analysis."
,10.1016/j.jedc.2018.01.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044503624&doi=10.1016%2fj.jedc.2018.01.021&partnerID=40&md5=589c3362a601a1eb27561eef6881e047,"Estimation of agent-based models is currently an intense area of research. Recent contributions have to a large extent resorted to simulation-based methods mostly using some form of simulated method of moments estimation (SMM). There is, however, an entire branch of statistical methods that should appear promising, but has to our knowledge never been applied so far to estimate agent-based models in economics and finance: Markov chain Monte Carlo methods designed for state space models or models with latent variables. This latter class of models seems particularly relevant as agent-based models typically consist of some latent and some observable variables since not all the characteristics of agents would mostly be observable. Indeed, one might often not only be interested in estimating the parameters of a model, but also to infer the time development of some latent variable. However, agent-based models when interpreted as latent variable models would be typically characterized by non-linear dynamics and non-Gaussian fluctuations and, thus, would require a computational approach to statistical inference. Here we resort to Sequential Monte Carlo (SMC) estimation based on a particle filter. This approach is used here to numerically approximate the conditional densities that enter into the likelihood function of the problem. With this approximation we simultaneously obtain parameter estimates and filtered state probabilities for the unobservable variable(s) that drive(s) the dynamics of the observable time series. In our examples, the observable series will be asset returns (or prices) while the unobservable variables will be some measure of agents’ aggregate sentiment. We apply SMC to two selected agent-based models of speculative dynamics with somewhat different flavor. The empirical application to a selection of financial data includes an explicit comparison of the goodness-of-fit of both models. © 2018 Elsevier B.V."
,10.1016/j.spl.2018.01.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044657258&doi=10.1016%2fj.spl.2018.01.020&partnerID=40&md5=89b93df813ab657c8e2b921e3217eb07,"Markov chain Monte Carlo(MCMC) is a popular approach to sample from high dimensional distributions, and the asymptotic variance is a commonly used criterion to evaluate the performance. While most popular MCMC algorithms are reversible, there is a growing literature on the development and analyses of nonreversible MCMC. Chen and Hwang (2013) showed that a reversible MCMC can be improved by adding an antisymmetric perturbation. They also raised a conjecture that it cannot be improved if there is no cycle in the corresponding graph. In this paper, we present a rigorous proof of this conjecture. The proof is based on the fact that the transition matrix with an acyclic structure will produce minimum commute time between vertices. © 2018 Elsevier B.V."
,10.1109/TPAMI.2017.2711024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021791520&doi=10.1109%2fTPAMI.2017.2711024&partnerID=40&md5=892c6c330eeb208f1d3e1d018e2ff433,"Learning from demonstration (LfD) is the process of building behavioral models of a task from demonstrations provided by an expert. These models can be used, e.g., for system control by generalizing the expert demonstrations to previously unencountered situations. Most LfD methods, however, make strong assumptions about the expert behavior, e.g., they assume the existence of a deterministic optimal ground truth policy or require direct monitoring of the expert's controls, which limits their practical use as part of a general system identification framework. In this work, we consider the LfD problem in a more general setting where we allow for arbitrary stochastic expert policies, without reasoning about the optimality of the demonstrations. Following a Bayesian methodology, we model the full posterior distribution of possible expert controllers that explain the provided demonstration data. Moreover, we show that our methodology can be applied in a nonparametric context to infer the complexity of the state representation used by the expert, and to learn task-appropriate partitionings of the system state space. © 1979-2012 IEEE."
,10.1080/00401706.2018.1429317,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048355285&doi=10.1080%2f00401706.2018.1429317&partnerID=40&md5=d966d1f4fa94e0616f5a24a167d40811,"A Bayesian inferential approach with a noninformative prior is introduced to analyze ordinal repeatability and reproducibility (R&R) data using the De Mast–Van Wieringen model. This approach is extended with a weakly informative prior and random effects to allow for the consideration of a population of raters and prediction of a new rater. This random-effects approach is also shown to result in partial pooling of estimates across raters. In addition, match-probability-based measures to decompose ordinal R&R study data into contributions due to repeatability and due to reproducibility are defined. All extensions involving Bayesian inference (for fixed or random effects) and measures are illustrated on real and simulated ordinal R&R study data and are applicable in business and industry settings. This methodology can be implemented using the supplemental R package ordinalRR available from CRAN. Additional supplementary material for this article is available online. © 2018 American Statistical Association and the American Society for Quality"
2,10.1111/biom.12763,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049006356&doi=10.1111%2fbiom.12763&partnerID=40&md5=06b01a7233bb3f8d541016681b0d1886,"The standard approach to fitting capture–recapture data collected in continuous time involves arbitrarily forcing the data into a series of distinct discrete capture sessions. We show how continuous-time models can be fitted as easily as discrete-time alternatives. The likelihood is factored so that efficient Markov chain Monte Carlo algorithms can be implemented for Bayesian estimation, available online in the R package ctime. We consider goodness-of-fit tests for behavior and heterogeneity effects as well as implementing models that allow for such effects. © 2017, The International Biometric Society"
,10.1111/biom.12782,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030480480&doi=10.1111%2fbiom.12782&partnerID=40&md5=a7d4fc741b8f755c29dbaf080d373da9,"Medical imaging data with thousands of spatially correlated data points are common in many fields. Methods that account for spatial correlation often require cumbersome matrix evaluations which are prohibitive for data of this size, and thus current work has either used low-rank approximations or analyzed data in blocks. We propose a method that accounts for nonstationarity, functional connectivity of distant regions of interest, and local signals, and can be applied to large multi-subject datasets using spectral methods combined with Markov Chain Monte Carlo sampling. We illustrate using simulated data that properly accounting for spatial dependence improves precision of estimates and yields valid statistical inference. We apply the new approach to study associations between cortical thickness and Alzheimer's disease, and find several regions of the cortex where patients with Alzheimer's disease are thinner on average than healthy controls. © 2017, The International Biometric Society"
,10.1109/TFUZZ.2017.2746064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028732547&doi=10.1109%2fTFUZZ.2017.2746064&partnerID=40&md5=5c642f83291f4b720e32b1490e065a7b,"In this paper, we propose a novel approach for learning from data using rule-based fuzzy inference systems where the model parameters are estimated using Bayesian inference and Markov Chain Monte Carlo techniques. We show the applicability of the method for regression and classification tasks using synthetic datasets and also a real world example in the financial services industry. Then, we demonstrate how the method can be extended for knowledge extraction to select the individual rules in a Bayesian way which best explains the given data. Finally, we discuss the advantages and pitfalls of using this method over state-of-the-art techniques and highlight the specific class of problems where this would be useful. © 1993-2012 IEEE."
,10.1007/s00216-018-1061-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045756308&doi=10.1007%2fs00216-018-1061-3&partnerID=40&md5=6d55869441bc955bf427f35e75660217,"It is relatively easy to collect chromatographic measurements for a large number of analytes, especially with gradient chromatographic methods coupled with mass spectrometry detection. Such data often have a hierarchical or clustered structure. For example, analytes with similar hydrophobicity and dissociation constant tend to be more alike in their retention than a randomly chosen set of analytes. Multilevel models recognize the existence of such data structures by assigning a model for each parameter, with its parameters also estimated from data. In this work, a multilevel model is proposed to describe retention time data obtained from a series of wide linear organic modifier gradients of different gradient duration and different mobile phase pH for a large set of acids and bases. The multilevel model consists of (1) the same deterministic equation describing the relationship between retention time and analyte-specific and instrument-specific parameters, (2) covariance relationships relating various physicochemical properties of the analyte to chromatographically specific parameters through quantitative structure–retention relationship based equations, and (3) stochastic components of intra-analyte and interanalyte variability. The model was implemented in Stan, which provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods. [Figure not available: see fulltext.]. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
1,10.1121/1.5042162,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049408009&doi=10.1121%2f1.5042162&partnerID=40&md5=f476990b6e9c9f8707039b335a38eaf6,"Coprime microphone arrays use sparse sensing to achieve greater degrees of freedom, while the coprimality of the microphone subarrays help resolve grating lobe ambiguities. The result is a narrow beam at frequencies higher than the spatial Nyquist limit allows, with residual side lobes arising from aliasing. These side lobes can be mitigated when observing broadband sources, as shown by Bush and Xiang [J. Acoust. Soc. Am. 138, 447-456 (2015)]. Peak positions may indicate directions of arrival in this case; however, one must first ask how many sources are present. In answering this question, this work employs a model describing scenes with potentially multiple concurrent sound sources. Bayesian inference is used to first select which model the data prefer from competing models before estimating model parameters, including the particular source locations. The model is a linear combination of Laplace distribution functions (one per sound source). The likelihood function is explored by a Markov Chain Monte Carlo method called nested sampling in order to evaluate Bayesian evidence for each model. These values increase monotonically with model complexity; however, diminished returns are penalized via an implementation of Occam's razor. © 2018 Acoustical Society of America."
,10.1016/j.advwatres.2017.11.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046421241&doi=10.1016%2fj.advwatres.2017.11.013&partnerID=40&md5=4605a9469bc811caa98f6956b4aec334,"Bayesian solutions to geophysical and hydrological inverse problems are dependent upon a forward model linking subsurface physical properties to measured data, which is typically assumed to be perfectly known in the inversion procedure. However, to make the stochastic solution of the inverse problem computationally tractable using methods such as Markov-chain-Monte-Carlo (MCMC), fast approximations of the forward model are commonly employed. This gives rise to model error, which has the potential to significantly bias posterior statistics if not properly accounted for. Here, we present a new methodology for dealing with the model error arising from the use of approximate forward solvers in Bayesian solutions to hydrogeophysical inverse problems. Our approach is geared towards the common case where this error cannot be (i) effectively characterized through some parametric statistical distribution; or (ii) estimated by interpolating between a small number of computed model-error realizations. To this end, we focus on identification and removal of the model-error component of the residual during MCMC using a projection-based approach, whereby the orthogonal basis employed for the projection is derived in each iteration from the K-nearest-neighboring entries in a model-error dictionary. The latter is constructed during the inversion and grows at a specified rate as the iterations proceed. We demonstrate the performance of our technique on the inversion of synthetic crosshole ground-penetrating radar travel-time data considering three different subsurface parameterizations of varying complexity. Synthetic data are generated using the eikonal equation, whereas a straight-ray forward model is assumed for their inversion. In each case, our developed approach enables us to remove posterior bias and obtain a more realistic characterization of uncertainty. © 2017 Elsevier Ltd"
,10.1017/apr.2018.27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050670165&doi=10.1017%2fapr.2018.27&partnerID=40&md5=11e130944505c2dbe13434d56888d308,"We develop a forward-reverse expectation-maximization (FREM) algorithm for estimating parameters of a discrete-time Markov chain evolving through a certain measurable state-space. For the construction of the FREM method, we develop forward-reverse representations for Markov chains conditioned on a certain terminal state. We prove almost sure convergence of our algorithm for a Markov chain model with curved exponential family structure. On the numerical side, we carry out a complexity analysis of the forward-reverse algorithm by deriving its expected cost. Two application examples are discussed. © Applied Probability Trust 2018."
,10.4230/LIPIcs.SoCG.2018.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048985368&doi=10.4230%2fLIPIcs.SoCG.2018.19&partnerID=40&md5=d790eed152d0009babb1484b9e16b0fb,"We examine volume computation of general-dimensional polytopes and more general convex bodies, defined as the intersection of a simplex by a family of parallel hyperplanes, and another family of parallel hyperplanes or a family of concentric ellipsoids. Such convex bodies appear in modeling and predicting financial crises. The impact of crises on the economy (labor, income, etc.) makes its detection of prime interest for the public in general and for policy makers in particular. Certain features of dependencies in the markets clearly identify times of turmoil. We describe the relationship between asset characteristics by means of a copula; each characteristic is either a linear or quadratic form of the portfolio components, hence the copula can be constructed by computing volumes of convex bodies. We design and implement practical algorithms in the exact and approximate setting, we experimentally juxtapose them and study the tradeoff of exactness and accuracy for speed. We analyze the following methods in order of increasing generality: rejection sampling relying on uniformly sampling the simplex, which is the fastest approach, but inaccurate for small volumes; exact formulae based on the computation of integrals of probability distribution functions, which are the method of choice for intersections with a single hyperplane; an optimized Lawrence sign decomposition method, since the polytopes at hand are shown to be simple with additional structure; Markov chain Monte Carlo algorithms using random walks based on the hit-and-run paradigm generalized to nonlinear convex bodies and relying on new methods for computing a ball enclosed in the given body, such as a second-order cone program; the latter is experimentally extended to non-convex bodies with very encouraging results. Our C++ software, based on CGAL and Eigen and available on github, is shown to be very effective in up to 100 dimensions. Our results offer novel, effective means of computing portfolio dependencies and an indicator of financial crises, which is shown to correctly identify past crises. © Ludovic Calès, Apostolos Chalkis, Ioannis Z. Emiris, and Vissarion Fisikopoulos; licensed under Creative Commons License CC-BY 34th Symposium on Computational Geometry (SoCG 2018)."
,10.1051/0004-6361/201731445,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048863170&doi=10.1051%2f0004-6361%2f201731445&partnerID=40&md5=c77dbfad42576e753ad4d635cf01cc6c,"Context. This paper is the fourth of a series evaluating the ASpiX cosmological method, based on X-ray diagrams, which are constructed from simple cluster observable quantities, namely: count rate (CR), hardness ratio (HR), core radius (rc), and redshift. Aims. Following extensive tests on analytical toy catalogues (Paper III), we present the results of a more realistic study over a 711 deg2 template-based maps derived from a cosmological simulation. Methods. Dark matter haloes from the Aardvark simulation have been ascribed luminosities, temperatures, and core radii, using local scaling relations and assuming self-similar evolution. The predicted X-ray sky-maps were converted into XMM event lists, using a detailed instrumental simulator. The XXL pipeline runs on the resulting sky images, produces an observed cluster catalogue over which the tests have been performed. This allowed us to investigate the relative power of various combinations of the CR, HR, rc, and redshift information. Two fitting methods were used: a traditional Markov chain Monte Carlo (MCMC) approach and a simple minimisation procedure (Amoeba) whose mean uncertainties are a posteriori evaluated by means of synthetic catalogues. The results were analysed and compared to the predictions from the Fisher analysis (FA). Results. For this particular catalogue realisation, assuming that the scaling relations are perfectly known, the CR-HR combination gives σ8 and ω at the 10% level, while CR-HR-rc-z improves this to ≤3%. Adding a second HR improves the results from the CRHR1- rc combination, but to a lesser extent than when adding the redshift information. When all coefficients of the mass-temperature relation (M-T, including scatter) are also fitted, the cosmological parameters are constrained to within 5-10% and larger for the M-T coefficients (up to a factor of two for the scatter). The errors returned by the MCMC, those by Amoeba and the FA predictions are in most cases in excellent agreement and always within a factor of two. We also study the impact of the scatter of the mass-size relation (M-Rc) on the number of detected clusters: for the cluster typical sizes usually assumed, the larger the scatter, the lower the number of detected objects. Conclusions. The present study confirms and extends the trends outlined in our previous analyses, namely the power of X-ray observable diagrams to successfully and easily fit at the same time, the cosmological parameters, cluster physics, and the survey selection, by involving all detected clusters. The accuracy levels quoted should not be considered as definitive. A number of simplifying hypotheses were made for the testing purpose, but this should affect any method in the same way. The next publication will consider in greater detail the impact of cluster shapes (selection and measurements) and of cluster physics on the final error budget by means of hydrodynamical simulations. © ESO 2018."
,10.1093/gji/ggy071,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052655207&doi=10.1093%2fgji%2fggy071&partnerID=40&md5=832af31a8eefd4c053889f2604f76e91,"This paper develops an efficient hierarchical trans-dimensional (trans-D) Bayesian algorithm to invert magnetotelluric (MT) data for subsurface geoelectrical structure, with unknown geophysical model parameterization (the number of conductivity-layer interfaces) and dataerror models parameterized by an auto-regressive (AR) process to account for potential error correlations. The reversible-jump Markov-chain Monte Carlo algorithm, which adds/removes interfaces and AR parameters in birth/death steps, is applied to sample the trans-D posterior probability density for model parameterization, model parameters, error variance and AR parameters, accounting for the uncertainties of model dimension and data-error statistics in the uncertainty estimates of the conductivity profile. To provide efficient sampling over the multiple subspaces of different dimensions, advanced proposal schemes are applied. Parameter perturbations are carried out in principal-component space, defined by eigen-decomposition of the unit-lag model covariance matrix, to minimize the effect of inter-parameter correlations and provide effective perturbation directions and length scales. Parameters of new layers in birth steps are proposed from the prior, instead of focused distributions centred at existing values, to improve birth acceptance rates. Parallel tempering, based on a series of parallel interacting Markov chains with successively relaxed likelihoods, is applied to improve chain mixing over model dimensions. The trans-D inversion is applied in a simulation study to examine the resolution of model structure according to the data information content. The inversion is also applied to a measured MT data set from south-central Australia. © The Author(s) 2018."
,10.1002/2017JB015359,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050113486&doi=10.1002%2f2017JB015359&partnerID=40&md5=f7ca8da59527f9b817e930276568029b,"This study presents a new statistical method to estimate a spatial stress pattern in the Earth's crust from P wave first-motions. In this method, it is assumed that the orientation of a fault plane is distributed randomly with a uniform distribution and that the direction of fault slip is parallel to the direction of maximum shear stress on the fault plane. Under these two assumptions, the spatial stress pattern that fits the data set of P wave first-motions is estimated with a smoothness constraint on the spatial variation of stress pattern. An advantage of this method is that it does not require the determination of the focal mechanisms of each event. This method was applied to synthetic data sets to demonstrate its performance. The estimated stress pattern agreed with that given in the generation of the data set if the given pattern changes smoothly in space. Then, the method was applied to the real data set that was taken from the aftershocks of the 2000 Western Tottori earthquake. The resulting stress pattern was consistent with that estimated in previous studies examining the same data set. Additionally, the spatial variation of stress rotation across the main fault was revealed. These applications show the validity and benefit of the new method. ©2018. American Geophysical Union. All Rights Reserved."
,10.1371/journal.pone.0198280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048165885&doi=10.1371%2fjournal.pone.0198280&partnerID=40&md5=d4fd403aa7d3192e1985be3bde90d4d1,"A campaign for malaria control, using Long Lasting Insecticide Nets (LLINs) was launched in South Sudan in 2009. The success of such a campaign often depends upon adequate available resources and reliable surveillance data which help officials understand existing infections. An optimal allocation of resources for malaria control at a sub-national scale is therefore paramount to the success of efforts to reduce malaria prevalence. In this paper, we extend an existing SIR mathematical model to capture the effect of LLINs on malaria transmission. Available data on malaria is utilized to determine realistic parameter values of this model using a Bayesian approach via Markov Chain Monte Carlo (MCMC) methods. Then, we explore the parasite prevalence on a continued rollout of LLINs in three different settings in order to create a sub-national projection of malaria. Further, we calculate the model’s basic reproductive number and study its sensitivity to LLINs’ coverage and its efficacy. From the numerical simulation results, we notice a basic reproduction number, R0, confirming a substantial increase of incidence cases if no form of intervention takes place in the community. This work indicates that an effective use of LLINs may reduce R0 and hence malaria transmission. We hope that this study will provide a basis for recommending a scaling-up of the entry point of LLINs’ distribution that targets households in areas at risk of malaria. © 2018 Mukhtar et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1142/S0218539318500122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040918978&doi=10.1142%2fS0218539318500122&partnerID=40&md5=967da6b6d04eafa6f8320260d0f07494,"This paper presents Bayesian reliability sampling plans for the Weibull distribution based on progressively Type-II censored data with binomial removals. In constructing sampling plans, the decision theoretic approach is used. A dependent bivariate nonconjugate prior is employed. The total cost of the sampling plan consists of sampling, time-consuming, rejection, and acceptance costs. The decision rule is based on the Bayes estimator of the survival function. Lindley's approximation is used to obtain Bayes estimates of the survival function under the quadratic and LINEX loss functions. However, the poor performance of Lindley's approximation with small sample sizes can be observed. The Metropolis-within-Gibbs Markov Chain Monte Carlo (MCMC) algorithm show significantly improved performance compared to Lindley's approximation. We use simulation studies to evaluate the Bayes risk and determine the optimal sampling plans for different sample sizes, observed number of failures, binomial removal probabilities and minimum acceptable reliability. © 2018 World Scientific Publishing Company."
1,10.1007/s12273-017-0413-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044464837&doi=10.1007%2fs12273-017-0413-9&partnerID=40&md5=797be9251dfc68bf55e24f10331e08fc,"Occupancy information in an office building is an important asset for determining energy-efficient operations and emergency evacuation of a building. In this study, we developed a method to estimate the occupancy distribution in a multi-room office building using Bayesian inference. The Markov chain Monte Carlo algorithm was used to estimate the real-time occupancy in individual rooms based on indoor carbon dioxide concentrations. The office building was made-up of five rooms with different physical configurations and dimensions, and the rooms were air-conditioned and ventilated by a central air handling unit. The carbon dioxide concentration data were generated by the simulation software CONTAMW according to a given schedule of occupancy and the supply airflow rates in each room. The objective of the present paper is to investigate the effects of various parameters of Bayesian inference on the accuracy of estimation results. The parameters include the probability of prior information, the uncertainty level of CO2 data, and the time interval for monitoring CO2. © 2017, Tsinghua University Press and Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1051/0004-6361/201731635,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048868961&doi=10.1051%2f0004-6361%2f201731635&partnerID=40&md5=6b99e52393d80719e1c53f5fc01cc7ae,"We present an investigation of radio luminosity functions (LFs) and number counts based on the Karl G. Jansky Very Large Array-COSMOS 3 GHz Large Project. The radio-selected sample of 7826 galaxies with robust optical/near-infrared counterparts with excellent photometric coverage allows us to construct the total radio LF since z ∼ 5.7. Using the Markov chain Monte Carlo algorithm, we fit the redshift dependent pure luminosity evolution model to the data and compare it with previously published VLA-COSMOS LFs obtained on individual populations of radio-selected star-forming galaxies and galaxies hosting active galactic nuclei classified on the basis of presence or absence of a radio excess with respect to the star-formation rates derived from the infrared emission. We find they are in excellent agreement, thus showing the reliability of the radio excess method in selecting these two galaxy populations at radio wavelengths. We study radio number counts down to submicrojansky levels drawn from different models of evolving LFs. We show that our evolving LFs are able to reproduce the observed radio sky brightness, even though we rely on extrapolations toward the faint end. Our results also imply that no new radio-emitting galaxy population is present below 1 μJy. Our work suggests that selecting galaxies with radio flux densities between 0.1 and 10 μJy will yield a star-forming galaxy in 90-95% of the cases with a high percentage of these galaxies existing around a redshift of z ∼ 2, thus providing useful constraints for planned surveys with the Square Kilometer Array and its precursors. © ESO 2018."
1,10.1111/sjos.12299,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046805205&doi=10.1111%2fsjos.12299&partnerID=40&md5=8ea270ca8b4ca18424537938b7982156,"Conditional simulation of max-stable processes allows for the analysis of spatial extremes taking into account additional information provided by the conditions. Instead of observations at given sites as usually done, we consider a single condition given by a more general functional of the process as may occur in the context of climate models. As the problem turns out to be intractable analytically, we make use of Markov chain Monte Carlo methods to sample from the conditional distribution. Simulation studies indicate fast convergence of the Markov chains involved. In an application to precipitation data, the utility of the procedure as a tool to downscale climate data is demonstrated. © 2017 Board of the Foundation of the Scandinavian Journal of Statistics"
,10.1051/0004-6361/201732128,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049575887&doi=10.1051%2f0004-6361%2f201732128&partnerID=40&md5=b3b89181c970b2b3f3c502042a753c11,"Context. The interstellar medium (ISM) is now widely acknowledged to display features ascribable to magnetized turbulence. With the public release of Planck data and the current balloon-borne and ground-based experiments, the growing amount of data tracing the polarized thermal emission from Galactic dust in the submillimetre provides choice diagnostics to constrain the properties of this magnetized turbulence. Aims. We aim to constrain these properties in a statistical way, focussing in particular on the power spectral index βB of the turbulent component of the interstellar magnetic field in a diffuse molecular cloud, the Polaris Flare. Methods. We present an analysis framework based on simulating polarized thermal dust emission maps using model dust density (proportional to gas density nH) and magnetic field cubes, integrated along the line of sight (LOS), and comparing these statistically to actual data. The model fields are derived from fractional Brownian motion (fBm) processes, which allows a precise control of their one-and two-point statistics. The parameters controlling the model are (1)-(2) the spectral indices of the density and magnetic field cubes, (3)-(4) the RMS-to-mean ratios for both fields, (5) the mean gas density, (6) the orientation of the mean magnetic field in the plane of the sky (POS), (7) the dust temperature, (8) the dust polarization fraction, and (9) the depth of the simulated cubes. We explore the nine-dimensional parameter space through a Markov chain Monte Carlo analysis, which yields best-fitting parameters and associated uncertainties. Results. We find that the power spectrum of the turbulent component of the magnetic field in the Polaris Flare molecular cloud scales with wavenumber as k-βB with a spectral index βB = 2.8 ± 0.2. It complements a uniform field whose norm in the POS is approximately twice the norm of the fluctuations of the turbulent component, and whose position angle with respect to the north-south direction is χ0-69°. The density field nH is well represented by a log-normally distributed field with a mean gas density (nH)40 cm-3, a fluctuation ratio σnH/(nH)1.6, and a power spectrum with an index βn=1.7-0.3 +0.4. We also constrain the depth of the cloud to be d 13 pc, and the polarization fraction p0 0.12. The agreement between the Planck data and the simulated maps for these best-fitting parameters is quantified by a χ2 value that is only slightly larger than unity. Conclusions. We conclude that our fBm-based model is a reasonable description of the diffuse, turbulent, magnetized ISM in the Polaris Flare molecular cloud, and that our analysis framework is able to yield quantitative estimates of the statistical properties of the dust density and magnetic field in this cloud. © 2018 ESO."
,10.21278/brod69208,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049691672&doi=10.21278%2fbrod69208&partnerID=40&md5=146ef1c3bebc8ea2b6e0d72c47db25b6,"Reliability-based inspection planning is one of the most popular methods in determining the time of inspection and repairs in various structures. In this way, inspection and repair times are determined mainly by putting a lower limit for the reliability index. The detection and measurement of cracks is one of the possible outputs at the time of inspecting fatigue cracking. One way to use this output is to update the parameters of the fatigue reliability equation. In this study, statistical distribution of the parameters of the problem is updated and fatigue reliability is calculated for inspection planning using the Bayesian updating concept through the Markov Chain Monte Carlo (MCMC) method and the Metropolis–Hasting algorithm. The distribution of crack growth equation material parameters and the initial crack length will be updated with this method. The application of the proposed method has been shown in a structural member of a ship. © 2018, Brodarski Institute. All rights reserved."
2,10.1214/17-BA1063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044401713&doi=10.1214%2f17-BA1063&partnerID=40&md5=95abab0ac10a45c8ac36f1142f185afc,"Markov chain Monte Carlo (MCMC) algorithms have become powerful tools for Bayesian inference. However, they do not scale well to large-data problems. Divide-and-conquer strategies, which split the data into batches and, for each batch, run independent MCMC algorithms targeting the corresponding subposterior, can spread the computational burden across a number of separate computer cores. The challenge with such strategies is in recombining the subposteriors to approximate the full posterior. By creating a Gaussian-process approximation for each log-subposterior density we create a tractable approximation for the full posterior. This approximation is exploited through three methodologies: firstly a Hamiltonian Monte Carlo algorithm targeting the expectation of the posterior density provides a sample from an approximation to the posterior; secondly, evaluating the true posterior at the sampled points leads to an importance sampler that, asymptotically, targets the true posterior expectations; finally, an alternative importance sampler uses the full Gaussian-process distribution of the approximation to the log-posterior density to re-weight any initial sample and provide both an estimate of the posterior expectation and a measure of the uncertainty in it. © 2018 International Society for Bayesian Analysis."
,10.1371/journal.pone.0198760,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048167676&doi=10.1371%2fjournal.pone.0198760&partnerID=40&md5=9b352b5ff983ba30aefb6436bac30b54,"Slaughterhouse surveillance through post-mortem meat inspection provides an important mechanism for detecting bovine tuberculosis (bTB) infections in cattle herds in Great Britain (GB), complementary to the live animal skin test based programme. We explore patterns in the numbers of herd breakdowns detected through slaughterhouse surveillance and develop a Bayesian hierarchical regression model to assess the associations of animal-level factors with the odds of an infected animal being detected in the slaughterhouse, allowing us to highlight slaughterhouses that show atypical patterns of detection. The analyses demonstrate that the numbers and proportions of breakdowns detected in slaughterhouses increased in GB over the period of study (1998–2013). The odds of an animal being a slaughterhouse case was strongly associated with the region of the country that the animal spent most of its life, with animals living in high-frequency testing areas of England having on average 21 times the odds of detection compared to animals living in Scotland. There was also a strong effect of age, with animals slaughtered at > 60 months of age having 5.3 times the odds of detection compared to animals slaughtered between 0–18 months of age. Smaller effects were observed for cattle having spent time on farms with a history of bTB, quarter of the year that the animal was slaughtered, movement and test history. Over-and-above these risks, the odds of detection increased by a factor of 1.1 for each year of the study. After adjustment for these variables, there were additional variations in risk between slaughterhouses and breed. Our framework has been adopted into the routine annual surveillance reporting carried out by the Animal Plant Health Agency and may be used to target more detailed investigation of meat inspection practices. © 2018 McKinley et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1093/jjfinec/nby010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051552102&doi=10.1093%2fjjfinec%2fnby010&partnerID=40&md5=d6420d5d81043f780b3b0aa6f2d2e67a,"We investigate high-frequency volatility models for analyzing intradaily tick by tick stock price changes using Bayesian estimation procedures. Our key interest is the extraction of intradaily volatility patterns from high-frequency integer price changes. We account for the discrete nature of the data via two different approaches: ordered probit models and discrete distributions. We allow for stochastic volatility by modeling the variance as a stochastic function of time, with intraday periodic patterns. We consider distributions with heavy tails to address occurrences of jumps in tick by tick discrete prices changes. In particular, we introduce a dynamic version of the negative binomial difference model with stochastic volatility. For each model, we develop a Markov chain Monte Carlo estimation method that takes advantage of auxiliary mixture representations to facilitate the numerical implementation. This new modeling framework is illustrated by means of tick by tick data for two stocks from the NYSE and for different periods. Different models are compared with each other based on the predictive likelihoods. We find evidence in favor of our preferred dynamic negative binomial difference model. © The Author(s) 2018. Published by Oxford University Press. All rights reserved."
1,10.1177/0013164417693666,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043387907&doi=10.1177%2f0013164417693666&partnerID=40&md5=39074350f8088eb3a0df26ddb9040744,"Stan is a new Bayesian statistical software program that implements the powerful and efficient Hamiltonian Monte Carlo (HMC) algorithm. To date there is not a source that systematically provides Stan code for various item response theory (IRT) models. This article provides Stan code for three representative IRT models, including the three-parameter logistic IRT model, the graded response model, and the nominal response model. We demonstrate how IRT model comparison can be conducted with Stan and how the provided Stan code for simple IRT models can be easily extended to their multidimensional and multilevel cases. © 2017, © The Author(s) 2017."
,10.1002/prot.25490,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044277394&doi=10.1002%2fprot.25490&partnerID=40&md5=804b1eff9523579a059b5bd8b9dc54e4,"Biological macromolecules often undergo large conformational rearrangements during a functional cycle. To simulate these structural transitions with full atomic detail typically demands extensive computational resources. Moreover, it is unclear how to incorporate, in a principled way, additional experimental information that could guide the structural transition. This article develops a probabilistic model for conformational transitions in biomolecules. The model can be viewed as a network of anharmonic springs that break, if the experimental data support the rupture of bonds. Hamiltonian Monte Carlo in internal coordinates is used to infer structural transitions from experimental data, thereby sampling large conformational transitions without distorting the structure. The model is benchmarked on a large set of conformational transitions. Moreover, we demonstrate the use of the probabilistic network model for integrative modeling of macromolecular complexes based on data from crosslinking followed by mass spectrometry. © 2018 Wiley Periodicals, Inc."
3,10.1177/1471082X18759140,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044338766&doi=10.1177%2f1471082X18759140&partnerID=40&md5=cb076b5cd8e0a490db0fd51bb58ee49a,"Bayesian methods have become increasingly popular in the past two decades. With the constant rise of computational power, even very complex models can be estimated on virtually any modern computer. Moreover, interest has shifted from conditional mean models to probabilistic distributional models capturing location, scale, shape and other aspects of a response distribution, where covariate effects can have flexible forms, for example, linear, non-linear, spatial or random effects. This tutorial article discusses how to select models in the Bayesian distributional regression setting, how to monitor convergence of the Markov chains and how to use simulation-based inference also for quantities derived from the original model parametrization. We exemplify the workflow using daily weather data on (a) temperatures on Germany's highest mountain and (b) extreme values of precipitation for the whole of Germany. © 2018, © 2018 SAGE Publications."
2,10.1007/s00362-016-0787-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976514994&doi=10.1007%2fs00362-016-0787-2&partnerID=40&md5=e9287a5e600e5ef85ddaba42fecfa87a,"In this paper, the estimation of parameters of a three-parameter Weibull–Gamma distribution based on progressively type-II right censored sample is studied. The maximum likelihood, Bayes, and parametric bootstrap methods are used for estimating the unknown parameters as well as some lifetime parameters reliability function, hazard function and coefficient of variation. Approximate confidence intervals for the unknown parameters as well as reliability function, hazard function and coefficient of variation are constructed based on the s-normal approximation to the asymptotic distribution of maximum likelihood estimators (MLEs), and log-transformed MLEs. In addition, two bootstrap CIs are also proposed. Bayes estimates of the unknown parameters and the corresponding credible intervals are obtained by using the Gibbs within Metropolis–Hasting samplers procedure. Furthermore, the results of Bayes method are obtained under both the balanced squared error loss and balanced linear-exponential loss. Analysis of a simulated data set has also been presented for illustrative purposes. Finally, a Monte Carlo simulation study is carried out to investigate the precision of the Bayes estimates with MLEs and two bootstrap estimates, also to compare the performance of different corresponding CIs considered. © 2016, The Author(s)."
7,10.1214/17-BA1051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040740190&doi=10.1214%2f17-BA1051&partnerID=40&md5=b818eb7a2e858b89334499bcf4c85c38,"In logistic regression, separation occurs when a linear combination of the predictors can perfectly classify part or all of the observations in the sample, and as a result, finite maximum likelihood estimates of the regression coefficients do not exist. Gelman et al. (2008) recommended independent Cauchy distributions as default priors for the regression coefficients in logistic regression, even in the case of separation, and reported posterior modes in their analyses. As the mean does not exist for the Cauchy prior, a natural question is whether the posterior means of the regression coefficients exist under separation. We prove theorems that provide necessary and sufficient conditions for the existence of posterior means under independent Cauchy priors for the logit link and a general family of link functions, including the probit link. We also study the existence of posterior means under multivariate Cauchy priors. For full Bayesian inference, we develop a Gibbs sampler based on Pólya-Gamma data augmentation to sample from the posterior distribution under independent Student-t priors including Cauchy priors, and provide a companion R package tglm, available at CRAN. We demonstrate empirically that even when the posterior means of the regression coefficients exist under separation, the magnitude of the posterior samples for Cauchy priors may be unusually large, and the corresponding Gibbs sampler shows extremely slow mixing. While alternative algorithms such as the No-U-Turn Sampler (NUTS) in Stan can greatly improve mixing, in order to resolve the issue of extremely heavy tailed posteriors for Cauchy priors under separation, one would need to consider lighter tailed priors such as normal priors or Student-t priors with degrees of freedom larger than one. © 2018 International Society for Bayesian Analysis."
,10.1007/s11009-017-9574-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020745040&doi=10.1007%2fs11009-017-9574-3&partnerID=40&md5=8e3517ca62f58558fa62cfb2dab0ebc4,"This paper proves convergence to stationarity of certain adaptive MCMC algorithms, under certain assumptions including easily-verifiable upper and lower bounds on the transition densities and a continuous target density. In particular, the transition and proposal densities are not required to be continuous, thus improving on the previous ergodicity results of Craiu et al. (Ann Appl Probab 25(6):3592–3623, 2015). © 2017, Springer Science+Business Media, LLC."
,10.1016/j.sste.2018.01.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041552848&doi=10.1016%2fj.sste.2018.01.003&partnerID=40&md5=b0ca11f23f82d4d746c88e8607fcd14c,"Model-based approaches for the analysis of areal count data are commonplace in spatiotemporal analysis. In Bayesian hierarchical models, a latent process is incorporated in the mean function to account for dependence in space and time. Typically, the latent process is modelled using a conditional autoregressive (CAR) prior. The aim of this paper is to offer an alternative approach to CAR-based priors for modelling the latent process. The proposed approach is based on a spatiotemporal generalization of a latent process Poisson regression model developed in a time series setting. Spatiotemporal dependence in the autoregressive model for the latent process is modelled through its transition matrix, with a structured covariance matrix specified for its error term. The proposed model and its parameterizations are fitted in a Bayesian framework implemented via MCMC techniques. Our findings based on real-life examples show that the proposed approach is at least as effective as CAR-based models. © 2018"
1,10.1111/rssa.12352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041745129&doi=10.1111%2frssa.12352&partnerID=40&md5=1f75c642ca723fa678b814e93af89310,"Statistical agencies are increasingly adopting synthetic data methods for disseminating microdata without compromising the privacy of respondents. Crucial to the implementation of these approaches are flexible models, able to capture the nuances of the multivariate structure in the original data. In the case of multivariate categorical data, preserving this multivariate structure also often involves satisfying constraints in the form of combinations of responses that cannot logically be present in any data set—like married toddlers or pregnant men—also known as structural zeros. Ignoring structural zeros can result in both logically inconsistent synthetic data and biased estimates. Here we propose the use of a Bayesian non-parametric method for generating discrete multivariate synthetic data subject to structural zeros. This method can preserve complex multivariate relationships between variables, can be applied to high dimensional data sets with massive collections of structural zeros, requires minimal tuning from the user and is computationally efficient. We demonstrate our approach by synthesizing an extract of 17 variables from the 2000 US census. Our method produces synthetic samples with high analytic utility and low disclosure risk. © 2018 Royal Statistical Society"
4,10.1016/j.envsoft.2018.03.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044166591&doi=10.1016%2fj.envsoft.2018.03.001&partnerID=40&md5=3cee4c858f7ece5670421ce053cdd7c5,"An efficient Bayesian analytical framework was developed to address the challenges of uncertainty analysis and assess the parameter identification problems of complex water quality models with high-dimensional parameter space. The inclusion of a multi-chain Markov Chain Monte Carlo method and comprehensive global sensitive analysis (GSA) guarantees the results to be robust. A high-frequency synthetic data case study was conducted in the EFDC water quality module including 54 parameters. The comprehensive GSA identified 39 completely or partially sensitive parameters for reducing dimensionality, among which only nine were identifiable without significant bias. The fundamental causes of the parameter identification problem could be traced to the cognitive limitations of the real water quality assessment process instead of data scarcity. The framework is powerful for exploring these limitations, generating reminders for model users to use Bayesian estimates in future forecasts, and providing directions for model developers to perfect a model in future work. © 2018 Elsevier Ltd"
1,10.1007/s00454-018-9992-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045057267&doi=10.1007%2fs00454-018-9992-1&partnerID=40&md5=18685e3cf5937b058864d5ae376bf993,"We extend the Langevin Monte Carlo (LMC) algorithm to compactly supported measures via a projection step, akin to projected stochastic gradient descent (SGD). We show that (projected) LMC allows to sample in polynomial time from a log-concave distribution with smooth potential. This gives a new Markov chain to sample from a log-concave distribution. Our main result shows in particular that when the target distribution is uniform, LMC mixes in O~ (n7) steps (where n is the dimension). We also provide preliminary experimental evidence that LMC performs at least as well as hit-and-run, for which a better mixing time of O~ (n4) was proved by Lovász and Vempala. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
1,10.1007/s00477-018-1538-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045121539&doi=10.1007%2fs00477-018-1538-9&partnerID=40&md5=c636d4a608fa787a06812651d3b5154e,"The physically based distributed hydrological models are ideal for hydrological simulations; however most of such models do not use the basic equations pertaining to mass, energy and momentum conservation, to represent the physics of the process. This is plausibly due to the lack of complete understanding of the hydrological process. The soil and water assessment tool (SWAT) is one such widely accepted semi-distributed, conceptual hydrological model used for water resources planning. However, the over-parameterization, difficulty in its calibration process and the uncertainty associated with predictions make its applications skeptical. This study considers assessing the predictive uncertainty associated with distributed hydrological models. The existing methods for uncertainty estimation demand high computational time and therefore make them challenging to apply on complex hydrological models. The proposed approach employs the concepts of generalized likelihood uncertainty estimation (GLUE) in an iterative procedure by starting with an assumed prior probability distribution of parameters, and by using mutual information (MI) index for sampling the behavioral parameter set. The distributions are conditioned on the observed information through successive cycles of simulations. During each cycle of simulation, MI is used in conjunction with Markov Chain Monte Carlo procedure to sample the parameter sets so as to increase the number of behavioral sets, which in turn helps reduce the number of cycles/simulations for the analysis. The method is demonstrated through a case study of SWAT model in Illinois River basin in the USA. A comparison of the proposed method with GLUE indicates that the computational requirement of uncertainty analysis is considerably reduced in the proposed approach. It is also noted that the model prediction band, derived using the proposed method, is more effective compared to that derived using the other methods considered in this study. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
1,10.1016/j.jse.2017.11.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039923971&doi=10.1016%2fj.jse.2017.11.013&partnerID=40&md5=cb02e01dd5848f41c0672d74af29cff3,"Background: The purpose of this study was to conduct a cost-effectiveness analysis of the arthroscopic Bankart and the open Latarjet in the treatment of primary shoulder instability. Methods: This cost-effectiveness study used a Markov decision chain and Monte-Carlo simulation. Existing literature was reviewed to determine the survivorship and complication rates of these procedures. Health utility states (EQ-5D and quality-adjusted life-years) of the Bankart and Latarjet were prospectively collected. Using these variables, the Monte-Carlo simulation was modeled 100,000 times. Results: In reviewing the literature, the overall recurrence rate is 14% after the arthroscopic Bankart and 8% after the open Latarjet. Postoperative health utility states were equal between the 2 procedures (mean EQ-5D, 0.930; P =.775). The Monte-Carlo simulation showed that the Bankart had an incremental cost-effectiveness ratio of $4214 and the Latarjet had an incremental cost-effectiveness ratio of $4681 (P <.001). Conclusion: Both the arthroscopic Bankart and open Latarjet are highly cost-effective; however, the Bankart is more cost-effective than the Latarjet, primarily because of a lower health utility state after a failed Latarjet. Ultimately, the clinical scenario may favor Latarjet (ie, critical glenoid bone loss) in certain circumstances, and decisions should be made on a case by case basis. © 2017 Journal of Shoulder and Elbow Surgery Board of Trustees"
1,10.1214/17-BA1049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044436139&doi=10.1214%2f17-BA1049&partnerID=40&md5=148f2330c99c8af349b8d8f3e24d3bef,"Evaluating the marginal likelihood in Bayesian analysis is essential for model selection. Estimators based on a single Markov chain Monte Carlo sample from the posterior distribution include the harmonic mean estimator and the inflated density ratio estimator. We propose a new class of Monte Carlo estimators based on this single Markov chain Monte Carlo sample. This class can be thought of as a generalization of the harmonic mean and inflated density ratio estimators using a partition weighted kernel (likelihood times prior). We show that our estimator is consistent and has better theoretical properties than the harmonic mean and inflated density ratio estimators. In addition, we provide guidelines on choosing optimal weights. Simulation studies were conducted to examine the empirical performance of the proposed estimator. We further demonstrate the desirable features of the proposed estimator with two real data sets: one is from a prostate cancer study using an ordinal probit regression model with latent variables; the other is for the power prior construction from two Eastern Cooperative Oncology Group phase III clinical trials using the cure rate survival model with similar objectives. © 2018 International Society for Bayesian Analysis."
,10.1007/s00705-018-3764-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041922059&doi=10.1007%2fs00705-018-3764-3&partnerID=40&md5=243f4005b09044431e548e99664008f8,"Previous local and national Iranian publications indicate that all Iranian hepatitis B virus (HBV) strains belong to HBV genotype D. The aim of this study was to analyze the evolutionary history of HBV infection in Iran for the first time, based on an intensive phylodynamic study. The evolutionary parameters, time to most recent common ancestor (tMRCA), and the population dynamics of infections were investigated using the Bayesian Monte Carlo Markov chain (BMCMC). The effective sample size (ESS) and sampling convergence were then monitored. After sampling from the posterior distribution of the nucleotide substitution rate and other evolutionary parameters, the point estimations (median) of these parameters were obtained. All Iranian HBV isolates were of genotype D, sub-type ayw2. The origin of HBV is regarded as having evolved first on the eastern border, before moving westward, where Isfahan province then hosted the virus. Afterwards, the virus moved to the south and west of the country. The tMRCA of HBV in Iran was estimated to be around 1894, with a 95% credible interval between the years 1701 and 1957. The effective number of infections increased exponentially from around 1925 to 1960. Conversely, from around 1992 onwards, the effective number of HBV infections has decreased at a very high rate. Phylodynamic inference clearly demonstrates a unique homogenous pattern of HBV genotype D compatible with a steady configuration of the decreased effective number of infections in the population in recent years, possibly due to the implementation of blood donation screening and vaccination programs. Adequate molecular epidemiology databases for HBV are crucial for infection prevention and treatment programs. © 2018, Springer-Verlag GmbH Austria, part of Springer Nature."
,10.1007/s13253-018-0319-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045147017&doi=10.1007%2fs13253-018-0319-8&partnerID=40&md5=532b7541abc143e142a9ec5fa103ad6e,"The Orbiting Carbon Observatory-2 (OCO-2) collects infrared spectra from which atmospheric properties are retrieved. OCO-2 operational data processing uses optimal estimation (OE), a state-of-the-art approach to inference of atmospheric properties from satellite measurements. One of the main advantages of the OE approach is computational efficiency, but it only characterizes the first two moments of the posterior distribution of interest. Here we obtain samples from the posterior using a Markov Chain Monte Carlo (MCMC) algorithm and compare this empirical estimate of the true posterior to the OE results. We focus on 600 simulated soundings that represent the variability of physical conditions encountered by OCO-2 between November 2014 and January 2016. We treat the two retrieval methods as ensemble and density probabilistic forecasts, where the MCMC yields an ensemble from the posterior and the OE retrieval result provide the first two moments of a normal distribution. To compare these methods, we apply both univariate and multivariate diagnostic tools and proper scoring rules. The general impression from our study is that when compared to MCMC, the OE retrieval performs reasonably well for the main quantity of interest, the column-averaged CO 2 concentration XCO2, but not for the full state vector X which includes a profile of CO 2 concentrations over 20 pressure levels, as well as several other atmospheric properties.Supplementary materials accompanying this paper appear on-line. © 2018, International Biometric Society."
1,10.1111/biom.12790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032803987&doi=10.1111%2fbiom.12790&partnerID=40&md5=7b99428c6f9922aca6bbc825fe870fef,"This work is motivated by a desire to quantify relationships between two time series of pulsing hormone concentrations. The locations of pulses are not directly observed and may be considered latent event processes. The latent event processes of pulsing hormones are often associated. It is this joint relationship we model. Current approaches to jointly modeling pulsing hormone data generally assume that a pulse in one hormone is coupled with a pulse in another hormone (one-to-one association). However, pulse coupling is often imperfect. Existing joint models are not flexible enough for imperfect systems. In this article, we develop a more flexible class of pulse association models that incorporate parameters quantifying imperfect pulse associations. We propose a novel use of the Cox process model as a model of how pulse events co-occur in time. We embed the Cox process model into a hormone concentration model. Hormone concentration is the observed data. Spatial birth and death Markov chain Monte Carlo is used for estimation. Simulations show the joint model works well for quantifying both perfect and imperfect associations and offers estimation improvements over single hormone analyses. We apply this model to luteinizing hormone (LH) and follicle stimulating hormone (FSH), two reproductive hormones. Use of our joint model results in an ability to investigate novel hypotheses regarding associations between LH and FSH secretion in obese and non-obese women. © 2017, The International Biometric Society"
1,10.1002/env.2504,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048059480&doi=10.1002%2fenv.2504&partnerID=40&md5=fced3b370fb4e938f0b22874bf772b7d,"There is substantial interest in assessing how exposure to environmental mixtures, such as chemical mixtures, affects child health. Researchers are also interested in identifying critical time windows of susceptibility to these complex mixtures. A recently developed method, called lagged kernel machine regression (LKMR), simultaneously accounts for these research questions by estimating the effects of time-varying mixture exposures and by identifying their critical exposure windows. However, LKMR inference using Markov chain Monte Carlo (MCMC) methods (MCMC-LKMR) is computationally burdensome and time intensive for large data sets, limiting its applicability. Therefore, we develop a mean field variational approximation method for Bayesian inference (MFVB) procedure for LKMR (MFVB-LKMR). The procedure achieves computational efficiency and reasonable accuracy as compared with the corresponding MCMC estimation method. Updating parameters using MFVB may only take minutes, whereas the equivalent MCMC method may take many hours or several days. We apply MFVB-LKMR to Programming Research in Obesity, Growth, Environment and Social Stressors (PROGRESS), a prospective cohort study in Mexico City. Results from a subset of PROGRESS using MFVB-LKMR provide evidence of significant and positive association between second trimester cobalt levels and z-scored birth weight. This positive association is heightened by cesium exposure. MFVB-LKMR is a promising approach for computationally efficient analysis of environmental health data sets, to identify critical windows of exposure to complex mixtures. Copyright © 2018 John Wiley & Sons, Ltd."
,10.1371/journal.pone.0199450,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049196848&doi=10.1371%2fjournal.pone.0199450&partnerID=40&md5=d8e8fa933b7172f1b979cb0f44993692,"Background The CYD-TDV vaccine was unusual in that the recommended target population for vaccination was originally defined not only by age, but also by transmission setting as defined by seroprevalence. WHO originally recommended countries consider vaccination against dengue with CYD-TDV vaccine in geographic settings only where prior infection with any dengue serotype, as measured by seroprevalence, was >170% in the target age group. Vaccine was not recommended in settings where seroprevalence was <50%. Test-and-vaccinate strategies suggested following new analysis by Sanofi will still require age-stratified seroprevalence surveys to optimise age-group targeting. Here we address considerations for serosurvey design in the context of vaccination program planning. Methods To explore how the design of seroprevalence surveys affects estimates of transmission intensity, 100 age-specific seroprevalence surveys were simulated using a beta-binomial distribution and a simple catalytic model for different combinations of age-range, survey size, transmission setting, and test sensitivity/specificity. We then used a Metropolis-Hastings Markov Chain Monte-Carlo algorithm to estimate the force of infection from each simulated dataset. Results Sampling from a wide age-range led to more accurate estimates than merely increasing sample size in a narrow age-range. This finding was consistent across all transmission settings. The optimum test sensitivity and specificity given an imperfect test differed by setting with high sensitivity being important in high transmission settings and high specificity important in low transmission settings. Conclusions When assessing vaccination suitability by seroprevalence surveys, countries should ensure an appropriate age-range is sampled, considering epidemiological evidence about the local burden of disease. © 2018 Imai, Ferguson. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1051/0004-6361/201730921,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048882050&doi=10.1051%2f0004-6361%2f201730921&partnerID=40&md5=2b0d776ad39434770206721950f786a8,"The Gaia mission is expected to make a significant contribution to the knowledge of exoplanet systems, both in terms of their number and of their physical properties. We develop Bayesian methods and detection criteria for orbital fitting, and revise the detectability of exoplanets in light of the in-flight properties of Gaia. Limiting ourselves to one-planet systems as a first step of the development, we simulate Gaia data for exoplanet systems over a grid of S/N, orbital period, and eccentricity. The simulations are then fit using Markov chain Monte Carlo methods. We investigate the detection rate according to three information criteria and the Δχ2. For the Δχ2, the effective number of degrees of freedom depends on the mission length. We find that the choice of the Markov chain starting point can affect the quality of the results; we therefore consider two limit possibilities: an ideal case, and a very simple method that finds the starting point assuming circular orbits. We use 6644 and 4402 simulations to assess the fraction of false positive detections in a 5 yr and in a 10 yr mission, respectively; and 4968 and 4706 simulations to assess the detection rate and how the parameters are recovered. Using Jeffreys' scale of evidence, the fraction of false positives passing a strong evidence criterion is ≤ 0.2% (0.6%) when considering a 5 yr (10 yr) mission and using the Akaike information criterion or the Watanabe-Akaike information criterion, and &lt;0.02% (&lt;0.06%) when using the Bayesian information criterion. We find that there is a 50% chance of detecting a planet with a minimum S/N = 2.3 (1.7). This sets the maximum distance to which a planet is detectable to ∼70 pc and ∼3.5 pc for a Jupiter-mass and Neptune-mass planets, respectively, assuming a 10 yr mission, a 4 au semi-major axis, and a 1 M⊙ star. We show the distribution of the accuracy and precision with which orbital parameters are recovered. The period is the orbital parameter that can be determined with the best accuracy, with a median relative difference between input and output periods of 4.2% (2.9%) assuming a 5 yr (10 yr) mission. The median accuracy of the semi-major axis of the orbit can be recovered with a median relative error of 7% (6%). The eccentricity can also be recovered with a median absolute accuracy of 0.07 (0.06). © ESO 2018."
,10.1371/journal.pcbi.1006235,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049368254&doi=10.1371%2fjournal.pcbi.1006235&partnerID=40&md5=1ca838b839b9e809047806b6eea9a5da,"Imaging data has become an essential tool to explore key biological questions at various scales, for example the motile behaviour of bacteria or the transport of mRNA, and it has the potential to transform our understanding of important transport mechanisms. Often these imaging studies require us to compare biological species or mutants, and to do this we need to quantitatively characterise their behaviour. Mathematical models offer a quantitative description of a system that enables us to perform this comparison, but to relate mechanistic mathematical models to imaging data, we need to estimate their parameters. In this work we study how collecting data at different temporal resolutions impacts our ability to infer parameters of biological transport models; performing exact inference for simple velocity jump process models in a Bayesian framework. The question of how best to choose the frequency with which data is collected is prominent in a host of studies because the majority of imaging technologies place constraints on the frequency with which images can be taken, and the discrete nature of observations can introduce errors into parameter estimates. In this work, we mitigate such errors by formulating the velocity jump process model within a hidden states framework. This allows us to obtain estimates of the reorientation rate and noise amplitude for noisy observations of a simple velocity jump process. We demonstrate the sensitivity of these estimates to temporal variations in the sampling resolution and extent of measurement noise. We use our methodology to provide experimental guidelines for researchers aiming to characterise motile behaviour that can be described by a velocity jump process. In particular, we consider how experimental constraints resulting in a trade-off between temporal sampling resolution and observation noise may affect parameter estimates. Finally, we demonstrate the robustness of our methodology to model misspecification, and then apply our inference framework to a dataset that was generated with the aim of understanding the localization of RNA-protein complexes. © 2018 Harrison, Baker. http://creativecommons.org/licenses/by/4.0/"
,10.1016/j.csda.2018.01.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041538675&doi=10.1016%2fj.csda.2018.01.013&partnerID=40&md5=b61118c5b90e5ce5cf89d768acb05309,"Parametric conditional copula models allow the copula parameters to vary with a set of covariates according to an unknown calibration function. Flexible Bayesian inference for the calibration function of a bivariate conditional copula is introduced. The prior distribution over the set of smooth calibration functions is built using a sparse Gaussian process (GP) prior for the single index model (SIM). The estimation of parameters from the marginal distributions and the calibration function is done jointly via Markov Chain Monte Carlo sampling from the full posterior distribution. A new Conditional Cross Validated Pseudo-Marginal (CCVML) criterion is used to perform copula selection and is modified using a permutation-based procedure to assess data support for the simplifying assumption. The performance of the estimation method and model selection criteria is studied via a series of simulations using correct and misspecified models with Clayton, Frank and Gaussian copulas and a numerical application involving red wine features. © 2018 Elsevier B.V."
,10.1007/s10827-018-0684-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046767121&doi=10.1007%2fs10827-018-0684-x&partnerID=40&md5=058c8aa193e67c820856437cc9f51740,"It was previously reported, that temperature may significantly influence neural dynamics on the different levels of brain function. Thus, in computational neuroscience, it would be useful to make models scalable for a wide range of various brain temperatures. However, lack of experimental data and an absence of temperature-dependent analytical models of synaptic conductance does not allow to include temperature effects at the multi-neuron modeling level. In this paper, we propose a first step to deal with this problem: A new analytical model of AMPA-type synaptic conductance, which is able to incorporate temperature effects in low-frequency stimulations. It was constructed based on Markov model description of AMPA receptor kinetics using the set of coupled ODEs. The closed-form solution for the set of differential equations was found using uncoupling assumption (introduced in the paper) with few simplifications motivated both from experimental data and from Monte Carlo simulation of synaptic transmission. The model may be used for computationally efficient and biologically accurate implementation of temperature effects on AMPA receptor conductance in large-scale neural network simulations. As a result, it may open a wide range of new possibilities for researching the influence of temperature on certain aspects of brain functioning. © 2018, The Author(s)."
1,10.1051/0004-6361/201731396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048884068&doi=10.1051%2f0004-6361%2f201731396&partnerID=40&md5=4471cceee7b94237e37640fa4db1e35c,"We have investigated the shape of the extinction curve in the infrared up to ∼25μm for the Orion A star-forming complex. The basis of this work is near-infrared data acquired with the Visual and Infrared Survey Telescope for Astronomy, in combination with Pan-STARRS and mid-infrared Spitzer photometry. We obtain colour excess ratios for eight passbands by fitting a series of colour-colour diagrams. The fits are performed using Markov chain Monte Carlo methods, together with a linear model under a Bayesian formalism. The resulting colour excess ratios are directly interpreted as a measure of the extinction law. We show that the Orion A molecular cloud is characterized by flat mid-infrared extinction, similar to many other recently studied sightlines. Moreover, we find statistically significant evidence that the extinction law from ∼1μm to at least ∼6μm varies across the cloud. In particular, we find a gradient along galactic longitude, where regions near the Orion Nebula Cluster show a different extinction law compared to L1641 and L1647, the low-mass star-forming sites in the cloud complex. These variations are of the order of only 3% and are most likely caused by the influence of the massive stars on their surrounding medium. While the observed general trends in our measurements are in agreement with model predictions, both well-established and new dust grain models are not able to fully reproduce our infrared extinction curve. We also present a new extinction map featuring a resolution of 1′ and revisit the correlation between extinction and dust optical depth. This analysis shows that cloud substructure, which is not sampled by background sources, affects the conversion factor between these two measures. In conclusion, we argue that specific characteristics of the infrared extinction law are still not well understood, but Orion A can serve as an unbiased template for future studies. © ESO 2018."
4,10.1051/0004-6361/201732327,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048894220&doi=10.1051%2f0004-6361%2f201732327&partnerID=40&md5=ecad373e4e82f5047e49230d57111f63,"Context. The first Gaia data release unlocked the access to photometric information for 1.1 billion sources in the G-band. Yet, given the high level of degeneracy between extinction and spectral energy distribution for large passbands such as the Gaia G-band, a correction for the interstellar reddening is needed in order to exploit Gaia data. Aims. The purpose of this manuscript is to provide the empirical estimation of the Gaia G-band extinction coefficient kG for both the red giants and main sequence stars in order to be able to exploit the first data release DR1. Methods. We selected two samples of single stars: one for the red giants and one for the main sequence. Both samples are the result of a cross-match between Gaia DR1 and 2MASS catalogues; they consist of high-quality photometry in the G-, J- and KS-bands. These samples were complemented by temperature and metallicity information retrieved from APOGEE DR13 and LAMOST DR2 surveys, respectively. We implemented a Markov chain Monte Carlo method where we used (G - KS)0 versus Teff and (J - KS)0 versus (G - KS)0, calibration relations to estimate the extinction coefficient kG and we quantify its corresponding confidence interval via bootstrap resampling. We tested our method on samples of red giants and main sequence stars, finding consistent solutions. Results. We present here the determination of the Gaia extinction coefficient through a completely empirical method. Furthermore we provide the scientific community with a formula for measuring the extinction coefficient as a function of stellar effective temperature, the intrinsic colour (G - KS)0, and absorption. © ESO 2018."
,10.1016/j.frl.2017.10.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033385926&doi=10.1016%2fj.frl.2017.10.021&partnerID=40&md5=59d89ba47cb793955d3da5e2d2f63167,"This study investigates the impact of stock market cycles on the volatility of Asian markets. It specifically addresses the combined effect of jumps, asymmetry and stochasticity while predicting the market volatility. Our results indicate that the stochastic volatility process is highly persistent across the countries. Leverage effect, size and frequency of jumps are found to be significant and play a prominent role in computing market volatility. The empirical results imply that the stochastic volatility model embedded with the jump and asymmetric component significantly helps in measuring volatility especially during the turbulent periods. Our results have major implications for policy makers, regulators, mutual funds, hedge funds as well for other institutional investors. © 2017 Elsevier Inc."
,10.15446/esrj.v22n2.65577,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050793625&doi=10.15446%2fesrj.v22n2.65577&partnerID=40&md5=75eff689f7ceee892ec5832f7d41f4e9,"Time series models are often used in hydrology and meteorology studies to model streamflows series in order to make forecasting and generate synthetic series which are inputs for the analysis of complex water resources systems. In this paper we introduce a new modeling approach for hydrologic and meteorological time series assuming a continuous distribution for the data, where both the conditional mean and conditional variance parameters are modeled. Bayesian methods using standard MCMC (Markov Chain Monte Carlo Methods) are used to simulate samples for the joint posterior distribution of interest. Two applications to real data sets illustrate the proposed methodology, assuming that the observations come from a normal, a gamma or a beta distribution. A first example is given by a time series of monthly averages of natural streamflows, measured in the year period ranging from 1931 to 2010 in Furnas hydroelectric dam, Brazil. A second example is given with a time series of 313 air humidity data measured in a weather station of Rio Claro, a Brazilian city located in southeastern of Brazil. These applications motivate us to introduce new classes of models to analyze hydrological and meteorological time series. © 2018, Universidad Nacional de Colombia. All rights reserved."
2,10.1016/j.amar.2018.04.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045719766&doi=10.1016%2fj.amar.2018.04.002&partnerID=40&md5=3278c2f13c94b3de0d13ea011f213784,"The existence of preponderant zero crash sites and/or sites with large crash counts can present challenges during the statistical analysis of crash count data. Additionally, unobserved heterogeneity in crash data due to the absence of important variables could negatively impact the estimated model parameters. The traditional negative binomial (NB) model with fixed parameters might not adequately handle highly over-dispersed data or unobserved heterogeneity. Many research efforts that have involved the negative binomial–Lindley (NB-L) model or the random parameters negative binomial (RPNB) model, for example, have attempted to improve the inference of estimated coefficients by explicitly accounting for extra variation in crash data. The NB-L is a mixed modeling approach which provides flexibility to account for additional dispersion in data. The RP modeling approach accommodates the effect of unobserved variables by allowing the model parameters to vary from one observation to another. The following study proposes a combination of these models – the random parameters NB-L (RPNB-L) generalized linear model (GLM) – to account for underlying heterogeneity and address excess over-dispersion. The results show that the RPNB-L model not only provides a superior goodness-of-fit (GOF) with the sample data, but also offers a better understanding about the effects of potential contributing factors. The paper uses the Bayesian framework to provide a strategy for eliminating the potential for poor mixing in the Markov Chain Monte Carlo (MCMC) chains during the estimation of the RPNB-L model. © 2018 Elsevier Ltd"
,10.1002/qre.2279,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047941131&doi=10.1002%2fqre.2279&partnerID=40&md5=6895e0fb032e0e9a55ede1e58efd3723,"Based on failures of a parallel-series system, a new distribution called geometric-Poisson-Rayleigh distribution is proposed. Some properties of the distribution are discussed. A real data set is used to compare the new distribution with other 6 distributions. The progressive-stress accelerated life tests are considered when the lifetime of an item under use condition is assumed to follow the geometric-Poisson-Rayleigh distribution. It is assumed that the scale parameter of the geometric-Poisson-Rayleigh distribution satisfies the inverse power law such that the stress is a nonlinear increasing function of time and the cumulative exposure model for the effect of changing stress holds. Based on type-I progressive hybrid censoring with binomial removals, the maximum likelihood and Bayes (using linear-exponential and general entropy loss functions) estimation methods are considered to estimate the involved parameters. Some point predictors such as the maximum likelihood, conditional median, best unbiased, and Bayes point predictors for future order statistics are obtained. The Bayes estimates are obtained using Markov chain Monte Carlo algorithm. Finally, a simulation study is performed, and numerical computations are performed to compare the performance of the implemented methods of estimation and prediction. Copyright © 2018 John Wiley & Sons, Ltd."
4,10.1016/j.kint.2018.01.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044330182&doi=10.1016%2fj.kint.2018.01.009&partnerID=40&md5=87ecc9f0b55344720736c2aa6c3e43b0,"Patients with chronic kidney disease and severely decreased glomerular filtration rate (GFR) are at high risk for kidney failure, cardiovascular disease (CVD) and death. Accurate estimates of risk and timing of these clinical outcomes could guide patient counseling and therapy. Therefore, we developed models using data of 264,296 individuals in 30 countries participating in the international Chronic Kidney Disease Prognosis Consortium with estimated GFR (eGFR)s under 30 ml/min/1.73m2. Median participant eGFR and urine albumin-to-creatinine ratio were 24 ml/min/1.73m2 and 168 mg/g, respectively. Using competing-risk regression, random-effect meta-analysis, and Markov processes with Monte Carlo simulations, we developed two- and four-year models of the probability and timing of kidney failure requiring kidney replacement therapy (KRT), a non-fatal CVD event, and death according to age, sex, race, eGFR, albumin-to-creatinine ratio, systolic blood pressure, smoking status, diabetes mellitus, and history of CVD. Hypothetically applied to a 60-year-old white male with a history of CVD, a systolic blood pressure of 140 mmHg, an eGFR of 25 ml/min/1.73m2 and a urine albumin-to-creatinine ratio of 1000 mg/g, the four-year model predicted a 17% chance of survival after KRT, a 17% chance of survival after a CVD event, a 4% chance of survival after both, and a 28% chance of death (9% as a first event, and 19% after another CVD event or KRT). Risk predictions for KRT showed good overall agreement with the published kidney failure risk equation, and both models were well calibrated with observed risk. Thus, commonly-measured clinical characteristics can predict the timing and occurrence of clinical outcomes in patients with severely decreased GFR. © 2018 International Society of Nephrology"
,10.1016/j.mgene.2018.02.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042286463&doi=10.1016%2fj.mgene.2018.02.005&partnerID=40&md5=f9629535b85b4ae2090c01180c8c6410,"Genus Impatiens is widely distributed in the areas of Old World tropics and subtropics. Of which, only European and United States Impatiens were examined for genetic diversity analysis. In this study, 10 Indian Impatiens were studied for their interspecies genetic diversity based on inter-simple sequence repeat (ISSR) markers and morphological characters. All 73 DNA fragments amplified using 6 ISSR markers were polymorphic. Diversity indices for ISSR markers were ranged from He = 0.2183 to 0.3733, I = 0.3533 to 0.5579, genetic similarity coefficient = 0.5135 to 0.8409 and average PIC value as 0.7523 for each marker. These results indicate the presence of high amount of genetic diversity among studied Impatiens species. Computed p-value (0.941) obtained for Mantel test reveals nonsignificant correlation of the diversity and relationship among Impatiens shown by ISSR markers and morphological characters. Studied Impatiens were divided in to 4 groups based on clustering results in unweighted pair group method with arithmetic mean (UPGMA) dendrogram of ISSR and morphological data. Resolution of these species obtained using ISSR data limitedly supports the sectional classification. Biogeographic history of Impatiens based on ITS phylogeny using Bayesian Binary Markov Chain Monte Carlo method (BBM) confirmed the South-East Asian origin of Impatiens. This study gave an idea about interspecies diversity in Indian Impatiens and suggests the suitability of ISSR markers for further interspecies diversity studies in Impatiens. © 2018 Elsevier B.V."
,10.1007/s10260-017-0384-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022185707&doi=10.1007%2fs10260-017-0384-0&partnerID=40&md5=e055549f7b6a251cfda5f399d9745145,"The use of Bayesian nonparametrics models has increased rapidly over the last few decades driven by increasing computational power and the development of efficient Markov chain Monte Carlo algorithms. We review some applications of these models in economic applications including: volatility modelling (using both stochastic volatility models and GARCH-type models) with Dirichlet process mixture models, uses in portfolio allocation problems, long memory models with flexible forms of time-dependence, flexible extension of the dynamic Nelson-Siegel model for interest rate yields and multivariate time series models used in macroeconometrics. © 2017, Springer-Verlag GmbH Germany."
,10.1063/1.5025545,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048321420&doi=10.1063%2f1.5025545&partnerID=40&md5=0a94b244245cc18ca964f296bd491d62,"We propose a Bayesian nonparametric approach for the noise reduction of a given chaotic time series contaminated by dynamical noise, based on Markov Chain Monte Carlo methods. The underlying unknown noise process (possibly) exhibits heavy tailed behavior. We introduce the Dynamic Noise Reduction Replicator model with which we reconstruct the unknown dynamic equations and in parallel we replicate the dynamics under reduced noise level dynamical perturbations. The dynamic noise reduction procedure is demonstrated specifically in the case of polynomial maps. Simulations based on synthetic time series are presented. © 2018 Author(s)."
,10.1111/jedm.12178,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048053036&doi=10.1111%2fjedm.12178&partnerID=40&md5=691eb4e6e724c669d33726415674e0ad,"Lord's Wald test for differential item functioning (DIF) has not been studied extensively in the context of the multidimensional item response theory (MIRT) framework. In this article, Lord's Wald test was implemented using two estimation approaches, marginal maximum likelihood estimation and Bayesian Markov chain Monte Carlo estimation, to detect uniform and nonuniform DIF under MIRT models. The Type I error and power rates for Lord's Wald test were investigated under various simulation conditions, including different DIF types and magnitudes, different means and correlations of two ability parameters, and different sample sizes. Furthermore, English usage data were analyzed to illustrate the use of Lord's Wald test with the two estimation approaches. Copyright © 2018 by the National Council on Measurement in Education"
,10.1080/10618600.2018.1438900,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048145700&doi=10.1080%2f10618600.2018.1438900&partnerID=40&md5=85c72002af2ac9f7982f02562197f18d,"While mixtures of Gaussian distributions have been studied for more than a century, the construction of a reference Bayesian analysis of those models remains unsolved, with a general prohibition of improper priors due to the ill-posed nature of such statistical objects. This difficulty is usually bypassed by an empirical Bayes resolution. By creating a new parameterization centered on the mean and possibly the variance of the mixture distribution itself, we manage to develop here a weakly informative prior for a wide class of mixtures with an arbitrary number of components. We demonstrate that some posterior distributions associated with this prior and a minimal sample size are proper. We provide Markov chain Monte Carlo (MCMC) implementations that exhibit the expected exchangeability. We only study here the univariate case, the extension to multivariate location-scale mixtures being currently under study. An R package called Ultimixt is associated with this article. Supplementary material for this article is available online. © 2018 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America"
,10.1063/1.5026855,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049408505&doi=10.1063%2f1.5026855&partnerID=40&md5=d4786deb0dfef61ee0ca09c168a61c18,"This paper contributes in detecting chaotic behaviors in dynamic complex social networks using a new feature diffusion-aware model from two perspectives of abnormal links as well as abnormal nodes. The proposed approach constructs a probabilistic model of dynamic complex social networks and subsequently, applies it to detect chaotic behaviors by measuring deviations from the model. The predictive model considers the main processes of features' dynamics, evolution of nodes' features, feature diffusion, and link generation processes in dynamic complex social networks. The feature diffusion process indicates the process in which each node former features influence the future features of its neighbors. The proposed approach is validated by experiments on two real dynamic complex social network datasets of Google+ and Twitter. The approach uses some Markov Chain Monte Carlo sampling methods like Metropolis-Hastings algorithm and Slice sampling strategy to extract the model parameters, given these real datasets. Experimental results indicate the improved performance characteristics of the proposed approach in comparison with baseline approaches in terms of the performance measures of accuracy, F1-score, Matthews Correlation Coefficient, recall, precision, area under ROC curve, and log-likelihood. © 2018 Author(s)."
3,10.1002/jrsm.1285,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040739921&doi=10.1002%2fjrsm.1285&partnerID=40&md5=09128200d7060a71964876e7d0435a81,"Network meta-analysis (NMA) is gaining popularity for comparing multiple treatments in a single analysis. Generalized linear mixed models provide a unifying framework for NMA, allow us to analyze datasets with dichotomous, continuous or count endpoints, and take into account multiarm trials, potential heterogeneity between trials and network inconsistency. To perform inference within such NMA models, the use of Bayesian methods is often advocated. The standard inference tool is Markov chain Monte Carlo (MCMC), which is computationally expensive and requires convergence diagnostics. A deterministic approach to do fully Bayesian inference for latent Gaussian models can be achieved by integrated nested Laplace approximations (INLA), which is a fast and accurate alternative to MCMC. We show how NMA models fit in the class of latent Gaussian models and how NMA models are implemented using INLA and demonstrate that the estimates obtained by INLA are in close agreement with the ones obtained by MCMC. Specifically, we emphasize the design-by-treatment interaction model with random inconsistency parameters (also known as the Jackson model). Also, we have proposed a network meta-regression model, which is constructed by incorporating trial-level covariates to the Jackson model to explain possible sources of heterogeneity and/or inconsistency in the network. A publicly available R package, nmaINLA, is developed to automate the INLA implementation of NMA models, which are considered in this paper. Three applications illustrate the use of INLA for a NMA. © 2017 The Authors. Research Synthesis Methods Published by John Wiley & Sons Ltd"
1,10.1111/biom.12766,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028412236&doi=10.1111%2fbiom.12766&partnerID=40&md5=f7531ef5021dfbaf57c333d7125dec8b,"Sightings of previously marked animals can extend a capture–recapture dataset without the added cost of capturing new animals for marking. Combined marking and resighting methods are therefore an attractive option in animal population studies, and there exist various likelihood-based non-spatial models, and some spatial versions fitted by Markov chain Monte Carlo sampling. As implemented to date, the focus has been on modeling sightings only, which requires that the spatial distribution of pre-marked animals is known. We develop a suite of likelihood-based spatial mark–resight models that either include the marking phase (“capture–mark–resight” models) or require a known distribution of marked animals (narrow-sense “mark–resight”). The new models sacrifice some information in the covariance structure of the counts of unmarked animals; estimation is by maximizing a pseudolikelihood with a simulation-based adjustment for overdispersion in the sightings of unmarked animals. Simulations suggest that the resulting estimates of population density have low bias and adequate confidence interval coverage under typical sampling conditions. Further work is needed to specify the conditions under which ignoring covariance results in unacceptable loss of precision, or to modify the pseudolikelihood to include that information. The methods are applied to a study of ship rats Rattus rattus using live traps and video cameras in a New Zealand forest, and to previously published data. © 2017, The International Biometric Society"
,10.1007/s10511-018-9525-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048458212&doi=10.1007%2fs10511-018-9525-x&partnerID=40&md5=e7de0f600712bc2871a3d5eb38a5ef95,"We present the γ -ray observations of the radio galaxy PKS 0625-35, using the Fermi Large Area Telescope data accumulated during 2008-2017. γ -rays up to 100 GeV have been detected with a detection significance of about 32.3σ. A power law spectrum with a photon index of 1.88±0.04 and an integrated flux of Fγ = (1.02 ± 0.10)×10-8 photon cm-2 s-1 above 100 MeV well describes the data averaged over 9 years of observations. There is a hint of deviation from a simple power-law shape around tens of GeV energies; however, the low statistics does not allow one to reject power law modeling. The spectral energy distributions during high and low X-ray states are modeled using one-zone leptonic models that include the synchrotron, synchrotron self Compton processes; the model parameters are estimated using the Markov Chain Monte Carlo method. The modeling shows that in the jet of PKS 0625-35 the particles (electrons) are accelerated to energies higher than 50 TeV. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
,10.1111/aogs.13310,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042528302&doi=10.1111%2faogs.13310&partnerID=40&md5=e7396dafdcadde96c89bf61d4891d3ed,"Introduction: Cesarean section rates continue to increase globally. Prediction of intrapartum cesarean section could lead to preventive measures. Our aim was to assess the association between sonographically measured cervical length at 37 weeks of gestation and cesarean section among women planning a vaginal birth. The population was women with a low-risk pregnancy or with gestational diabetes. Material and methods: This was a prospective cohort study conducted in a tertiary referral hospital in Sydney, Australia. In all, 212 women with a low-risk pregnancy or with gestational diabetes were recruited including 158 nulliparous and 54 parous women. Maternal demographic, clinical and ultrasound characteristics were collected at 37 weeks of gestation. Semi-Bayesian logistic regression and Markov chain Monte Carlo simulation were used to assess the relation between cervical length and cesarean section in labor. Results: Rates of cesarean section were 5% (2/55) for cervical length ≤20 mm, 17% (17/101) for cervical length 20–32 mm, and 27% (13/56) for cervical length >32 mm. These rates were 4, 22 and 33%, respectively, in nulliparous women. In the semi-Bayesian analysis, the odds ratio for cesarean section was 6.2 (95% confidence interval 2.2–43) for cervical length 20–32 mm and 10 (95% confidence interval 4.8–74) for cervical length >32 mm compared with the lowest quartile of cervical length, after adjusting for maternal age, parity, height, prepregnancy body mass index, gestational diabetes, induction of labor, neonatal sex and birthweight centile. Conclusions: Cervical length at 37 weeks of gestation is associated with intrapartum cesarean section. © 2018 Nordic Federation of Societies of Obstetrics and Gynecology"
1,10.1111/2041-210X.13009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045841727&doi=10.1111%2f2041-210X.13009&partnerID=40&md5=ea7507b4ed8a8c1f1e18dd97d5fedfa9,"Stable isotope analysis provides a powerful tool to identify the energy sources which fuel consumers, to understand trophic interactions and to infer consumer trophic position (TP), an important concept that describes the ecological role of consumers in food webs. However, current methods for estimating TP using stable isotopes are limited and do not fulfil the complete potential of the isotopic approach. For instance, researchers typically use point estimates for key parameters including trophic discrimination factors and isotopic baselines, and do not explicitly include variance associated with these parameters when calculating TP. We present “tRophicPosition,” an r package incorporating a Bayesian model for the calculation of consumer TP at the population level using stable isotopes, with one or two baselines. It combines Markov Chain Monte Carlo simulations through JAGS and statistical and graphical analyses using R. We model consumer and baseline observations using relevant statistical distributions, allowing them to be treated as random variables. The calculation of TP—a random parameter—for one baseline follows standard equations linking 15N enrichment per trophic level and the trophic position of the baseline (e.g. a primary producer or primary consumer). In the case of two baselines, a simple mixing model incorporating δ13C allows for the differentiation between two distinct sources of nitrogen, thus including heterogeneity derived from alternatives sources of δ15N. Methods currently implemented in “tRophicPosition” include loading, plotting and summarizing stable isotope data either from multiple sites and/or communities or a local assemblage; loading trophic discrimination factors from an internal database or generating them; defining and initializing a Bayesian model of TP; sampling posterior parameters; analysing, comparing and plotting posterior estimates of TP and other parameters; and calculating a parametric (non-Bayesian) TP estimate. Additionally, full documentation including examples, multiple vignettes and code are available for download. © 2018 The Authors. Methods in Ecology and Evolution © 2018 British Ecological Society"
1,10.3390/atmos9060213,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047838085&doi=10.3390%2fatmos9060213&partnerID=40&md5=815ce29718f081ec65f6dc7a52d4eaae,"This paper presents a fully non-Gaussian filter for sequential data assimilation. The filter is named the ""cluster sampling filter"", and works by directly sampling the posterior distribution following a Markov Chain Monte-Carlo (MCMC) approach, while the prior distribution is approximated using a Gaussian Mixture Model (GMM). Specifically, a clustering step is introduced after the forecast phase of the filter, and the prior density function is estimated by fitting a GMM to the prior ensemble. Using the data likelihood function, the posterior density is then formulated as a mixture density, and is sampled following an MCMC approach. Four versions of the proposed filter, namely CℓMCMC, CℓHMC, MC-CℓHMC, and MC-CℓHMC are presented. CℓMCMC uses a Gaussian proposal density to sample the posterior, and CℓHMC is an extension to the Hamiltonian Monte-Carlo (HMC) sampling filter. MC-CℓMCMC and MC-CℓHMC are multi-chain versions of the cluster sampling filters CℓMCMC and CℓHMC respectively. The multi-chain versions are proposed to guarantee that samples are taken from the vicinities of all probability modes of the formulated posterior. The new methodologies are tested using a simple one-dimensional example, and a quasi-geostrophic (QG) model with double-gyre wind forcing and bi-harmonic friction. Numerical results demonstrate the usefulness of using GMMs to relax the Gaussian prior assumption especially in the HMC filtering paradigm. © 2018 by the authors."
,10.1145/3194554.3194577,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049476449&doi=10.1145%2f3194554.3194577&partnerID=40&md5=9547e3533bc3673766ceda3f078ce6f1,"The paper presents ""MC3A""- Markov Chain Monte Carlo ManyCore Accelerator, a high-throughput, domain-specific, programmable manycore accelerator, which effectively generates samples from a provided target distribution. MCMC samplers are used in machine learning, image and signal processing applications that are computationally intensive. In such scenarios, high-throughput samplers are of paramount importance. To achieve a high-throughput platform, we add two domain-specific instructions with dedicated hardware whose functions are extensively used in MCMC algorithms. These instructions bring down the number of clock cycles needed to implement the respective functions by 10× and 21×. A 64-cluster architecture of the MC3A is fully placed and routed in 65 nm, TSMC CMOS technology, where the VLSI layout of each cluster occupies an area of 0.577 mm2 while consuming a power of 247 mW running at 1 GHz clock frequency. Our proposed MC3A achieves 6× higher throughput than its equivalent predecessor (PENC) and consumes 4× lower energy per sample. Also, when compared to other off-the-shelf platforms, such as, Jetson TX1 and TX2 SoC, MC3A results in 195× and 191× higher throughput, and consumes 808× and 726× lower energy per sample generation, respectively. © 2018 Association for Computing Machinery."
,10.1109/ACCESS.2018.2842088,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047787956&doi=10.1109%2fACCESS.2018.2842088&partnerID=40&md5=c616bce0615fb7887c3ed5417d7e57da,"Transferring complex computing to the cloud server side leverages cloud-based intelligent service robots that are capable of highly complex computing tasks such as video analysis. In practical behavior surveillance applications, the captured videos from intelligent service robots are continuous. Action extraction from continuous unconstrained video is an important prerequisite for action analysis, such as action classification and recognition, abnormal event detection, and crowd emotion sensing. This paper proposes a novel approach for action extraction in continuous unconstrained video, which has three parts: spatial location estimation, temporal action path searching; and spatial-temporal action compensation. Spatial location estimation utilizes both human appearance and motion cues to obtain frame-level bounding boxes. Then, with the spatial action proposal results as a priori, the searching of temporal action paths is formulated as an optimal estimation problem by accounting for the missed detections and false alarms of the spatial location estimation. To solve the temporal action path searching problem, we propose the Markov Chain Monte Carlo algorithm and illustrate its convergence property. Extensive experiments on the challenging UCF-Sports and UCF-101 data sets show the effectiveness of our approach and obtain superior performance compared with the state-of-the-arts. © 2013 IEEE."
,10.1109/CSPA.2018.8368705,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048821948&doi=10.1109%2fCSPA.2018.8368705&partnerID=40&md5=1caa383c4f3384dad1ddf41d7ba31126,"Dynamical systems are a natural and convenient way to model the evolution of processes observed in practice. When uncertainty is considered and incorporated, these system become known as stochastic dynamical systems. Based on observations made from stochastic dynamical systems, we consider the issue of parameter learning, and a related state estimation problem. We develop a Markov Chain Monte Carlo (MCMC) algorithm, which is an iterative method, for parameter inference. Within the parameter learning steps, the MCMC algorithm requires to perform state estimation for which the target distribution is constructed by using the Ensemble Kalman filter (EnKF). The methodology is illustrated using two examples of nonlinear stochastic dynamical systems. © 2018 IEEE."
,10.1080/03610918.2017.1311915,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021409068&doi=10.1080%2f03610918.2017.1311915&partnerID=40&md5=89967c212fba360ff8f23b136afb4288,"In this paper, we develop a Bayesian estimation procedure for semiparametric models under shape constrains. The approach uses a hierarchical Bayes framework and characterizations of shape-constrained B-splines. We employ Markov chain Monte Carlo methods for model fitting, using a truncated normal distribution as the prior for the coefficients of basis functions to ensure the desired shape constraints. The small sample properties of the function estimators are provided via simulation and compared with existing methods. A real data analysis is conducted to illustrate the application of the proposed method. © 2018, © 2018 Taylor & Francis Group, LLC."
,10.1109/ACCESS.2018.2840536,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047803394&doi=10.1109%2fACCESS.2018.2840536&partnerID=40&md5=c56bf9fdfd38a8adca8ac21dbaea9897,"Accurate prior knowledge of future driving cycle is quite essential in many research and applications related to optimal control of the vehicle and transportation, especially for model predictive control-based energy management for hybrid electric vehicles. Therefore, an adaptive online prediction method with variable prediction horizon is proposed for future driving cycle prediction in this paper. In particular, two aspects of efforts have been explored. First, combining Markov chain and Monte Carlo theory, a multi-scale single-step prediction method is proposed and compared with traditional fixed-scale multi-step method, improving by about 7% in prediction accuracy. Second, to further adapt to variable actual driving cycles, online reconstructions of driving cycle and state filling are introduced to guarantee continuous and robust online application; principal component analysis and cluster analysis are employed to adjust real-time prediction horizons for better overall prediction accuracy. In the end, the proposed method is verified by the experiment of hardware-in-loop simulation, showing more than 20% improvement in prediction accuracy than fixed-horizon prediction method, and relatively good robustness and universality in different driving conditions. © 2013 IEEE."
,10.1080/03610918.2017.1317804,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021836002&doi=10.1080%2f03610918.2017.1317804&partnerID=40&md5=8008b7935ba0363f2475e65a4150d6e4,"This article addresses the various properties and different methods of estimation of the unknown parameter of length and area-biased Maxwell distributions. Although, our main focus is on estimation from both frequentist and Bayesian point of view, yet, various mathematical and statistical properties of length and area-biased Maxwell distributions (such as moments, moment-generating function (mgf), hazard rate function, mean residual lifetime function, residual lifetime function, reversed residual life function, conditional moments and conditional mgf, stochastic ordering, and measures of uncertainty) are derived. We briefly describe different frequentist approaches, namely, maximum likelihood estimator, moments estimator, least-square and weighted least-square estimators, maximum product of spacings estimator and compare them using extensive numerical simulations. Next we consider Bayes estimation under different types of loss function (symmetric and asymmetric loss functions) using inverted gamma prior for the scale parameter. Furthermore, Bayes estimators and their respective posterior risks are computed and compared using Markov chain Monte Carlo (MCMC) algorithm. Also, bootstrap confidence intervals using frequentist approaches are provided to compare with Bayes credible intervals. Finally, a real dataset has been analyzed for illustrative purposes. © 2018, © 2018 Taylor &amp; Francis Group, LLC."
,10.1016/j.jsv.2018.02.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044306993&doi=10.1016%2fj.jsv.2018.02.012&partnerID=40&md5=1fe3febd60782b76bbb52f6de737ca3e,"A novel approach is presented in this work to localize simultaneously multiple damaged elements in a structure along with the estimation of damage severity for each of the damaged elements. For detection of damaged elements, a best achievable eigenvector based formulation has been derived. To deal with noisy data, Bayesian inference is employed in the formulation wherein the likelihood of the Bayesian algorithm is formed on the basis of errors between the best achievable eigenvectors and the measured modes. In this approach, the most probable damage locations are evaluated under Bayesian inference by generating combinations of various possible damaged elements. Once damage locations are identified, damage severities are estimated using a Bayesian inference Markov chain Monte Carlo simulation. The efficiency of the proposed approach has been demonstrated by carrying out a numerical study involving a 12-story shear building. It has been found from this study that damage scenarios involving as low as 10% loss of stiffness in multiple elements are accurately determined (localized and severities quantified) even when 2% noise contaminated modal data are utilized. Further, this study introduces a term parameter impact (evaluated based on sensitivity of modal parameters towards structural parameters) to decide the suitability of selecting a particular mode, if some idea about the damaged elements are available. It has been demonstrated here that the accuracy and efficiency of the Bayesian quantification algorithm increases if damage localization is carried out a-priori. An experimental study involving a laboratory scale shear building and different stiffness modification scenarios shows that the proposed approach is efficient enough to localize the stories with stiffness modification. © 2018 Elsevier Ltd"
,10.1109/3DV.2017.00043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048769435&doi=10.1109%2f3DV.2017.00043&partnerID=40&md5=80ab2c0a825045b789b8fc635ddce4f0,"We address the problem of generating variations of captured 4D models automatically, and we particularly focus on dynamic human shapes as observed from multi-view videos. Variation is an essential component of motion realism, however recent mesh animation datasets and tools lack such richness. Given a few 4D models representing movements of the same type, our method builds a probabilistic low dimensional embedding of shape poses using Gaussian Process Dynamical Models, and novel variants of motions are obtained by sampling trajectories from this manifold using Monte Carlo Markov Chain. We can synthesise an unlimited number of variations of any of the input movements, and also any blended version of them, without costly non-linear interpolation of input movement variations in mesh domain. The output variations are statistically similar to the input movements but yet slightly different in poses and timings. As we show through our results, the generated mesh sequences match the training examples in realism, which facilitates 4D model dataset augmentation. © 2017 IEEE."
,10.16339/j.cnki.hdxbzkb.2018.05.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052913846&doi=10.16339%2fj.cnki.hdxbzkb.2018.05.005&partnerID=40&md5=3c696b1566b004a50ae35835c285fb80,"The issue related to multi-model structural identification (MM St-Id) was experimentally researched based on sampling method of Bayesian theory. The concept and basic framework of MM St-Id method based on Bayesian theory were introduced, and then, the Markov chain - Monte Carlo simulation (MCMC) was utilized to build finite element (FE) model libraries. Since MCMC is not easy to converge and it has low calculation efficiency when the parameters have high dimensions, an improved MCMC sampling method for MM St-Id was introduced. The Matlab-Strand7 Application Programming Interface (API) strategy can be used to update the parameters of large structural FE model automatically. After the calibrated FE model libraries were established, they can be used to predict the responses based on the posterior probability distribution of the FE models. In order to verify the feasibility and effectiveness of the proposed theory, a numerical example of a simply-supported beam and an on-site large concrete-steel tubular truss arch bridge St-Id were investigated based on Bayesian theory and response prediction. A simple model St-Id method -genetic algorithm (GA) was used for comparison. The results showed that the proposed MM St-Id method based on Bayesian theory was much better in structural response prediction. © 2018, Editorial Department of Journal of Hunan University. All right reserved."
,10.1109/ISPA/IUCC.2017.00188,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048376574&doi=10.1109%2fISPA%2fIUCC.2017.00188&partnerID=40&md5=eb2d39509b79aead793b840806903be3,"MrBayes is a popular bioinformatics software that is widely used in phylogenetic analysis. The core algorithm of Mrbayes is Metropolis Coupled Markov Chain Monte Carlo (MC3). However, when dealing with large data sets, MC3 algorithm is too slow to meet researcher's requirements. Although several parallelizations have been proposed for MrBayes, such as MPI (Message Passing Interface) based MrBayes, GPU (Graphics Processing Unit) based MrBayes, there is still no efficient parallel algorithm to fully utilize computing power of modern CPU and computer architecture. This paper (a) presents a new three-level hybrid parallel algorithm, include data-level parallelism (DLP), thread-level parallelism (TLP), and process-level parallelism (PLP), which can be used on most modern multi-core computers; (b) compares the performance of different combinations of parallel strategies on real-world protein data sets. The experimental results show that, this hybrid parallel algorithm does convert more computing powers into higher speedup. Furthermore, the proposed algorithm's speedup is near the speedup on one GPU at the same data sets. This algorithm is fit for practical use in phylogenetic inferences. © 2017 IEEE."
,10.1080/00949655.2018.1442469,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042914584&doi=10.1080%2f00949655.2018.1442469&partnerID=40&md5=1fd31067546aa07363de1a066c64530f,"Combining the multivariate probit models with the multivariate partially linear single-index models, we propose new semiparametric latent variable models for multivariate ordinal response data. Based on the reversible jump Markov chain Monte Carlo technique, we develop a fully Bayesian method with free-knot splines to analyse the proposed models. To address the problem that the ordinary Gibbs sampler usually converges slowly, we make use of the partial-collapse and parameter-expansion techniques in our algorithm. The proposed methodology are demonstrated by simulated and real data examples. © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1109/EDUCON.2018.8363352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048078626&doi=10.1109%2fEDUCON.2018.8363352&partnerID=40&md5=5f8a4c37e3a84ababfddd8b4f568d55d,"Estimating grades for courses that are yet to be enrolled by students can help them in making decisions towards timely graduation and achieving better overall results. This paper presents an evaluation of grade prediction for future courses using the model-based collaborative filtering methods: Probabilistic Matrix Factorization and Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo. The prediction model was evaluated in a simulated scenario of an enrollment cycle in a winter and summer semester, based on a real data-set of enrollments and grades over several years at the authors' institution. Several evaluation metrics were used in order to assess the accuracy of predictions and analyze the distribution of the prediction deviation across study programs and grades. Beside the standard approach in predicting the final grade that is to be achieved by a student in a future course, we have also devised a method to estimate if the student will fail the course, so that he will have to re-enroll it at least once. The results showed that the predicted grades were in the range ±1 compared to the actual grades in more than 80% of the records. © 2018 IEEE."
,10.1109/ATSIP.2018.8364492,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048493696&doi=10.1109%2fATSIP.2018.8364492&partnerID=40&md5=95073f0994977364113830ca1f016d2f,"In this paper, we study the estimation of olive tree biophysical properties driven by Sentinel-2 (S2) image inversion. The latter is based on the forward/backward radiative transfer model (RTM). The forward step is done simulating the DART model on a realistic olive tree mock-up, whereas the backward is done based on a coupling between the Look UP Table (LUT) and the Markov Chain Monte Carlo (MCMC). The parameters Leaf area index (LAI), chlorophyll (Cab) water (Cw) contents and mesophyll structure (N) are therefore derived. Soil reflectance is pre-calculated based on an upscaling of the S2 resolution to 3m using Planet images. Moreover to obtain a significant representation of the local heterogeneity, S2 are upscaled to the 80m resolution. The estimation results are promising. © 2018 IEEE."
,10.1007/s00500-018-3244-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047275326&doi=10.1007%2fs00500-018-3244-4&partnerID=40&md5=e257aac58471aa5b4334f999b689200d,"The goal of constructing models from examples has been approached from different perspectives. Statistical methods have been widely used and proved effective in generating accurate models. Finite Gaussian mixture models have been widely used to describe a wide variety of random phenomena and have played a prominent role in many attempts to develop expressive statistical models in machine learning. However, their effectiveness is limited to applications where underlying modeling assumptions (e.g., the per-components densities are Gaussian) are reasonably satisfied. Thus, much research efforts have been devoted to developing better alternatives. In this paper, we focus on constructing statistical models from positive vectors (i.e., vectors whose elements are strictly greater than zero) for which the generalized inverted Dirichlet (GID) mixture has been shown to be a flexible and powerful parametric framework. In particular, we propose a Bayesian density estimation method based upon mixtures of GIDs. The consideration of Bayesian learning is interesting in several respects. It allows to take uncertainty into account by introducing prior information about the parameters, it allows simultaneous parameters estimation and model selection, and it allows to overcome learning problems related to over- or under-fitting. Indeed, we develop a reversible jump Markov Chain Monte Carlo sampler for GID mixtures that we apply for simultaneous clustering and feature selection in the context of some challenging real-world applications concerning scene classification, action recognition, and video forgery detection. © 2018 Springer-Verlag GmbH Germany, part of Springer Nature"
,10.1109/TCYB.2018.2831792,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047221211&doi=10.1109%2fTCYB.2018.2831792&partnerID=40&md5=b91f7b87b21ca91595a73931f1530403,"In this paper, a unified Bayesian max-margin discriminant projection framework is proposed, which is able to jointly learn the discriminant feature space and the max-margin classifier with different relationships between the latent representations and observations. We assume that the latent representation follows a normal distribution whose sufficient statistics are functions of the observations. The function can be flexibly realized through either shallow or deep structures. The shallow structure includes linear, nonlinear kernel-based functions, and even the convolutional projection, which can be further trained layerwisely to build a multilayered convolutional feature learning model. To take the advantage of the deep neural networks, especially their highly expressive ability and efficient parameter learning, we integrate Bayesian modeling and the popular neural networks, for example, mltilayer perceptron and convolutional neural network, to build an end-to-end Bayesian deep discriminant projection under the proposed framework, which degenerated into the existing shallow linear or convolutional projection with the single-layer structure. Moreover, efficient scalable inferences for the realizations with different functions are derived to handle large-scale data via a stochastic gradient Markov chain Monte Carlo. Finally, we demonstrate the effectiveness and efficiency of the proposed models by the experiments on real-world data, including four image benchmarks (MNIST, CIFAR-10, STL-10, and SVHN) and one measured radar high-resolution range profile dataset, with the detailed analysis about the parameters and computational complexity. IEEE"
,10.1080/02664763.2017.1367368,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028554674&doi=10.1080%2f02664763.2017.1367368&partnerID=40&md5=542073202bb5adb51ba6975eed392612,"In the course of hypertension, cardiovascular disease events (e.g. stroke, heart failure) occur frequently and recurrently. The scientific interest in such study may lie in the estimation of treatment effect while accounting for the correlation among event times. The correlation among recurrent event times comes from two sources: subject-specific heterogeneity (e.g. varied lifestyles, genetic variations, and other unmeasurable effects) and event dependence (i.e. event incidences may change the risk of future recurrent events). Moreover, event incidences may change the disease progression so that there may exist event-varying covariate effects (the covariate effects may change after each event) and event effect (the effect of prior events on the future events). In this article, we propose a Bayesian regression model that not only accommodates correlation among recurrent events from both sources, but also explicitly characterizes the event-varying covariate effects and event effect. This model is especially useful in quantifying how the incidences of events change the effects of covariates and risk of future events. We compare the proposed model with several commonly used recurrent event models and apply our model to the motivating lipid-lowering trial (LLT) component of the Antihypertensive and Lipid-Lowering Treatment to Prevent Heart Attack Trial (ALLHAT) (ALLHAT-LLT). © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.3389/fphar.2018.00508,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047120193&doi=10.3389%2ffphar.2018.00508&partnerID=40&md5=c1ec565aa993ae42aca47fba0c15b476,"A computational workflow was developed to facilitate the process of quantitative in vitro to in vivo extrapolation (QIVIVE), specifically the translation of in vitro concentration-response to in vivo dose-response relationships and subsequent derivation of a benchmark dose value (BMD). The workflow integrates physiologically based pharmacokinetic (PBPK) modeling; global sensitivity analysis (GSA), Approximate Bayesian Computation (ABC) and Markov Chain Monte Carlo (MCMC) simulation. For a given set of in vitro concentration and response data the algorithm returns the posterior distribution of the corresponding in vivo, population-based dose-response values, for a given route of exposure. The novel aspect of the workflow is a rigorous statistical framework for accommodating uncertainty in both the parameters of the PBPK model (both parameter uncertainty and population variability) and in the structure of the PBPK model itself recognizing that the model is an approximation to reality. Both these sources of uncertainty propagate through the workflow and are quantified within the posterior distribution of in vivo dose for a fixed representative in vitro concentration. To demonstrate this process and for comparative purposes a similar exercise to previously published work describing the kinetics of ethylene glycol monoethyl ether (EGME) and its embryotoxic metabolite methoxyacetic acid (MAA) in rats was undertaken. The computational algorithm can be used to extrapolate from in vitro data to any organism, including human. Ultimately, this process will be incorporated into a user-friendly, freely available modeling platform, currently under development, that will simplify the process of QIVIVE. © 2018 McNally, Hogg and Loizou."
1,10.1016/j.fuel.2018.02.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041840308&doi=10.1016%2fj.fuel.2018.02.018&partnerID=40&md5=6829289d33127400a905b5d0ece6770a,"Embedding complex discrete fractures in reservoir simulation is required to attain more realistic flow behavior of shale reservoirs. However, using local grid refinement to model discrete fractures is computationally expensive. Nevertheless, recent developments in a methodology called Embedded Discrete Fracture Model (EDFM) have overcome the computational complexity. In this study, we develop an efficient assisted history matching (AHM) workflow using proxy-based Markov chain Monte Carlo algorithm and integrating with the EDFM preprocessor. The workflow can automatically perform history matching, production forecasting and uncertainty quantification. It has been successfully applied on a shale oil well in Vaca Muerta formation. © 2018 Elsevier Ltd"
,10.1016/j.neuroimage.2018.01.047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043311755&doi=10.1016%2fj.neuroimage.2018.01.047&partnerID=40&md5=4e1347b65956b04e4837a14e8c50998f,"We introduce a new approach to Bayesian pRF model estimation using Markov Chain Monte Carlo (MCMC) sampling for simultaneous estimation of pRF and hemodynamic parameters. To obtain high performance on commonly accessible hardware we present a novel heuristic consisting of interpolation between precomputed responses for predetermined stimuli and a large cross-section of receptive field parameters. We investigate the validity of the proposed approach with respect to MCMC convergence, tuning and biases. We compare different combinations of pRF - Compressive Spatial Summation (CSS), Dumoulin-Wandell (DW) and hemodynamic (5-parameter and 3-parameter Balloon-Windkessel) models within our framework with and without the usage of the new heuristic. We evaluate estimation consistency and log probability across models. We perform as well a comparison of one model with and without lookup table within the RStan framework using its No-U-Turn Sampler. We present accelerated computation of whole-ROI parameters for one subject. Finally, we discuss risks and limitations associated with the usage of the new heuristic as well as the means of resolving them. We found that the new algorithm is a valid sampling approach to joint pRF/hemodynamic parameter estimation and that it exhibits very high performance. © 2018"
,10.1088/1742-6596/1022/1/012002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048137361&doi=10.1088%2f1742-6596%2f1022%2f1%2f012002&partnerID=40&md5=d23102cbd29bc6b01723dee0e7d5f7ca,"Bayesian method is a method that can be used to estimate the parameters of multivariate multiple regression model. Bayesian method has two distributions, there are prior and posterior distributions. Posterior distribution is influenced by the selection of prior distribution. Jeffreys' prior distribution is a kind of Non-informative prior distribution. This prior is used when the information about parameter not available. Non-informative Jeffreys' prior distribution is combined with the sample information resulting the posterior distribution. Posterior distribution is used to estimate the parameter. The purposes of this research is to estimate the parameters of multivariate regression model using Bayesian method with Non-informative Jeffreys' prior distribution. Based on the results and discussion, parameter estimation of β and Σ which were obtained from expected value of random variable of marginal posterior distribution function. The marginal posterior distributions for β and Σ are multivariate normal and inverse Wishart. However, in calculation of the expected value involving integral of a function which difficult to determine the value. Therefore, approach is needed by generating of random samples according to the posterior distribution characteristics of each parameter using Markov chain Monte Carlo (MCMC) Gibbs sampling algorithm. © Published under licence by IOP Publishing Ltd."
2,10.1103/PhysRevD.97.103020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048087811&doi=10.1103%2fPhysRevD.97.103020&partnerID=40&md5=712d5d32627c30249d0a5b49878286f5,"Leveraging Markov chain Monte Carlo optimization of the F statistic, we introduce a method for the hierarchical follow-up of continuous gravitational wave candidates identified by wide-parameter space semicoherent searches. We demonstrate parameter estimation for continuous wave sources and develop a framework and tools to understand and control the effective size of the parameter space, critical to the success of the method. Monte Carlo tests of simulated signals in noise demonstrate that this method is close to the theoretical optimal performance. © 2018 American Physical Society."
,10.1145/3200921.3200934,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048424778&doi=10.1145%2f3200921.3200934&partnerID=40&md5=298a5da2f2a931d6f46f18fcc5edbf43,"Markov models have a long tradition in modeling and simulation of dynamic systems. In this paper, we look at certain properties of a discrete time Markov chain including entropy, trace and 2nd largest eigenvalue to better understand their role for time series analysis. We simulate a number of possible input signals, fit a discrete time Markov chain and explore properties with the help of Sobol indices. This research is motivated by recent results in the analysis of cell development for Xenopus laevis in cell biology that relied on the considered entropy measure to distinguish development stages from time series data of Calcium levels in cells. © 2018 Copyright held by the owner/author(s)."
,10.1002/sim.7613,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042193224&doi=10.1002%2fsim.7613&partnerID=40&md5=2bf6e4a7607a499b68c7d8cc2a5b465d,"Juvenile dermatomyositis (JDM) is a rare autoimmune disease that may lead to serious complications, even to death. We develop a 2-state Markov regression model in a Bayesian framework to characterise disease progression in JDM over time and gain a better understanding of the factors influencing disease risk. The transition probabilities between disease and remission state (and vice versa) are a function of time-homogeneous and time-varying covariates. These latter types of covariates are introduced in the model through a latent health state function, which describes patient-specific health over time and accounts for variability among patients. We assume a nonparametric prior based on the Dirichlet process to model the health state function and the baseline transition intensities between disease and remission state and vice versa. The Dirichlet process induces a clustering of the patients in homogeneous risk groups. To highlight clinical variables that most affect the transition probabilities, we perform variable selection using spike and slab prior distributions. Posterior inference is performed through Markov chain Monte Carlo methods. Data were made available from the UK JDM Cohort and Biomarker Study and Repository, hosted at the UCL Institute of Child Health. Copyright © 2018 John Wiley & Sons, Ltd."
,10.3390/w10050621,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046880180&doi=10.3390%2fw10050621&partnerID=40&md5=3c2f83bae86f57ae23717dcca93c2ca3,"The widely used, partly-deterministic Soil andWater Assessment Tool (SWAT) requires a large amount of spatial input data, such as a digital elevation model (DEM), land use, and soil maps. Modelers make an effort to apply the most specific data possible for the study area to reflect the heterogeneous characteristics of landscapes. Regional data, especially with fine resolution, is often preferred. However, such data is not always available and can be computationally demanding. Despite being coarser, global data are usually free and available to the public. Previous studies revealed the importance for single investigations of different input maps. However, it remains unknown whether higher-resolution data can lead to reliable results. This study investigates how global and regional input datasets affect parameter uncertainty when estimating river discharges. We analyze eight different setups for the SWAT model for a catchment in Luxembourg, combining different land-use, elevation, and soil input data. The Metropolis-Hasting Markov Chain Monte Carlo (MCMC) algorithm is used to infer posterior model parameter uncertainty. We conclude that our higher resolved DEM improves the general model performance in reproducing low flows by 10%. The less detailed soil-map improved the fit of low flows by 25%. In addition, more detailed land-use maps reduce the bias of the model discharge simulations by 50%. Also, despite presenting similar parameter uncertainty (P-factor ranging from 0.34 to 0.41 and R-factor from 0.41 to 0.45) for all setups, the results show a disparate parameter posterior distribution. This indicates that no assessment of all sources of uncertainty simultaneously is compensated by the fitted parameter values. We conclude that our result can give some guidance for future SWAT applications in the selection of the degree of detail for input data. © 2018 by the authors."
,10.1002/sim.7612,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041606986&doi=10.1002%2fsim.7612&partnerID=40&md5=292e9a062c4865685cf0a9f93ee31a1e,"Researchers collected multiple measurements on patients with schizophrenia and their relatives, as well as control subjects and their relatives, to study vulnerability factors for schizophrenics and their near relatives. Observations across individuals from the same family are correlated, and also the multiple outcome measures on the same individuals are correlated. Traditional data analyses model outcomes separately and thus do not provide information about the interrelationships among outcomes. We propose a novel Bayesian family factor model (BFFM), which extends the classical confirmatory factor analysis model to explain the correlations among observed variables using a combination of family-member and outcome factors. Traditional methods for fitting confirmatory factor analysis models, such as full-information maximum likelihood (FIML) estimation using quasi-Newton optimization (QNO), can have convergence problems and Heywood cases (lack of convergence) caused by empirical underidentification. In contrast, modern Bayesian Markov chain Monte Carlo handles these inference problems easily. Simulations compare the BFFM to FIML-QNO in settings where the true covariance matrix is identified, close to not identified, and not identified. For these settings, FIML-QNO fails to fit the data in 13%, 57%, and 85% of the cases, respectively, while MCMC provides stable estimates. When both methods successfully fit the data, estimates from the BFFM have smaller variances and comparable mean-squared errors. We illustrate the BFFM by analyzing data on data from schizophrenics and their family members. Copyright © 2018 John Wiley & Sons, Ltd."
,10.1088/1681-7575/aabd57,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048238027&doi=10.1088%2f1681-7575%2faabd57&partnerID=40&md5=b7129b7adc9dc3a95d569c4f605b8619,"A Bayesian statistical procedure is proposed for value assignment and uncertainty evaluation for the mass fraction of the elemental analytes in single-element solutions distributed as NIST standard reference materials. The principal novelty that we describe is the use of information about relative differences observed historically between the measured values obtained via gravimetry and via high-performance inductively coupled plasma optical emission spectrometry, to quantify the uncertainty component attributable to between-method differences. This information is encapsulated in a prior probability distribution for the between-method uncertainty component, and it is then used, together with the information provided by current measurement data, to produce a probability distribution for the value of the measurand from which an estimate and evaluation of uncertainty are extracted using established statistical procedures. © 2018 Not subject to copyright in the USA. Contribution of NIST."
,10.1021/acs.biochem.7b01264,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046621838&doi=10.1021%2facs.biochem.7b01264&partnerID=40&md5=a5a82f388f8fc6fa14ae9ff5735ef209,"Here we describe pytc, an open-source Python package for global fits of thermodynamic models to multiple isothermal titration calorimetry experiments. Key features include simplicity, the ability to implement new thermodynamic models, a robust maximum likelihood fitter, a fast Bayesian Markov-Chain Monte Carlo sampler, rigorous implementation, extensive documentation, and full cross-platform compatibility. pytc fitting can be done using an application program interface or via a graphical user interface. It is available for download at https://github.com/harmslab/pytc. © Copyright 2018 American Chemical Society."
6,10.1080/10705511.2017.1406803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039160037&doi=10.1080%2f10705511.2017.1406803&partnerID=40&md5=997d7e51baf0f3745064561ad13adec9,"This article presents dynamic structural equation modeling (DSEM), which can be used to study the evolution of observed and latent variables as well as the structural equation models over time. DSEM is suitable for analyzing intensive longitudinal data where observations from multiple individuals are collected at many points in time. The modeling framework encompasses previously published DSEM models and is a comprehensive attempt to combine time-series modeling with structural equation modeling. DSEM is estimated with Bayesian methods using the Markov chain Monte Carlo Gibbs sampler and the Metropolis–Hastings sampler. We provide a detailed description of the estimation algorithm as implemented in the Mplus software package. DSEM can be used for longitudinal analysis of any duration and with any number of observations across time. Simulation studies are used to illustrate the framework and study the performance of the estimation method. Methods for evaluating model fit are also discussed. Copyright © Taylor & Francis Group, LLC."
1,10.1080/00273171.2018.1428892,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041819812&doi=10.1080%2f00273171.2018.1428892&partnerID=40&md5=e454fd175be268bbc3214105597c7be9,"This article is a how-to guide on Bayesian computation using Gibbs sampling, demonstrated in the context of Latent Class Analysis (LCA). It is written for students in quantitative psychology or related fields who have a working knowledge of Bayes Theorem and conditional probability and have experience in writing computer programs in the statistical language R. The overall goals are to provide an accessible and self-contained tutorial, along with a practical computation tool. We begin with how Bayesian computation is typically described in academic articles. Technical difficulties are addressed by a hypothetical, worked-out example. We show how Bayesian computation can be broken down into a series of simpler calculations, which can then be assembled together to complete a computationally more complex model. The details are described much more explicitly than what is typically available in elementary introductions to Bayesian modeling so that readers are not overwhelmed by the mathematics. Moreover, the provided computer program shows how Bayesian LCA can be implemented with relative ease. The computer program is then applied in a large, real-world data set and explained line-by-line. We outline the general steps in how to extend these considerations to other methodological applications. We conclude with suggestions for further readings. © 2018 Taylor & Francis Group, LLC."
,10.1109/ICCChinaW.2017.8355278,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049621172&doi=10.1109%2fICCChinaW.2017.8355278&partnerID=40&md5=6354152a8039ba84cb1fdd8643b7949d,"One very popular IoT system is characterized by the very bursty traffic in which the transmission occurrence can be as lower as just 200 milliseconds in multiple hours. The traditional Monte Carlo behavioral simulation is very time-consuming for a usual IoT system where the traffic burst is significantly smaller than the hibernation time and the number of terminals is big. Alternatives are analytical approaches using random probability and stochastic process theory, and there have been numerous papers on Slotted ALOHA (S-ALOHA) system. Unfortunately almost all the results obtained so far are based on full buffer traffic model so there is not an efficient approach to evaluate the system performance of an IoT system supporting massive number of terminals with very bursty traffic patterns. Two analytic methodologies are proposed and studied in this paper for very bursty traffic pattern in a contention based access system employing S-ALOHA access with Binary Exponential Back-off (BEB), and a set of closed-form formulae are derived. The first method is based on the probability modeling and the second method is a Markov chain based 2-D analytical model. The fundamental parameter in both methods is the expression for packet transmission probability, and the close-form expressions are derived for both methods. It was verified that the two expressions agree with each other very well. Based on the packet transmission probability, the close form expressions for other system performance indicators are also derived, which includes packet loss rate, transmission time, transmission delay, re-transmission times, collision possibility, and channel utilization efficiency. The proposed methods were verified to match real system performance very well, and they can be the efficient and accurate analytical tools for IoT system performance evaluation and optimization supporting very bursty traffic. © 2017 IEEE."
,10.1088/1361-6420/aabce7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047256837&doi=10.1088%2f1361-6420%2faabce7&partnerID=40&md5=63a88b5e2872a798e706e450443e77e0,"In this study, we rely on a Bayesian approach to estimate the seismic velocity from first arrival travel times. The advantage of the Bayesian approach compared to linearized ones is its ability to properly quantify the uncertainties associated with the solution. However, this approach remains fairly expensive, and the Markov chain-Monte Carlo algorithms that are used to sample the posterior distribution are efficient only when the number of parameters remains within reason. Therefore, a first step toward an efficient implementation of the Bayesian approach is to properly parameterize the model to reduce its dimensionality. In this article, we introduce new parsimonious parameterizations which enable us to accurately reproduce the wave velocity field and the associated uncertainties. The first parametric model that we propose uses a random Johnson-Mehl tessellation, a generalization of the Voronoi tessellation. The main difference of the Johnson-Mehl model when compared to the Voronoi model is that the shapes of the generated cells are much more general. The cells of a Voronoi tessellation are indeed convex polytopes, while the Johnson-Mehl tessellation model yields cells whose boundaries are portions of hyperboles and which are not necessarily convex, hence allowing for a greater variety of shapes. We demonstrate the gain in efficiency and the better convergence when compared to the Voronoi model. The second parameterization uses Gaussian kernels as basis functions. Its purpose is to provide a way to reproduce localized variations in the seismic velocity field. We first illustrate the tomography results with a synthetic velocity model which contains two small anomalies. We then apply our methodology to a more advanced and realistic synthetic model that serves as a benchmark in the oil industry. We finally present an example where Gaussian kernels outperform Voronoi and Johnson-Mehl models. The tomography results reveal the ability of our algorithm to map the velocity heterogeneities accurately using few parameters. © 2018 IOP Publishing Ltd."
,10.1080/02331888.2018.1435661,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041902812&doi=10.1080%2f02331888.2018.1435661&partnerID=40&md5=bac6da770fe11b7a151ae843600ba1fe,"In an attempt to produce more realistic stress–strength models, this article considers the estimation of stress–strength reliability in a multi-component system with non-identical component strengths based on upper record values from the family of Kumaraswamy generalized distributions. The maximum likelihood estimator of the reliability, its asymptotic distribution and asymptotic confidence intervals are constructed. Bayes estimates under symmetric squared error loss function using conjugate prior distributions are computed and corresponding highest probability density credible intervals are also constructed. In Bayesian estimation, Lindley approximation and the Markov Chain Monte Carlo method are employed due to lack of explicit forms. For the first time using records, the uniformly minimum variance unbiased estimator and the closed form of Bayes estimator using conjugate and non-informative priors are derived for a common and known shape parameter of the stress and strength variates distributions. Comparisons of the performance of the estimators are carried out using Monte Carlo simulations, the mean squared error, bias and coverage probabilities. Finally, a demonstration is presented on how the proposed model may be utilized in materials science and engineering with the analysis of high-strength steel fatigue life data. © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1109/AsianHOST.2017.8353992,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050941131&doi=10.1109%2fAsianHOST.2017.8353992&partnerID=40&md5=d11e6c31f1baedef38d3e2ea18456340,"True random number generators (TRNGs) are pivotal in cryptography, Markov Chain Monte Carlo analysis, neural network simulation, industrial testing, gambling, etc., where deterministic pseudo-random number sequences are inadequate to produce satisfactory results. The demand for fast low-power TRNG is growing as such sophisticated applications are increasingly moving into mobile. This paper presents an energy-efficient on-chip TRNG design. Its random digital bits are extracted from the jitter noise of two free running current starved ring oscillators (ROs). The current starved ROs exhibit a larger jitter noise than the regular inverter based ROs because the jitter is boosted by lowering the oscillation frequency and the drain current of the transistors in the ROs. In addition, the jitter source ROs, which are the most power-hungry components of conventional oscillator based TRNGs, are biased in the subthreshold region in our proposed design to reduce their power consumption. Simulation results based on 65nm 1.2V CMOS technology show that the proposed TRNG consumes only 123 μW at a throughput rate of 96 Mbps. It outperforms the state-of-art on-chip TRNGs with a figure-of-merit of 1.28 pJ/bit. Its generated bit sequence passes all the fifteen randomness tests of the National Institute of Standards and Technology (NIST) statistical test suite. © 2017 IEEE."
1,10.1061/(ASCE)HE.1943-5584.0001646,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043505631&doi=10.1061%2f%28ASCE%29HE.1943-5584.0001646&partnerID=40&md5=0fb1e0ce19d57759650463f22a97d0aa,"It is imperative for cities to develop sustainable water management and planning strategies in order to best serve urban communities that are currently facing increasing population and water demand. Water resources managers are often chastened by experiencing failures attributed to natural extreme droughts and floods. However, recent changes in water management systems have been responding to these uncertain conditions.Water managers have become thoughtful about the adverse effects of uncertain extreme events on the performance of water supply systems. Natural hydrologic variability and inherent uncertainties associated with the future climate variations make the simulation and management of water supplies a greater challenge. The hydrologic simulation process is one of the main components in integrated water resources management. Hydrologic simulations incorporate uncertain input values, model parameters, and a model structure. Therefore, stochastic streamflow simulation and prediction, and consideration of uncertainty propagation on performance of water supply systems (WSSs) are essential phases for efficient management of these systems. The proposed integrated framework in this study models a WSS by taking into account the dynamic nature of the system and utilizing a Markov chain Monte Carlo (MCMC) algorithm to capture the uncertainties associated with hydrologic simulation. Hydrologic responses from the results of a rainfall-runoff model for three watersheds of Karaj, Latyan, and Lar in Tehran, Iran, as the case study are used as inputs to the reservoirs. Results confirm that uncertainties associated with the hydrologic model's parameters propagate through the simulation and lead to a wide variation in reservoir storage and WSS performance metrics such as vulnerability and reliability. For example, water storage simulation in the Karaj Reservoir can vary up to 70% compared with the observed values. This causes contradiction and conflict in the management of reservoirs and water systems and decision making. The results emphasize the importance of analyzing WSS performance under uncertain conditions to improve the simulation of natural processes and support water managers for a more efficient decision-making process. © 2018 American Society of Civil Engineers."
1,10.1177/1475921717717106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042227761&doi=10.1177%2f1475921717717106&partnerID=40&md5=76e2aadeadd55292f7aafb5659e5d5bb,"This article reports the development of a Bayesian method for assessing the damage status of railway ballast under a concrete sleeper based on vibration data of the in situ sleeper. One of the important contributions of the proposed method is to describe the variation of stiffness distribution of ballast using Lagrange polynomial, for which the order of the polynomial is decided by the Bayesian approach. The probability of various orders of polynomial conditional on a given set of measured vibration data is calculated. The order of polynomial with the highest probability is selected as the most plausible order and used for updating the ballast stiffness distribution. Due to the uncertain nature of railway ballast, the corresponding model updating problem is usually unidentifiable. To ensure the applicability of the proposed method even in unidentifiable cases, a computational efficient Markov chain Monte Carlo–based Bayesian method was employed in the proposed method for generating a set of samples in the important region of parameter space to approximate the posterior (updated) probability density function of ballast stiffness. The proposed ballast damage detection method was verified with roving hammer test data from a segment of full-scale ballasted track. The experimental verification results positively show the potential of the proposed method in ballast damage detection. © 2017, © The Author(s) 2017."
,10.1007/s11222-017-9743-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016124939&doi=10.1007%2fs11222-017-9743-9&partnerID=40&md5=951c8c86163c5c03fa54cf4bf9813c7f,"A new Bayesian state and parameter learning algorithm for multiple target tracking models with image observations are proposed. Specifically, a Markov chain Monte Carlo algorithm is designed to sample from the posterior distribution of the unknown time-varying number of targets, their birth, death times and states as well as the model parameters, which constitutes the complete solution to the specific tracking problem we consider. The conventional approach is to pre-process the images to extract point observations and then perform tracking, i.e. infer the target trajectories. We model the image generation process directly to avoid any potential loss of information when extracting point observations using a pre-processing step that is decoupled from the inference algorithm. Numerical examples show that our algorithm has improved tracking performance over commonly used techniques, for both synthetic examples and real florescent microscopy data, especially in the case of dim targets with overlapping illuminated regions. © 2017, Springer Science+Business Media New York."
2,10.1190/geo2017-0387.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045062368&doi=10.1190%2fgeo2017-0387.1&partnerID=40&md5=c55caed807bab8f2efe3124aad144645,"We have evaluated a two-step Bayesian algorithm for seismic-reservoir characterization, which, thanks to some simplifying assumptions, is computationally very efficient. The applicability and reliability of this method are assessed by comparison with a more sophisticated and computer-intensive Markov-chain Monte Carlo (MCMC) algorithm, which in a single loop directly estimates petrophysical properties and lithofluid facies from prestack data. The two-step method first combines a linear rock-physics model (RPM) with the analytical solution of a linearized amplitude versus angle (AVA) inversion, to directly estimate the petrophysical properties, and related uncertainties, from prestack data under the assumptions of a Gaussian prior model and weak elastic contrasts at the reflecting interface. In particular, we use an empirical, linear RPM, properly calibrated for the investigated area, to reparameterize the linear time-continuous P-wave reflectivity equation in terms of petrophysical contrasts instead of elastic constants. In the second step, a downward 1D Markov-chain prior model is used to infer the lithofluid classes from the outcomes of the first step. The single-loop (SL) MCMC algorithm uses a convolutional forward modeling based on the exact Zoeppritz equations, and it adopts a nonlinear RPM. Moreover, it assumes a more realistic Gaussian mixture distribution for the petrophysical properties. Both approaches are applied on an onshore 3D seismic data set for the characterization of a gas-bearing, clastic reservoir. Notwithstanding the differences in the forward-model parameterization, in the considered RPM, and in the assumed a priori probability density functions, the two methods yield maximum a posteriori solutions that are consistent with well-log data, although the Gaussian mixture assumption adopted by the SL method slightly improves the description of the multimodal behavior of the petrophysical parameters. However, in the considered reservoir, the main difference between the two approaches remains the very different computational times, the SL method being much more computationally intensive than the two-step approach. © 2018 Society of Exploration Geophysicists."
3,10.3847/1538-4365/aab76e,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047256754&doi=10.3847%2f1538-4365%2faab76e&partnerID=40&md5=02fd8f949d6ad0b48ca2052fd49d77e3,"Markov Chain Monte Carlo (MCMC) methods for sampling probability density functions (combined with abundant computational resources) have transformed the sciences, especially in performing probabilistic inferences, or fitting models to data. In this primarily pedagogical contribution, we give a brief overview of the most basic MCMC method and some practical advice for the use of MCMC in real inference problems. We give advice on method choice, tuning for performance, methods for initialization, tests of convergence, troubleshooting, and use of the chain output to produce or report parameter estimates with associated uncertainties. We argue that autocorrelation time is the most important test for convergence, as it directly connects to the uncertainty on the sampling estimate of any quantity of interest. We emphasize that sampling is a method for doing integrals; this guides our thinking about how MCMC output is best used. © 2018. The American Astronomical Society.."
,10.1109/TPWRS.2017.2757980,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030776418&doi=10.1109%2fTPWRS.2017.2757980&partnerID=40&md5=0e3a9df3ca5960a2a6fe25e81a5a1bf9,"With increased competition in wholesale electricity markets, the need for new decision-making tools for strategic producers has arisen. Optimal bidding strategies have traditionally been modeled as stochastic profit maximization problems. However, for producers with non-negligible market power, modeling the interactions with rival participants is fundamental. This can be achieved through equilibrium and hierarchical optimization models. The efficiency of these methods relies on the strategic producer's ability to model rival participants' behavior and supply curve, but a substantial gap remains in the literature on modeling this uncertainty. In this study, we introduce a Bayesian inference approach to reveal the aggregate supply curve in a day-Ahead electricity market. The proposed algorithm relies on Markov Chain Monte Carlo and sequential Monte Carlo methods. The major appeal of this approach is that it provides a complete model of the uncertainty of the aggregate supply curve, through an estimate of its posterior distribution. We show on a small case study that we are able to reveal accurately the aggregate supply curve with no prior information on rival participants. Finally, we show how this piece of information can be used by a price-maker producer in order to devise an optimal bidding strategy. © 1969-2012 IEEE."
,10.1098/rspa.2017.0700,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047566297&doi=10.1098%2frspa.2017.0700&partnerID=40&md5=e70428a3f77c2e1d68a302364f6b6ef1,"The building of mathematical and computer models of cities has a long history. The core elements are models of flows (spatial interaction) and the dynamics of structural evolution. In this article, we develop a stochastic model of urban structure to formally account for uncertainty arising from less predictable events. Standard practice has been to calibrate the spatial interaction models independently and to explore the dynamics through simulation. We present two significant results that will be transformative for both elements. First, we represent the structural variables through a single potential function and develop stochastic differential equations to model the evolution. Second, we show that the parameters of the spatial interaction model can be estimated from the structure alone, independently of flow data, using the Bayesian inferential framework. The posterior distribution is doubly intractable and poses significant computational challenges that we overcome using Markov chain Monte Carlo methods. We demonstrate our methodology with a case study on the London, UK, retail system. © 2018 The Author(s) Published by the Royal Society. All Rights Reserved."
,10.1109/LSP.2018.2822550,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044725237&doi=10.1109%2fLSP.2018.2822550&partnerID=40&md5=ee2902bd7e322b92c2c5899010b0ea08,"Parallel factor analysis (PARAFAC) is one of the most popular tensor factorization models. Even though it has proven successful in diverse application fields, the performance of PARAFAC usually hinges up on the rank of the factorization, which is typically specified manually by the practitioner. In this study, we develop a novel parallel and distributed Bayesian model selection technique for rank estimation in large-scale PARAFAC models. The proposed approach integrates ideas from the emerging field of stochastic gradient Markov Chain Monte Carlo, statistical physics, and distributed stochastic optimization. As opposed to the existing methods, which are based on some heuristics, our method has a clear mathematical interpretation, and has significantly lower computational requirements, thanks to data subsampling and parallelization. We provide formal theoretical analysis on the bias induced by the proposed approach. Our experiments on synthetic and large-scale real datasets show that our method is able to find the optimal model order while being significantly faster than the state-of-the-art. © 1994-2012 IEEE."
,10.1109/TPS.2018.2795084,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041858477&doi=10.1109%2fTPS.2018.2795084&partnerID=40&md5=16891f22296e0f0c8b713514a975eb25,"The joint deformation contributes significantly to final end-effector displacement of a manipulator, especially when the manipulator is in the form a serial kinematics with long links. When the manipulators employed in the DEMO are under the heavy payload, the deformation of manipulator is inevitable and the magnitude is significant. In order to maneuver the large object through several via points with high positioning accuracy in the remote handling process of DEMO, the real-time computation of manipulator deformation has to be conducted, which is crucial to the DEMO adaptive position control system for the displacement compensation. Three computation-effective deformation modeling methods are proposed in this paper, which are parametric modeling method, nonparametric deterministic artificial neural network (ANN) modeling method, and nonparametric Bayesian ANN modeling method, respectively. A specific joint in a boom equipped in a telescopic articulated remote mast is taken as the study object in this paper. A nodal deformation in the joint is investigated by three modeling methods, respectively. The parametric deformation model is derived by using the structural mechanics, whose parameters are identified by using the Markov chain Monte Carlo (MCMC) method; the deformation model of deterministic ANN is trained by using the Levenberg-Marquardt method; and the deformation model of the Bayesian ANN is trained by using the MCMC method. The results show that the parametric model from the structural mechanics is linear and is incompetent in the deformation modeling when the nonlinearity presents; both the deterministic and Bayesian ANNs are capable of model the nodal deformation of joint. The performance of both the deterministic network and Bayesian network cannot rival for one another in the application scenario of this paper. The training of the Bayesian network can provide the criterions for estimation of possible ranges of the modeling outputs from its probabilistic distribution curves, and the judgment of proper size of network. © 1973-2012 IEEE."
,10.1029/2017GC007399,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047391965&doi=10.1029%2f2017GC007399&partnerID=40&md5=244adcde3c4c9adfb55e6b8d9458fdaa,"Seismic-wave velocities offer essential constraints on the temperature, thickness, and composition of the lithosphere of cratons. We invert broadband, Rayleigh-wave phase and Love-wave phase velocities measured across the Kaapvaal Craton and Limpopo Belt for depth distributions of shear-wave velocity and radial anisotropy, from the upper-crust down to deep upper mantle. Our probabilistic, Bayesian inversion addresses model nonuniqueness by means of direct parameter-space sampling. An increase in Vs between the Moho and 100–150 km depths occurs across the region and can be explained by the gradual emergence of garnet below 80 km, due to the spinel peridotite-garnet peridotite transformation and due to the exsolution of garnet from mantle orthopyroxene. Lateral variations in this Vs gradient can provide new information on lateral compositional variations. Cold cratonic lithosphere is manifest in very high shear velocities, up to 4.8 km/s. The depth extent of the shear-velocity anomaly and the inferred lithospheric thickness increase from ∼200 km beneath the central and southwestern Kaapvaal to ∼300 km beneath the Limpopo Belt. Curiously, surface elevation decreases monotonically with the increasing lithospheric thickness. The relationship between the lithospheric thickness and topography depends on the lithospheric composition and, with the crustal structure taken into account, our results imply that the bottom part of the Limpopo lithosphere (200–300 km) is weakly-to-moderately depleted (Mg# 89.7–90.8). Our results also show that the central-southwestern Kaapvaal lithosphere is thinner than it was (according to kimberlites) 100–200 m.y. ago. It may have been thinned by the same mantle plume that, initially, triggered the kimberlite eruptions. © 2018. American Geophysical Union. All Rights Reserved."
2,10.1016/j.trc.2018.02.019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043239879&doi=10.1016%2fj.trc.2018.02.019&partnerID=40&md5=7c737585140d0bebcee227c083626fe9,"Railway big data technologies are transforming the existing track inspection and maintenance policy deployed for railroads in North America. This paper develops a data-driven condition-based policy for the inspection and maintenance of track geometry. Both preventive maintenance and spot corrective maintenance are taken into account in the investigation of a 33-month inspection dataset that contains a variety of geometry measurements for every foot of track. First, this study separates the data based on the time interval of the inspection run, calculates the aggregate track quality index (TQI) for each track section, and predicts the track spot geo-defect occurrence probability using random forests. Then, a Markov chain is built to model aggregated track deterioration, and the spot geo-defects are modeled by a Bernoulli process. Finally, a Markov decision process (MDP) is developed for track maintenance decision making, and it is optimized by using a value iteration algorithm. Compared with the existing maintenance policy using Markov chain Monte Carlo (MCMC) simulation, the maintenance policy developed in this paper results in an approximately 10% savings in the total maintenance costs for every 1 mile of track. © 2018 Elsevier Ltd"
,10.3788/OPE.20182605.1201,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052688805&doi=10.3788%2fOPE.20182605.1201&partnerID=40&md5=82619df2ff9ef0c9905a65897ce24a83,"In order to realize the arbitrary shape object extraction from LiDAR point cloud data, a method based on irregular marked point process was proposed. Firstly, a random point process was defined on ground plan, in which random point positioned the object projection on the plan. Then the marks associating individual points were defined with a set of nodes to depict the shape of object on the ground plan. Assumed that the elevation values of ground points followed an independent and identical Gauss distribution, and that of objects were also characterized by Gauss distributions individually. According to the Bayesian inference, the object extraction model was obtained; The RJMCMC algorithm was designed to simulate the posterior distribution and estimate the parameters. Finally, the optimal target extraction model was obtained according to the maximum a posteriori. LiDAR point cloud data was extracted by using the proposed method. According to the experimental results, it can be seen that the detection accuracy of the algorithm is above 80%, the highest accuracy is 99.43%. In this paper, the traditional rule mark process is extended to irregular marking process, and it can be used to fit the geometry of arbitrary shape target effectively. Experimental results show that this method can effectively fit the arbitrary shape objects. © 2018, Science Press. All right reserved."
,10.14358/PERS.84.5.279,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047411307&doi=10.14358%2fPERS.84.5.279&partnerID=40&md5=486ee07fe8c31838440e2ed59eda8476,"The estimation of land cover fractions from remote sensing images is a frequently used indicator of the environmental quality. This paper focuses on the quantification of land cover fractions in an urban area of Berlin, Germany, using simulated hyperspectral EnMAP data with a spatial resolution of 30 m × 30 m. We use constrained sparse representation, where each pixel with unknown surface characteristics is expressed by a weighted linear combination of elementary spectra with known land cover class. We automatically determine the elementary spectra from image reference data using archetypal analysis by simplex volume maximization, and combine it with reversible jump Markov chain Monte Carlo method. In our experiments, the estimation of the automatically derived elementary spectra is compared to the estimation obtained by a manually designed spectral library by means of reconstruction error, mean absolute error of the fraction estimates, sum of fractions, R2, and the number of used elementary spectra. The experiments show that a collection of archetypes can be an adequate and efficient alternative to the manually designed spectral library with respect to the mentioned criteria. © 2018 American Society for Photogrammetry and Remote Sensing."
,10.1016/j.ijmedinf.2018.02.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043290149&doi=10.1016%2fj.ijmedinf.2018.02.004&partnerID=40&md5=6966940a89cad569500476a9b4455b2e,"Background: Telephone nursing is the first line of contact for many care-seekers and aims at optimizing the performance of the healthcare system by supporting and guiding patients to the correct level of care and reduce the amount of unscheduled visits. Good statistical models that describe the effects of telephone nursing are important in order to study its impact on healthcare resources and evaluate changes in telephone nursing procedures. Objective: To develop a valid model that captures the complex relationships between the nurse's recommendations, the patients’ intended actions and the patients’ health seeking behavior. Using the model to estimate the effects of telephone nursing on patient behavior, healthcare utilization, and infer potential cost savings. Methods: Bayesian ordinal regression modeling of data from randomly selected patients that received telephone nursing. Inference is based on Markov Chain Monte Carlo (MCMC) methods, model selection using the Watanabe–Akaike Information Criteria (WAIC), and model validation using posterior predictive checks on standard discrepancy measures. Results and conclusions: We present a robust Bayesian ordinal regression model that predicts three-quarters of the patients’ healthcare utilization after telephone nursing and we found no evidence of model deficiencies. A patient's compliance to the nurse's recommendation varies and depends on the recommended level of care, its agreement with and level of the patient's prior intention, and the availability of different care options at the time. The model reveals a risk reducing behavior among patients and the effect of the telephone nursing recommendation is 7 times higher than the effect of the patient's intended action prior to consultation if the recommendation is the highest level of care. But the effect of the nurse's recommendation is lower, or even non-existing, if the recommendation is self-care. Telephone nursing was found to have a constricting effect on healthcare utilization, however, the compliance to nurse's recommendation is closely tied to perceptions of risk, emphasizing the importance to address caller's needs of reassurance. © 2018 Elsevier B.V."
,10.11908/j.issn.0253-374x.2018.05.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051075198&doi=10.11908%2fj.issn.0253-374x.2018.05.014&partnerID=40&md5=30c5e09f3e81b85872c79f7e7cadcd1d,"Based on the random parameter mixed logit model, a discrete choice model for customer preference heterogeneity was established for the hierarchy of auto-body product. According to the sampled data obtained from SP (stated preference) survey and parameter prior distribution setting, the Markov chain Monte Carlo simulation method was used to make the Bayesian estimation of parameters. Finally, the McFadden's likelihood ratio test proves that the random parameter mixed logit model is of optimal goodness-of-fit, and better than others to elucidate where the customer preference heterogeneity rooted in. This modeling approach helps to capture personalized customer needs, and helps manufacturers to anticipate mutiple preferences of potential customers and assists in the design and the development of auto-body products. © 2018, Editorial Department of Journal of Tongji University. All right reserved."
2,10.1016/j.automatica.2018.01.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041514402&doi=10.1016%2fj.automatica.2018.01.011&partnerID=40&md5=e76290780ea2cc6677f370cad811693c,"We present a new method of identifying a specific module in a dynamic network, possibly with feedback loops. Assuming known topology, we express the dynamics by an acyclic network composed of two blocks where the first block accounts for the relation between the known reference signals and the input to the target module, while the second block contains the target module. Using an empirical Bayes approach, we model the first block as a Gaussian vector with covariance matrix (kernel) given by the recently introduced stable spline kernel. The parameters of the target module are estimated by solving a marginal likelihood problem with a novel iterative scheme based on the Expectation–Maximization algorithm. Additionally, we extend the method to include additional measurements downstream of the target module. Using Markov Chain Monte Carlo techniques, it is shown that the same iterative scheme can solve also this formulation. Numerical experiments illustrate the effectiveness of the proposed methods. © 2018 Elsevier Ltd"
,10.1109/TCST.2017.2692723,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019905138&doi=10.1109%2fTCST.2017.2692723&partnerID=40&md5=bde2ecf6649047ad234d755aab2fe4d7,"This paper develops and compares two stochastic strategies that manage glider flight in a dynamic environment, to tackle an open problem concerning the necessity, applicability, complexity, and validity of using an environment model when making flight management decisions. Strategy performance is compared on two surveillance and pursuit tasks that require optimal control: the maximization of expected range while maintaining altitude within given limits, and the maximization of expected range while following a moving ground vehicle within a prescribed distance and maintaining altitude within given limits. Both tasks involve flight management decisions of glider airspeed and the time-to-spend climbing in a randomly encountered thermal. The first strategy is stochastic drift counteraction optimal control (SDCOC), a dynamic programming method that relies on a model. Here, SDCOC uses a computationally inexpensive yet accurate environment model that reflects transition probabilities between updrafts and downdrafts as well as thermal locations and strengths based on existing glider flight data. The second strategy is selective evolutionary generation, a model-free Markov chain Monte Carlo method that is not subject to the curse of dimensionality, efficiently searches for an optimal control in an online and tunable fashion, and easily adapts to a dynamic environment. However, this strategy requires a tolerance for learning and decision-space exploration. Both strategies perform satisfactorily, and showcase an environment/decision-space exploitation-exploration tradeoff. © 2017 IEEE."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047533341&partnerID=40&md5=1e819d9b2baab844f528f0018c6ca883,"Recent progress has increased our understanding of key controls on the productivity of shale reservoirs. The quantitative relations between regional Eagle Ford Shale production trends and geologic parameters were investigated to clarify which geologic parameters exercise dominant control on well-production rates. Previously, qualitative correlations for the Eagle Ford Shale were demonstrated among depth, thickness, total organic carbon (TOC), distribution of limestone beds, and average bed thickness with regional production. Eagle Ford production wells are horizontal, but it was necessary to use vertical wells that penetrated the Eagle Ford to map reservoir properties. No wells in the database had both production and geological parameters, and thus geological parameters could not be directly related to individual-well production. Therefore, spatial-interpolation methods derived from the Kriging and Bayesian methods with Markov-chain Monte Carlo (MCMC) sampling algorithms were used to integrate data sets and predict geological properties at production-well locations. The spatial Gaussianprocess- regression modeling was conducted to investigate the primary controls on production. Results suggest that the 6-month cumulative production from the Eagle Ford Shale, in barrels of oil equivalent (BOE), increases consistently with depth, with Eagle Ford thickness (up to 180-ft thickness), and with TOC (up to 7%). Also, when the number of limestone beds exceeds 12, production increases with the number of limestone beds. The corresponding significance code indicates that the parameters most significant to production are TOC and depth (which relates to pressure and thermal maturation). Concepts and models developed in this study may assist operators in making critical Eagle Ford Shale development decisions and should be transferable to other shale plays. Copyright ©2018 Society of Petroleum Engineers."
1,10.1002/ecy.2189,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044341861&doi=10.1002%2fecy.2189&partnerID=40&md5=d2ec2f68aa6342bf55fdf065c2fe4e58,"Metapopulation ecology and landscape ecology aim to understand how spatial structure influences ecological processes, yet these disciplines address the problem using fundamentally different modeling approaches. Metapopulation models describe how the spatial distribution of patches affects colonization and extinction, but often do not account for the heterogeneity in the landscape between patches. Models in landscape ecology use detailed descriptions of landscape structure, but often without considering colonization and extinction dynamics. We present a novel spatially explicit modeling framework for narrowing the divide between these disciplines to advance understanding of the effects of landscape structure on metapopulation dynamics. Unlike previous efforts, this framework allows for statistical inference on landscape resistance to colonization using empirical data. We demonstrate the approach using 11 yr of data on a threatened amphibian in a desert ecosystem. Occupancy data for Lithobates chiricahuensis (Chiricahua leopard frog) were collected on the Buenos Aires National Wildlife Refuge (BANWR), Arizona, USA from 2007 to 2017 following a reintroduction in 2003. Results indicated that colonization dynamics were influenced by both patch characteristics and landscape structure. Landscape resistance increased with increasing elevation and distance to the nearest streambed. Colonization rate was also influenced by patch quality, with semi-permanent and permanent ponds contributing substantially more to the colonization of neighboring ponds relative to intermittent ponds. Ponds that only hold water intermittently also had the highest extinction rate. Our modeling framework can be widely applied to understand metapopulation dynamics in complex landscapes, particularly in systems in which the environment between habitat patches influences the colonization process. © 2018 by the Ecological Society of America"
,10.1016/j.ijpvp.2018.01.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042733710&doi=10.1016%2fj.ijpvp.2018.01.004&partnerID=40&md5=b9a23ab0530d723d8ed751779dc5fb82,"A novel approach for reliability assessment of aging gas pipeline systems based on a Bayesian network methodology is proposed in this paper with a focus on the improvement of the pipeline strength prediction. A multimodal diagnosis is performed by assessing the variation in the mechanical property (e.g., yield strength) within the pipe in terms of material property measurements, such as microstructure, composition, and hardness. The multimodality measurements are then integrated with the Bayesian network information fusion model. Prototype testing is carried out for model verification, validation and demonstration. The model updating scheme employs a Markov Chain Monte Carlo algorithm to infer the posterior distribution of the pipe strength using the multimodality measurements, whereas, the priors are derived from the literature knowledge of such systems. Moreover, through-thickness studies of pipe cross-sections are performed to demonstrate the mechanical property variation from the surface to bulk. Finally, data training of the model is employed to obtain a more accurate measure of the probabilistic pipe strength. Discussions on the observations and future work are provided. © 2018 Elsevier Ltd"
1,10.1007/s11432-016-9070-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028950675&doi=10.1007%2fs11432-016-9070-4&partnerID=40&md5=e5e14bf91ef5b21c65acf612720e99a4,"Characterizing and understanding the structure and the evolution of networks is an important problem for many different fields. While in the real-world networks, especially the spatial networks, the influence from one node to another tends to vary over both space and time due to the different space distances and propagation speeds between nodes. Thus the time lag plays an essential role in interpreting the temporal causal dependency among nodes and also brings a big challenge in network structure learning. However most of the previous researches aiming to learn the dynamic network structure only treat the time lag as a predefined constant, which may miss important information or include noisy information if the time lag is set too small or too large. In this paper, we propose a dynamic Bayesian model with adaptive lags (DBAL) which simultaneously integrates two usually separate tasks, i.e., learning the dynamic dependency network structure and estimating time lags, within one unified framework. Specifically, we propose a novel weight kernel approach for time series segmenting and sampling via leveraging samples from adjacent segments to avoid the sample scarcity. Besides, an effective Bayesian scheme cooperated with reversible jump Markov chain Monte Carlo (RJMCMC) and expectation propagation (EP) algorithm is proposed for parameter inference. Extensive empirical evaluations are conducted on both synthetic and two real-world datasets, and the results demonstrate that our proposed model is superior to the traditional methods in learning the network structure and the temporal dependency. © 2017, Science China Press and Springer-Verlag GmbH Germany."
1,10.1016/j.ymssp.2017.10.033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032792122&doi=10.1016%2fj.ymssp.2017.10.033&partnerID=40&md5=43a343ecf652987eb0456039fb354b38,"Probabilistic modeling provides the capability to represent and manipulate uncertainty in data, models, predictions and decisions. We are concerned with the problem of learning probabilistic models of dynamical systems from measured data. Specifically, we consider learning of probabilistic nonlinear state-space models. There is no closed-form solution available for this problem, implying that we are forced to use approximations. In this tutorial we will provide a self-contained introduction to one of the state-of-the-art methods—the particle Metropolis-Hastings algorithm—which has proven to offer a practical approximation. This is a Monte Carlo based method, where the particle filter is used to guide a Markov chain Monte Carlo method through the parameter space. One of the key merits of the particle Metropolis-Hastings algorithm is that it is guaranteed to converge to the “true solution” under mild assumptions, despite being based on a particle filter with only a finite number of particles. We will also provide a motivating numerical example illustrating the method using a modeling language tailored for sequential Monte Carlo methods. The intention of modeling languages of this kind is to open up the power of sophisticated Monte Carlo methods—including particle Metropolis-Hastings—to a large group of users without requiring them to know all the underlying mathematical details. © 2017 Elsevier Ltd"
,10.18576/amis/120310,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047100150&doi=10.18576%2famis%2f120310&partnerID=40&md5=a0a05bf1456e439dbb547774f4992991,"In this article, based on progressively Type-II censored schemes under step-stress partially accelerated life test model, the maximum likelihood, Bayes, and two parametric bootstrap methods are used for estimating the unknown parameters of the Kumaraswamy inverse Weibull distribution and the acceleration factor. Asymptotic confidence interval estimates of the model parameters and the acceleration factor are also evaluated by using Fisher information matrix. The classical Bayes estimators cannot be obtained in explicit form, so Markov chain Monte Carlo method is used to tackle this problem, which allows us to construct the credible interval of the involved parameters. Finally, analysis of a simulated data set has been also presented for illustrative purposes. © 2018 NSP Natural Sciences Publishing Cor."
4,10.1111/bmsp.12114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028842025&doi=10.1111%2fbmsp.12114&partnerID=40&md5=aa6d1c5fa3ca248041a5b2f0e3885051,"To provide more refined diagnostic feedback with collateral information in item response times (RTs), this study proposed joint modelling of attributes and response speed using item responses and RTs simultaneously for cognitive diagnosis. For illustration, an extended deterministic input, noisy ‘and’ gate (DINA) model was proposed for joint modelling of responses and RTs. Model parameter estimation was explored using the Bayesian Markov chain Monte Carlo (MCMC) method. The PISA 2012 computer-based mathematics data were analysed first. These real data estimates were treated as true values in a subsequent simulation study. A follow-up simulation study with ideal testing conditions was conducted as well to further evaluate model parameter recovery. The results indicated that model parameters could be well recovered using the MCMC approach. Further, incorporating RTs into the DINA model would improve attribute and profile correct classification rates and result in more accurate and precise estimation of the model parameters. © 2017 The British Psychological Society"
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048634346&partnerID=40&md5=2e1d6f95652d8006c0c520b5a66f90e3,"Objective: To compare the effect of different approaches of missing data replacement on the regression coefficient estimates r of ""length of stay"" on ""hospital expenditure"". Methods: Data were extracted from the medical records of patients with head and neck neoplasms who were admitted to Sichuan Cancer Hospital. R 3.4.1 was used for generating and processing simulated datasets. Various scenarios were established by setting up different proportions of missing data and missing mechanisms using Monte Carlo method. Three strategies were tested for replacing missing data: Complete Case method, Expectation Maximization (EM): and Markov Chain Monte Carlo method (MCMC). The regression coefficient estimates r of standardized ""length of stay"" on standardized logarithmic ""hospital expenditure"" were calculated using these strategies and compared with that of the original complete dataset, in terms of their accuracy (magnitude of differences in r) and precision (differences in the standard error of r). Results: The three replacement methods were all acceptable (within the limit re±0.5se) when missing data were generated using MAR (2:1) mechanism, or less than 30% data were simulated as missing using the MCAR and MAR (1:2) mechanism. The EM method had the best estimation precision. Conclusion: Missing data replacement should consider the proportion of missing data and potential mechanisms involved. © Sichuan University. All rights reserved."
,10.12989/sss.2018.21.5.601,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049567661&doi=10.12989%2fsss.2018.21.5.601&partnerID=40&md5=0f35e463797d996ae7eebb2dbe73240d,"The estimated probabilistic model of wind data based on the conventional approach may have high discrepancy compared with the true distribution because of the uncertainty caused by the instrument error and limited monitoring data. A sequential quadratic programming (SQP) algorithm-based finite mixture modeling method has been developed in the companion paper and is conducted to formulate the joint probability density function (PDF) of wind speed and direction using the wind monitoring data of the investigated bridge. The established bivariate model of wind speed and direction only represents the features of available wind monitoring data. To characterize the stochastic properties of the wind parameters with the subsequent wind monitoring data, in this study, Bayesian inference approach considering the uncertainty is proposed to update the wind parameters in the bivariate probabilistic model. The slice sampling algorithm of Markov chain Monte Carlo (MCMC) method is applied to establish the multi-dimensional and complex posterior distribution which is analytically intractable. The numerical simulation examples for univariate and bivariate models are carried out to verify the effectiveness of the proposed method. In addition, the proposed Bayesian inference approach is used to update and optimize the parameters in the bivariate model using the wind monitoring data from the investigated bridge. The results indicate that the proposed Bayesian inference approach is feasible and can be employed to predict the bivariate distribution of wind speed and direction with limited monitoring data. Copyright © 2018 Techno-Press, Ltd."
,10.1029/2018WR022735,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046723556&doi=10.1029%2f2018WR022735&partnerID=40&md5=024a1669879dc0853d512c0d57239a16,"Monte Carlo (MC) simulations of transport in random porous networks indicate that for high variances of the lognormal permeability distribution, the transport of a passive tracer is non-Fickian. Here we model this non-Fickian dispersion in random porous networks using discrete temporal Markov models. We show that such temporal models capture the spreading behavior accurately. This is true despite the fact that the slow velocities are strongly correlated in time, and some studies have suggested that the persistence of low velocities would render the temporal Markovian model inapplicable. Compared to previously proposed temporal stochastic differential equations with case-specific drift and diffusion terms, the models presented here require fewer modeling assumptions. Moreover, we show that discrete temporal Markov models can be used to represent dispersion in unstructured networks, which are widely used to model porous media. A new method is proposed to extend the state space of temporal Markov models to improve the model predictions in the presence of extremely low velocities in particle trajectories and extend the applicability of the model to higher temporal resolutions. Finally, it is shown that by combining multiple transitions, temporal models are more efficient for computing particle evolution compared to correlated CTRW with spatial increments that are equal to the lengths of the links in the network. © 2018. American Geophysical Union. All Rights Reserved."
1,10.1007/s11222-017-9750-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018798470&doi=10.1007%2fs11222-017-9750-x&partnerID=40&md5=40a1d21a3532d1d5f9cbdb1a512af937,"COM-Poisson regression is an increasingly popular model for count data. Its main advantage is that it permits to model separately the mean and the variance of the counts, thus allowing the same covariate to affect in different ways the average level and the variability of the response variable. A key limiting factor to the use of the COM-Poisson distribution is the calculation of the normalisation constant: its accurate evaluation can be time-consuming and is not always feasible. We circumvent this problem, in the context of estimating a Bayesian COM-Poisson regression, by resorting to the exchange algorithm, an MCMC method applicable to situations where the sampling model (likelihood) can only be computed up to a normalisation constant. The algorithm requires to draw from the sampling model, which in the case of the COM-Poisson distribution can be done efficiently using rejection sampling. We illustrate the method and the benefits of using a Bayesian COM-Poisson regression model, through a simulation and two real-world data sets with different levels of dispersion. © 2017, The Author(s)."
,10.1016/j.resconrec.2018.01.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041408379&doi=10.1016%2fj.resconrec.2018.01.016&partnerID=40&md5=e73a5062409562f7fe7e3e05d3fc6308,"This paper analyses the production of ethanol in Brazil using an extensive, plant-based, ethanol and sugar production database, including multiple variables involved in the ethanol production chain. To this end, a generalized mixed model was used with the Markov Chain and Monte Carlo methods by applying the MCMCglmm package in the R software environment. The results obtained not only confirmed the expected signs between ethanol production and its major drivers or contextual variables, but also shed light in terms of their relative importance and their nature: whether structural, conjunctural or exogenous. The main conclusions of this paper are that the contextual variables that contribute the most to the increase in ethanol production in Brazil were, in order of importance, sugarcane milling, sugar production, and the price ratios between ethanol and sugar. Policy implications to the sector are derived. © 2018 Elsevier B.V."
1,10.1093/sysbio/syx087,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050723466&doi=10.1093%2fsysbio%2fsyx087&partnerID=40&md5=c55588fbafaa7c7bfdcd35671343dbc4,"Phylogenetics, the inference of evolutionary trees from molecular sequence data such as DNA, is an enterprise that yields valuable evolutionary understanding of many biological systems. Bayesian phylogenetic algorithms, which approximate a posterior distribution on trees, have become a popular if computationally expensive means of doing phylogenetics. Modern data collection technologies are quickly adding newsequences to already substantial databases.With all current techniques for Bayesian phylogenetics, computation must start anew each time a sequence becomes available, making it costly to maintain an up-to-date estimate of a phylogenetic posterior. These considerations highlight the need for an online Bayesian phylogenetic method which can update an existing posterior with new sequences. Here, we provide theoretical results on the consistency and stability of methods for online Bayesian phylogenetic inference based on Sequential Monte Carlo (SMC) and Markov chain Monte Carlo. We first show a consistency result, demonstrating that the method samples from the correct distribution in the limit of a large number of particles. Next, we derive the first reported set of bounds on how phylogenetic likelihood surfaces change when new sequences are added. These bounds enable us to characterize the theoretical performance of sampling algorithms by bounding the effective sample size (ESS) with a given number of particles from below.We show that the ESS is guaranteed to grow linearly as the number of particles in an SMC sampler grows. Surprisingly, this result holds even though the dimensions of the phylogenetic model grow with each new added sequence. © 2017 The Author(s)."
,10.1534/genetics.117.300673,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046354287&doi=10.1534%2fgenetics.117.300673&partnerID=40&md5=8cfc21024f9e40706ec4df7005283865,"Recent technical and methodological advances have greatly enhanced genome-wide association studies (GWAS). The advent of low-cost, whole-genome sequencing facilitates high-resolution variant identification, and the development of linear mixed models (LMM) allows improved identification of putatively causal variants. While essential for correcting false positive associations due to sample relatedness and population stratification, LMMs have commonly been restricted to quantitative variables. However, phenotypic traits in association studies are often categorical, coded as binary case-control or ordered variables describing disease stages. To address these issues, we have devised a method for genomic association studies that implements a generalized LMM (GLMM) in a Bayesian framework, called Bayes-GLMM. Bayes-GLMM has four major features: (1) support of categorical, binary, and quantitative variables; (2) cohesive integration of previous GWAS results for related traits; (3) correction for sample relatedness by mixed modeling; and (4) model estimation by both Markov chain Monte Carlo sampling and maximal likelihood estimation. We applied Bayes-GLMM to the whole-genome sequencing cohort of the Alzheimer’s Disease Sequencing Project. This study contains 570 individuals from 111 families, each with Alzheimer’s disease diagnosed at one of four confidence levels. Using Bayes-GLMM we identified four variants in three loci significantly associated with Alzheimer’s disease. Two variants, rs140233081 and rs149372995, lie between PRKAR1B and PDGFA. The coded proteins are localized to the glial-vascular unit, and PDGFA transcript levels are associated with Alzheimer’s disease-related neuropathology. In summary, this work provides implementation of a flexible, generalized mixed-model approach in a Bayesian framework for association studies. © 2018 by the Genetics Society of America."
1,10.1016/j.ymssp.2017.10.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037808799&doi=10.1016%2fj.ymssp.2017.10.023&partnerID=40&md5=3657e71662f75cb6845cc23d3827f42c,"In a previous paper, the authors introduced a flexible methodology for reconstructing mechanical sources in the frequency domain from prior local information on both their nature and location over a linear and time invariant structure. The proposed approach was derived from Bayesian statistics, because of its ability in mathematically accounting for experimenter's prior knowledge. However, since only the Maximum a Posteriori estimate was computed, the posterior uncertainty about the regularized solution given the measured vibration field, the mechanical model and the regularization parameter was not assessed. To answer this legitimate question, this paper fully exploits the Bayesian framework to provide, from a Markov Chain Monte Carlo algorithm, credible intervals and other statistical measures (mean, median, mode) for all the parameters of the force reconstruction problem. © 2017 Elsevier Ltd"
2,10.1109/TSG.2016.2606700,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044154332&doi=10.1109%2fTSG.2016.2606700&partnerID=40&md5=3ff7bc71784439adf3766454465e7b1c,"The influences of network reconfiguration and distributed generations (DGs) on distribution system state estimation (DSSE) and measurement placement should not be ignored in active distribution systems. In this paper, considering network reconfiguration and the output uncertainties of DGs, a robust measurement placement method for active distribution systems is proposed based on measurement saturation analysis and heuristic algorithm. First, the saturation number determined by measurement saturation characteristic is chosen as the measurement number. Then, the impacts of different network topologies are represented by various weights in the robust measurement placement model, which are computed by Markov chain and analytic hierarchy process. In addition, Gaussian mixture model is applied to approximate the power fluctuations of DGs. Uncertainties caused by measurements are considered by Monte Carlo simulations. The accuracy of DSSE in different network topologies can be guaranteed by the proposed robust measurement placement method. Simulation results based on the IEEE 33-bus and 119-bus distribution systems demonstrate the effectiveness of the proposed method. © 2016 IEEE."
,10.1109/TII.2018.2802497,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041527866&doi=10.1109%2fTII.2018.2802497&partnerID=40&md5=6cade3f6ce666b72aaa49b938ea9ec4f,"Recently proposed IEEE 802.15.4-2015 MAC introduced a new prioritized contention access (PCA) for transfer of time-critical packets with lower channel access latency compared to carrier sense multiple access - collision avoidance (CSMA/CA). In this paper, we first propose a novel Markov-chain-based analytical model for unslotted CSMA/CA and PCA for industrial applications. The unslotted model is further extended to derive the analytical model for slotted CSMA/CA and PCA. Primary emphasis is laid on understanding the performance of PCA compared to CSMA/CA for different traffic classes in industrial applications. The performance analysis shows that the slotted PCA achieves a reduction of 63.3% and 97% in delay and power consumption respectively compared to slotted CSMA/CA, whereas unslotted PCA achieves a delay reduction of 53.3% and reduction of power consumption by 96% compared to unslotted CSMA/CA without any significant loss of reliability. The proposed analytical models for both slotted and unslotted IEEE 802.15.4-2015 MAC offer satisfactory performance with less than 5% error when validated using Monte Carlo simulations. Also, the performance is verified using real-time testbed. © 2005-2012 IEEE."
,10.5731/pdajpst.2017.008094,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049746479&doi=10.5731%2fpdajpst.2017.008094&partnerID=40&md5=cee0216026dea1e6f89048a350852c05,"For biotherapeutics and vaccines, potency is measured in a bioassay that compares the concentrationresponse curves of a new batch to that of a reference standard. Acceptable accuracy and precision of potency measurement is critical to the manufacturing of these products. These characteristics of a bioassay are typically assessed in a procedure that is carried out with samples spanning the acceptable range for the product. During early development, however, a full validation study such as that which is carried out in late development can be costly as it relates to the likelihood of eventual program success. For these reasons, the laboratory may look for alternative ways to ensure the validity of the bioassay across a range that will support product development. One such alternative combines information from a reduced procedure using only reference standard and 100% relative potency concentration- response data sets, together with computer simulation, to estimate missing relative potency values across the desired range. Fits to the reduced dataset provide estimates of bioassay model parameters such as those for an S-shaped potency assay that follows a four-parameter logistic relationship, along with estimates of their variancecovariance structure and independent experimental unit (e.g., well-to-well or animal-to-animal) errors. Using Bayesian Markov Chain Monte Carlo modeling, the predictive distribution of the concentration-response data for the desired levels of relative potency is generated. Results from use of the reduced procedure are compared to results calculated from a full dataset in Monte Carlo simulation and in a motivating example. © PDA, Inc. 2018."
,10.1371/journal.pone.0196435,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047177670&doi=10.1371%2fjournal.pone.0196435&partnerID=40&md5=764b6ac5497596be3be0da7984b7a38f,"A major challenge in systems biology is to infer the parameters of regulatory networks that operate in a noisy environment, such as in a single cell. In a stochastic regime it is hard to distinguish noise from the real signal and to infer the noise contribution to the dynamical behavior. When the genetic network displays oscillatory dynamics, it is even harder to infer the parameters that produce the oscillations. To address this issue we introduce a new estimation method built on a combination of stochastic simulations, mass action kinetics and ensemble network simulations in which we match the average periodogram and phase of the model to that of the data. The method is relatively fast (compared to Metropolis-Hastings Monte Carlo Methods), easy to parallelize, applicable to large oscillatory networks and large (~2000 cells) single cell expression data sets, and it quantifies the noise impact on the observed dynamics. Standard errors of estimated rate coefficients are typically two orders of magnitude smaller than the mean from single cell experiments with on the order of ~1000 cells. We also provide a method to assess the goodness of fit of the stochastic network using the Hilbert phase of single cells. An analysis of phase departures from the null model with no communication between cells is consistent with a hypothesis of Stochastic Resonance describing single cell oscillators. Stochastic Resonance provides a physical mechanism whereby intracellular noise plays a positive role in establishing oscillatory behavior, but may require model parameters, such as rate coefficients, that differ substantially from those extracted at the macroscopic level from measurements on populations of millions of communicating, synchronized cells. © 2018 Caranica et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1002/env.2497,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045665550&doi=10.1002%2fenv.2497&partnerID=40&md5=8b0e7d0700adeced89242022e9b9d5cf,"Knowledge of the characteristics of earthquake ground motion is fundamental for earthquake hazard assessments. Over small distances, relative to the source–site distance, where uniform site conditions are expected, the ground motion variability is also expected to be insignificant. However, despite being located on what has been characterized as a uniform lava-rock site condition, considerable peak ground acceleration (PGA) variations were observed on stations of a small-aperture array (covering approximately 1 km2) of accelerographs in Southwest Iceland during the Ölfus earthquake of magnitude 6.3 on May 29, 2008 and its sequence of aftershocks. We propose a novel Bayesian hierarchical model for the PGA variations accounting separately for earthquake event effects, station effects, and event-station effects. An efficient posterior inference scheme based on Markov chain Monte Carlo (MCMC) simulations is proposed for the new model. The variance of the station effect is certainly different from zero according to the posterior density, indicating that individual station effects are different from one another. The Bayesian hierarchical model thus captures the observed PGA variations and quantifies to what extent the source and recording sites contribute to the overall variation in ground motions over relatively small distances on the lava-rock site condition. Copyright © 2018 John Wiley & Sons, Ltd."
,10.2112/SI85-058.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051372260&doi=10.2112%2fSI85-058.1&partnerID=40&md5=35b9d19d400445b060d12b48eede9abb,"A coastal erosion was caused by the tractive force of wave, and is accelerating in some areas. In this study, a hydraulic experiment considered the pickup concept was carried out to understand this phenomenon. Since existing methods for deriving suspended sediment concentrations (SSC) were likely to affect the results, image processing techniques was used. Preliminary experiments were conducted for this process. Because relevant data necessarily involves uncertainty, a two-step Bayesian Markov Chain Monte Carlo method (MCMC) was used to quantitatively derive it. The first step was about the relationship between image grayscale and turbidity, and then relationship between SSC and turbidity was presented for quantative analysis to show sediment pickup rate. A sluice gate was designed for rapid openning to generate a solitary wave, and the optimum opening speed was derived. The experimental result indicated that the physical characteristics of wave and suspended sediment pickup rate were closely related, and this relationship was changed according to wave breaking. And solitary wave pickup function was presented by adapting Einstein (1950)'s essential concepts. More precise pickup rate can be used as basic data for prevention of coast erosion and management of shoreline. © Coastal Education and Research Foundation, Inc. 2018."
,10.1016/j.adhoc.2018.01.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044473466&doi=10.1016%2fj.adhoc.2018.01.006&partnerID=40&md5=b2d07e606d1ff5f3cabae2e54a28b588,"To provide a tool for performance evaluation of IEEE 802.15.4 with sleep mode enabled, a novel model based on real time queueing analysis is proposed in this paper. A low-rate wireless personal area network (LR-WPAN), composed of multiple nodes which send packets to the coordinator, is considered. The queueing behaviour of IEEE 802.15.4 node with sleep mode enabled differs from others because the packet arrivals in sleep period accumulate at the beginning of the active period, which makes a heavier load in the beginning than at any other time. This model analyses this behaviour by dividing the active portion of the superframe into backoff slots and then using an embedded discrete-time Markov chain model. The concept of virtual service time is introduced into this model which makes the proposed queueing model novel and different from typical ones. The accuracy of the proposed model is validated by Monte Carlo simulations in existing typical application scenarios, which indicates that the proposed queueing model can accurately evaluate the performance of IEEE 802.15.4 in the context of the application scenarios described in the simulations. © 2018 Elsevier B.V."
6,10.1016/j.jpdc.2018.01.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041471445&doi=10.1016%2fj.jpdc.2018.01.001&partnerID=40&md5=576673abbc8895a6baceff271cd94e01,"In this paper, we propose a coupled multiplex network framework to model the epidemic spreading and its corresponding information diffusion among a population. In the model, as far as the information perception on the epidemics is concerned, the individuals can be divided into two classes, namely aware or unaware ones; Meanwhile, the awareness diffusion is depicted by utilizing the traditional contact process. From the perspective of infectious disease spreading, the contagion dynamics among nodes can be characterized with the classic SIR (susceptible–infective–recovered) model. Based on the microscopic Markov chain approach, we build the probability tree to describe the switching process between different states, and then intensively perform the theoretical analysis for the state transition. In particular, we analytically derive the epidemic threshold regarding the disease propagation, which is correlated with the multiplex network topology and the coupling relationship between two transmission dynamics. After being compared with extensive numerical Monte Carlo (MC) simulations, it is clearly found that the achieved analytical results concur with the MC simulations. Current results will be beneficial to substantially enhance the predictability of the epidemic outbreaks within many realistic dissemination cases. © 2018 Elsevier Inc."
,10.1534/g3.117.300406,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046621018&doi=10.1534%2fg3.117.300406&partnerID=40&md5=9ca2c11abc9c857c8dc61d03bacd8709,"Genomic selection (GS) has become a tool for selecting candidates in plant and animal breeding programs. In the case of quantitative traits, it is common to assume that the distribution of the response variable can be approximated by a normal distribution. However, it is known that the selection process leads to skewed distributions. There is vast statistical literature on skewed distributions, but the skew normal distribution is of particular interest in this research. This distribution includes a third parameter that drives the skewness, so that it generalizes the normal distribution. We propose an extension of the Bayesian whole-genome regression to skew normal distribution data in the context of GS applications, where usually the number of predictors vastly exceeds the sample size. However, it can also be applied when the number of predictors is smaller than the sample size. We used a stochastic representation of a skew normal random variable, which allows the implementation of standard Markov Chain Monte Carlo (MCMC) techniques to efficiently fit the proposed model. The predictive ability and goodness of fit of the proposed model were evaluated using simulated and real data, and the results were compared to those obtained by the Bayesian Ridge Regression model. Results indicate that the proposed model has a better fit and is as good as the conventional Bayesian Ridge Regression model for prediction, based on the DIC criterion and cross-validation, respectively. A computing program coded in the R statistical package and C programming language to fit the proposed model is available as supplementary material. © 2018 Pérez-Rodríguez et al."
,10.1371/journal.pone.0197954,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047473699&doi=10.1371%2fjournal.pone.0197954&partnerID=40&md5=7435d26dfe7a7425e9e01131364363c2,"Statistical inference is a widely-used, powerful tool for learning about natural processes in diverse fields. The statistical software platforms AD Model Builder (ADMB) and Template Model Builder (TMB) are particularly popular in the ecological literature, where they are typically used to perform frequentist inference of complex models. However, both lack capabilities for flexible and efficient Markov chain Monte Carlo (MCMC) integration. Recently, the no-U-turn sampler (NUTS) MCMC algorithm has gained popularity for Bayesian inference through the software Stan because it is efficient for high dimensional, complex hierarchical models. Here, we introduce the R packages adnuts and tmbstan, which provide NUTS sampling in parallel and interactive diagnostics with ShinyStan. The ADMB source code was modified to provide NUTS, while TMB models are linked directly into Stan. We describe the packages, provide case studies demonstrating their use, and contrast performance against Stan. For TMB models, we show how to test the accuracy of the Laplace approximation using NUTS. For complex models, the performance of ADMB and TMB was typically within +/- 50% the speed of Stan. In one TMB case study we found inaccuracies in the Laplace approximation, potentially leading to biased inference. adnuts provides a new method for estimating hierarchical ADMB models which previously were infeasible. TMB users can fit the same model in both frequentist and Bayesian paradigms, including using NUTS to test the validity of the Laplace approximation of the marginal likelihood for arbitrary subsets of parameters. These software developments extend the available statistical methods of the ADMB and TMB user base with no additional effort by the user. © 2018 Monnahan, Kristensen. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1371/journal.pcbi.1006181,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048195917&doi=10.1371%2fjournal.pcbi.1006181&partnerID=40&md5=cd4d79f51a31fc5076cdc8b634cba60a,"A common challenge in systems biology is quantifying the effects of unknown parameters and estimating parameter values from data. For many systems, this task is computationally intractable due to expensive model evaluations and large numbers of parameters. In this work, we investigate a new method for performing sensitivity analysis and parameter estimation of complex biological models using techniques from uncertainty quantification. The primary advance is a significant improvement in computational efficiency from the replacement of model simulation by evaluation of a polynomial surrogate model. We demonstrate the method on two models of mating in budding yeast: a smaller ODE model of the heterotrimeric G-protein cycle, and a larger spatial model of pheromone-induced cell polarization. A small number of model simulations are used to fit the polynomial surrogates, which are then used to calculate global parameter sensitivities. The surrogate models also allow rapid Bayesian inference of the parameters via Markov chain Monte Carlo (MCMC) by eliminating model simulations at each step. Application to the ODE model shows results consistent with published single-point estimates for the model and data, with the added benefit of calculating the correlations between pairs of parameters. On the larger PDE model, the surrogate models allowed convergence for the distribution of 15 parameters, which otherwise would have been computationally prohibitive using simulations at each MCMC step. We inferred parameter distributions that in certain cases peaked at values different from published values, and showed that a wide range of parameters would permit polarization in the model. Strikingly our results suggested different diffusion constants for active versus inactive Cdc42 to achieve good polarization, which is consistent with experimental observations in another yeast species S. pombe. © 2018 Renardy et al. http://creativecommons.org/licenses/by/4.0/"
,10.1063/1.5025627,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047653592&doi=10.1063%2f1.5025627&partnerID=40&md5=4134b161dafae9ca95775bab0406d2c2,"We introduce a Monte Carlo algorithm to efficiently compute transport properties of chaotic dynamical systems. Our method exploits the importance sampling technique that favors trajectories in the tail of the distribution of displacements, where deviations from a diffusive process are most prominent. We search for initial conditions using a proposal that correlates states in the Markov chain constructed via a Metropolis-Hastings algorithm. We show that our method outperforms the direct sampling method and also Metropolis-Hastings methods with alternative proposals. We test our general method through numerical simulations in 1D (box-map) and 2D (Lorentz gas) systems. © 2018 Author(s)."
3,10.1061/(ASCE)CP.1943-5487.0000750,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041702334&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000750&partnerID=40&md5=f6aa0b1d2156a6e400acf8a84833f716,"The aim of model-based structural identification is to identify suitable models and values for model parameters that determine structure behavior through comparing measurements with predictions. Well-known methodologies, such as traditional implementations of Bayesian model updating, have been shown to be inaccurate in cases characterized by systematic uncertainties and unknown spatial correlations. Error-domain model falsification (EDMF) is another approach to structural identification. This approach is easy to understand for practicing engineers and can provide robust parameter identification without assumptions on spatial correlations. The performance of all approaches involving sampling is affected by the number of model evaluations that is generated based on prior knowledge of parameter-value distributions. This paper focuses on a new sampling technique, called radial-basis function sampling (RBFS), and its application to EDMF, to generate a set of candidate models that represents the behavior of the structure with a certain confidence level. Radial-basis function sampling provides a good exploration of the parameter space even with a limited number of samples, which results in reduced computation times. A full-scale bridge in Singapore has been tested and a new index of sampling quality is proposed to compare this approach with other sampling techniques such as Latin hypercube sampling (LHS) and Markov-chain Monte Carlo (MCMC). Finally, a cross-validation method is used to verify the robustness of the approach and the sensitivity of sampling on prediction reliability. © 2018 American Society of Civil Engineers."
1,10.1016/j.clon.2018.01.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041899239&doi=10.1016%2fj.clon.2018.01.007&partnerID=40&md5=29112011da77b6ce10a64ab3715461b8,"Proton beam therapy (PBT) is still relatively new in cancer treatment and the clinical evidence base is relatively sparse. Mathematical modelling offers assistance when selecting patients for PBT and predicting the demand for service. Discrete event simulation, normal tissue complication probability, quality-adjusted life-years and Markov Chain models are all mathematical and statistical modelling techniques currently used but none is dominant. As new evidence and outcome data become available from PBT, comprehensive models will emerge that are less dependent on the specific technologies of radiotherapy planning and delivery. © 2018 The Royal College of Radiologists"
1,10.1016/j.fct.2018.03.040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044953341&doi=10.1016%2fj.fct.2018.03.040&partnerID=40&md5=12586b52b158bbb15d4c8e1ba031dbe8,"An investigation of some heavy metals content in rice (Oryza sativa) and associated health risks was carried out for residents of Iranshahr city, Iran. Average daily rice consumption of the citizens and most widely used rice brands in the market of Iranshahr were determined using a questionnaire. Besides, the concentration of heavy metals in the gathered rice samples was measured by inductively coupled plasma mass spectrometry (ICP-MS). Monte Carlo uncertainty simulation was utilized in conducting exposure assessment and investigating the non-carcinogenic effects of the studied elements as well as the carcinogenic effect of As. Concentrations of As, Cd, Pb, Cu, Al, and Mo were 0.369 ± 0.094, 0.0337 ± 0.039, 0.123 ± 0.14, 3.095 ± 439.42, 39.6 ± 14.73, and 1.106 ± 0.133 mg kg−1, respectively. Al (0.18 ± 0.15 mg kg−1 d−1) and Cd (0.00015 ± 0.00034 mg kg−1 d−1) were the highest and lowest estimated daily intake, respectively. Except As (5.23 ± 4.01), the calculated hazard quotient for investigated elements showed no non-carcinogenic health risk. Besides, the simulation of the carcinogenic risk effect for As (2.37E-3) revealed that the ingestion of the studied rice brands would cause cancer risk due to lifetime consumption. Results show that consumption of rice in the Iranshahr city is a potential source of exposure to the studied elements. © 2018 Elsevier Ltd"
1,10.1016/S2214-109X(18)30059-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045197275&doi=10.1016%2fS2214-109X%2818%2930059-7&partnerID=40&md5=0651553e32ba4abe79e70608f636a303,"Background: The progress to achieve the fourth Millennium Development Goal in reducing mortality rate in children younger than 5 years since 1990 has been remarkable. However, work remains to be done in the Sustainable Development Goal era. Estimates of under-5 mortality rates at the national level can hide disparities within countries. We assessed disparities in under-5 mortality rates by household economic status in low-income and middle-income countries (LMICs). Method: We estimated country-year-specific under-5 mortality rates by wealth quintile on the basis of household wealth indices for 137 LMICs from 1990 to 2016, using a Bayesian statistical model. We estimated the association between quintile-specific and national-level under-5 mortality rates. We assessed the levels and trends of absolute and relative disparity in under-5 mortality rate between the poorest and richest quintiles, and among all quintiles. Findings: In 2016, for all LMICs (excluding China), the aggregated under-5 mortality rate was 64·6 (90% uncertainty interval [UI] 61·1–70·1) deaths per 1000 livebirths in the poorest households (first quintile), 31·3 (29·5–34·2) deaths per 1000 livebirths in the richest households (fifth quintile), and in between those outcomes for the middle quintiles. Between 1990 and 2016, the largest absolute decline in under-5 mortality rate occurred in the two poorest quintiles: 77·6 (90% UI 71·2–82·6) deaths per 1000 livebirths in the poorest quintile and 77·9 (72·0–82·2) deaths per 1000 livebirths in the second poorest quintile. The difference in under-5 mortality rate between the poorest and richest quintiles decreased significantly by 38·8 (90% UI 32·9–43·8) deaths per 1000 livebirths between 1990 and 2016. The poorest to richest under-5 mortality rate ratio, however, remained similar (2·03 [90% UI 1·94–2·11] in 1990, 1·99 [1·91–2·08] in 2000, and 2·06 [1·92–2·20] in 2016). During 1990–2016, around half of the total under-5 deaths occurred in the poorest two quintiles (48·5% in 1990 and 2000, 49·5% in 2016) and less than a third were in the richest two quintiles (30·4% in 1990, 30·5% in 2000, 29·9% in 2016). For all regions, differences in the under-5 mortality rate between the first and fifth quintiles decreased significantly, ranging from 20·6 (90% UI 15·9–25·1) deaths per 1000 livebirths in eastern Europe and central Asia to 59·5 (48·5–70·4) deaths per 1000 livebirths in south Asia. In 2016, the ratios of under-5 mortality rate in the first quintile to under-5 mortality rate in the fifth quintile were significantly above 2·00 in two regions, with 2·49 (90% UI 2·15–2·87) in east Asia and Pacific (excluding China) and 2·41 (2·05–2·80) in south Asia. Eastern and southern Africa had the smallest ratio in 2016 at 1·62 (90% UI 1·48–1·76). Our model suggested that the expected ratio of under-5 mortality rate in the first quintile to under-5 mortality rate in the fifth quintile increases as national-level under-5 mortality rate decreases. Interpretation: For all LMICs (excluding China) combined, the absolute disparities in under-5 mortality rate between the poorest and richest households have narrowed significantly since 1990, whereas the relative differences have remained stable. To further narrow the rich-and-poor gap in under-5 mortality rate on the relative scale, targeted interventions that focus on the poorest populations are needed. Funding: National University of Singapore, UN Children's Fund, United States Agency for International Development, and the Bill & Melinda Gates Foundation. © 2018 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license"
,10.1038/s41372-017-0026-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039171776&doi=10.1038%2fs41372-017-0026-2&partnerID=40&md5=ba2e45da6c2be3e67440ca2a2be4e37d,[No abstract available]
,10.1158/1078-0432.CCR-17-3542,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047795245&doi=10.1158%2f1078-0432.CCR-17-3542&partnerID=40&md5=68b47c0bee761a1bbb8b67c0a2056dac,"Purpose: To compare PREDICT and CancerMath, two widely used prognostic models for invasive breast cancer, taking into account their clinical utility. Furthermore, it is unclear whether these models could be improved. Experimental Design: A dataset of 5,729 women was used for model development. A Bayesian variable selection algorithm was implemented to stochastically search for important interaction terms among the predictors. The derived models were then compared in three independent datasets (n ¼ 5,534). We examined calibration, discrimination, and performed decision curve analysis. Results: CancerMath demonstrated worse calibration performance compared with PREDICT in estrogen receptor (ER)–positive and ER-negative tumors. The decline in discrimination performance was 4.27% (6.39 to 2.03) and 3.21% (5.9 to 0.48) for ER-positive and ER-negative tumors, respectively. Our new models matched the performance of PREDICT in terms of calibration and discrimination, but offered no improvement. Decision curve analysis showed predictions for all models were clinically useful for treatment decisions made at risk thresholds between 5% and 55% for ER-positive tumors and at thresholds of 15% to 60% for ER-negative tumors. Within these threshold ranges, CancerMath provided the lowest clinical utility among all the models. Conclusions: Survival probabilities from PREDICT offer both improved accuracy and discrimination over CancerMath. Using PREDICT to make treatment decisions offers greater clinical utility than CancerMath over a range of risk thresholds. Our new models performed as well as PREDICT, but no better, suggesting that, in this setting, including further interaction terms offers no predictive benefit. © 2018 American Association for Cancer Research."
,10.1002/bimj.201700176,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042385693&doi=10.1002%2fbimj.201700176&partnerID=40&md5=258d36afb18fb4dbdd9f750294a72d6a,"In this paper, the panel count data analysis for recurrent events is considered. Such analysis is useful for studying tumor or infection recurrences in both clinical trial and observational studies. A bivariate Gaussian Cox process model is proposed to jointly model the observation process and the recurrent event process. Bayesian nonparametric inference is proposed for simultaneously estimating regression parameters, bivariate frailty effects, and baseline intensity functions. Inference is done through Markov chain Monte Carlo, with fully developed computational techniques. Predictive inference is also discussed under the Bayesian setting. The proposed method is shown to be efficient via simulation studies. A clinical trial dataset on skin cancer patients is analyzed to illustrate the proposed approach. © 2018 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim"
,10.1007/s11306-018-1351-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044506676&doi=10.1007%2fs11306-018-1351-y&partnerID=40&md5=8fe48e23287c0c812cba0ec9ebe23175,"Introduction: To aid the development of better algorithms for 1H NMR data analysis, such as alignment or peak-fitting, it is important to characterise and model chemical shift changes caused by variation in pH. The number of protonation sites, a key parameter in the theoretical relationship between pH and chemical shift, is traditionally estimated from the molecular structure, which is often unknown in untargeted metabolomics applications. Objective: We aim to use observed NMR chemical shift titration data to estimate the number of protonation sites for a range of urinary metabolites. Methods: A pool of urine from healthy subjects was titrated in the range pH 2–12, standard 1H NMR spectra were acquired and positions of 51 peaks (corresponding to 32 identified metabolites) were recorded. A theoretical model of chemical shift was fit to the data using a Bayesian statistical framework, using model selection procedures in a Markov Chain Monte Carlo algorithm to estimate the number of protonation sites for each molecule. Results: The estimated number of protonation sites was found to be correct for 41 out of 51 peaks. In some cases, the number of sites was incorrectly estimated, due to very close pKa values or a limited amount of data in the required pH range. Conclusions: Given appropriate data, it is possible to estimate the number of protonation sites for many metabolites typically observed in 1H NMR metabolomics without knowledge of the molecular structure. This approach may be a valuable resource for the development of future automated metabolite alignment, annotation and peak fitting algorithms. © 2018, The Author(s)."
3,10.3150/15-BEJ785,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034834546&doi=10.3150%2f15-BEJ785&partnerID=40&md5=698842a0e0b428b98492975da3db80f0,"We establish quantitative bounds for rates of convergence and asymptotic variances for iterated conditional sequential Monte Carlo (i-cSMC) Markov chains and associated particle Gibbs samplers [J. R. Stat. Soc. Ser. B. Stat. Methodol. 72 (2010) 269-342]. Our main findings are that the essential boundedness of potential functions associated with the i-cSMC algorithm provide necessary and sufficient conditions for the uniform ergodicity of the i-cSMCMarkov chain, as well as quantitative bounds on its (uniformly geometric) rate of convergence. Furthermore, we show that the i-cSMC Markov chain cannot even be geometrically ergodic if this essential boundedness does not hold in many applications of interest. Our sufficiency and quantitative bounds rely on a novel non-asymptotic analysis of the expectation of a standard normalizing constant estimate with respect to a ""doubly conditional"" SMC algorithm. In addition, our results for i-cSMC imply that the rate of convergence can be improved arbitrarily by increasing N, the number of particles in the algorithm, and that in the presence of mixing assumptions, the rate of convergence can be kept constant by increasing N linearly with the time horizon. We translate the sufficiency of the boundedness condition for i-cSMC into sufficient conditions for the particle Gibbs Markov chain to be geometrically ergodic and quantitative bounds on its geometric rate of convergence, which imply convergence of properties of the particle Gibbs Markov chain to those of its corresponding Gibbs sampler. These results complement recently discovered, and related, conditions for the particle marginal Metropolis-Hastings (PMMH) Markov chain. © 2018 ISI/BS."
,10.1142/S0219024918500218,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046808746&doi=10.1142%2fS0219024918500218&partnerID=40&md5=a2a325ea0fac63767d7e82f5af40475e,"This paper studies the fitting of Markov chain potential models to interest-rate derivative prices in four currencies simultaneously, using sequential Monte Carlo methodology (particle filtering). The potential approach starts from some Markov process which is supposed to drive the random observations, and there have been many studies where this Markov process is taken to be a diffusion; fewer studies have worked from a finite-state Markov chain, and this seems to be the first study to attempt to fit such models to data. With the available data, we show impressive agreement of the fitted models with market prices. © 2018 World Scientific Publishing Company."
1,10.1002/2017GC007347,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046551872&doi=10.1002%2f2017GC007347&partnerID=40&md5=7727625279bb09a9c9aac521b88c915e,"Understanding the enigmatic intraplate volcanism in the Tristan da Cunha region requires knowledge of the temperature of the lithosphere and asthenosphere beneath it. We measured phase-velocity curves of Rayleigh waves using cross-correlation of teleseismic seismograms from an array of ocean-bottom seismometers around Tristan, constrained a region-average, shear-velocity structure, and inferred the temperature of the lithosphere and asthenosphere beneath the hotspot. The ocean-bottom data set presented some challenges, which required data-processing and measurement approaches different from those tuned for land-based arrays of stations. Having derived a robust, phase-velocity curve for the Tristan area, we inverted it for a shear wave velocity profile using a probabilistic (Markov chain Monte Carlo) approach. The model shows a pronounced low-velocity anomaly from 70 to at least 120 km depth. (Formula presented.) in the low velocity zone is 4.1–4.2 km/s, not as low as reported for Hawaii (∼4.0 km/s), which probably indicates a less pronounced thermal anomaly and, possibly, less partial melting. Petrological modeling shows that the seismic and bathymetry data are consistent with a moderately hot mantle (mantle potential temperature of 1,410–1,430°C, an excess of about 50–120°C compared to the global average) and a melt fraction smaller than 1%. Both purely seismic inversions and petrological modeling indicate a lithospheric thickness of 65–70 km, consistent with recent estimates from receiver functions. The presence of warmer-than-average asthenosphere beneath Tristan is consistent with a hot upwelling (plume) from the deep mantle. However, the excess temperature we determine is smaller than that reported for some other major hotspots, in particular Hawaii. © 2018. American Geophysical Union. All Rights Reserved."
11,10.1016/j.prevetmed.2017.02.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017168674&doi=10.1016%2fj.prevetmed.2017.02.017&partnerID=40&md5=c9bb3f0461c9f9b0ea8ca0bd98185828,"Bovine Tuberculosis (bTB) in cattle is a global health problem and eradication of the disease requires accurate estimates of diagnostic test performance to optimize their efficiency. The objective of this study was, through statistical meta-analyses, to obtain estimates of sensitivity (Se) and specificity (Sp), for 14 different ante-mortem and post-mortem diagnostic tests for bTB in cattle. Using data from a systematic review of the scientific literature (published 1934–2009) diagnostic Se and Sp were estimated using Bayesian logistic regression models adjusting for confounding factors. Random effect terms were used to account for unexplained heterogeneity. Parameters in the models were implemented using Markov Chain Monte Carlo (MCMC), and posterior distributions for the diagnostic parameters with adjustment for covariates (confounding factors) were obtained using the inverse logit function. Estimates for Se and/or Sp of the tuberculin skin tests and the IFN-γ blood test were compared with estimates published 2010–2015. Median Se for the single intradermal comparative cervical tuberculin skin (SICCT) test (standard interpretation) was 0.50 and Bayesian credible intervals (CrI) were wide (95% CrI 0.26, 0.78). Median Sp for the SICCT test was 1.00 (95% CrI 0.99, 1.00). Estimates for the IFN-γ blood test Bovine Purified Protein Derivative (PPD)-Avian PPD and Early Secreted Antigen target 6 and Culture Filtrate Protein 10 (ESAT-6/CFP10) ESAT6/CFP10 were 0.67 (95% CrI 0.49, 0.82) and 0.78 (95% CrI 0.60, 0.90) respectively for Se, and 0.98 (95% CrI 0.96, 0.99) and 0.99 (95% CrI 0.99, 1.00) for Sp. The study provides an overview of the accuracy of a range of contemporary diagnostic tests for bTB in cattle. Better understanding of diagnostic test performance is essential for the design of effective control strategies and their evaluation. © 2017"
,10.1214/16-BJPS345,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045506660&doi=10.1214%2f16-BJPS345&partnerID=40&md5=b9eb11d20b43c4152fa0f9e969d56fbf,"Mixture models provide a flexible representation of heterogeneity in a finite number of latent classes. From the Bayesian point of view, Markov Chain Monte Carlo methods provide a way to draw inferences from these models. In particular, when the number of subpopulations is considered unknown, more sophisticated methods are required to perform Bayesian analysis. The Reversible Jump Markov Chain Monte Carlo is an alternative method for computing the posterior distribution by simulation in this case. Some problems associated with the Bayesian analysis of these class of models are frequent, such as the so-called “label-switching” problem. However, as the level of heterogeneity in the population increases, these problems are expected to become less frequent and the model’s performance to improve. Thus, the aim of this work is to evaluate the normal mixture model fit using simulated data under different settings of heterogeneity and prior information about the mixture proportions. A simulation study is also presented to evaluate the model’s performance considering the number of components known and estimating it. Finally, the model is applied to a censored real dataset containing antibody levels of Cytomegalovirus in individuals. © Brazilian Statistical Association, 2018."
2,10.1103/PhysRevD.97.092007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048687794&doi=10.1103%2fPhysRevD.97.092007&partnerID=40&md5=8afa6fc5d791c401e3f876f986a5f31f,"We report on the response of liquid xenon to low energy electronic recoils below 15 keV from beta decays of tritium at drift fields of 92 V/cm, 154 V/cm and 366 V/cm using the XENON100 detector. A data-to-simulation fitting method based on Markov Chain Monte Carlo is used to extract the photon yields and recombination fluctuations from the experimental data. The photon yields measured at the two lower fields are in agreement with those from literature; additional measurements at a higher field of 366 V/cm are presented. The electronic and nuclear recoil discrimination as well as its dependence on the drift field and photon detection efficiency are investigated at these low energies. The results provide new measurements in the energy region of interest for dark matter searches using liquid xenon. © 2018 American Physical Society."
3,10.1002/jeab.330,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048044760&doi=10.1002%2fjeab.330&partnerID=40&md5=83e567acfd193ff69718c1d3cf52d2c6,"Models that generate event records have very general scope regarding the dimensions of the target behavior that we measure. From a set of predicted event records, we can generate predictions for any dependent variable that we could compute from the event records of our subjects. In this sense, models that generate event records permit us a freely multivariate analysis. To explore this proposition, we conducted a multivariate examination of Catania's Operant Reserve on single VI schedules in transition using a Markov Chain Monte Carlo scheme for Approximate Bayesian Computation. Although we found systematic deviations between our implementation of Catania's Operant Reserve and our observed data (e.g., mismatches in the shape of the interresponse time distributions), the general approach that we have demonstrated represents an avenue for modelling behavior that transcends the typical constraints of algebraic models. © 2018 Society for the Experimental Analysis of Behavior"
,10.5812/ijcm.62863,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048713898&doi=10.5812%2fijcm.62863&partnerID=40&md5=3c847bf8131802c7c48067e1c33623c4,"Background: Rapid progression in medical and health sciences have caused survival studies, where some patients have long-term survival, especially for chronic diseases such as breast cancer. Cure models can be applicable to analyze such data. Objectives: The aim of this study was to determine the risk factors associated with breast cancer, using mixture cure fraction model. Methods: We studied data for 438 patients, who were referred to cancer research center in Shahid Beheshti University of Medical Sciences. The patients were visited and treated during 1992 to 2012 and followed-up until October 2014. The data were analyzed by mixture cure fraction model based on GMW (generalized modified Weibull) distribution and inferences were obtained with Bayesian approach, using standard MCMC (Markov Chain Monte Carlo) methods. All analyses were performed, using SPSS v20 and OpenBUGS software. The significant level was considered at 0.05. Results: During the follow-up period, 75 (17.12%) deaths occurred by breast cancer and the one-year overall survival rate was 98%. Covariates such as numbers of metastatic lymph nodes and histologic grade were statistically significant. Also, the cure fraction estimation was obtained 58%. Conclusions: When some patients have a long-term survival, cure models can be an interesting model to study survival and these models estimate parameters better than the traditional models such as cox model. In this paper, the mixture cure fraction model based on GMW was fitted for analysing survival times in patients with breast cancer. © 2018, Cancer Research Center (CRC), Shahid Beheshti University of Medical Sciences."
,10.1007/s12517-018-3576-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046868202&doi=10.1007%2fs12517-018-3576-5&partnerID=40&md5=f3caea146f7203a4d85863f3a4aa494a,"Elastic properties of rock are being used to help evaluate reservoir properties. Distinctive properties of shale, such as low porosity and permeability, anisotropy, and complicated mineral components, limit the application of traditional petrophysical experimental method. The elastic properties of shale are not easy to obtain. In this work, we proposed a workflow and used an advanced Markov Chain Monte Carlo (MCMC) method to reconstruct 3D digital cores of rocks in a shale reservoir. Then, we used a finite element method (FEM) to calculate the equivalent static elastic moduli of shale of the 3D digital cores. This method overcomes the limitation of the direct elastic property measurement. The equivalent bulk and shear moduli of cores with different compositions are obtained. The result reveals that the moduli are anisotropic and have different sensitivities to different components. Equivalent moduli linearly decrease in three cases, namely (a) with the increase of clays, (b) with the increase of porosity, and (c) with the increase of organic matter content. On the other hand, equivalent moduli linearly increase in three cases, namely (a) with the increase of plagioclase content, (b) with the increase of calcite content, and (c) with the increase of pyrite content. For different clays, the equivalent moduli of the rocks vary with the elastic moduli of the clay in the form of power functions. Meanwhile, the bulk modulus can be affected by the type of pore fluid and the shear modulus is insensitive. © 2018, Saudi Society for Geosciences."
,10.1177/0013164418777569,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047832862&doi=10.1177%2f0013164418777569&partnerID=40&md5=ec2d1def9e7e6cf4640e5276af9f1447,"Plausible values can be used to either estimate population-level statistics or compute point estimates of latent variables. While it is well known that five plausible values are usually sufficient for accurate estimation of population-level statistics in large-scale surveys, the minimum number of plausible values needed to obtain accurate latent variable point estimates is unclear. This is especially relevant when an item response theory (IRT) model is estimated with MCMC (Markov chain Monte Carlo) methods in Mplus and point estimates of the IRT ability parameter are of interest, as Mplus only estimates the posterior distribution of each ability parameter. In order to obtain point estimates of the ability parameter, a number of plausible values can be drawn from the posterior distribution of each individual ability parameter and their mean (the posterior mean ability estimate) can be used as an individual ability point estimate. In this note, we conducted a simulation study to investigate how many plausible values were needed to obtain accurate posterior mean ability estimates. The results indicate that 20 is the minimum number of plausible values required to obtain point estimates of the IRT ability parameter that are comparable to marginal maximum likelihood estimation(MMLE)/expected a posteriori (EAP) estimates. A real dataset was used to demonstrate the comparison between MMLE/EAP point estimates and posterior mean ability estimates based on different number of plausible values. © 2018, The Author(s) 2018."
1,10.1177/0962280216662298,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043468751&doi=10.1177%2f0962280216662298&partnerID=40&md5=0b2d7798a3eca2d1819052310cec45f6,"Many medical (and ecological) processes involve the change of shape, whereby one trajectory changes into another trajectory at a specific time point. There has been little investigation into the study design needed to investigate these models. We consider the class of fixed effect change-point models with an underlying shape comprised two joined linear segments, also known as broken-stick models. We extend this model to include two sub-groups with different trajectories at the change-point, a change and no change class, and also include a missingness model to account for individuals with incomplete follow-up. Through a simulation study, we consider the relationship of sample size to the estimates of the underlying shape, the existence of a change-point, and the classification-error of sub-group labels. We use a Bayesian framework to account for the missing labels, and the analysis of each simulation is performed using standard Markov chain Monte Carlo techniques. Our simulation study is inspired by cognitive decline as measured by the Mini-Mental State Examination, where our extended model is appropriate due to the commonly observed mixture of individuals within studies who do or do not exhibit accelerated decline. We find that even for studies of modest size (n = 500, with 50 individuals observed past the change-point) in the fixed effect setting, a change-point can be detected and reliably estimated across a range of observation-errors. © 2016, © The Author(s) 2016."
,10.1016/j.jmva.2017.11.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038256959&doi=10.1016%2fj.jmva.2017.11.006&partnerID=40&md5=763ec6244f4cc4c167c090d86be872b8,"This paper introduces a multivariate circular–linear (or poly-cylindrical) distribution obtained by combining the projected and the skew-normal. We show the flexibility of our proposal, its closure under marginalization, and how to quantify multivariate dependence. Due to a non-identifiability issue that our proposal inherits from the projected normal, a computational problem arises. We overcome it in a Bayesian framework, adding suitable latent variables and showing that posterior samples can be obtained with a post-processing of the estimation algorithm output. Under specific prior choices, this approach enables us to implement a Markov chain Monte Carlo algorithm relying only on Gibbs steps, where the updates of the parameters are done as if we were working with a multivariate normal likelihood. The proposed approach can also be used with the projected normal. As a proof of concept, on simulated examples we show the ability of our algorithm in recovering the parameter values and to solve the identification problem. Then the proposal is used in a real data example, where the turning-angles (circular variables) and the logarithm of the step-lengths (linear variables) of four zebras are modeled jointly. © 2017 Elsevier Inc."
3,10.3847/1538-3881/aab95c,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047335032&doi=10.3847%2f1538-3881%2faab95c&partnerID=40&md5=7b4fdd75886ce83653ed6b588fa3e507,"Space-based high-contrast imaging mission concepts for studying rocky exoplanets in reflected light are currently under community study. We develop an inverse modeling framework to estimate the science return of such missions given different instrument design considerations. By combining an exoplanet albedo model, instrument noise model, and ensemble Markov chain Monte Carlo sampler, we explore retrievals of atmospheric and planetary properties for Earth twins as a function of signal-to-noise ratio (S/N) and resolution (R). Our forward model includes Rayleigh-scattering, single-layer water clouds with patchy coverage, and pressure-dependent absorption due to water vapor, oxygen, and ozone. We simulate data at R = 70 and 140 from 0.4 to 1.0 μm with S/N = 5, 10, 15, and 20 at 550 nm (i.e., for HabEx/LUVOIR-type instruments). At these same S/Ns, we simulate data for WFIRST paired with a starshade, which includes two photometric points between 0.48 and 0.6 μm and R = 50 spectroscopy from 0.6 to 0.97 μm. Given our noise model for WFIRST-type detectors, we find that weak detections of water vapor, ozone, and oxygen can be achieved with observations with at least R = 70/S/N = 15 or R = 140/S/N = 10 for improved detections. Meaningful constraints are only achieved with R = 140/S/N = 20 data. The WFIRST data offer limited diagnostic information, needing at least S/N = 20 to weakly detect gases. Most scenarios place limits on planetary radius but cannot constrain surface gravity and, thus, planetary mass. © 2018. The American Astronomical Society. All rights reserved."
2,10.1093/mnras/sty291,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043496919&doi=10.1093%2fmnras%2fsty291&partnerID=40&md5=ac4022b3b594253a3493fa3b31d94852,"We report mid-UV (MUV) observations taken with Hubble Space Telescope (HST)/WFC3, Swift/UVOT, and GALEX/NUV of the transitional millisecond pulsars XSS J12270-4859 and PSR J1023+0038 during their radio pulsar states. Both systems were detected in our images and showed MUV variability. At similar orbital phases, the MUV luminosities of both pulsars are comparable. This suggests that the emission processes involved in both objects are similar. We estimated limits on the mass ratio, companion's temperature, inclination, and distance to XSS J12270-4859 by using a Markov Chain Monte Carlo algorithm to fit published folded optical light curves. Using the resulting parameters, we modelled MUV light curves in our HST filters. The resulting models failed to fit our MUV observations. Fixing the mass ratio of XSS J12270-4859 to the value reported in other studies, we obtained a distance of ~3.2 kpc. This is larger than the one derived from dispersion measure (~1.4 kpc). Assuming a uniform prior for the mass ratio, the distance is similar to that from radio measurements. However, it requires an undermassive companion (~0.01M⊙). We conclude that a direct heating model alone cannot fully explain the observations in optical and MUV. Therefore, an additional radiation source is needed. The source could be an intrabinary shock which contributes to the MUV flux and likely to the optical one as well. During the radio pulsar state, the MUV orbital variations of PSR J1023+0038 detected with GALEX, suggest the presence of an asymmetric intrabinary shock. © 2018 The Author(s). Published by Oxford University Press on behalf of the Royal Astronomical Society."
1,10.1111/ddi.12707,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042368055&doi=10.1111%2fddi.12707&partnerID=40&md5=892ca9f991c23864b5fcedb1f8140d32,"Aim: We develop a novel modelling framework for analysing the spatio-temporal spread of biological invasions. The framework integrates different invasion drivers and disentangles their roles in determining observed invasion patterns by fitting models to historical distribution data. As a case study application, we analyse the spread of common ragweed (Ambrosia artemisiifolia). Location: Central Europe. Methods: A lattice system represents actual landscapes with environmental heterogeneity. Modelling covers the spatio-temporal invasion sequence in this grid and integrates the effects of environmental conditions on local invasion suitability, the role of invaded cells and spatially implicit “background” introductions as propagule sources, within-cell invasion level bulk-up and multiple dispersal means. A modular framework design facilitates flexible numerical representation of the modelled invasion processes and customization of the model complexity. We used the framework to build and contrast increasingly complex models, and fitted them using a Bayesian inference approach with parameters estimated by Markov chain Monte Carlo (MCMC). Results: All modelled invasion drivers codetermined the A. artemisiifolia invasion pattern. Inferences about individual drivers depended on which processes were modelled concurrently, and hence changed both quantitatively and qualitatively between models. Among others, the roles of environmental variables were assessed substantially differently subject to whether models included explicit source-recipient cell relationships, spatio-temporal variability in source cell strength and human-mediated dispersal means. The largest fit improvements were found by integrating filtering effects of the environment and spatio-temporal availability of propagule sources. Main conclusions: Our modelling framework provides a straightforward means to build integrated invasion models and address hypotheses about the roles and mutual relationships of different putative invasion drivers. Its statistical nature and generic design make it suitable for studying many observed invasions. For efficient invasion modelling, it is important to represent changes in spatio-temporal propagule supply by explicitly tracking the species’ colonization sequence and establishment of new populations. © 2018 John Wiley & Sons Ltd"
1,10.1111/geb.12722,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042176962&doi=10.1111%2fgeb.12722&partnerID=40&md5=224f1ee63a958e9badeaa5c78e36231b,"Aim: Community assembly is traditionally assumed to result from speciation and colonization mediated by available niche space. This paradigm is expanded by the theory that niche space can also be saturated by intersexual adaptive divergence (ecological sexual dimorphism) when interspecific competition is relaxed. This theory (here termed ‘niche-packing equivalence’) predicts that the evolution of ecological sexual dimorphism constrains the ecological opportunity that would otherwise lead to ecological speciation or colonization, and that saturation of niches by different species constrains divergent selection for divergence between the sexes. Therefore, sexes and species are equivalent, yet antagonistic units of niche occupation. We present the most comprehensive test of the niche-packing equivalence theory at ecological time-scales (assemblage level) to date. Location: South America. Major taxa studied: Liolaemus lizards. Methods: We identified 23 Liolaemus assemblages varying in species richness and sexual size dimorphism (SSD), distributed across a wide environmental range. We used mixed effects models, permutation tests and Markov Chain Monte Carlo (MCMC) regressions to quantify the relationship between SSD and species richness. We then partitioned the body size niche dimension between the sexes and amongst species, and tested for non-overlapping body size distributions. We regressed SSD and species richness of each assemblage against environmental predictors, using multi-model inference and structural equation modelling. Results: Sexual dimorphism declines with increasing species richness, and a strong signal of tension between the two remains following phylogenetic control. This pattern is accompanied by evidence of constraints on body-size partitioning amongst species and between the sexes: the two units of niche saturation tend not to overlap. However, across assemblages, species richness and SSD correlate with different environmental variables, suggesting that their tension is context-specific. Main conclusions: Our evidence supports the prediction that sexual dimorphism and species richness are alternative outcomes of adaptive radiation. However, this antagonism is mediated by a suite of environmental predictors that influence dimorphism and species richness differentially. © 2018 John Wiley & Sons Ltd"
,10.1093/mnras/sty079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043489597&doi=10.1093%2fmnras%2fsty079&partnerID=40&md5=c1a08eb18922b0446cf04793b574b1a3,"The ground-breaking detections of gravitational waves from black hole mergers by LIGOhave rekindled interest in primordial black holes (PBHs) and the possibility of dark matterbeing composed of PBHs. It has been suggested that PBHs of tens of solar masses could serveas dark matter candidates. Recent analytical studies demonstrated that compact ultra-faintdwarf galaxies can serve as a sensitive test for the PBH dark matter hypothesis, since stars insuch a halo-dominated system would be heated by the more massive PBHs, their present-daydistribution can provide strong constraints on PBH mass. In this study, we further explore thisscenario with more detailed calculations, using a combination of dynamical simulations andBayesian inference methods. The joint evolution of stars and PBH dark matter is followedwith a Fokker-Planck code PHASEFLOW. We run a large suite of such simulations for differentdark matter parameters, then use a Markov chain Monte Carlo approach to constrain the PBHproperties with observations of ultra-faint galaxies. We find that two-body relaxation betweenthe stars and PBH drives up the stellar core size, and increases the central stellar velocitydispersion. Using the observed half-light radius and velocity dispersion of stars in the compactultra-faint dwarf galaxies as joint constraints, we infer that these dwarfs may have a cored darkmatter halo with the central density in the range of 1-2 M pc-3, and that the PBHs may havea mass range of 2-14 M if they constitute all or a substantial fraction of the dark matter. © 2018 The Author(s). Published by Oxford University Press on behalf of the Royal Astronomical Society."
,10.1002/ece3.4006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045411195&doi=10.1002%2fece3.4006&partnerID=40&md5=0ed37a7a5ea0e86133f09393efeb412e,"Thermal acclimation is hypothesized to offer a selective advantage in seasonal habitats and may underlie disparities in geographic range size among closely-related species with similar ecologies. Understanding this relationship is also critical for identifying species that are more sensitive to warming climates. Here, we study North American plethodontid salamanders to investigate whether acclimation ability is associated with species’ latitudinal extents and the thermal range of the environments they inhabit. We quantified variation in thermal physiology by measuring standard metabolic rate (SMR) at different test and acclimation temperatures for 16 species of salamanders with varying latitudinal extents. A phylogenetically-controlled Markov chain Monte Carlo generalized linear mixed model (MCMCglmm) was then employed to determine whether there are differences in SMR between wide- and narrow-ranging species at different acclimation temperatures. In addition, we tested for a relationship between the acclimation ability of species and the environmental temperature ranges they inhabit. Further, we investigated if there is a trade-off between critical thermal maximum (CTMax) and thermal acclimation ability. MCMCglmm results show a significant difference in acclimation ability between wide and narrow-ranging temperate salamanders. Salamanders with wide latitudinal distributions maintain or slightly increase SMR when subjected to higher test and acclimation temperatures, whereas several narrow-ranging species show significant metabolic depression. We also found significant, positive relationships between acclimation ability and environmental thermal range, and between acclimation ability and CTMax. Wide-ranging salamander species exhibit a greater capacity for thermal acclimation than narrow-ranging species, suggesting that selection for acclimation ability may have been a key factor enabling geographic expansion into areas with greater thermal variability. Further, given that narrow-ranging salamanders are found to have both poor acclimation ability and lower tolerance to warm temperatures, they are likely to be more susceptible to environmental warming associated with anthropogenic climate change. © 2018 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd."
,10.1016/j.spinee.2017.09.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033489191&doi=10.1016%2fj.spinee.2017.09.009&partnerID=40&md5=38e413d4ee6cacf27f69b588a157145a,"Background Context: Chronic opioid therapy is associated with worse patient-reported outcomes (PROs) following spine surgery. However, little literature exists on the relationship between opioid use and PROs following epidural steroid injections for radicular pain. Purpose: We evaluated the association between pre-injection opioid use and PROs following spine epidural steroid injection. Study Design: This study is a retrospective analysis of a prospective longitudinal registry database. Patient Sample: A total of 392 patients within our database who were undergoing epidural steroid injections (ESIs) at our institution for degenerative structural spine diagnoses and met our inclusion criteria were included in this study. Outcome Measures: Patient-reported outcomes for disability (Oswestry Disability Index/Neck Disability Index [ODI/NDI)]), quality of life (EuroQol-5D [EQ-5D]), and pain (Numerical Rating Scale scores for back pain, neck pain, leg pain, and arm pain [NRS-BP/NP/LP/AP]) were assessed at baseline and at 3 and 12 months post-injection. Methods: Multivariable proportional odds logistic regression models were created to examine the relationship between pre-injection opioid use and post-injection PROs. A logistic regression with Bayesian Markov chain Monte Carlo parameter estimation was used to investigate a possible cutoff value of pre-injection opioid use above which the effectiveness of ESI (as measured by minimum clinically important difference [MCID] for ODI/NDI) decreases. Results: A total of 276 patients with complete 12-month follow-up following ESI were analyzed. The mean pre-injection daily morphine equivalent amount (MEA) was 14.7 mg (95% confidence interval [CI] 12.4 mg–19.1 mg) for the cohort. Pre-injection opioid use was associated with slightly higher odds of worse disability (odds ratio [OR] 1.03, p=.03) and leg/arm pain (OR 1.01, p=.04) scores at 3 months post-injection only. No significant association between pre-injection opioid use and MCID for ODI/NDI was found, although a cutoff of 55.5 mg/day might serve as a significant threshold. Conclusion: Increased pre-injection opioid use does not impact long-term outcomes after ESIs for degenerative spine diseases. A pre-injection MEA around 50 mg/day may represent a threshold above which the 3-month effectiveness of ESI for back- and neck-related disability decreases. Epidural steroid injection is an effective treatment modality for pain in patients using opioids, and can be part of a multimodal strategy for opioid independence. © 2017"
,10.1109/TVCG.2018.2832097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046363098&doi=10.1109%2fTVCG.2018.2832097&partnerID=40&md5=5d4370e88c086f12ddb0f31793aa4682,"We introduce a data-driven method to generate a large number of plausible, closely interacting 3D human pose-pairs, for a given motion category, e.g., wrestling or salsa dance. With much difficulty in acquiring close interactions using 3D sensors, our approach utilizes abundant existing video data which cover many human activities. Instead of treating the data generation problem as one of reconstruction, either through 3D acquisition or direct 2D-to-3D data lifting from video annotations, we present a solution based on Markov Chain Monte Carlo (MCMC) sampling. With a focus on efficient sampling over the space of close interactions, rather than pose spaces, we develop a novel representation called interaction coordinates (IC) to encode both poses and their interactions in an integrated manner. Plausibility of a 3D pose-pair is then defined based on the ICs and with respect to the annotated 2D pose-pairs from video. We show that our sampling-based approach is able to efficiently synthesize a large volume of plausible, closely interacting 3D pose-pairs which provide a good coverage of the input 2D pose-pairs. IEEE"
,10.1002/sim.7583,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040655380&doi=10.1002%2fsim.7583&partnerID=40&md5=be499a776309af311b77e7c11af9dc90,"The controlled imputation method refers to a class of pattern mixture models that have been commonly used as sensitivity analyses of longitudinal clinical trials with nonignorable dropout in recent years. These pattern mixture models assume that participants in the experimental arm after dropout have similar response profiles to the control participants or have worse outcomes than otherwise similar participants who remain on the experimental treatment. In spite of its popularity, the controlled imputation has not been formally developed for longitudinal binary and ordinal outcomes partially due to the lack of a natural multivariate distribution for such endpoints. In this paper, we propose 2 approaches for implementing the controlled imputation for binary and ordinal data based respectively on the sequential logistic regression and the multivariate probit model. Efficient Markov chain Monte Carlo algorithms are developed for missing data imputation by using the monotone data augmentation technique for the sequential logistic regression and a parameter-expanded monotone data augmentation scheme for the multivariate probit model. We assess the performance of the proposed procedures by simulation and the analysis of a schizophrenia clinical trial and compare them with the fully conditional specification, last observation carried forward, and baseline observation carried forward imputation methods. Copyright © 2018 John Wiley & Sons, Ltd."
,10.1080/15598608.2018.1460885,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046105147&doi=10.1080%2f15598608.2018.1460885&partnerID=40&md5=be35b6195afe1a6197e4887327e74515,"In this article we introduce a new distribution, namely, the defective Dagum distribution (DDD). This improper distribution can be seen as an extension of the Type I Dagum distribution and it is useful to accommodate survival data in the presence of a cure fraction. In the applications of survival methods to medical data, the cure fraction is defined as the proportion of patients who are cured of disease and become long-term survivors. The great advantage of the DDD is that the cure fraction can be written as a function of only one parameter. We also considered the presence of censored data and covariates. Maximum likelihood and Bayesian methods for estimation of the model parameters are presented. A simulation study is provided to evaluate the performance of the maximum likelihood method in estimating parameters. In the Bayesian analysis, posterior distributions of the parameters are estimated using the Markov-chain Monte Carlo (MCMC) method. An example involving a real data set is presented. The model based on the new distribution is easy to use and it is a good alternative for the analysis of real time-to-event data in the presence of censored information and a cure fraction. © 2018 Grace Scientific Publishing, LLC"
,10.1063/1.5021242,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046142657&doi=10.1063%2f1.5021242&partnerID=40&md5=1bc2a3cdf118a97e552072208ac6aa8c,"Stochastic simulations of biochemical networks are of vital importance for understanding complex dynamics in cells and tissues. However, existing methods to perform such simulations are associated with computational difficulties and addressing those remains a daunting challenge to the present. Here we introduce the selected-node stochastic simulation algorithm (snSSA), which allows us to exclusively simulate an arbitrary, selected subset of molecular species of a possibly large and complex reaction network. The algorithm is based on an analytical elimination of chemical species, thereby avoiding explicit simulation of the associated chemical events. These species are instead described continuously in terms of statistical moments derived from a stochastic filtering equation, resulting in a substantial speedup when compared to Gillespie's stochastic simulation algorithm (SSA). Moreover, we show that statistics obtained via snSSA profit from a variance reduction, which can significantly lower the number of Monte Carlo samples needed to achieve a certain performance. We demonstrate the algorithm using several biological case studies for which the simulation time could be reduced by orders of magnitude. © 2018 Author(s)."
,10.1007/s00446-018-0332-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046015826&doi=10.1007%2fs00446-018-0332-8&partnerID=40&md5=613e98e770f2638101740e84617ec408,"The local computation of Linial [FOCS’87] and Naor and Stockmeyer [STOC’93] studies whether a locally defined distributed computing problem is locally solvable. In classic local computation tasks, the goal of distributed algorithms is to construct a feasible solution for some constraint satisfaction problem (CSP) locally defined on the network. In this paper, we consider the problem of sampling a uniform CSP solution by distributed algorithms in the (Formula presented.) model, and ask whether a locally definable joint distribution is locally sample-able. We use Markov random fields and Gibbs distributions to model locally definable joint distributions. We give two distributed algorithms based on Markov chains, called LubyGlauber and LocalMetropolis, which we believe to represent two basic approaches for distributed Gibbs sampling. The algorithms achieve respective mixing times (Formula presented.) and (Formula presented.) under typical mixing conditions, where n is the number of vertices and (Formula presented.) is the maximum degree of the graph. We show that the time bound (Formula presented.) is optimal for distributed sampling. We also show a strong (Formula presented.) lower bound: in particular for sampling independent set in graphs with maximum degree (Formula presented.). This gives a strong separation between sampling and constructing locally checkable labelings. © 2018 The Author(s)"
,10.1080/02626667.2018.1457219,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046032909&doi=10.1080%2f02626667.2018.1457219&partnerID=40&md5=c7b2d14c6c0f4e47c6f0ff230974d8c3,"A statistical study was made of the temporal trend in extreme rainfall in the region of Extremadura (Spain) during the period 1961–2009. A hierarchical spatio-temporal Bayesian model with a GEV parameterization of the extreme data was employed. The Bayesian model was implemented in a Markov chain Monte Carlo framework that allows the posterior distribution of the parameters that intervene in the model to be estimated. The results show a decrease of extreme rainfall in winter and spring and a slight increase in autumn. The uncertainty in the trend parameters obtained with the hierarchical approach is much smaller than the uncertainties obtained from the GEV model applied locally. Also found was a negative relationship between the NAO index and the extreme rainfall in Extremadura during winter. An increase was observed in the intensity of the NAO index in winter and spring, and a slight decrease in autumn. © 2018 IAHS."
,10.1109/ICOIACT.2018.8350803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050465590&doi=10.1109%2fICOIACT.2018.8350803&partnerID=40&md5=db0957e88e880948c36df57d2d956b2e,"National Examination (UN) is one of the standard evaluation systems of education in Indonesia. The results of the UN can be used as a consideration for the development and provision of assistance to educational units in an effort to improve the quality of education. This research is done to get the best model of the average of UN value which is hierarchically structured. This paper would employ a two-level hierarchical linear model with nine characteristics of the school in the first level and four characteristics of the district/city in the second level. The complexity of the model is increasing due to the pattern of average UN value follows a normal mixture distribution pattern. A Bayesian hierarchical mixture normal approach coupled with Markov Chain Monte Carlo (MCMC), therefore, would be employed to estimate the model numerically. The results show that based on DIC value, the hierarchical normal mixture model with four components on the UN value has a better performance than the only one level mixture regression model in explaining the variability of the average value of UN. © 2018 IEEE."
1,10.3389/fpsyg.2018.00607,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046096859&doi=10.3389%2ffpsyg.2018.00607&partnerID=40&md5=460007f5ccbc0303b84063aa22932adf,"In joint models for item response times (RTs) and response accuracy (RA), local item dependence is composed of local RA dependence and local RT dependence. The two components are usually caused by the same common stimulus and emerge as pairs. Thus, the violation of local item independence in the joint models is called paired local item dependence. To address the issue of paired local item dependence while applying the joint cognitive diagnosis models (CDMs), this study proposed a joint testlet cognitive diagnosis modeling approach. The proposed approach is an extension of Zhan et al. (2017) and it incorporates two types of random testlet effect parameters (one for RA and the other for RTs) to account for paired local item dependence. The model parameters were estimated using the full Bayesian Markov chain Monte Carlo (MCMC) method. The 2015 PISA computer-based mathematics data were analyzed to demonstrate the application of the proposed model. Further, a brief simulation study was conducted to demonstrate the acceptable parameter recovery and the consequence of ignoring paired local item dependence. © 2018 Zhan, Liao and Bian."
1,10.1109/ICOMET.2018.8346456,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050985121&doi=10.1109%2fICOMET.2018.8346456&partnerID=40&md5=30da782e69b0bbb01956684e720db7d5,"Markov chain Monte Carlo (MCMC) methods are widely used in various areas of study to appraise the posterior inference in a Bayesian framework and to analyze the properties of complex systems. Prevailing theory and investigations demonstrate convergence of well-constructed MCMC schemes to the appropriate limiting distribution under a variety of different conditions. Diversification in the use of MCMC schemes urges the modelers to exploit it for the calibration of environmental models. Especially, calibration of hydrological models has always remained a key challenge for the hydrologic as well as hydraulic engineers because the designing of hydraulic structures primarily depends on the truthfulness of these models. In this study, an MCMC sampler scheme is utilized for the calibration and uncertainty analysis of a hydrological model. This sampler scheme, named Differential Evolution Adaptive Metropolis or DREAM, runs multiple chains simultaneously for global exploration, and automatically tunes the scale and orientation of the proposal distribution in randomized subspaces during the search. Application of DREAM does not only improve the authenticity of the model parameters, it also provides information about the uncertainty limits of the predictions which helps in deciding the factor of safety in design procedures. A conceptual model is used and the model parameters are acquired through MCMC sampling procedure. The calibration and validation is done using different time slots and efficacy of DREAM method is expressed in both temporal scenarios using Nash Sutcliffe efficiency measure. © 2018 IEEE."
,10.1007/s00366-018-0610-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045855864&doi=10.1007%2fs00366-018-0610-x&partnerID=40&md5=faaae17fdd307458449181a6dd1be7ba,"Field measured data reflect real response of soil slopes under rainfall infiltration and can provide representative estimates of in situ soil properties. In this study, an efficient probabilistic back analysis method for characterization of spatial variability of soil properties is used to investigate the effects of field responses with various monitoring schemes on characterization of spatial variability in unsaturated soil slope. A hypothetical heterogeneous slope of spatially varied saturated hydraulic conductivity subjecting to steady-state rainfall infiltration is analyzed as a numerical example. The spatially varied soil saturated hydraulic conductivity is parameterized by the Karhunen–Loève expansion (KLE) with a given covariance. The random variables corresponding to the truncated KLE terms are considered as variables to be estimated with Bayesian inverse method. Synthetic pore water pressure data corrupted with artificial noise are utilized as measurement data. Nine schemes with various locations, spacings and depths of monitoring sections are discussed. The results show that the local variability can be reduced substantially around the monitoring points of pore pressure. The spatial variability can be estimated more accurately with a smaller spacing of measurement points. When measurement points are installed with a spacing of 16.5 m, the posterior average COV of ks field is around 2% and the RMSE of the MAP field is only 5.90 × 10− 7 m/s. For schemes with different depths, the RMSEs of the MAP field does not change much but the posterior uncertainty of the estimated field is reduced with the increase of borehole depth. © 2018 Springer-Verlag London Ltd., part of Springer Nature"
,10.5194/isprs-annals-IV-3-127-2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046789328&doi=10.5194%2fisprs-annals-IV-3-127-2018&partnerID=40&md5=4af747efbc263c024f68835ce8e498e4,"The aftermath of wartime attacks is often felt long after the war ended, as numerous unexploded bombs may still exist in the ground. Typically, such areas are documented in so-called impact maps which are based on the detection of bomb craters. This paper proposes a method for the automatic detection of bomb craters in aerial wartime images that were taken during the Second World War. The object model for the bomb craters is represented by ellipses. A probabilistic approach based on marked point processes determines the most likely configuration of objects within the scene. Adding and removing new objects to and from the current configuration, respectively, changing their positions and modifying the ellipse parameters randomly creates new object configurations. Each configuration is evaluated using an energy function. High gradient magnitudes along the border of the ellipse are favored and overlapping ellipses are penalized. Reversible Jump Markov Chain Monte Carlo sampling in combination with simulated annealing provides the global energy optimum, which describes the conformance with a predefined model. For generating the impact map a probability map is defined which is created from the automatic detections via kernel density estimation. By setting a threshold, areas around the detections are classified as contaminated or uncontaminated sites, respectively. Our results show the general potential of the method for the automatic detection of bomb craters and its automated generation of an impact map in a heterogeneous image stock. © Authors 2018."
,10.1007/s11045-018-0577-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045751326&doi=10.1007%2fs11045-018-0577-1&partnerID=40&md5=f84a1dbee828da16e49128eb8460fcba,"A new stochastic nonlocal denoising method based on adaptive patch-size is presented. The quality of restored image is improved by choosing the optimal nonlocal similar patch-size for each site of image individually. The method contains two phase. The first phase is to search the similar patches base on adaptive patch-size. The second phase is to design the denoising algorithm by making use of similar image patches obtained in the first step. The multiple clusters of similar patches for each pixel point are searched by using Markov-chain Monte Carlo sampling many times. Following, we adjust the patch-size according to the consistency of multiple clusters. This processing is repeated until we obtain the optimal patch-size and corresponding optimal patch cluster. We get the estimation of noise-free patch cluster by employing modified two-directional non-local method. Furthermore, the denoised image is obtained by using the method of superposition approach. The theoretical analysis and simulation results show that the method is feasible and effective. © 2018 Springer Science+Business Media, LLC, part of Springer Nature"
,10.1109/ICCSNT.2017.8343738,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049662771&doi=10.1109%2fICCSNT.2017.8343738&partnerID=40&md5=ccad54e41ea41f9d46ec084953d76ff0,"Based on the fact that some image signals possess the block sparsity in practical application environment, a novel Compressed Sensing (CS) algorithm for block sparse image is proposed in this paper. Namely, a Double-level Binary Tree (DBT) Bayesian model is proposed for the block sparse image at the same time the relationship of the root node and the leaf node of this DBT structure is defined as 'genetic characteristic'. Then, the block clustering for the block sparse image can be executed successfully and effectively by utilizing Markov Chain Monte Carlo (MCMC) method. The simulation results prove that, our proposed method for the block sparse image signal can get better recovery results with less computation time. © 2017 IEEE."
,10.1007/s11538-018-0430-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045666945&doi=10.1007%2fs11538-018-0430-6&partnerID=40&md5=8a5beee9e057fec436c75bac065bfb5d,"A number of coupling strategies are presented for stochastically modeled biochemical processes with time-dependent parameters. In particular, the stacked coupling is introduced and is shown via a number of examples to provide an exceptionally low variance between the generated paths. This coupling will be useful in the numerical computation of parametric sensitivities and the fast estimation of expectations via multilevel Monte Carlo methods. We provide the requisite estimators in both cases. © 2018 Society for Mathematical Biology"
1,10.3390/s18041243,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045733767&doi=10.3390%2fs18041243&partnerID=40&md5=b4565b61d050faef42b842b1d67ca67c,"Microscale uncertainties related to the geometry and morphology of polycrystalline silicon films, constituting the movable structures of micro electro-mechanical systems (MEMS), were investigated through a joint numerical/experimental approach. An on-chip testing device was designed and fabricated to deform a compliant polysilicon beam. In previous studies, we showed that the scattering in the input-output characteristics of the device can be properly described only if statistical features related to the morphology of the columnar polysilicon film and to the etching process adopted to release the movable structure are taken into account. In this work, a high fidelity finite element model of the device was used to feed a transitional Markov chain Monte Carlo (TMCMC) algorithm for the estimation of the unknown parameters governing the aforementioned statistical features. To reduce the computational cost of the stochastic analysis, a synergy of proper orthogonal decomposition (POD) and kriging interpolation was adopted. Results are reported for a batch of nominally identical tested devices, in terms of measurement error-affected probability distributions of the overall Young’s modulus of the polysilicon film and of the overetch depth. © 2018 by the authors. Licensee MDPI, Basel, Switzerland."
1,10.1016/j.geoderma.2017.12.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038207851&doi=10.1016%2fj.geoderma.2017.12.010&partnerID=40&md5=13ba11ed1cfb30b158aec5869daf7499,"One of the first soil forming processes in marine and fluviatile clay soils is ripening, the irreversible change of physical and chemical soil properties, especially consistency, under influence of air. We used Bayesian binomial logistic regression (BBLR) to update the map showing unripened subsoils for a reclamation area in the west of The Netherlands. Similar to conventional binomial logistic regression (BLR), in BBLR the binary target variable (the subsoil is ripened or unripened) is modelled by a Bernoulli distribution. The logit transform of the `probability of success' parameter of the Bernoulli distribution was modelled as a linear combination of the covariates soil type, freeboard (the desired water level in the ditches, compared to surface level) and mean lowest groundwater table. To capture all available information, Bayesian statistics combines legacy data summarized in a ‘prior’ probability distribution for the regression coefficients with actual observations. Our research focused on quantifying the influence of priors with different information levels, in combination with different sample sizes, on the resulting parameters and maps. We combined subsamples of different size (ranging from 5% to 50% of the original dataset of 676 observations) with priors representing different levels of trust in legacy data and investigated the effect of sample size and prior distribution on map accuracy. The resulting posterior parameter distributions, calculated by Markov chain Monte Carlo simulation, vary in centrality as well as in dispersion, especially for the smaller datasets. More informative priors decreased dispersion and pushed posterior central values towards prior central values. Interestingly, the resulting probability maps were almost similar. However, the associated uncertainty maps were different: a more informative prior decreased prediction uncertainty. When using the ‘overall accuracy’ validation metric, we found an optimal value for the prior information level, indicating that the standard deviation of the legacy data regression parameters should be multiplied by 10. This effect is only detectable for smaller datasets. The Area Under Curve validation statistic did not provide a meaningful optimal multiplier for the standard deviation. Bayesian binomial logistic regression proved to be a flexible mapping tool but the accuracy gain compared to conventional logistic regression was marginal and may not outweigh the extra modelling and computing effort. © 2017 Elsevier B.V."
,10.1002/hyp.11464,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044226097&doi=10.1002%2fhyp.11464&partnerID=40&md5=84ce5b260998e2443375a3ac7359df29,"Finding an operational parameter vector is always challenging in the application of hydrologic models, with over-parameterization and limited information from observations leading to uncertainty about the best parameter vectors. Thus, it is beneficial to find every possible behavioural parameter vector. This paper presents a new methodology, called the patient rule induction method for parameter estimation (PRIM-PE), to define where the behavioural parameter vectors are located in the parameter space. The PRIM-PE was used to discover all regions of the parameter space containing an acceptable model behaviour. This algorithm consists of an initial sampling procedure to generate a parameter sample that sufficiently represents the response surface with a uniform distribution within the “good-enough” region (i.e., performance better than a predefined threshold) and a rule induction component (PRIM), which is then used to define regions in the parameter space in which the acceptable parameter vectors are located. To investigate its ability in different situations, the methodology is evaluated using four test problems. The PRIM-PE sampling procedure was also compared against a Markov chain Monte Carlo sampler known as the differential evolution adaptive Metropolis (DREAMZS) algorithm. Finally, a spatially distributed hydrological model calibration problem with two settings (a three-parameter calibration problem and a 23-parameter calibration problem) was solved using the PRIM-PE algorithm. The results show that the PRIM-PE method captured the good-enough region in the parameter space successfully using 8 and 107 boxes for the three-parameter and 23-parameter problems, respectively. This good-enough region can be used in a global sensitivity analysis to provide a broad range of parameter vectors that produce acceptable model performance. Moreover, for a specific objective function and model structure, the size of the boxes can be used as a measure of equifinality. Copyright © 2018 John Wiley & Sons, Ltd."
,10.1103/PhysRevE.97.042119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045390322&doi=10.1103%2fPhysRevE.97.042119&partnerID=40&md5=c4d6de63033846ecf8a85fc5cad4c444,"We present a path integral formulation of Darcy's equation in one dimension with random permeability described by a correlated multivariate lognormal distribution. This path integral is evaluated with the Markov chain Monte Carlo method to obtain pressure distributions, which are shown to agree with the solutions of the corresponding stochastic differential equation for Dirichlet and Neumann boundary conditions. The extension of our approach to flow through random media in two and three dimensions is discussed. © 2018 American Physical Society."
,10.1186/s12859-018-2099-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045277015&doi=10.1186%2fs12859-018-2099-0&partnerID=40&md5=f4684fdaaf86ba5b48286ad5599b0afd,"Background: Somatic copy number alternations (SCNAs) can be utilized to infer tumor subclonal populations in whole genome seuqncing studies, where usually their read count ratios between tumor-normal paired samples serve as the inferring proxy. Existing SCNA based subclonal population inferring tools consider the GC bias of tumor and normal sample is of the same fature, and could be fully offset by read count ratio. However, we found that, the read count ratio on SCNA segments presents a Log linear biased pattern, which influence existing read count ratios based subclonal inferring tools performance. Currently no correction tools take into account the read ratio bias. Results: We present Pre-SCNAClonal, a tool that improving tumor subclonal population inferring by correcting GC-bias at SCNAs level. Pre-SCNAClonal first corrects GC bias using Markov chain Monte Carlo probability model, then accurately locates baseline DNA segments (not containing any SCNAs) with a hierarchy clustering model. We show Pre-SCNAClonal's superiority to exsiting GC-bias correction methods at any level of subclonal population. Conclusions: Pre-SCNAClonal could be run independently as well as serving as pre-processing/gc-correction step in conjuntion with exsiting SCNA-based subclonal inferring tools. © 2018 The Author(s)."
,10.1186/s12955-018-0879-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045246890&doi=10.1186%2fs12955-018-0879-x&partnerID=40&md5=c92cff5675557faf7468d9fda39ed41c,"Background: Longitudinal invariance is a perquisite for a valid comparison of oral health-related quality of life (OHRQoL) scores over time. Item response theory (IRT) models can assess measurement invariance and allow better estimation of the associations between predictors and latent construct. By extending IRT models, this study aimed to investigate the longitudinal invariance of the two 8-item short forms of the Child Perception Questionnaire (CPQ11-14) regression short form (RSF:8) and item-impact short form (ISF:8) and identify factors associated with adolescents' OHRQoL and its change. Methods: All students from S1 and S2 (equivalent to US grades 6 and 7) who were born in April 1997 and May 1997 (at age 12) from 45 randomly selected secondary schools were invited to participate in this study and followed up after 3years. Data on the CPQ11-14 RSF:8 and CPQ11-14 ISF:8, demographics, oral health behavior and status were collected. Explanatory graded response models were fitted to both short forms of the CPQ11-14 data for assessing longitudinal invariance and factors associated with OHRQoL. The Bayesian estimation method - Monte Carlo Markov Chain (MCMC) with Gibbs sampling was adopted for parameter estimation and the credible intervals were used for inference. Results: Data from 649 children at age 12 at baseline and 415 children at age 15 at follow up were analyzed. For the 12years old children, healthier oral health behavior, better gum status, families with both parents employed and parents' education level were found to be associated with better OHRQoL. Four items among the 2 short forms lacked longitudinal invariance. With statistical adjustment of longitudinal invariance, OHRQoL were found improved in general over the 3years but no predictor was associated with OHRQoL in follow-up. For those with decreased family income, their OHRQoL had worsened over 3years. Conclusions: IRT explanatory analysis enables a more valid identification of the factors associated with OHRQoL and its changes over time. It provides important information to oral healthcare researchers and policymakers. © 2018 The Author(s)."
,10.1109/TCBB.2018.2825327,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045296828&doi=10.1109%2fTCBB.2018.2825327&partnerID=40&md5=9107ad277c25b200f249a664b2e34b18,"Ordinary differential equations (ODEs) provide a powerful formalism to model molecular networks mechanistically. However, inferring the model structure, given a set of time course measurements and a large number of alternative molecular mechanisms, is a challenging and open research question. Existing search heuristics are designed only for finding a single best model configuration and cannot account for the uncertainty in selecting the network components. In this study, we present a novel Markov chain Monte Carlo approach for performing Bayesian model structure inference over ODE models. We formulate a Metropolis algorithm that explores the model space efficiently and is suitable for obtaining probabilistic inferences about the network structure. The method and its special parallelization possibilities are demonstrated using simulated data. Furthermore, we apply the method to a time course RNA sequencing data set to infer the structure of the transiently evolving core regulatory network that steers the T helper 17 (Th17) cell differentiation. Our results are in agreement with the earlier finding that the Th17 lineage-specific differentiation program evolves in three sequential phases. Further, the analysis provides us with probabilistic predictions on the molecular interactions that are active in different phases of Th17 cell differentiation. OAPA"
,10.1186/s12885-018-4308-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045020949&doi=10.1186%2fs12885-018-4308-7&partnerID=40&md5=fac38cb14adc63d03a2bc4a5286e46f6,"Background: Sorafenib and transarterial chemoembolization (TACE) might both provide survival benefit for advanced hepatocellular carcinoma (HCC). Adopting either as a first-line therapy carries major cost and resource implications. We aimed to estimate the cost-effectiveness of sorafenib and TACE in advanced HCC. Methods: A Markov model was constructed in a hypothetical cohort of patients aged 60years with advanced HCC and Child-Pugh A/B cirrhosis over a 2-year time frame. Three strategies (full or dose-adjusted sorafenib and TACE) were compared in two cost settings: China and the USA. Transition probabilities, utility and costs were extracted from systematic review of 27 articles. Sensitivity analysis and Monte Carlo analysis were conducted. Results: Full and dose-adjusted sorafenib respectively produced 0.435 and 0.482 quality-adjusted life years (QALYs) while TACE produced 0.375 QALYs. The incremental cost-effectiveness ratio (ICER) of full-dose sorafenib versus TACE was $101,028.83/QALY in China whereas full-dose sorafenib is a dominant strategy (ICER of -$1,014,507.20/ QALY) compared with TACE in the USA. Compared to full-dose sorafenib, dose-adjusted sorafenib was the dominant strategy with the negative ICERs in both China (-$132,238.94/QALY) and the USA (-$230,058.09/QALY). However, dose-adjusted sorafenib is not available currently, so full-dose sorafenib should be compared with TACE. As the acceptability curves shown, full-dose sorafenib was the optimal strategy at the accepted thresholds of WTP in these two countries. Specifically, full-dose sorafenib was the cost-effective treatment compared with TACE if a WTP was set above $21,670 in the USA, whereas in China, TACE could be more favorable than full-dose sorafenib if a WTP was set below $10,473. Conclusions: Dose-adjusted sorafenib may be cost-effective compared to full-dose sorafenib or TACE for advanced HCC patients. However, when confining the comparisons between full-dose sorafenib and TACE, full-dose sorafenib was cost-effective for these patients, under the accepted thresholds of WTP. © 2018 The Author(s)."
,10.1103/PhysRevE.97.042406,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045006650&doi=10.1103%2fPhysRevE.97.042406&partnerID=40&md5=a4590ab0da0409917c1d89263bcb6c3b,"Competition is the main driver of population dynamics, which shapes the genetic composition of populations and the assembly of ecological communities. Neutral models assume that all the individuals are equivalent and that the dynamics is governed by demographic (shot) noise, with a steady state species abundance distribution (SAD) that reflects a mutation-extinction equilibrium. Recently, many empirical and theoretical studies emphasized the importance of environmental variations that affect coherently the relative fitness of entire populations. Here we consider two generic time-averaged neutral models; in both the relative fitness of each species fluctuates independently in time but its mean is zero. The first (model A) describes a system with local competition and linear fitness dependence of the birth-death rates, while in the second (model B) the competition is global and the fitness dependence is nonlinear. Due to this nonlinearity, model B admits a noise-induced stabilization mechanism that facilitates the invasion of new mutants. A self-consistent mean-field approach is used to reduce the multispecies problem to two-species dynamics, and the large-N asymptotics of the emerging set of Fokker-Planck equations is presented and solved. Our analytic expressions are shown to fit the SADs obtained from extensive Monte Carlo simulations and from numerical solutions of the corresponding master equations. © 2018 American Physical Society."
,10.1109/TNSE.2018.2823324,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045221359&doi=10.1109%2fTNSE.2018.2823324&partnerID=40&md5=2eea0493d8c5735fb0c0ff2d87a75172,"This work proposes novel hybrid mixed-membership blockmodels (HMMB) that integrate three canonical network models to capture the characteristics of real-world interactions: community structure with mixed-membership, power-law-distributed node degrees, and sparsity. This hybrid model provides the capacity needed for realism, enabling control and inference on individual attributes of interest such as mixed-membership and popularity. A rigorous inference procedure is developed for estimating the parameters of this model through iterative Bayesian updates, with targeted initialization to improve identifiability. For the estimation of mixed-membership parameters, the Cram&#x00E9;r-Rao bound is derived by quantifying the information content in terms of the Fisher information matrix. The effectiveness of the proposed inference is demonstrated in simulations where the estimates achieve covariances close to the Cram&#x00E9;r-Rao bound while maintaining good truth coverage. We illustrate the utility of the proposed model and inference procedure in the application of detecting a community from a few cue nodes, where success depends on accurately estimating the mixed-memberships. Performance evaluations on both simulated and real-world data show that inference with HMMB is able to recover mixed-memberships in the presence of challenging community overlap, leading to significantly improved detection performance over algorithms based on network modularity and simpler models. OAPA"
,10.1080/17415977.2017.1322078,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019217535&doi=10.1080%2f17415977.2017.1322078&partnerID=40&md5=a8786be8c16ebbe141c4baa964cdb897,"This paper presents an innovative application of a new class of parallel interacting Markov chains Monte Carlo to solve the Bayesian history matching (BHM) problem. BHM consists of sampling a posterior distribution given by the Bayesian theorem. Markov chain Monte Carlo (MCMC) is well suited for sampling, in principle, any type of distribution; however the number of iteration required by the traditional single-chain MCMC can be prohibitive in BHM applications. Furthermore, history matching is typically a highly nonlinear inverse problem, which leads in very complex posterior distributions, characterized by many separated modes. Therefore, single chain can be trapped into a local mode. Parallel interacting chains is an interesting way to overcome this problem, as shown in this paper. In addition, we presented new approaches to define starting points for the parallel chains. For validation purposes, the proposed methodology is firstly applied in a simple but challenging cross section reservoir model with many modes in the posterior distribution. Afterwards, the application to a realistic case integrated to geostatistical modelling is also presented. The results showed that the combination of parallel interacting chain with the capabilities of distributed computing commonly available nowadays is very promising to solve the BHM problem. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
2,10.1080/01621459.2017.1294075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040770199&doi=10.1080%2f01621459.2017.1294075&partnerID=40&md5=25e3dffd8186e7104b384c6a6d98ce96,"Many Markov chain Monte Carlo techniques currently available rely on discrete-time reversible Markov processes whose transition kernels are variations of the Metropolis–Hastings algorithm. We explore and generalize an alternative scheme recently introduced in the physics literature (Peters and de With 2012) where the target distribution is explored using a continuous-time nonreversible piecewise-deterministic Markov process. In the Metropolis–Hastings algorithm, a trial move to a region of lower target density, equivalently of higher “energy,” than the current state can be rejected with positive probability. In this alternative approach, a particle moves along straight lines around the space and, when facing a high energy barrier, it is not rejected but its path is modified by bouncing against this barrier. By reformulating this algorithm using inhomogeneous Poisson processes, we exploit standard sampling techniques to simulate exactly this Markov process in a wide range of scenarios of interest. Additionally, when the target distribution is given by a product of factors dependent only on subsets of the state variables, such as the posterior distribution associated with a probabilistic graphical model, this method can be modified to take advantage of this structure by allowing computationally cheaper “local” bounces, which only involve the state variables associated with a factor, while the other state variables keep on evolving. In this context, by leveraging techniques from chemical kinetics, we propose several computationally efficient implementations. Experimentally, this new class of Markov chain Monte Carlo schemes compares favorably to state-of-the-art methods on various Bayesian inference tasks, including for high-dimensional models and large datasets. Supplementary materials for this article are available online. © 2018, © 2018 American Statistical Association."
,10.1080/01621459.2017.1287080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048095380&doi=10.1080%2f01621459.2017.1287080&partnerID=40&md5=e7765e6af19539d1aa9a16c3deb8c7b3,"Factor models are used in a wide range of areas. Two issues with Bayesian versions of these models are a lack of invariance to ordering of and scaling of the variables and computational inefficiency. This article develops invariant and efficient Bayesian methods for estimating static factor models. This approach leads to inference that does not depend upon the ordering or scaling of the variables, and we provide arguments to explain this invariance. Beginning from identified parameters which are subject to orthogonality restrictions, we use parameter expansions to obtain a specification with computationally convenient conditional posteriors. We show significant gains in computational efficiency. Identifying restrictions that are commonly employed result in interpretable factors or loadings and, using our approach, these can be imposed ex-post. This allows us to investigate several alternative identifying (noninvariant) schemes without the need to respecify and resample the model. We illustrate the methods with two macroeconomic datasets. © 2018, © 2018 American Statistical Association."
2,10.1080/07350015.2016.1164707,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018180266&doi=10.1080%2f07350015.2016.1164707&partnerID=40&md5=a005e73d2038818097e5d22523f35e4e,"Restrictions on the risk-pricing in dynamic term structure models (DTSMs) tighten the link between cross-sectional and time-series variation of interest rates, and make absence of arbitrage useful for inference about expectations. This article presents a new econometric framework for estimation of affine Gaussian DTSMs under restrictions on risk prices, which addresses the issues of a large model space and of model uncertainty using a Bayesian approach. A simulation study demonstrates the good performance of the proposed method. Data for U.S. Treasury yields calls for tight restrictions on risk pricing: only level risk is priced, and only changes in the slope affect term premia. Incorporating the restrictions changes the model-implied short-rate expectations and term premia. Interest rate persistence is higher than in a maximally flexible model, hence expectations of future short rates are more variable—restrictions on risk prices help resolve the puzzle of implausibly stable short-rate expectations in this literature. Consistent with survey evidence and conventional macro wisdom, restricted models attribute a large share of the secular decline in long-term interest rates to expectations of future nominal short rates. Supplementary materials for this article are available online. © 2018 American Statistical Association."
1,10.1080/07350015.2016.1141096,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044229110&doi=10.1080%2f07350015.2016.1141096&partnerID=40&md5=21bebda8c36afdf5d0f30f0a4be3be57,"Email marketing has been an increasingly important tool for today’s businesses. In this article, we propose a counting-process-based Bayesian method for quantifying the effectiveness of email marketing campaigns in conjunction with customer characteristics. Our model explicitly addresses the seasonality of data, accounts for the impact of customer characteristics on their purchasing behavior, and evaluates effects of email offers as well as their interactions with customer characteristics. Using the proposed method, together with a propensity-score-based unit-matching technique for alleviating potential confounding, we analyze a large email marketing dataset of an online ticket marketplace to evaluate the short- and long-term effectiveness of their email campaigns. It is shown that email offers can increase customer purchase rate both immediately and during a longer term. Customers’ characteristics such as length of shopping history, purchase recency, average ticket price, average ticket count, and number of genres purchased also affect customers’ purchase rate. A strong positive interaction is uncovered between email offer and purchase recency, suggesting that customers who have been inactive recently are more likely to take advantage of promotional offers. Supplementary materials for this article are available online. © 2018 American Statistical Association."
,10.1080/03639045.2017.1405430,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035766963&doi=10.1080%2f03639045.2017.1405430&partnerID=40&md5=f8b3888bdd18562fc7005894b473192c,"In this paper, we propose a stochastic gamma process model for assessing the similarity of two dissolution profiles. Based on the proposed stochastic model, we utilize the difference factor and similarity factor to test the similarity of two dissolution profiles based on bootstrap confidence intervals. The performances of the proposed methods are compared with a multivariate test procedure via Monte Carlo simulation studies. The proposed testing methods are shown to be powerful and effectively controlling the error rate. The proposed model provides a simple yet useful alternative parametric statistical model for assessing the similarity of two dissolution profiles. All the methods are illustrated with a numerical example. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1093/jas/sky041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045137534&doi=10.1093%2fjas%2fsky041&partnerID=40&md5=af7cb947a828b1281e3b00504c9b0985,"Reproductive performance is the most important component of cattle production from the standpoint of economic sustainability of commercial beef enterprises. Heifer Pregnancy (HPG) and Stayability (STAY) genetic predictions are 2 selection tools published by the Red Angus Association of America (RAAA) to assist with improvements in reproductive performance. Given the importance of HPG and STAY to the profitability of commercial beef enterprises, the objective of this study was to identify QTL associated with both HPG and STAY in Red Angus cattle. A genome-wide association study (GWAS) was performed using deregressed HPG and STAY EBV, calculated using a single-trait animal model and a 3-generation pedigree with data from the Spring 2015 RAAA National Cattle Evaluation. Each individual animal possessed 74,659 SNP genotypes. Individual animals with a deregressed EBV reliability &gt; 0.05 were merged with the genotype file and marker quality control was performed. Criteria for sifting genotypes consisted of removing those markers where any of the following were found: average call rate less than 0.85, minor allele frequency &lt; 0.01, lack of Hardy–Weinberg equilibrium (P &lt; 0.0001), or extreme linkage dis-equilibrium (r2 &gt; 0.99). These criteria resulted in 2,664 animals with 62,807 SNP available for GWAS. Association studies were performed using a Bayes Cπ model in the BOLT software package. Marker significance was calculated as the posterior probability of inclusion (PPI), or the number of instances a specific marker was sampled divided by the total number of samples retained from the Markov chain Monte Carlo chains. Nine markers, with a PPI ≥ 3% were identified as QTL associated with HPG on BTA 1, 11, 13, 23, and 29. Twelve markers, with a PPI ≥ 75% were identified as QTL associated with STAY on BTA 6, 8, 9, 12, 15, 18, 22, and 23. © The Author(s) 2018."
,10.1080/01621459.2017.1286241,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048085888&doi=10.1080%2f01621459.2017.1286241&partnerID=40&md5=244d057eb665c5685114de4ca4630ce9,"We introduce a wavelet-domain method for functional analysis of variance (fANOVA). It is based on a Bayesian hierarchical model that employs a graphical hyperprior in the form of a Markov grove (MG)—that is, a collection of Markov trees—for linking the presence/absence of factor effects at all location-scale combinations, thereby incorporating the natural clustering of factor effects in the wavelet-domain across locations and scales. Inference under the model enjoys both analytical simplicity and computational efficiency. Specifically, the posterior of the full hierarchical model is available in closed form through a pyramid algorithm operationally similar to Mallat’s pyramid algorithm for discrete wavelet transform (DWT), achieving for exact Bayesian inference the same computational efficiency—linear in both the number of observations and the number of locations—as for carrying out the DWT. In particular, posterior probabilities of the presence of factor contributions to functional variation are directly available from the pyramid algorithm, while posterior samples for the factor effects can be drawn directly from the exact posterior through standard (not Markov chain) Monte Carlo. We investigate the performance of our method through extensive simulation and show that it substantially outperforms existing wavelet-domain fANOVA methods in a variety of common settings. We illustrate the method through analyzing the orthosis data. Supplementary materials for this article are available online. © 2018, © 2018 American Statistical Association."
,10.1080/15366367.2018.1437306,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044833588&doi=10.1080%2f15366367.2018.1437306&partnerID=40&md5=3be3e3c88e1a5d2334b024f0a5ae6c0c,"Interest in Bayesian analysis of item response theory (IRT) models has grown tremendously due to the appeal of the paradigm among psychometricians, advantages of these methods when analyzing complex models, and availability of general-purpose software. Possible models include models which reflect multidimensionality due to designed test structure, construct-irrelevant variance, and mixed item formats. Using Markov Chain Monte Carlo methods, models can be estimated and evaluated for model-fit. In addition to discussing Bayesian analysis, analyses of three IRT models designed to account for extreme response style are illustrated: IRTree, multidimensional nominal response model (MNRM), and modified generalized partial credit model (MPCM). While results indicated there may be little impact of model choice on substantive trait estimates for the data set studied herein, model-fit results favored the MNRM and MPCM over the IRTree Model. © 2018 Taylor & Francis."
,10.1080/15598608.2017.1343693,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025596312&doi=10.1080%2f15598608.2017.1343693&partnerID=40&md5=5c10cc66fa7535ea8f353901fa9eb705,"In recent years, there have been many efforts to develop a new statistical distribution with more flexibility that can be fitted well to complex data. In this article we consider the statistical inference of the six-parameter McDonald extended Weibull distribution (McEW) based on the progressively Type-II censored sample. The maximum likelihood estimates (MLEs) of the six parameters and their asymptotic distribution are obtained. Based on the asymptotic distribution, the asymptotic confidence limits of its parameters can be computed. We also propose bootstrap confidence intervals of the parameters. The Bayes estimates and the associated highest posterior density credible intervals are computed using the Markov-chain Monte Carlo (MCMC) method, including the Gibbs sampling technique and Metropolis–Hastings algorithm. Simulation experiments are performed to compare the proposed methods and the corresponding confidence intervals under the different censoring schemes. Finally, concluding remarks are given. © 2018 Grace Scientific Publishing, LLC."
2,10.1080/01621459.2016.1264957,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041512198&doi=10.1080%2f01621459.2016.1264957&partnerID=40&md5=f4b2d68cbd6b796c88395857482ca931,"Decision tree ensembles are an extremely popular tool for obtaining high-quality predictions in nonparametric regression problems. Unmodified, however, many commonly used decision tree ensemble methods do not adapt to sparsity in the regime in which the number of predictors is larger than the number of observations. A recent stream of research concerns the construction of decision tree ensembles that are motivated by a generative probabilistic model, the most influential method being the Bayesian additive regression trees (BART) framework. In this article, we take a Bayesian point of view on this problem and show how to construct priors on decision tree ensembles that are capable of adapting to sparsity in the predictors by placing a sparsity-inducing Dirichlet hyperprior on the splitting proportions of the regression tree prior. We characterize the asymptotic distribution of the number of predictors included in the model and show how this prior can be easily incorporated into existing Markov chain Monte Carlo schemes. We demonstrate that our approach yields useful posterior inclusion probabilities for each predictor and illustrate the usefulness of our approach relative to other decision tree ensemble approaches on both simulated and real datasets. Supplementary materials for this article are available online. © 2018, © 2018 American Statistical Association."
,10.1080/10807039.2017.1394174,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035123230&doi=10.1080%2f10807039.2017.1394174&partnerID=40&md5=bd52f8c810e3df6846246e29a9fe868d,"The influence of various heterogeneous parameters, stochastic uncertain factors, and pollutant particles from the industrial effluents in the water system is investigated using advection dispersion equation (ADE) and the Bayesian approximation. Here, the decay coefficient is decomposed into the exact part and the deviation part. The coefficient is used to find out the errors and deviation in decay during the flow of pollutants. Two Bayesian models are developed to analyze the posterior distributions and to find out the Bayes factor for the stochastic covariance estimation. The Bayesian calibration focused the characteristics of both on mechanistic and statistical approximation. The efficiency and accuracy of the developed models are checked from the results obtained on the basis of the confidence interval. Markov chain Monte Carlo simulation is used to acquire the convergence point of parameters for the posterior estimation. The stochastic covariance or white noise represents the effect of random factors on the river system. The analysis revealed that the rate of decay is dependent upon the duration and distance traveled by the pollutants. The collaboration of ADE and Bayesian approximation encourage the water-quality management and environmental modeling. © 2018 Taylor & Francis Group, LLC."
1,10.1080/10618600.2017.1366913,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046690915&doi=10.1080%2f10618600.2017.1366913&partnerID=40&md5=b5311952640d24ce40ac026cdf59cc03,"Topic models, and more specifically the class of latent Dirichlet allocation (LDA), are widely used for probabilistic modeling of text. Markov chain Monte Carlo (MCMC) sampling from the posterior distribution is typically performed using a collapsed Gibbs sampler. We propose a parallel sparse partially collapsed Gibbs sampler and compare its speed and efficiency to state-of-the-art samplers for topic models on five well-known text corpora of differing sizes and properties. In particular, we propose and compare two different strategies for sampling the parameter block with latent topic indicators. The experiments show that the increase in statistical inefficiency from only partial collapsing is smaller than commonly assumed, and can be more than compensated by the speedup from parallelization and sparsity on larger corpora. We also prove that the partially collapsed samplers scale well with the size of the corpus. The proposed algorithm is fast, efficient, exact, and can be used in more modeling situations than the ordinary collapsed sampler. Supplementary materials for this article are available online. © 2018, © 2018 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
3,10.1016/j.cageo.2018.01.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044630004&doi=10.1016%2fj.cageo.2018.01.011&partnerID=40&md5=c7b8e7ac7073f2c1b142725ec63d7e1f,"This paper presents a new computer code developed to solve the 1D magnetotelluric (MT) inverse problem using a Bayesian trans-dimensional Markov chain Monte Carlo algorithm. MT data are sensitive to the depth-distribution of rock electric conductivity (or its reciprocal, resistivity). The solution provided is a probability distribution - the so-called posterior probability distribution (PPD) for the conductivity at depth, together with the PPD of the interface depths. The PPD is sampled via a reversible-jump Markov Chain Monte Carlo (rjMcMC) algorithm, using a modified Metropolis-Hastings (MH) rule to accept or discard candidate models along the chains. As the optimal parameterization for the inversion process is generally unknown a trans-dimensional approach is used to allow the dataset itself to indicate the most probable number of parameters needed to sample the PPD. The algorithm is tested against two simulated datasets and a set of MT data acquired in the Clare Basin (County Clare, Ireland). For the simulated datasets the correct number of conductive layers at depth and the associated electrical conductivity values is retrieved, together with reasonable estimates of the uncertainties on the investigated parameters. Results from the inversion of field measurements are compared with results obtained using a deterministic method and with well-log data from a nearby borehole. The PPD is in good agreement with the well-log data, showing as a main structure a high conductive layer associated with the Clare Shale formation. In this study, we demonstrate that our new code go beyond algorithms developend using a linear inversion scheme, as it can be used: (1) to by-pass the subjective choices in the 1D parameterizations, i.e. the number of horizontal layers in the 1D parameterization, and (2) to estimate realistic uncertainties on the retrieved parameters. The algorithm is implemented using a simple MPI approach, where independent chains run on isolated CPU, to take full advantage of parallel computer architectures. In case of a large number of data, a master/slave appoach can be used, where the master CPU samples the parameter space and the slave CPUs compute forward solutions. © 2018 The Authors"
,10.13195/j.kzyjc.2017.0213,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048893443&doi=10.13195%2fj.kzyjc.2017.0213&partnerID=40&md5=d4dbcda3fe15b4c5cf7a9382fb9623fb,"The prediction models are the basis for scientific formulation of emergency disposal and rescue measures. In order to quickly and accurately construct the forecasting model of sudden water pollution accidents, this paper regards the problem as the Bayesian estimation problem and obtains the posterior probability density function of the model parameters according to the finite difference method and Bayesian inference. Then, by using the improved Metropolis-Hastings sampling method, more reasonable prediction model parameters are obtained. Taking the sudden water pollution event in a certain open channel as an example, the effects of different observation noises on the parameters calibration results are discussed under the two scenarios of non-uniform flow control with non-uniform flow and non-equal capacity control, and compared with the parameter and true value of the Bayesian-Markov chain Monte Carlo method. The results show that, the improved Bayesian-Markov chain Monte Carlo method can give a better parameter value, more application and has a good anti-noise, which can provide a new approach to build the forecast model of sudden water pollution accidents. © 2018, Editorial Office of Control and Decision. All right reserved."
,10.1016/j.soildyn.2018.01.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044639054&doi=10.1016%2fj.soildyn.2018.01.014&partnerID=40&md5=6bfcc768e40c637efd2ead743a9dd4bc,"We conduct numerical experiments to estimate the variability of 1D linear and nonlinear soil amplifications due to the uncertainty in shallow S-wave velocity profiles derived from surface-wave phase velocity inversions using the Markov-chain Monte Carlo method. We first generate synthetic, observed phase velocities of Rayleigh waves for two- and three-layer models of shallow soil. Our final models from sampling can explain well the true S-wave velocity profiles and the phase velocities. We also estimate the uncertainties of each model parameter. A synthetic strong motion is applied to the engineering bedrock of the sampled models to obtain the surface motion assuming linear and nonlinear amplifications. It is found that the nonlinear amplification shows less variability and also has a flatter spectral shape than the linear amplification, particularly at high frequencies. The distributions of ground motion proxies generally have less uncertainty for the nonlinear amplification as well. We also find that the observational errors of the phase velocities have less influence on the variability of the nonlinear amplification than the linear case. This result is caused by the high damping factor applied in the nonlinear soil response. © 2018 Elsevier Ltd"
1,10.1002/stc.2140,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040604080&doi=10.1002%2fstc.2140&partnerID=40&md5=bba025d8329c56917c428333724ed844,"This paper proposes a Bayesian method for structural model updating and damage detection using modal data. A recently developed Markov chain Monte Carlo algorithm is adopted to handle the model updating problem. The proposed Bayesian method focuses on calculation of the posterior probability distribution function of uncertain model parameters. In addition to the most probable values of the uncertain parameters, the associated uncertainties can be calculated with consideration of the effects of both the modeling error and the measurement noise. An experimental case study was carried out with a shear building model under laboratory conditions to study the identifiability of the model-updating problem following the proposed Bayesian method. The results demonstrate the change in the posterior probability distribution function of the uncertain parameters with the amount of measured information. It also demonstrates the ability of the proposed method to handle unidentifiable problems. The proposed Bayesian method is then applied for structural damage detection by calculating the probability distribution of the extent of damage to various structural components. To demonstrate the proposed Bayesian damage-detection method, ambient vibration tests were carried out on a 2-story steel frame with bolted connections. Joint damage was simulated by loosening some bolts at the target beam-column connection. The model-updating results show that the uncertainty associated with the rotational stiffness of the steel joints was very high, rendering the problem almost unidentifiable. Although the problem is almost unidentifiable, the calculated probability distribution of the damage extent can still locate the damaged joint and estimate the damage extent (i.e., the percentage reduction in rotational stiffness) together with the associated uncertainty. Copyright © 2018 John Wiley & Sons, Ltd."
,10.3997/1873-0604.2017064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047529657&doi=10.3997%2f1873-0604.2017064&partnerID=40&md5=ca0b18546473bb4f39cdbe0058a0ce00,"Surface nuclear magnetic resonance is a technique capable of providing insight into subsurface aquifer properties. To produce estimates of aquifer properties (such as the spatial distribution of water content and parameters controlling the duration of the nuclear magnetic resonance signal), an inversion is required. Essential to the reliable interpretation of the estimated subsurface models is an understanding of the uncertainty and correlation between the parameters in the estimated models. To quantify parameter uncertainty and correlation in the surface nuclear magnetic resonance inversion, a Markov chain Monte Carlo approach is demonstrated. Markov chain Monte Carlo approaches have been previously employed to invert surface nuclear magnetic resonance data, but the primary focus has been on quantifying parameter uncertainty. The focus of this paper is to further investigate whether the parameters in the estimated models exhibit correlation with one another; equally important to building a reliable interpretation of the subsurface is an understanding of the parameter uncertainty. The utility of the Markov chain Monte Carlo approach is demonstrated through the investigation of three questions. The first question investigates whether the parameters describing the water content and thickness of a layer exhibit a strong correlation. This question stems from applying concepts known to electromagnetic surveys (that the layer thickness and layer resistivity parameters are strongly correlated) to the surface nuclear magnetic resonance inversion. A water content–layer thickness correlation in surface nuclear magnetic resonance would not have large effects for quantifying total water content but would affect the ability to identify layer boundaries. The second question examines whether the parameter controlling the duration of the nuclear magnetic resonance signal exhibits a correlation with the water content and layer thickness parameters. The resolution of surface nuclear magnetic resonance typically does not consider the duration of the signal and focuses primarily on the distribution of current amplitudes that form the suite of transmit pulses. It is common to treat regions with short-duration signal with greater uncertainty, but it is important to understand whether the signal duration controls resolution for medium to long duration signals as well. The third question explores if the parameter uncertainty produced by the Markov chain Monte Carlo approach is consistent with that produced by an alternative approach based upon the posterior covariance matrix (for the linearised inversion). The ability of the Markov chain Monte Carlo approach to more thoroughly explore the model space provides a means to improve the reliability of surface nuclear magnetic resonance aquifer characterisations by quantifying parameter uncertainty and correlation. © 2018 European Association of Geoscientists & Engineers."
1,10.1007/s13137-017-0101-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044538117&doi=10.1007%2fs13137-017-0101-z&partnerID=40&md5=08a9db2bb7693dd26bc7ce30e285935b,"Monte Carlo methods deal with generating random variates from probability density functions in order to estimate unknown parameters or general functions of unknown parameters and to compute their expected values, variances and covariances. One generally works with the multivariate normal distribution due to the central limit theorem. However, if random variables with the normal distribution and random variables with a different distribution are combined, the normal distribution is not valid anymore. The Monte Carlo method is then needed to get the expected values, variances and covariances for the random variables with distributions different from the normal distribution. The error propagation by Monte Carlo methods is discussed and methods for generating random variates from the multivariate normal distribution and from the multivariate uniform distribution. The Monte Carlo integration is presented leading to the sampling–importance-resampling algorithm. Markov chain Monte Carlo methods provide by the Metropolis algorithm and the Gibbs sampler additional ways of generating random variates. A special topic is the Gibbs sampler for computing and propagating large covariance matrices. This task arises, for instance, when the geopotential is determined from satellite observations. The example of the minimal detectable outlier shows, how the Monte Carlo method is used to determine the power of a hypothesis test. © 2017, Springer-Verlag GmbH Germany, part of Springer Nature."
2,10.1016/j.dsp.2018.01.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041405475&doi=10.1016%2fj.dsp.2018.01.004&partnerID=40&md5=fa5d06a065568755e65f502d30318925,"Many applications in signal processing require the estimation of some parameters of interest given a set of observed data. More specifically, Bayesian inference needs the computation of a-posteriori estimators which are often expressed as complicated multi-dimensional integrals. Unfortunately, analytical expressions for these estimators cannot be found in most real-world applications, and Monte Carlo methods are the only feasible approach. A very powerful class of Monte Carlo techniques is formed by the Markov Chain Monte Carlo (MCMC) algorithms. They generate a Markov chain such that its stationary distribution coincides with the target posterior density. In this work, we perform a thorough review of MCMC methods using multiple candidates in order to select the next state of the chain, at each iteration. With respect to the classical Metropolis–Hastings method, the use of multiple try techniques foster the exploration of the sample space. We present different Multiple Try Metropolis schemes, Ensemble MCMC methods, Particle Metropolis–Hastings algorithms and the Delayed Rejection Metropolis technique. We highlight limitations, benefits, connections and differences among the different methods, and compare them by numerical simulations. © 2018 Elsevier Inc."
,10.1051/ro/2017076,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049627997&doi=10.1051%2fro%2f2017076&partnerID=40&md5=73780c6a022bc567cee0da6c6d294171,"One of the fundamental problems in supply chain management is to design the effective inventory control policies for models with stochastic demands because efficient inventory management can both maintain a high customers' service level and reduce unnecessary over and under-stock expenses which are significant key factors of profit or loss of an organization. In this study, a new formulation of an inventory system is analyzed under discrete Markov-modulated demand. We employ simulation-based optimization that combines simulated annealing pattern search and ranking selection (SAPS&RS) methods to approximate near-optimal solutions of this problem. After determining the values of demand, we employ novel approach to achieve minimum cost of total SCM (Supply Chain Management) network. In our proposed approach, hybrid improved cuckoo search algorithm (ICS) and genetic algorithm (GA) are presented as main platform to solve this problem. The computational results demonstrate the effectiveness and applicability of the proposed approach. © EDP Sciences, ROADEF, SMAI 2018."
1,10.1142/S0218539318500109,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040703321&doi=10.1142%2fS0218539318500109&partnerID=40&md5=a9018d69311048309664d8920a41a532,"The three-parameter Burr type XII distribution (3pBXIID) is quite flexible and contains a wide range of distribution shapes for fitting lifetime data. However, it is difficult to obtain reliable estimates of the 3pBXIID quantiles from censored samples for evaluating the reliability of lifetime data. In this work, a Metropolis-Hastings Markov chain Monte Carlo (M-H MCMC) procedure is proposed to obtain reliable maximum likelihood estimates (MLEs) of the 3pBXIID quantiles from a type II censored sample. Moreover, the parametric bootstrap percentile procedure is used to obtain the confidence interval of the quantile of the 3pBXIID. The performance of the proposed M-H MCMC method is evaluated in view of Monte Carlo simulations. Two examples, regarding the survival lifetimes of breast cancer patients and the reliability inference on the lifetimes of oil-well pumps for sucker-rod oil pumping systems, are applied to illustrate the applications of the proposed M-H MCMC method and bootstrap procedure. © 2018 World Scientific Publishing Company."
1,10.1016/j.jhydrol.2018.02.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043474435&doi=10.1016%2fj.jhydrol.2018.02.026&partnerID=40&md5=65272fe4e1e3756e3b484e9ab9de144f,"This essay illustrates some recent developments to the DiffeRential Evolution Adaptive Metropolis (DREAM) MATLAB toolbox of Vrugt (2016) to delineate and sample the behavioural solution space of set-theoretic likelihood functions used within the GLUE (Limits of Acceptability) framework (Beven and Binley, 1992, 2014; Beven and Freer, 2001; Beven, 2006). This work builds on the DREAM(ABC) algorithm of Sadegh and Vrugt (2014) and enhances significantly the accuracy and CPU-efficiency of Bayesian inference with GLUE. In particular it is shown how lack of adequate sampling in the model space might lead to unjustified model rejection. © 2018 Elsevier B.V."
,10.5705/ss.202016.0378,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045687807&doi=10.5705%2fss.202016.0378&partnerID=40&md5=cb07fd6dd4081a30450be00889fd3452,"The naive importance sampling estimator, based on samples from a single importance density, can be numerically unstable. We consider generalized importance sampling estimators where samples from more than one probability distribution are combined. We study this problem in the Markov chain Monte Carlo context, where independent samples are replaced with Markov chain samples. If the chains converge to their respective target distributions at a polynomial rate, then under two finite moment conditions, we show a central limit theorem holds for the generalized estimators. We develop an easy-to-implement method to calculate valid asymptotic standard errors based on batch means. We provide a batch means estimator for calculating asymptotically valid standard errors of Geyer's (1994) reverse logistic estimator. We illustrate the method via three examples. In particular, the generalized importance sampling estimator is used for Bayesian spatial modeling of binary data and to perform empirical Bayes variable selection where the batch means estimator enables standard error calculations in high-dimensional settings. © 2018 Institute of Statistical Science. All rights reserved."
,10.1002/net.21805,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040249259&doi=10.1002%2fnet.21805&partnerID=40&md5=3f32299533c375c94d33995ff5deecce,"Counting the number of independent sets is an important problem in graph theory, combinatorics, optimization, and social sciences. However, a polynomial-time exact calculation, or even a reasonably close approximation, is widely believed to be impossible, since their existence implies an efficient solution to various problems in the non-deterministic polynomial-time complexity class. To cope with the approximation challenge, we express the independent set counting problem as a rare-event estimation problem. We develop a multilevel splitting algorithm which is generally capable of delivering accurate results, while using a manageable computational effort, even when applied to large graphs. We apply the algorithm to both counting and optimization (finding a maximum independent set) problems, and show that it compares favorably with the existing state of the art. © 2018 Wiley Periodicals, Inc."
,10.1016/j.jeconom.2017.11.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042878221&doi=10.1016%2fj.jeconom.2017.11.009&partnerID=40&md5=ea6217ce4788c58cd2bb70abb5683e41,"Vector autoregressive (VAR) models are the main work-horse models for macroeconomic forecasting, and provide a framework for the analysis of complex dynamics that are present between macroeconomic variables. Whether a classical or a Bayesian approach is adopted, most VAR models are linear with Gaussian innovations. This can limit the model's ability to explain the relationships in macroeconomic series. We propose a nonparametric VAR model that allows for nonlinearity in the conditional mean, heteroscedasticity in the conditional variance, and non-Gaussian innovations. Our approach differs from that of previous studies by modelling the stationary and transition densities using Bayesian nonparametric methods. Our Bayesian nonparametric VAR (BayesNP-VAR) model is applied to US and UK macroeconomic time series, and compared to other Bayesian VAR models. We show that BayesNP-VAR is a flexible model that is able to account for nonlinear relationships as well as heteroscedasticity in the data. In terms of short-run out-of-sample forecasts, we show that BayesNP-VAR predictively outperforms competing models. © 2018 The Authors"
1,10.1007/s10208-016-9340-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995773214&doi=10.1007%2fs10208-016-9340-x&partnerID=40&md5=6821402a9d52732c4951b9826c27e685,"Metropolis algorithms for approximate sampling of probability measures on infinite dimensional Hilbert spaces are considered, and a generalization of the preconditioned Crank–Nicolson (pCN) proposal is introduced. The new proposal is able to incorporate information on the measure of interest. A numerical simulation of a Bayesian inverse problem indicates that a Metropolis algorithm with such a proposal performs independently of the state-space dimension and the variance of the observational noise. Moreover, a qualitative convergence result is provided by a comparison argument for spectral gaps. In particular, it is shown that the generalization inherits geometric convergence from the Metropolis algorithm with pCN proposal. © 2016, SFoCM."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047839332&partnerID=40&md5=bd5a5b9fca658d3066cf6a6dba55aa63,"Ranking and comparing items is crucial for collecting information about preferences in many areas, from marketing to politics. The Mallows rank model is among the most successful approaches to analyse rank data, but its computational complexity has limited its use to a particular form based on Kendall distance. We develop new computationally tractable methods for Bayesian inference in Mallows models that work with any right-invariant distance. Our method performs inference on the consensus ranking of the items, also when based on partial rankings, such as top-k items or pairwise comparisons. We prove that items that none of the assessors has ranked do not influence the maximum a posteriori consensus ranking, and can therefore be ignored. When assessors are many or heterogeneous, we propose a mixture model for clustering them in homogeneous subgroups, with cluster-specific consensus rankings. We develop approximate stochastic algorithms that allow a fully probabilistic analysis, leading to coherent quantifications of uncertainties. We make probabilistic predictions on the class membership of assessors based on their ranking of just some items, and predict missing individual preferences, as needed in recommendation systems. We test our approach using several experimental and benchmark datasets. c 2018 Valeria Vitelli, Øystein Sørensen, Marta Crispino, Arnoldo Frigessi and Elja Arjas."
1,10.1016/j.im.2017.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027521493&doi=10.1016%2fj.im.2017.08.001&partnerID=40&md5=5f7ec6d38377710d817fd7643263631e,"This study investigates what influences consumers’ extent of online search (i.e., the number of relevant web stores visited) before a purchase. A dataset containing website visitation and transaction activities from a panel of US consumers is used to test the hypotheses developed in the study. The results indicate a diminishing effect of competitive density on the extent of search, and the use of advanced information technologies induces more searches. Consumers also search more for experience products than for search products in contrast to the prediction in the nonelectronic market. Furthermore, online purchase experience increases, while product-specific experience reduces, prepurchase search. © 2017 Elsevier B.V."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048047668&partnerID=40&md5=91757f78b18e8cdb83ced5d16e4ec71c,"Latent Dirichlet Allocation (LDA) is a well known topic model that is often used to make inference regarding the properties of collections of text documents. LDA is a hierarchical Bayesian model, and involves a prior distribution on a set of latent topic variables. The prior is indexed by certain hyperparameters, and even though these have a large impact on inference, they are usually chosen either in an ad-hoc manner, or by applying an algorithm whose theoretical basis has not been firmly established. We present a method, based on a combination of Markov chain Monte Carlo and importance sampling, for estimating the maximum likelihood estimate of the hyperparameters. The method may be viewed as a computational scheme for implementation of an empirical Bayes analysis. It comes with theoretical guarantees, and a key feature of our approach is that we provide theoretically-valid error margins for our estimates. Experiments on both synthetic and real data show good performance of our methodology. Keywords: Empirical Bayes inference, latent Dirichlet allocation, Markov chain Monte Carlo, model selection, topic modelling. © 2018 Clint P. George and Hani Doss."
,10.1177/0962280216659312,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042844754&doi=10.1177%2f0962280216659312&partnerID=40&md5=0915106a495df3fa369314bcddd9cca5,"Longitudinal zero-inflated count data are encountered frequently in substance-use research when assessing the effects of covariates and risk factors on outcomes. Often, both the time to a terminal event such as death or dropout and repeated measure count responses are collected for each subject. In this setting, the longitudinal counts are censored by the terminal event, and the time to the terminal event may depend on the longitudinal outcomes. In the study described herein, we expand the class of joint models for longitudinal and survival data to accommodate zero-inflated counts and time-to-event data by using a Cox proportional hazards model with piecewise constant baseline hazard. We use a Bayesian framework via Markov chain Monte Carlo simulations implemented in the BUGS programming language. Via an extensive simulation study, we apply the joint model and obtain estimates that are more accurate than those of the corresponding independence model. We apply the proposed method to an alpha-tocopherol, beta-carotene lung cancer prevention study. © 2016, © The Author(s) 2016."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047752050&partnerID=40&md5=7f037032bac92d28fcbe20acbf3df491,"To develop a traffic conflict model at signalized intersections, traffic conflict data were extracted from 260 h of video data at 29 signalized intersections. Accounting for the traffic conflict heterogeneity, the random parameters Poisson-lognormal (RP-PLN) traffic conflict model and the random effects Poisson-lognormal (RE-PLN) traffic conflict model were developed. The posterior distributions of the models' parameters were estimated by the Bayesian method and the Markov chain Monte Carlo (MCMC) simulation. Using the deviance information criterion and the residual standardization decision coefficient, the goodness-of-fit of the models were compared. The results show that both the traffic conflict models can handle the traffic conflict heterogeneity; however, the goodness-of-fit of the RP-PLN traffic conflict model performs the RE-PLN traffic conflict model. Given that the other variables remained unchanged, a 1% increase in the crossing-through volume could increase the right-turn & crossing-through (RC) traffic conflict by 0.54%; a 1% increase in the right-turn volume could increase the RC traffic conflict by 0.65%; a 1% increase in the proportion of large vehicles could increase the RC traffic conflict by 1.06%; the raised and pavement channelized islands could decrease the RC traffic conflict by 18.9% and 17.3%, respectively; the acceleration lane could decrease the RC traffic conflict by 22.9%; a 1% increase in the right-turn radius could decrease the RC traffic conflict by 10.5%; the installation of a right-turn yield sign could decrease the RC traffic conflict by 16.5%; the setting up of the protected right-turn phase could decrease the RC traffic conflict by 29.8%. © 2018, Editorial Department of China Journal of Highway and Transport. All right reserved."
,10.1051/0004-6361/201731313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046660610&doi=10.1051%2f0004-6361%2f201731313&partnerID=40&md5=cce3513bb6a3f8ab251af9cfcf2a9b85,"Aims. We fit a log-normal function to the M-dwarf orbital surface density distribution of gas giant planets, over the mass range 1-10 times that of Jupiter, from 0.07 to 400 AU. Methods. We used a Markov chain Monte Carlo approach to explore the likelihoods of various parameter values consistent with point estimates of the data given our assumed functional form. Results. This fit is consistent with radial velocity, microlensing, and direct-imaging observations, is well-motivated from theoretical and phenomenological points of view, and predicts results of future surveys. We present probability distributions for each parameter and a maximum likelihood estimate solution. Conclusions. We suggest that this function makes more physical sense than other widely used functions, and we explore the implications of our results on the design of future exoplanet surveys. © ESO 2018."
1,10.1016/j.ejor.2017.09.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030452527&doi=10.1016%2fj.ejor.2017.09.014&partnerID=40&md5=9c2446a7a5a043658812b694fdeecc93,"In large cities in emerging economies, traditional retail is present in a very high density, with multiple independently owned small stores in each city block. Consequently, when faced with a stockout, consumers may not only substitute with a different product in the same store, but also switch to a neighboring store. Suppliers may take advantage of this behavior by strategically supplying these stores in a coherent manner. We study this problem using consumer choice models. We build two consumer choice models for this consumer behavior. First, we build a Nested Logit model for the consumer choice process, where the consumer chooses the store at the first level and selects the product at the second level. Then, we consider an Exogenous Substitution model. In both models, a consumer may substitute at either the store level or the product level. Furthermore, we estimate the parameters of the two models using a Markov chain Monte Carlo algorithm in a Bayesian manner. We numerically find that the Nested Logit model outperforms the Exogenous Substitution model in estimating substitution probabilities. Further, the information on consumers’ purchase records helps improve the estimation accuracies of both the first-choice probabilities and the substitution probabilities when the beginning inventory level is low. Finally, we show that explicitly including such substitution behavior in the inventory optimization process can significantly increase the expected profit. © 2017 Elsevier B.V."
,10.3390/en11040802,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045415099&doi=10.3390%2fen11040802&partnerID=40&md5=ba4b5591e8a256cb9162e7b01272b273,"The performance gap between the expected and actual energy performance of buildings and elements has stimulated interest in in-situ measurements. Most research has employed quasi-static analysis methods that estimate heat loss metrics such as U-values, without taking advantage of the rich time series data that is often recorded. This paper presents a dynamic Bayesian-based method to estimate the thermophysical properties of building elements from in-situ measurements. The analysis includes Markov chain Monte Carlo (MCMC) estimation, priors, uncertainty analysis, and model comparison to select the most appropriate model. Data from two case study dwellings is used to illustrate model performance; U-value estimates from the dynamic and static methods are within error estimates, with the dynamic model generally requiring much shorter time series than the static model. The dynamic model produced robust results at all times of year, including when the average indoor-to-outdoor temperature difference was low, when external temperatures had large daily variation, and measurements were subjected to direct solar radiation. Further, the probability distributions of parameters may provide insights into the thermal performance of elements. Dynamic methods such as that presented herein may enable wider characterisation of the performance of building elements as built, supporting work to reduce the performance gap. © 2018 by the authors."
,10.1007/s12046-018-0861-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045691299&doi=10.1007%2fs12046-018-0861-7&partnerID=40&md5=b7eea394315ca6c2d54f4f87d95dfe4b,"This paper reports the estimation of the unknown boundary heat flux from a fin using the Bayesian inference method. The setup consists of a rectangular mild steel fin of dimensions 250×150×6 mm3 and an aluminium base plate of dimensions 250×150×8 mm3. The fin is subjected to constant heat flux at the base and the fin setup is modelled using ANSYS14.5. The problem considered is a conjugate heat transfer from the fin, and the Navier–Stokes equation is solved to obtain the flow parameters. Grid independence study is carried out to fix the number of grids for the study considered. To reduce the computational cost, computational fluid dynamics (CFD) is replaced with artificial neural network (ANN) as the forward model. The Markov Chain Monte Carlo (MCMC) powered by Metropolis–Hastings sampling algorithm along with the Bayesian framework is used to explore the estimation space. The sensitivity analysis of the estimated temperature with respect to the unknown parameter is discussed to know the dependency of the temperature with the parameter. This paper signifies the effect of a prior model on the execution of the inverse algorithm at different noise levels. The unknown heat flux is estimated for the surrogated temperature and the estimates are reported as mean, Maximum a Posteriori (MAP) and standard deviation. The effect of a-priori information on the estimated parameter is also addressed. The standard deviation in the estimation process is referred to as the uncertainty associated with the estimated parameters. © 2018, Indian Academy of Sciences."
,10.1016/j.jcde.2017.10.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044143240&doi=10.1016%2fj.jcde.2017.10.002&partnerID=40&md5=45950d2a75684470e59de2780f3ba642,"In this study, a model for probabilistic fatigue life that is based on the Zhurkov model is suggested using stochastically and statistically estimated lethargy coefficients. The fatigue life model was derived using the Zhurkov life model, and it was deterministically validated using real fatigue life data as a reference. For this process, firstly, a lethargy coefficient that is related to the failure of materials must be obtained with rupture time and stress from a quasi-static tensile test. These experiments are performed using HS40R steel. However, the lethargy coefficient has discrepancies due to the inherent uncertainty and the variation of material properties in the experiments. The Bayesian approach was employed for estimating the lethargy coefficient of the fatigue life model using the Markov Chain Monte Carlo (MCMC) sampling method and considering its uncertainties. Once the samples are obtained, one can proceed to the posterior predictive inference of the fatigue life. This life model was shown to be reasonable when compared with experimental fatigue life data. As a result, predicted fatigue life was observed to significantly decrease in accordance with increasing relative stress conditions. © 2017 Society for Computational Design and Engineering"
1,10.3390/e20040262,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045839139&doi=10.3390%2fe20040262&partnerID=40&md5=bd3b8b8d9e12ed32afb65765464cc888,"In a regression analysis, a sample-selection bias arises when a dependent variable is partially observed as a result of the sample selection. This study introduces a Maximum Entropy (MaxEnt) process regression model that assumes aMaxEnt prior distribution for its nonparametric regression function and finds that the MaxEnt process regression model includes the well-known Gaussian process regression (GPR) model as a special case. Then, this special MaxEnt process regression model, i.e., the GPR model, is generalized to obtain a robust sample-selection Gaussian process regression (RSGPR) model that deals with non-normal data in the sample selection. Various properties of the RSGPR model are established, including the stochastic representation, distributional hierarchy, and magnitude of the sample-selection bias. These properties are used in the paper to develop a hierarchical Bayesianmethodology to estimate themodel. This involves a simple and computationally feasible Markov chain Monte Carlo algorithm that avoids analytical or numerical derivatives of the log-likelihood function of the model. The performance of the RSGPR model in terms of the sample-selection bias correction, robustness to non-normality, and prediction, is demonstrated through results in simulations that attest to its good finite-sample performance. © 2018 by the authors."
,10.1016/j.spasta.2018.03.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045698255&doi=10.1016%2fj.spasta.2018.03.002&partnerID=40&md5=e227a5805715b4b269e39e5cbbd7d093,"Spatial econometric models have been widely used for analyzing cross-sectional data in which spatial dependence is of primary interest. Although proven successful, Bayesian estimation via Markov chain Monte Carlo (MCMC) for spatial econometric models can be computationally demanding as the size of data and complexity of models grow. This paper proposes two variational Bayes methods that are more scalable and computationally faster in estimating general spatial autoregressive and matrix exponential spatial specification models: the hybrid mean-field variational Bayes (MFVB) method and the integrated nonfactorized variational Bayes (INFVB) method. The hybrid MFVB method assumes posterior independence and, when applicable, can yield accurate results but tends to underestimate posterior variances. In comparison, the INFVB method provides more robust results by accounting for posterior dependence and is computationally appealing due to parallelization. We demonstrate that variational Bayesian inference can be a faster and more scalable alternative to the MCMC approach for Bayesian spatial econometric modeling. The effectiveness of our proposed methods for spatial econometric models is demonstrated through simulated examples and a real-world data application. © 2018 Elsevier B.V."
,10.1007/s41549-018-0027-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045751686&doi=10.1007%2fs41549-018-0027-z&partnerID=40&md5=c0299cef7ba9a95e305e4986de0710e0,"This study examines the impact of outlier-adjusted data on business cycle inferences using coincident indicators of the composite index (CI) in Japan. To estimate the CI and business cycles, this study proposes a Markov switching dynamic factor model incorporating Student’s t-distribution in both the idiosyncratic noise and the factor equation. Furthermore, the model includes a stochastic volatility process to identify whether a large shock is associated with a business cycle. From the empirical analysis, both the factor and the idiosyncratic component have fat-tail error distributions, and the estimated CI and recession probabilities are close to those published by the Economic and Social Research Institute. Compared with the estimated CI using the adjusted data set, the outlier adjustment reduces the depth of the recession. Moreover, the results of the shock decomposition show that the financial crisis in mid-2008 was caused by increase of clustering shocks and large unexpected shocks. In contrast, the Great East Japan Earthquake in 2011 was derived from idiosyncratic noise and did not cause a recession. When analyzing whether to use a sample that includes outliers associated with the business cycle, it is not desirable to use the outlier-adjusted data set. © 2018, Springer International Publishing AG, part of Springer Nature."
1,10.1007/s00357-018-9248-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044072631&doi=10.1007%2fs00357-018-9248-z&partnerID=40&md5=b81209221d53bf89fe41396e6f6775ab,"This paper discusses the challenges presented by tall data problems associated with Bayesian classification (specifically binary classification) and the existing methods to handle them. Current methods include parallelizing the likelihood, subsampling, and consensus Monte Carlo. A new method based on the two-stage Metropolis-Hastings algorithm is also proposed. The purpose of this algorithm is to reduce the exact likelihood computational cost in the tall data situation. In the first stage, a new proposal is tested by the approximate likelihood based model. The full likelihood based posterior computation will be conducted only if the proposal passes the first stage screening. Furthermore, this method can be adopted into the consensus Monte Carlo framework. The two-stage method is applied to logistic regression, hierarchical logistic regression, and Bayesian multivariate adaptive regression splines. © 2018, Classification Society of North America."
,10.1002/ecy.2174,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043711634&doi=10.1002%2fecy.2174&partnerID=40&md5=ef4de60b1dcb7ba852b317a34b442d39,"Recently there have been major theoretical advances in the quantification and partitioning of diversity within and among communities, regions, and ecosystems. However, applying those advances to real data remains a challenge. Ecologists often end up describing their samples rather than estimating the diversity components of an underlying study system, and existing approaches do not easily provide statistical frameworks for testing ecological questions. Here we offer one avenue to do all of the above using a hierarchical Bayesian approach. We estimate posterior distributions of the underlying “true” relative abundances of each species within each unit sampled. These posterior estimates of relative abundance can then be used with existing formulae to estimate and partition diversity. The result is a posterior distribution of diversity metrics describing our knowledge (or beliefs) about the study system. This approach intuitively leads to statistical inferences addressing biologically motivated hypotheses via Bayesian model comparison. Using simulations, we demonstrate that our approach does as well or better at approximating the “true” diversity of a community relative to naïve or ad-hoc bias-corrected estimates. Moreover, model comparison correctly distinguishes between alternative hypotheses about the distribution of diversity within and among samples. Finally, we use an empirical ecological dataset to illustrate how the approach can be used to address questions about the makeup and diversities of assemblages at local and regional scales. © 2018 by the Ecological Society of America"
,10.1177/0962280215626947,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042847045&doi=10.1177%2f0962280215626947&partnerID=40&md5=86c8998fc1dd1ac0c86cc9b7251f43dd,"Accelerated failure time model is a popular model to analyze censored time-to-event data. Analysis of this model without assuming any parametric distribution for the model error is challenging, and the model complexity is enhanced in the presence of large number of covariates. We developed a nonparametric Bayesian method for regularized estimation of the regression parameters in a flexible accelerated failure time model. The novelties of our method lie in modeling the error distribution of the accelerated failure time nonparametrically, modeling the variance as a function of the mean, and adopting a variable selection technique in modeling the mean. The proposed method allowed for identifying a set of important regression parameters, estimating survival probabilities, and constructing credible intervals of the survival probabilities. We evaluated operating characteristics of the proposed method via simulation studies. Finally, we apply our new comprehensive method to analyze the motivating breast cancer data from the Surveillance, Epidemiology, and End Results Program, and estimate the five-year survival probabilities for women included in the Surveillance, Epidemiology, and End Results database who were diagnosed with breast cancer between 1990 and 2000. © 2016, © The Author(s) 2016."
,10.1016/j.econlet.2018.02.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044282225&doi=10.1016%2fj.econlet.2018.02.005&partnerID=40&md5=beb4629ee3827fd542f98905cd7e10dc,"A Bayesian alternative to Zhuo (2018) is presented. The method is of general interest as it presents an explicit formula for the local sensitivity of log marginal likelihood when observations vary by a small amount. The remarkable feature is that the formula is very easy to compute and does not require knowledge of the marginal likelihood which is, invariably, extremely difficult to compute. Similar expressions are derived for posterior moments and other functions of interest, including inefficiency. Methods for examining prior sensitivity in a straightforward way are also presented. The methods are illustrated in the context of a stochastic production frontier. © 2018"
6,10.1534/genetics.117.300489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045020060&doi=10.1534%2fgenetics.117.300489&partnerID=40&md5=9015e91e40365336d1d805e7ca8c16cb,"An open question in human evolution is the importance of polygenic adaptation: adaptive changes in the mean of a multifactorial trait due to shifts in allele frequencies across many loci. In recent years, several methods have been developed to detect polygenic adaptation using loci identified in genome-wide association studies (GWAS). Though powerful, these methods suffer from limited interpretability: they can detect which sets of populations have evidence for polygenic adaptation, but are unable to reveal where in the history of multiple populations these processes occurred. To address this, we created a method to detect polygenic adaptation in an admixture graph, which is a representation of the historical divergences and admixture events relating different populations through time. We developed a Markov chain Monte Carlo (MCMC) algorithm to infer branch-specific parameters reflecting the strength of selection in each branch of a graph. Additionally, we developed a set of summary statistics that are fast to compute and can indicate which branches are most likely to have experienced polygenic adaptation. We show via simulations that this method—which we call PolyGraph—has good power to detect polygenic adaptation, and applied it to human population genomic data from around the world. We also provide evidence that variants associated with several traits, including height, educational attainment, and self-reported unibrow, have been influenced by polygenic adaptation in different populations during human evolution. © 2018 by the Genetics Society of America."
,10.1115/1.4038934,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041903882&doi=10.1115%2f1.4038934&partnerID=40&md5=32aeb23f5cf769a43589d2d86ce02dd8,"Operations and maintenance activities have a significant impact on the energy cost for offshore wind turbines. Analytical methods such as reliability block diagrams and Markov processes along with simulation approaches have been widely used in planning and optimizing operations and maintenance actions in industrial systems. Generalized stochastic Petri nets (GSPNs) with predicates coupled with Monte Carlo simulation (MCS) are applied in this paper to model the planning of operations and maintenance activities of an offshore wind turbine. The merits of GSPN in modeling complex and multicomponent systems are addressed. Three maintenance categories classified according to the size and weight of the components to be replaced and the logistics involved, such as vessels, maintenance crew and spares and, the associated delays, and costs are included in the model. The weather windows for accessing the wind turbine are also modeled. Corrective maintenance (CM) based on replacements and age-dependent preventive maintenance (PM) with imperfect repair are modeled and compared in terms of the wind turbine's performance (e.g., availability and loss production) and of the operations and maintenance costs. © 2018 by ASME."
,10.1016/j.meegid.2018.01.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041509404&doi=10.1016%2fj.meegid.2018.01.021&partnerID=40&md5=74b4308505009398574d56b23a44177b,"We performed detailed genetic analyses of the partial hemagglutinin–neuraminidase (HN) gene in 34 human respirovirus 3 (HRV3) strains from children with acute respiratory illness during 2013–2015 in Iwate Prefecture, Japan. In addition, we performed analyses of the evolutionary timescale of the gene using the Bayesian Markov chain Monte Carlo (MCMC) method. Furthermore, we analyzed pairwise distances and performed selective pressure analyses followed by linear B-cell epitope mapping and N-glycosylation and phylodynamic analyses. A phylogenetic tree showed that the strains diversified at around 1939, and the rate of molecular evolution was 7.6 × 10−4 substitutions/site/year. Although the pairwise distances were relatively short (0.03 ± 0.018 [mean ± standard deviation, SD]), two positive selection sites (Cys544Trp and Leu555Ser) and no amino acid substitutions were found in the active/catalytic sites. Six epitopes were estimated in this study, and three mouse monoclonal antibody binding sites (amino acid positions 278, 281, and 461) overlapped with two epitopes belonging to subcluster C3 strains. Bayesian skyline plot analyses indicated that subcluster C3 strains have been increasing from 2004, whereas subcluster C1 strains have declined from 2004. Based on these results, Iwate strains were divided into two subclusters and each subcluster evolved independently. Moreover, our results suggested that some predicted linear epitopes (epitopes 3 and 5) are candidates for an HRV3 vaccine motif. To better understand the details of the molecular evolution of HRV, further studies are needed. © 2018 Elsevier B.V."
1,10.1371/journal.pone.0195117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045275859&doi=10.1371%2fjournal.pone.0195117&partnerID=40&md5=1cd1313df6f75fb83413733a2eceee7d,"Background Hepatitis C is the second fastest growing infectious disease in China. The standard-of-care for chronic hepatitis C in China is Pegylated interferon plus ribavirin (PR), which is associated with tolerability and efficacy issues. An interferon- and ribavirin-free, all-oral regimen comprising daclatasvir (DCV) and asunaprevir (ASV), which displays higher efficacy and tolerability, has recently been approved in China. Objectives This study is to estimate the cost-effectiveness of DCV+ASV (24 weeks) for chronic hepatitis C genotype 1b treatment-naïve patients compared with PR regimen (48 weeks) in China. Methods A cohort-based Markov model was developed from Chinese payer perspective to project the lifetime outcomes of treating 10,000 patients with an average age of 44.5 with two hypothetical regimens, DCV+ASV and PR. Chinese-specific health state costs and efficacy data were used. The annual discount rate was 5%. Base-case analysis and sensitivity analysis were conducted. Results For HCV Genotype 1b treatment-naïve patients, DCV+ASV proved to be dominant over PR, with a cost saving of ¥33,480(5,096 USD) and gains in QALYs and life years of 1.29 and 0.85, respectively. The lifetime risk of compensated cirrhosis, decompensated cirrhosis, hepatocellular carcinoma and liver-related death was greatly reduced with DCV+ASV. Univariate sensitivity analysis demonstrated that key influencers were the discount rate, time horizon, initial disease severity and sustained virological response rate of DCV+ASV, with all scenarios resulting in additional benefit. Probabilistic sensitivity analysis demonstrated that DCV+ASV has a high likelihood (100%) of being cost-effective. Conclusion DCV+ASV is not only an effective and well-tolerated regimen to treat chronic HCV genotype 1b infection treatment-naïve patients, but also is more cost-effective than PR regimen. DCV +ASV can benefit both the public health and reimbursement system in China. © 2018 Lu et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1177/0962280216660127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042848765&doi=10.1177%2f0962280216660127&partnerID=40&md5=f3efebdd6e4242074ce0dfd6f5da1a37,"Background: Biomarker series can indicate disease progression and predict clinical endpoints. When a treatment is prescribed depending on the biomarker, confounding by indication might be introduced if the treatment modifies the marker profile and risk of failure. Objective: Our aim was to highlight the flexibility of a two-stage model fitted within a Bayesian Markov Chain Monte Carlo framework. For this purpose, we monitored the prostate-specific antigens in prostate cancer patients treated with external beam radiation therapy. In the presence of rising prostate-specific antigens after external beam radiation therapy, salvage hormone therapy can be prescribed to reduce both the prostate-specific antigens concentration and the risk of clinical failure, an illustration of confounding by indication. We focused on the assessment of the prognostic value of hormone therapy and prostate-specific antigens trajectory on the risk of failure. Methods: We used a two-stage model within a Bayesian framework to assess the role of the prostate-specific antigens profile on clinical failure while accounting for a secondary treatment prescribed by indication. We modeled prostate-specific antigens using a hierarchical piecewise linear trajectory with a random changepoint. Residual prostate-specific antigens variability was expressed as a function of prostate-specific antigens concentration. Covariates in the survival model included hormone therapy, baseline characteristics, and individual predictions of the prostate-specific antigens nadir and timing and prostate-specific antigens slopes before and after the nadir as provided by the longitudinal process. Results: We showed positive associations between an increased prostate-specific antigens nadir, an earlier changepoint and a steeper post-nadir slope with an increased risk of failure. Importantly, we highlighted a significant benefit of hormone therapy, an effect that was not observed when the prostate-specific antigens trajectory was not accounted for in the survival model. Conclusion: Our modeling strategy was particularly flexible and accounted for multiple complex features of longitudinal and survival data, including the presence of a random changepoint and a time-dependent covariate. © 2016, © The Author(s) 2016."
,10.1371/journal.pone.0196548,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046012648&doi=10.1371%2fjournal.pone.0196548&partnerID=40&md5=38475a49ee46c296de4c1c27f97da1d0,"Background Yunnan has the greatest share of reported human immunodeficiency virus (HIV)/acquired immunodeficiency syndrome (AIDS) cases in China. In recent years, HIV prevalence and incidence remained stubbornly high in men who have sex with men (MSM). To follow the dynamics of the HIV-1 epidemic among MSM, HIV-1 genetic characteristics and genetic transmission networks were investigated. Methods Blood samples from 190 newly diagnosed HIV-1 cases among MSM were continuously collected at fixed sites from January 2013 to December 2015 in Kunming City, Yunnan Province. Partial gag, pol and env genes were sequenced and used for phylogenetic and genotypic drug resistance analyses. The genetic characteristics of the predominant HIV-1 strains were analyzed by the Bayesian Markov Chain Monte Carlo (MCMC) method. The genetic transmission networks were identified with a genetic distance of 0.03 substitutions/ site and 90% bootstrap support. Results Among the 190 HIV-1 positive MSM reported during 2013–2105, various genotypes were identified, including CRF01_AE (45.3%), CRF07_BC (35.8%), unique recombinant forms (URFs) (11.6%), CRF08_BC (3.2%), CRF55_01B (2.1%), subtype B (1.6%) and CRF59_01B (0.5%). The effective population sizes (EPS) for CRF01_AE and CRF07_BC increased exponentially from approximately 2001–2010 and 2005–2009, respectively. Genetic transmission networks were constructed with 308 pol sequences from MSM diagnosed during 2010–2015. Of the 308 MSM, 109 (35.4%) were identified in 38 distinct clusters. Having multiple male partners was associated with a high probability of identification in the genetic transmission networks. Of the 38 clusters, 27 (71.1%) contained individuals diagnosed in different years. Of the 109 individuals in the networks, 26 (23.9%) had 2 potential transmission partners (2 links). The proportion of MSM with 2 links was higher among those diagnosed from 2010–2012. The constituent ratios of their potential transmission partners by areas showed no significant difference among MSM from Kunming, other cities in Yunnan and other provinces. Additionally, surveillance drug resistance mutations (SDRMs) were identified in 5% of individuals. Conclusion This study revealed the various HIV-a genotypes circulating among MSM in Kunming. MSM with more partners were more easily detected in transmission networks, and early-diagnosed MSM remained active in transmission networks. These findings suggested that the routine interventions should be combined with HIV testing and linkage to care and early antiretroviral therapy among HIV-positive MSM. © 2018 Chen et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1007/s13198-017-0688-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045975536&doi=10.1007%2fs13198-017-0688-3&partnerID=40&md5=80067a567331ee47155041d3eace1365,"This article deals with the classical and Bayesian estimation of the parameters of log-logistic distribution using random censorship model. The maximum likelihood estimators and the asymptotic confidence intervals based on observed Fisher information matrix of the parameters are derived. Bayes estimators of the parameters under generalized entropy loss function using independent gamma priors are obtained. For Bayesian computation, Tierney–Kadane’s approximation and Markov chain Monte Carlo (MCMC) methods are used. Also, the highest posterior credible intervals of the parameters based on MCMC method are constructed. A Monte Carlo simulation study is carried out to compare the behavior of various estimators developed in this article. Finally, a real data analysis is performed for illustration purposes. © 2017, The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden."
1,10.1016/j.patcog.2017.11.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040353756&doi=10.1016%2fj.patcog.2017.11.021&partnerID=40&md5=727e109148d5621b7b38a217dedca57f,"The partially observable hidden Markov model is an extension of the hidden Markov Model in which the hidden state is conditioned on an independent Markov chain. This structure is motivated by the presence of discrete metadata, such as an event type, that may partially reveal the hidden state but itself emanates from a separate process. Such a scenario is encountered in keystroke dynamics whereby a user's typing behavior is dependent on the text that is typed. Under the assumption that the user can be in either an active or passive state of typing, the keyboard key names are event types that partially reveal the hidden state due to the presence of relatively longer time intervals between words and sentences than between letters of a word. Using five public datasets, the proposed model is shown to consistently outperform other anomaly detectors, including the standard HMM, in biometric identification and verification tasks and is generally preferred over the HMM in a Monte Carlo goodness of fit test. © 2017"
,10.1017/S0950268818000651,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044622571&doi=10.1017%2fS0950268818000651&partnerID=40&md5=d64d9d0ea92f86b4993dbe8931f251ba,"Because of a lack of gold standard diagnostics, a combination of multiple diagnostic tests, or composite diagnostic standard, has been used to measure pneumococcal pneumonia (PP) in pneumococcal vaccine trials. We estimated the accuracy of composite diagnostic standards for PP used in previous randomised controlled trials by simple formulas. A systematic literature review identified five eligible trials and all trials had used different combinations of diagnostic tests for PP. The estimated values of sensitivity and minimum specificity of composite diagnostic standards varied substantially between trials: 48.4% to 98.1% and 71.0% to 97.3%, respectively. Without standardizing the outcome measurements, pneumococcal vaccine efficacy estimates against PP are not comparable between trials and their pooled estimates are biased. Copyright © Cambridge University Press 2018."
1,10.1109/TVCG.2018.2793618,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041637332&doi=10.1109%2fTVCG.2018.2793618&partnerID=40&md5=5e7c420dc55a4b2c48954956d45549ab,"Games and experiences designed for virtual or augmented reality usually require the player to move physically to play. This poses substantial challenge for level designers because the player's physical experience in a level will need to be considered, otherwise the level may turn out to be too exhausting or not challenging enough. This paper presents a novel approach to optimize level designs by considering the physical challenge imposed upon the player in completing a level of motion-based games. A game level is represented as an assembly of chunks characterized by the exercise intensity levels they impose on players. We formulate game level synthesis as an optimization problem, where the chunks are assembled in a way to achieve an optimized level of intensity. To allow the synthesis of game levels of varying lengths, we solve the trans-dimensional optimization problem with a Reversible-jump Markov chain Monte Carlo technique. We demonstrate that our approach can be applied to generate game levels for s of motion-based virtual reality games. A user evaluation validates the effectiveness of our approach in generating levels with the desired amount of physical challenge. © 1995-2012 IEEE."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049146967&partnerID=40&md5=9d9a7eabdd35a06145f83826465a4aa9,"Objective: The current treatment options available for patients with unresectable hilar cholangiocarcinoma [CCA] are endoscopic biliary drainage [EBD] using a metal stent, percutaneous transhepaticbiliary drainage [PTBD], and palliative care. However, information regarding their cost-effectiveness is not available.This study aimed to compare the cost utility between palliative biliary drainage [EBD or PTBD] and palliative care. Materials and Methods: We used 2 methods for evaluation, direct calculation and the Markov decision analysis model. The cost of treatment and quality-adjusted life years [QALY] in the EBD, PTBD and palliative care groups were collected from the cohorts of unresectable hilar CCA database at a tertiary care hospital in Thailand. Transition probabilities were derived from international literature and the cohorts. Base-case and sensitivity analysis was also performed. Results: Compared with palliative care, the incremental cost per additional QALY gained from EBD and PTBD using the direct calculation method were 422,822 baht (US$ 12,622) and 490,578 baht (US$ 14,644) per QALY gained, respectively. This result was in concordance with the Markov model. The ICER from EBD and PTBD were 655,520 baht (US$19,568) and 6,548,398 baht (US$195,475) per QALY gained, respectively. According to probabilistic sensitivity analysis using the Markov model, EBD is preferable to palliative care if the willingness to pay [WTP] is higher than 650,000 baht (US$19,403) per QALY gained. PTBD is not cost-effective compared to palliative care at any WTP threshold. At a WTP threshold of 160,000 Thai baht (the threshold of Thailand; US$ 4,776 per QALY gained) neither EBD nor PTBD were found to be cost- effective. At this threshold, only palliative care is cost-effective. Conclusion: EBD is more cost-effective than PTBD when compared with palliative care in cases of unresectable hilar CCA, but at the WTP threshold of Thailand only palliative care is cost-effective. © 2018, Medical Association of Thailand. All rights reserved."
3,10.1124/dmd.117.078790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042715142&doi=10.1124%2fdmd.117.078790&partnerID=40&md5=3ea1608afc96f40df40e58f3ceb46ad4,"Understanding liver exposure of hepatic transporter substrates in clinical studies is often critical, as it typically governs pharmacodynamics, drug-drug interactions, and toxicity for certain drugs. However, this is a challenging task since there is currently no easy method to directly measure drug concentration in the human liver. Using bosentan as an example, we demonstrate a new approach to estimate liver exposure based on observed systemic pharmacokinetics from clinical studies using physiologically based pharmacokinetic modeling. The prediction was verified to be both accurate and precise using sensitivity analysis. For bosentan, the predicted pseudo steady-state unbound liver-to-unbound systemic plasma concentration ratio was 34.9 (95% confidence interval: 4.2, 50). Drug-drug interaction (i.e., CYP3A and CYP2B6 induction) and inhibition of hepatic transporters (i.e., bile salt export pump, multidrug resistance-associated proteins, and sodium-taurocholate cotransporting polypeptide) were predicted based on the estimated unbound liver tissue or plasma concentrations. With further validation and refinement, we conclude that this approach may serve to predict human liver exposure and complement other methods involving tissue biopsy and imaging. Copyright © 2018 by The American Society for Pharmacology and Experimental Therapeutics."
,10.1371/journal.pone.0193525,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045989470&doi=10.1371%2fjournal.pone.0193525&partnerID=40&md5=603f2793a272353d446705863514edfa,"Respiratory syncytial virus (RSV) is an important pathogen of global significance. The BA9 is one of the most predominant lineages of the BA genotype of group B RSV that has acquired a 60bp duplication in its G protein gene. We describe the local and global evolutionary dynamics of the second hyper variable region in the C- terminal of the G protein gene of the BA9 lineage. A total of 418 sequences (including 31 study and 387 GenBank strains) from 29 different countries were used for phylogenetic analysis. This analysis showed that the study strains clustered with BA (BA9 and BA8) and SAB4 genotype of group B RSV. We performed time-scaled evolutionary clock analyses using Bayesian Markov chain Monte Carlo methods. We also carried out glycosylation, selection pressure, mutational, entropy and Network analyses of the BA9 lineage. The time to the most recent common ancestor (tMRCA) of the BA genotype and BA9 lineage were estimated to be the years 1995 (95% HPD; 1987–1997) and 2000 (95% HPD; 1998–2001), respectively. The nucleotide substitution rate of the BA genotype [(4.58×10−3 (95% HPD; 3.89–5.29×10−3) substitution/site/year] was slightly faster than the BA9 lineage [4.03×10−3 (95% HPD; 4.65–5.2492×10−3)]. The BA9 lineage was categorized into 3 sub lineages (I, II and III) based on the Bayesian and Network analyses. The local transmission pattern suggested that BA9 is the predominant lineage of BA viruses that has been circulating in India since 2002 though showing fluctuations in its effective population size. The BA9 lineage established its global distribution with report from 23 different countries over the past 16 years. The present study augments our understanding of RSV infection, its epidemiological dynamics warranting steps towards its overall global surveillance. © 2018 Haider et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
1,10.1177/0146621618762743,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045422625&doi=10.1177%2f0146621618762743&partnerID=40&md5=fde62f24f0705ade663fb1706164da53,"It is commonly known that respondents exhibit different response styles when responding to Likert-type items. For example, some respondents tend to select the extreme categories (e.g., strongly disagree and strongly agree), whereas some tend to select the middle categories (e.g., disagree, neutral, and agree). Furthermore, some respondents tend to disagree with every item (e.g., strongly disagree and disagree), whereas others tend to agree with every item (e.g., agree and strongly agree). In such cases, fitting standard unfolding item response theory (IRT) models that assume no response style will yield a poor fit and biased parameter estimates. Although there have been attempts to develop dominance IRT models to accommodate the various response styles, such models are usually restricted to a specific response style and cannot be used for unfolding data. In this study, a general unfolding IRT model is proposed that can be combined with a softmax function to accommodate various response styles via scoring functions. The parameters of the new model can be estimated using Bayesian Markov chain Monte Carlo algorithms. An empirical data set is used for demonstration purposes, followed by simulation studies to assess the parameter recovery of the new model, as well as the consequences of ignoring the impact of response styles on parameter estimators by fitting standard unfolding IRT models. The results suggest the new model to exhibit good parameter recovery and seriously biased estimates when the response styles are ignored. © 2018, The Author(s) 2018."
,10.1016/j.spasta.2018.03.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044647476&doi=10.1016%2fj.spasta.2018.03.005&partnerID=40&md5=e05f55a00afb0a7332a66ae94292ea79,"In this paper we construct a hierarchical model for spatial compositional data which is used to reconstruct past land-cover compositions (in terms of coniferous forest, broadleaved forest, and unforested/open land) for five time periods during the past 6000 years over Europe. The model consists of a Gaussian Markov Random Field (GMRF) with Dirichlet observations. A block updated Markov chain Monte Carlo (MCMC), including an adaptive Metropolis adjusted Langevin step, is used to estimate model parameters. The sparse precision matrix in the GMRF provides computational advantages leading to a fast MCMC algorithm. Reconstructions are obtained by combining pollen-based estimates of vegetation cover at a limited number of locations with scenarios of past deforestation and output from a dynamic vegetation model. To evaluate uncertainties in the predictions a novel way of constructing joint confidence regions for the entire composition at each prediction location is proposed. The hierarchical model's ability to reconstruct past land cover is evaluated through cross validation for all time periods, and by comparing reconstructions for the recent past to a present day European forest map. The evaluation results are promising, and the model is able to capture known structures in past land-cover compositions. © 2018 Elsevier B.V."
2,10.1111/2041-210X.12952,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040785365&doi=10.1111%2f2041-210X.12952&partnerID=40&md5=57d265908e4130f490063a0179708f13,"State-and-transition simulation models (STSMs) provide a general framework for forecasting landscape dynamics, including projections of both vegetation and land-use/land-cover (LULC) change. The STSM method divides a landscape into spatially referenced cells and then simulates the state of each cell forward in time, as a discrete-time stochastic process using a Monte Carlo approach, in response to any number of possible transitions. A current limitation of the STSM method, however, is that all of the state variables must be discrete. Here we present a new approach for extending a STSM, in order to account for continuous state variables, called a STSM with stocks and flows (STSM-SF). The STSM–SF method allows for any number of continuous stocks to be defined for every spatial cell in the STSM, along with a suite of continuous flows specifying the rates at which stock levels change over time. The change in the level of each stock is then simulated forward in time, for each spatial cell, as a discrete-time stochastic process. The method differs from the traditional systems dynamics approach to stock-flow modelling in that the stocks and flows can be spatially explicit, and the flows can be expressed as a function of the STSM states and transitions. We demonstrate the STSM-SF method by integrating a spatially explicit carbon (C) budget model with a STSM of LULC change for the state of Hawai'i, USA. In this example, continuous stocks are pools of terrestrial C, whereas the flows are the possible fluxes of C between these pools. Importantly, several of these C fluxes are triggered by corresponding LULC transitions in the STSM. Model outputs include changes in the spatial and temporal distribution of C pools and fluxes across the landscape in response to projected future changes in LULC over the next 50 years. The new STSM-SF method allows both discrete and continuous state variables to be integrated into a STSM, including interactions between them. With the addition of stocks and flows, STSMs provide a conceptually simple yet powerful approach for characterizing uncertainties in projections of a wide range of questions regarding landscape change. © 2017 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of the British Ecological Society"
2,10.1093/gji/ggx497,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041854469&doi=10.1093%2fgji%2fggx497&partnerID=40&md5=9bc126244b1b6560d8e39e55205190b2,"Seismic anisotropy provides important information on the deformation history of the Earth's interior. Rayleigh and Love surface-waves are sensitive to and can be used to determine both radial and azimuthal shear-wave anisotropies at depth, but parameter trade-offs give rise to substantial model non-uniqueness. Here, we explore the trade-offs between isotropic and anisotropic structure parameters and present a suite of methods for the inversion of surface-wave, phase-velocity curves for radial and azimuthal anisotropies. One Markov chain Monte Carlo (McMC) implementation inverts Rayleigh and Love dispersion curves for a radially anisotropic shear velocity profile of the crust and upper mantle. Another McMC implementation inverts Rayleigh phase velocities and their azimuthal anisotropy for profiles of vertically polarized shear velocity and its depth-dependent azimuthal anisotropy. The azimuthal anisotropy inversion is fully non-linear, with the forward problem solved numerically at different azimuths for every model realization, which ensures that any linearization biases are avoided. The computations are performed in parallel, in order to reduce the computing time. The often challenging issue of data noise estimation is addressed by means of a Hierarchical Bayesian approach, with the variance of the noise treated as an unknown during the radial anisotropy inversion. In addition to the McMC inversions, we also present faster, non-linear gradient-search inversions for the same anisotropic structure. The results of the two approaches are mutually consistent; the advantage of the McMC inversions is that they provide a measure of uncertainty of the models. Applying the method to broad-band data from the Baikal-central Mongolia region, we determine radial anisotropy from the crust down to the transition-zone depths. Robust negative anisotropy (Vsh &lt; Vsv) in the asthenosphere, at 100-300 km depths, presents strong new evidence for a vertical component of asthenospheric flow. This is consistent with an upward flow from below the thick lithosphere of the Siberian Craton to below the thinner lithosphere of central Mongolia, likely to give rise to decompression melting and the scattered, sporadic volcanism observed in the Baikal Rift area, as proposed previously. Inversion of phase-velocity data from west-central Italy for azimuthal anisotropy reveals a clear change in the shear-wave fast-propagation direction at 70-100 km depths, near the lithosphere-asthenosphere boundary. The orientation of the fabric in the lithosphere is roughly E-W, parallel to the direction of stretching over the last 10 m.y. The orientation of the fabric in the asthenosphere is NW-SE, matching the fast directions inferred from shear-wave splitting and probably indicating the direction of the asthenospheric flow. The Author(s) 2017. Published by Oxford University Press. All rights reserved."
,10.7782/JKSR.2018.21.3.233,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050944926&doi=10.7782%2fJKSR.2018.21.3.233&partnerID=40&md5=398b9c96ea3a77b8f13493b6f013b572,"This paper intends to describe methodological ideas for analyzing of field-failure-data for components operating in different environments. To accomplish this, a statistical method of predicting reliability life is proposed using Bayesian inference. We used as a main algorithm the Markov Chain Monte Carlo (MCMC) algorithm, which allows us to approximate the posterior distribution as calculated using the Bayesian inference. We evaluated the difference between the two groups using the Bayesian p.value. As a result, it was confirmed that the reliability characteristics of the air compressors were different when the operating environments were different. © 2018 The Korean Society for Railway. All rights reserved."
,10.1088/1674-4527/18/4/47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045623187&doi=10.1088%2f1674-4527%2f18%2f4%2f47&partnerID=40&md5=8b18558bf835a7a6e88d4390db996309,"We present new analyses of variations in O - C diagrams of three Algol-type eclipsing binary stars: AD And, TW Cas and IV Cas. We have used all published minima times (including visual and photographic) as well as newly determined ones from our and SuperWasp observations. We determined orbital parameters of 3rd bodies in the systems with statistically significant errors, using our code based on genetic algorithms and Markov chain Monte Carlo simulations. We confirmed the multiple nature of AD And and the triple-star model of TW Cas, and we proposed a quadruple-star model of IV Cas. © 2018 National Astronomical Observatories, CAS and IOP Publishing Ltd."
,10.12928/TELKOMNIKA.v16i2.7510,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044372100&doi=10.12928%2fTELKOMNIKA.v16i2.7510&partnerID=40&md5=cb36f317e9bd4fa2e544a62fc9f3b90f,"This paper proposes the important issues in signal segmentation. The signal is disturbed by multiplicative noise where the number of segments is unknown. A Bayesian approach is proposed to estimate the parameter. The parameter includes the number of segments, the location of the segment, and the amplitude. The posterior distribution for the parameter does not have a simple equation so that the Bayes estimator is not easily determined. Reversible Jump Markov chain Monte Carlo (MCMC) method is adopted to overcome the problem. The Reversible Jump MCMC method creates a Markov chain whose distribution is close to the posterior distribution. The performance of the algorithm is shown by simulation data. The result of this simulation shows that the algorithm works well. As an application, the algorithm is used to segment a Synthetic Aperture Radar (SAR) signal. The advantage of this method is that the number of segments, the position of the segment change, and the amplitude are estimated simultaneously. © 2018 Universitas Ahmad Dahlan."
8,10.1088/1538-3873/aaaaa8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044197959&doi=10.1088%2f1538-3873%2faaaaa8&partnerID=40&md5=c7bc8080b35fc6af3c612745a9ed54a5,"RadVel is an open-source Python package for modeling Keplerian orbits in radial velocity (RV) timeseries. RadVel provides a convenient framework to fit RVs using maximum a posteriori optimization and to compute robust confidence intervals by sampling the posterior probability density via Markov Chain Monte Carlo (MCMC). RadVel allows users to float or fix parameters, impose priors, and perform Bayesian model comparison. We have implemented real-time MCMC convergence tests to ensure adequate sampling of the posterior. RadVel can output a number of publication-quality plots and tables. Users may interface with RadVel through a convenient command-line interface or directly from Python. The code is object-oriented and thus naturally extensible. We encourage contributions from the community. © 2018. The Astronomical Society of the Pacific. All rights reserved."
,10.1103/PhysRevD.97.074507,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046728016&doi=10.1103%2fPhysRevD.97.074507&partnerID=40&md5=0d1eb6320e2c8778337c621aa5350bf6,"We investigate the lattice spacing dependence of the equilibration time for a recently proposed multiscale thermalization algorithm for Markov chain Monte Carlo simulations. The algorithm uses a renormalization-group matched coarse lattice action and prolongation operation to rapidly thermalize decorrelated initial configurations for evolution using a corresponding target lattice action defined at a finer scale. Focusing on nontopological long-distance observables in pure SU(3) gauge theory, we provide quantitative evidence that the slow modes of the Markov process, which provide the dominant contribution to the rethermalization time, have a suppressed contribution toward the continuum limit, despite their associated timescales increasing. Based on these numerical investigations, we conjecture that the prolongation operation used herein will produce ensembles that are indistinguishable from the target fine-action distribution for a sufficiently fine coupling at a given level of statistical precision, thereby eliminating the cost of rethermalization. © 2018 authors. Published by the American Physical Society. Published by the American Physical Society under the terms of the »https://creativecommons.org/licenses/by/4.0/» Creative Commons Attribution 4.0 International license. Further distribution of this work must maintain attribution to the author(s) and the published article's title, journal citation, and DOI. Funded by SCOAP3."
,10.1093/mnras/stx3322,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046116929&doi=10.1093%2fmnras%2fstx3322&partnerID=40&md5=9a3f25d2bd8d974bf4f68060b2a5e750,"Three gamma-ray millisecond pulsars (MSPs), PSR J1939+2134, PSR J1959+2048, and PSR J0034-0534, have been confirmed to have a common feature of phase-aligned in radio and gamma-ray bands. With a geometric (two-pole caustic) model and a physical outer gap model (revised 3D outer gap model) in a three dimensional (3D) retarded magnetic dipole with a perturbation magnetic field, the observed features of these MSPs are studied. In order to obtained the best-fitting model parameters, the Markov chain Monte Carlo technique is used and reasonable GeV band light curves for three MSPs are given. Our calculations indicate that MSPs emit high energy photons with smaller inclination angles (α ≈ 10°-50°), larger view angles (ζ ≈ 65°-100°), and smaller perturbation factor (∈ ≈ -0.15-0.1). Note that the factor ∈, describing the strength of the perturbed magnetic field, is all less than zero in these two models, so the magnetic field caused by current-induced play a leading role in the pulsed location of MSPs. © 2018 The Author(s)."
1,10.1007/s11069-016-2549-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984822347&doi=10.1007%2fs11069-016-2549-9&partnerID=40&md5=e3e3e263ea860424dd7c8d36ba3b3dc8,"Earthquakes cluster in space and time resulting in nonlinear damage effects. We compute earthquake interactions using the Coulomb stress transfer theory and dynamic vulnerability from the concept of ductility capacity reduction. We combine both processes in the generic multi-risk framework where risk scenarios are simulated using a variant of the Markov chain Monte Carlo method. We apply the proposed approach to the thrust fault system of northern Italy, considering earthquakes with characteristic magnitudes in the range ~[6, 6.5], different levels of tectonic loading τ˙ = {10−4, 10−3, 10−2} bar/year and a generic stock of fictitious low-rise buildings with different ductility capacities μΔ = {2, 4, 6}. We describe the process’ stochasticity by non-stationary Poisson earthquake probabilities and by binomial damage state probabilities. We find that earthquake clustering yields a tail fattening of the seismic risk curve, the effect of which is amplified by damage-dependent fragility due to clustering. The impact of clustering alone is in average more important than dynamic vulnerability, the spatial extent of the former phenomenon being greater than of the latter one. © 2016, The Author(s)."
1,10.1093/mnras/stx3299,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041326572&doi=10.1093%2fmnras%2fstx3299&partnerID=40&md5=07157a4e4769f67e4b65a06e3121de9d,"Previous studies of the rotation law in the outer Galactic disc have mainly used gas tracers or clump giants. Here, we explore A and F stars as alternatives: these provide a much denser sampling in the outer disc than gas tracers and have experienced significantly less velocity scattering than older clump giants. This first investigation confirms the suitability of A stars in this role. Our work is based on spectroscopy of ~1300 photometrically selected stars in the red calcium-triplet region, chosen to mitigate against the effects of interstellar extinction. The stars are located in two low Galactic latitude sightlines, at longitudes L = 118°, sampling strong Galactic rotation shear, and L = 178°, near the anticentre. With the use of Markov Chain Monte Carlo parameter fitting, stellar parameters and radial velocities are measured, and distances computed. The obtained trend of radial velocity with distance is inconsistent with existing flat or slowly rising rotation laws from gas tracers (Brand &amp; Blitz 1993; Reid et al. 2014). Instead, our results fit in with those obtained by Huang et al. (2016) from disc clump giants that favoured rising circular speeds. An alternative interpretation in terms of spiral arm perturbation is not straight forward. We assess the role that undetected binaries in the sample and distance error may have in introducing bias, and show that the former is a minor factor. The random errors in our trend of circular velocity are within ±5 kms-1. © 2018 The Author(s)."
2,10.1007/s40840-016-0311-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044731277&doi=10.1007%2fs40840-016-0311-9&partnerID=40&md5=906351d141e63765318bd5dc7ac496f9,"Accelerated life testing is widely used in product life testing experiments since it provides significant reduction in time and cost of testing. In this article, we assume that the lifetime of items under use condition follows the two-parameter distributions having power hazard function, and partially accelerated life tests based on progressive type-II censored samples are considered. The maximum likelihood, Bayes, and parametric bootstrap methods are used for estimating the unknown parameters. Based on normal approximation to the asymptotic distribution of MLEs, the approximate confidence intervals for the parameters are derived. Two bootstrap confidence intervals are also proposed. The classical Bayes estimates cannot be obtained in explicit form, so we propose to apply the Markov chain Monte Carlo (MCMC) technique. Gibbs within the Metropolis–Hasting algorithm has been applied to generate MCMC samples from the posterior density function. Based on the generated samples, the Bayes estimates and the highest posterior density credible intervals of the unknown parameters have been computed. Finally, analysis of a simulated data set has also been presented to illustrate the proposed estimation methods developed here. © 2016, Malaysian Mathematical Sciences Society and Penerbit Universiti Sains Malaysia."
1,10.1007/s11203-016-9153-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006341971&doi=10.1007%2fs11203-016-9153-1&partnerID=40&md5=b9c6c519cd1288df8f25b99503355510,"Given a sample from a discretely observed compound Poisson process, we consider non-parametric estimation of the density f0 of its jump sizes, as well as of its intensity λ0. We take a Bayesian approach to the problem and specify the prior on f0 as the Dirichlet location mixture of normal densities. An independent prior for λ0 is assumed to be compactly supported and to possess a positive density with respect to the Lebesgue measure. We show that under suitable assumptions the posterior contracts around the pair (λ0,f0) at essentially (up to a logarithmic factor) the nΔ-rate, where n is the number of observations and Δ is the mesh size at which the process is sampled. The emphasis is on high frequency data, Δ → 0 , but the obtained results are also valid for fixed Δ. In either case we assume that nΔ → ∞. Our main result implies existence of Bayesian point estimates converging (in the frequentist sense, in probability) to (λ0,f0) at the same rate. We also discuss a practical implementation of our approach. The computational problem is dealt with by inclusion of auxiliary variables and we develop a Markov chain Monte Carlo algorithm that samples from the joint distribution of the unknown parameters in the mixture density and the introduced auxiliary variables. Numerical examples illustrate the feasibility of this approach. © 2016, The Author(s)."
,10.1016/j.spasta.2018.03.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045245009&doi=10.1016%2fj.spasta.2018.03.007&partnerID=40&md5=b8f57fab9e7b67f874171b1c5df900f5,"We propose a formal specification for sum-zero constrained intrinsic conditional autoregressive (ICAR) models. Our specification first projects a vector of proper conditional autoregressive spatial random effects onto a subspace where the projected vector is constrained to sum to zero, and after that takes the limit when the proper conditional autoregressive model approaches the ICAR model. As a result, we show that the sum-zero constrained ICAR model has a singular Gaussian distribution with zero mean vector and a unique covariance matrix. Previously, sum-zero constraints have typically been imposed on the vector of spatial random effects in ICAR models within a Markov chain Monte Carlo (MCMC) algorithm in what is known as centering-on-the-fly. This mathematically informal way to impose the sum-zero constraint obscures the actual joint density of the spatial random effects. By contrast, the present work elucidates a unique distribution for ICAR random effects. The explicit expressions for the resulting unique covariance matrix and density function are useful for the development of Bayesian methodology in spatial statistics which will be useful to practitioners. We illustrate the practical relevance of our results by using Bayesian model selection to jointly assess both spatial dependence and fixed effects. © 2018 Elsevier B.V."
1,10.1093/mnras/sty095,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045769001&doi=10.1093%2fmnras%2fsty095&partnerID=40&md5=54ac0bd4f88dd617dd9483c7784c8e8c,"We present a large-scale galaxy structure Cl J021734-0513 at z ~ 0.65 discovered in the UKIDSS UDS field, made of ~20 galaxy groups and clusters, spreading over 10Mpc. We report on a VLT/VIMOS spectroscopic follow-up program that, combined with past spectroscopy, allowed us to confirm four galaxy clusters (M200 ~ 1014 M⊙) and a dozen associated groups and star-forming galaxy overdensities. Two additional filamentary structures at z~0.62 and 0.69 and foreground and background clusters at 0.6 &lt; z &lt; 0.7 were also confirmed along the line of sight. The structure subcomponents are at different formation stages. The clusters have a core dominated by passive galaxies and an established red sequence. The remaining structures are a mix of star-forming galaxy overdensities and forming groups. The presence of quiescent galaxies in the core of the latter shows that 'pre-processing' has already happened before the groups fall into theirmoremassive neighbours. Our spectroscopy allows us to derive spectral index measurements e.g. emission/absorption line equivalent widths, strength of the 4000Å break, valuable to investigate the star formation history of structure members. Based on these line measurements, we select a population of 'post-starburst' galaxies. These galaxies are preferentially found within the virial radius of clusters, supporting a scenario in which their recent quenching could be prompted by gas stripping by the dense intracluster medium. We derive stellar age estimates using Markov Chain Monte Carlo-based spectral fitting for quiescent galaxies and find a correlation between ages and colours/stellar masses which favours a top-down formation scenario of the red sequence. A catalogue of ~650 redshifts in UDS is released alongside the paper (via MNRAS online data). © 2017 The Authors."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044177231&partnerID=40&md5=988f79e2420d3a1f3f6a9ad9fe7615b4,"Introduction: Bipolar disorder is severe, chronic, pleomorphic and often recurrent, which can lead to severe abnormalities in lif's function. The purpose of this study was to determine some factors related to the time interval between relapse in bipolar disorder. Materials and Methods: In a retrospective study, 606 patients with bipolar disorder at Avicenna hospital in Mashhad were collected. In this study, 329 patients with at least one relapse of bipolar disorder were included. Survival time was defined as elapsed time from discharge to readmission due to relapse of disease. These patients wereadmitted from the beginning of 2008 to the end of 2009 due to their illness and were include in the study and followed until the end of 2013. The log-skew-normal accelerated failure time model was fitted to identify the factors related to the time of relapse. Estimation of parameters was obtained based on the Bayesian approach using Markov chain Monte Carlo algorithm by Open BUGS. Results: The estimate of mean and median time between discharge and readmission were 25.6 and 15 months, respectively. Age, family history, drug use, and gender had a significant association with the time of relapse, such that, male, younger patients, drug users, and positive family history of disease experienced a recurrence of disease earlier. With the mentioned variables in the model, stress and education were not associated with the recurrence of disease. Conclusion: Given that the time interval from discharge to recurrence of type 1 bipolar disorder is low in men, young people, family history of positive and drug users, it is recommended to identify strategies to prevent relapse or delay in these groups. © 2018, Semnan University of Medical Sciences. All rights reserved."
,10.1093/molbev/msx294,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044739219&doi=10.1093%2fmolbev%2fmsx294&partnerID=40&md5=df83c598aaf95c779207bf5cf2523d79,"Most phylogenetic models assume that the evolutionary process is stationary and reversible. In addition to being biologically improbable, these assumptions also impair inference by generating models under which the likelihood does not depend on the position of the root. Consequently, the root of the tree cannot be inferred as part of the analysis. Yet identifying the root position is a key component of phylogenetic inference because it provides a point of reference for polarizing ancestor-descendant relationships and therefore interpreting the tree. In this paper, we investigate the effect of relaxing the unrealistic reversibility assumption and allowing the position of the root to be another unknown. We propose two hierarchical models that are centered on a reversible model but perturbed to allow nonreversibility. The models differ in the degree of structure imposed on the perturbations. The analysis is performed in the Bayesian framework using Markov chain Monte Carlo methods for which software is provided. We illustrate the performance of the two nonreversible models in analyses of simulated data using two types of topological priors. We then apply the models to a real biological data set, the radiation of polyploid yeasts, for which there is robust biological opinion about the root position. Finally, we apply the models to a second biological alignment for which the rooted tree is controversial: The ribosomal tree of life. We compare the two nonreversible models and conclude that both are useful in inferring the position of the root from real biological data. © The Author(s) 2017. Published by Oxford University Press on behalf of the Society for Molecular Biology and Evolution."
1,10.1111/2041-210X.12931,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034660398&doi=10.1111%2f2041-210X.12931&partnerID=40&md5=9fd8597b5d88588b366dc0b87920d394,"Several new growth models have been proposed to account for the life-history trade-offs that occur when indeterminately growing species allocate energy between somatic growth and reproduction. These models can improve the understanding of lifetime growth and life history, but can be more difficult to fit than conventional growth models. Increased data demands, multiple growth phases and increased parameterization all serve as barriers to the adoption and proper use of these new models. We review and comment on confounding issues during model fitting for several of these models, and provide advice on surmounting such issues. We then simulation-test an example model, the Lester biphasic growth model, using several common fitting approaches. We highlight the biases and precision of each approach and provide guiding documents using r and jags code. The Bayesian Markov chain Monte Carlo and likelihood profiling approaches generally provided the best fits. Simpler approaches can be unbiased and precise if sampled data are of relatively high quality (e.g. moderate sample sizes for juvenile and adult phases) and model assumptions are met. Bayesian hierarchical approaches can accommodate more complicated data scenarios (e.g. unbalanced design across multiple populations); we provide an example of such an approach by recovering growth trajectories and inferring growth-associated trait variation and environmental effects across multiple populations. Conventional growth models provide limited inference on life history. Many biphasic growth models can provide direct inference on multiple life-history traits, but can be difficult to fit. The recommended approaches herein provide a path forward for fitting biphasic growth models in a variety of scenarios, allowing for wider application and tests of life history and ecological theory. © 2017 The Authors. Methods in Ecology and Evolution © 2017 British Ecological Society"
,10.1007/s12021-018-9369-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042946733&doi=10.1007%2fs12021-018-9369-x&partnerID=40&md5=bcb5fd5a459979b04d546408740c674f,"Mathematical modeling is a powerful tool that enables researchers to describe the experimentally observed dynamics of complex systems. Starting with a robust model including model parameters, it is necessary to choose an appropriate set of model parameters to reproduce experimental data. However, estimating an optimal solution of the inverse problem, i.e., finding a set of model parameters that yields the best possible fit to the experimental data, is a very challenging problem. In the present work, we use different optimization algorithms based on a frequentist approach, as well as Monte Carlo Markov Chain methods based on Bayesian inference techniques to solve the considered inverse problems. We first probe two case studies with synthetic data and study models described by a stochastic non-delayed linear second-order differential equation and a stochastic linear delay differential equation. In a third case study, a thalamo-cortical neural mass model is fitted to the EEG spectral power measured during general anesthesia induced by anesthetics propofol and desflurane. We show that the proposed neural mass model fits very well to the observed EEG power spectra, particularly to the power spectral peaks within δ − (0 − 4 Hz) and α − (8 − 13 Hz) frequency ranges. Furthermore, for each case study, we perform a practical identifiability analysis by estimating the confidence regions of the parameter estimates and interpret the corresponding correlation and sensitivity matrices. Our results indicate that estimating the model parameters from analytically computed spectral power, we are able to accurately estimate the unknown parameters while avoiding the computational costs due to numerical integration of the model equations. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
,10.1002/sim.7568,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042913946&doi=10.1002%2fsim.7568&partnerID=40&md5=ab4242ef96ce6a09155845d31696c9a0,"The first goal of the United Nations' 90–90–90 HIV/AIDS elimination strategy is to ensure that, by 2020, 90% of HIV-positive people know their HIV status. Estimating the prevalence of HIV among people eligible for screening allows assessment of the number of additional cases that might be diagnosed through continued screening efforts in this group. Here, we present methods for estimating prevalence when HIV status is verified by a gold standard only among those who test positive on an initial, imperfect screening test with known sensitivity and specificity. We develop maximum likelihood estimators and asymptotic confidence intervals for use in 2 scenarios: when the total number of test negatives is known (Scenario 1) and unknown (Scenario 2). We derive Bayesian prevalence estimators to account for non-negligible uncertainty in previous estimates of the sensitivity and specificity. The Scenario 1 estimator consistently outperformed the Scenario 2 estimator in simulations, demonstrating the use of recording the number of test negatives in public health screening programs. For less accurate tests (sensitivity and specificity < 90%), the performance of the 2 estimators was comparable, suggesting that, under these circumstances, prevalence can still be estimated with adequate precision when the number of test negatives is unknown. However, use of the Bayesian approach to account for uncertainty in the sensitivity and specificity is especially recommended for the Scenario 2 estimator, which was particularly sensitive to misspecification of these values. R code for implementing these methods is available at hsph.harvard.edu/donna-spiegelman/software. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1002/sim.7570,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037666251&doi=10.1002%2fsim.7570&partnerID=40&md5=39a076963dfd0fe296dcc93492813ca2,"The long-term health effects of air pollution are often estimated using a spatio-temporal ecological areal unit study, but this design leads to the following statistical challenges: (1) how to estimate spatially representative pollution concentrations for each areal unit; (2) how to allow for the uncertainty in these estimated concentrations when estimating their health effects; and (3) how to simultaneously estimate the joint effects of multiple correlated pollutants. This article proposes a novel 2-stage Bayesian hierarchical model for addressing these 3 challenges, with inference based on Markov chain Monte Carlo simulation. The first stage is a multivariate spatio-temporal fusion model for predicting areal level average concentrations of multiple pollutants from both monitored and modelled pollution data. The second stage is a spatio-temporal model for estimating the health impact of multiple correlated pollutants simultaneously, which accounts for the uncertainty in the estimated pollution concentrations. The novel methodology is motivated by a new study of the impact of both particulate matter and nitrogen dioxide concentrations on respiratory hospital admissions in Scotland between 2007 and 2011, and the results suggest that both pollutants exhibit substantial and independent health effects. © 2017 The Authors. Statistics in Medicine Published by John Wiley & Sons Ltd."
,10.1007/s11749-018-0580-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044591373&doi=10.1007%2fs11749-018-0580-8&partnerID=40&md5=830316248dc09247567214b7a52d1434,"Bayesian inference on the covariance matrix is usually performed after placing an inverse-Wishart or a multivariate Jeffreys as a prior density, but both of them, for different reasons, present some drawbacks. As an alternative, the covariance matrix can be modelled by separating out the standard deviations and the correlations. This separation strategy takes advantage of the fact that usually it is more straightforward and flexible to set priors on the standard deviations and the correlations rather than on the covariance matrix. On the other hand, the priors must preserve the positive definiteness of the correlation matrix. This can be obtained by considering the Cholesky decomposition of the correlation matrix, whose entries are reparameterized using trigonometric functions. The efficiency of the trigonometric separation strategy (TSS) is shown through an application to hidden Markov models (HMMs), with conditional distributions multivariate normal. In the case of an unknown number of hidden states, estimation is conducted using a reversible jump Markov chain Monte Carlo algorithm based on the split-and-combine and birth-and-death moves whose design is straightforward because of the use of the TSS. Finally, an example in remote sensing is described, where a HMM containing the TSS is used for the segmentation of a multi-colour satellite image. © 2018 Sociedad de Estadística e Investigación Operativa"
,10.1109/INCISCOS.2017.26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050945578&doi=10.1109%2fINCISCOS.2017.26&partnerID=40&md5=50271f29acdc28fc62c0c442a4603f4e,"El presente documento muestra un análisis de los perfiles de voltaje, que son un indicador de la confiabilidad de un sistema eléctrico, ya que la red se comporta implementando la carga de vehículos eléctricos (VE) en una curva de demanda típica residencial. En función de varios escenarios de carga de los vehículos eléctricos, es posible determinar si la red actual de la empresa de distribución tiene la infraestructura necesaria para poder suministrar la carga de los vehículos eléctricos y la demanda residencial. Este análisis se realiza a través del algoritmo de Monte Carlo utilizando cadenas de Markov, lo que nos permite obtener un resultado óptimo y confiable de este estudio gracias a las probabilidades de las cadenas de Markov. El análisis está asociado a la predicción de la potencia máxima requerida para satisfacer el período pico de demanda en los sistemas de distribución con predominio de carga residencial, buscando también la planificación de las redes para aumentar la eficiencia, calidad y confiabilidad del suministro de energía en horas punta. © 2017 IEEE."
,10.5194/gmd-11-1199-2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044718023&doi=10.5194%2fgmd-11-1199-2018&partnerID=40&md5=0c561faa08b99967f352fa9395a43558,"Estimating methane (CH4) emissions from natural wetlands is complex, and the estimates contain large uncertainties. The models used for the task are typically heavily parameterized and the parameter values are not well known. In this study, we perform a Bayesian model calibration for a new wetland CH4 emission model to improve the quality of the predictions and to understand the limitations of such models. The detailed process model that we analyze contains descriptions for CH4 production from anaerobic respiration, CH4 oxidation, and gas transportation by diffusion, ebullition, and the aerenchyma cells of vascular plants. The processes are controlled by several tunable parameters. We use a hierarchical statistical model to describe the parameters and obtain the posterior distributions of the parameters and uncertainties in the processes with adaptive Markov chain Monte Carlo (MCMC), importance resampling, and time series analysis techniques. For the estimation, the analysis utilizes measurement data from the Siikaneva flux measurement site in southern Finland. The uncertainties related to the parameters and the modeled processes are described quantitatively. At the process level, the flux measurement data are able to constrain the CH4 production processes, methane oxidation, and the different gas transport processes. The posterior covariance structures explain how the parameters and the processes are related. Additionally, the flux and flux component uncertainties are analyzed both at the annual and daily levels. The parameter posterior densities obtained provide information regarding importance of the different processes, which is also useful for development of wetland methane emission models other than the square root HelsinkI Model of MEthane buiLd-up and emIssion for peatlands (sqHIMMELI). The hierarchical modeling allows us to assess the effects of some of the parameters on an annual basis. The results of the calibration and the cross validation suggest that the early spring net primary production could be used to predict parameters affecting the annual methane production. Even though the calibration is specific to the Siikaneva site, the hierarchical modeling approach is well suited for larger-scale studies and the results of the estimation pave way for a regional or global-scale Bayesian calibration of wetland emission models. © Author(s) 2018."
,10.1080/02664763.2018.1454893,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044469211&doi=10.1080%2f02664763.2018.1454893&partnerID=40&md5=851bfd38899f2743f26043b9ecbd010d,"In proteomics, identification of proteins from complex mixtures of proteins extracted from biological samples is an important problem. Among the experimental technologies, mass spectrometry (MS) is the most popular one. Protein identification from MS data typically relies on a ‘two-step’ procedure of identifying the peptide first followed by the separate protein identification procedure next. In this setup, the interdependence of peptides and proteins is neglected resulting in relatively inaccurate protein identification. In this article, we propose a Markov chain Monte Carlo based Bayesian hierarchical model, a first of its kind in protein identification, which integrates the two steps and performs joint analysis of proteins and peptides using posterior probabilities. We remove the assumption of independence of proteins by using clustering group priors to the proteins based on the assumption that proteins sharing the same biological pathway are likely to be present or absent together and are correlated. The complete conditionals of the proposed joint model being tractable, we propose and implement a Gibbs sampling scheme for full posterior inference that provides the estimation and statistical uncertainties of all relevant parameters. The model has better operational characteristics compared to two existing ‘one-step’ procedures on a range of simulation settings as well as on two well-studied datasets. © 2018 Informa UK Limited, trading as Taylor & Francis Group"
2,10.3390/atmos9040126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044430963&doi=10.3390%2fatmos9040126&partnerID=40&md5=5d68e26a5acc3d39e56daafba24486bc,"In this paper, we propose an efficient EnKF implementation for non-Gaussian data assimilation based on Gaussian Mixture Models and Markov-Chain-Monte-Carlo (MCMC) methods. The proposed method works as follows: based on an ensemble of model realizations, prior errors are estimated via a Gaussian Mixture density whose parameters are approximated by means of an Expectation Maximization method. Then, by using an iterative method, observation operators are linearized about current solutions and posterior modes are estimated via a MCMC implementation. The acceptance/rejection criterion is similar to that of the Metropolis-Hastings rule. Experimental tests are performed on the Lorenz 96 model. The results show that the proposed method can decrease prior errors by several order of magnitudes in a root-mean-square-error sense for nearly sparse or dense observational networks. © 2018 by the authors."
,10.3389/fnins.2018.00184,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044871556&doi=10.3389%2ffnins.2018.00184&partnerID=40&md5=786233cebba3e3667c3ef213cd93024d,"Relating disease status to imaging data stands to increase the clinical significance of neuroimaging studies. Many neurological and psychiatric disorders involve complex, systems-level alterations that manifest in functional and structural properties of the brain and possibly other clinical and biologic measures. We propose a Bayesian hierarchical model to predict disease status, which is able to incorporate information from both functional and structural brain imaging scans. We consider a two-stage whole brain parcellation, partitioning the brain into 282 subregions, and our model accounts for correlations between voxels from different brain regions defined by the parcellations. Our approach models the imaging data and uses posterior predictive probabilities to perform prediction. The estimates of our model parameters are based on samples drawn from the joint posterior distribution using Markov Chain Monte Carlo (MCMC) methods. We evaluate our method by examining the prediction accuracy rates based on leave-one-out cross validation, and we employ an importance sampling strategy to reduce the computation time. We conduct both whole-brain and voxel-level prediction and identify the brain regions that are highly associated with the disease based on the voxel-level prediction results. We apply our model to multimodal brain imaging data from a study of Parkinson's disease. We achieve extremely high accuracy, in general, and our model identifies key regions contributing to accurate prediction including caudate, putamen, and fusiform gyrus as well as several sensory system regions. © 2018 Xue, Bowman and Kang."
,10.1186/s13690-018-0264-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044844860&doi=10.1186%2fs13690-018-0264-6&partnerID=40&md5=dd5944523eb6bea19c09008c0cc0a33b,"Background: The term malnutrition generally refers to both under-nutrition and over-nutrition, but this study uses the term to refer solely to a deficiency of nutrition. In Ethiopia, child malnutrition is one of the most serious public health problem and the highest in the world. The purpose of the present study was to identify the high risk factors of malnutrition and test different statistical models for childhood malnutrition and, thereafter weighing the preferable model through model comparison criteria. Methods: Bayesian Gaussian regression model was used to analyze the effect of selected socioeconomic, demographic, health and environmental covariates on malnutrition under five years old child's. Inference was made using Bayesian approach based on Markov Chain Monte Carlo (MCMC) simulation techniques in BayesX. Results: The study found that the variables such as sex of a child, preceding birth interval, age of the child, father's education level, source of water, mother's body mass index, head of household sex, mother's age at birth, wealth index, birth order, diarrhea, child's size at birth and duration of breast feeding showed significant effects on children's malnutrition in Ethiopia. The age of child, mother's age at birth and mother's body mass index could also be important factors with a non linear effect for the child's malnutrition in Ethiopia. Conclusions: Thus, the present study emphasizes a special care on variables such as sex of child, preceding birth interval, father's education level, source of water, sex of head of household, wealth index, birth order, diarrhea, child's size at birth, duration of breast feeding, age of child, mother's age at birth and mother's body mass index to combat childhood malnutrition in developing countries. © 2018 The Author(s)."
,10.1109/CompComm.2017.8322960,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049744026&doi=10.1109%2fCompComm.2017.8322960&partnerID=40&md5=c6801ebabed9ac76fa75d12559ff1614,"Many statistical problems can be formulated as the missing data problems. The data augmentation algorithm and the inverse Bayes formula are important tools for constructing iterative optimization or samplings via the introduction of unobserved data or latent variables. As a Markov Chain Monte Carlo method, the data augmentation algorithm has its autocorrelation. Nevertheless, the convergence rate of the data augmentation algorithm is not known in the Genetic Linkage Model, and the same is true for the inverse Bayes formula method. In this article, we analyze the convergence rates of the data augmentation algorithm and the inverse Bayes formula method in the genetic linkage model, and through simulation results we get their convergence rates. © 2017 IEEE."
,10.1080/00218464.2016.1268055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010638186&doi=10.1080%2f00218464.2016.1268055&partnerID=40&md5=e582c11b7a49e59a350d965be92b63d5,Adhesively bonded joints are used in several industrial sectors. Cohezive Zone Modes can be used to predict the adhesive mechanical behaviour. This work presents an approach to calibrate Cohesive Zone Models (CZM) by means of Statistical Inverse Analysis. The Bayesian framework for Inverse Problems is used to infer about the CZM model parameters. The solution corresponds to the exploration of the posterior probability density function of the model parameters. The exploration of the posterior density is performed by means of Markov Chain Monte Carlo (MCMC) methods mixing Population-Based MCMC with Adaptive Metropolis (AD) strategies. The assessment of the approach is performed using measured data from a single-lap shear experimental set-up. Measured data from 5 test-specimens is used for calibration and measured data from five other test-specimens is used for model validation. It is proposed a stochastic effective model for the CZM parameters. The predictions of maximum force and maximum displacement that are provided by the effective model are in accordance with measured data that is used for validation. © 2017 Taylor & Francis.
,10.1098/rsos.171519,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044257439&doi=10.1098%2frsos.171519&partnerID=40&md5=60c0aebb56efe50738ee183a05ba057f,"While model evidence is considered by Bayesian statisticians as a gold standard for model selection (the ratio in model evidence between two models giving the Bayes factor), its calculation is often viewed as too computationally demanding for many applications. By contrast, the widely used deviance information criterion (DIC), a different measure that balances model accuracy against complexity, is commonly considered a much faster alternative. However, recent advances in computational tools for efficient multi-temperature Markov chain Monte Carlo algorithms, such as steppingstone sampling (SS) and thermodynamic integration schemes, enable efficient calculation of the Bayesian model evidence. This paper compares both the capability (i.e. ability to select the true model) and speed (i.e. CPU time to achieve a given accuracy) of DIC with model evidence calculated using SS. Three important model classes are considered: linear regression models, mixed models and compartmental models widely used in epidemiology. While DIC was found to correctly identify the true model when applied to linear regression models, it led to incorrect model choice in the other two cases. On the other hand, model evidence led to correct model choice in all cases considered. Importantly, and perhaps surprisingly, DIC and model evidence were found to run at similar computational speeds, a result reinforced by analytically derived expressions. © 2018 The Authors."
,10.5194/hess-22-1917-2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044238613&doi=10.5194%2fhess-22-1917-2018&partnerID=40&md5=21435a0c415dc3ad3ec0a1fee6db33f8,"Profiles of temperature time series are commonly used to determine hyporheic flow patterns and hydraulic dynamics in the streambed sediments. Although hyporheic flows are 3-D, past research has focused on determining the magnitude of the vertical flow component and how this varies spatially. This study used a portable 56-sensor, 3-D temperature array with three heat pulse sources to measure the flow direction and magnitude up to 200gmm below the water-sediment interface. Short, 1gmin heat pulses were injected at one of the three heat sources and the temperature response was monitored over a period of 30gmin. Breakthrough curves from each of the sensors were analysed using a heat transport equation. Parameter estimation and uncertainty analysis was undertaken using the differential evolution adaptive metropolis (DREAM) algorithm, an adaption of the Markov chain Monte Carlo method, to estimate the flux and its orientation. Measurements were conducted in the field and in a sand tank under an extensive range of controlled hydraulic conditions to validate the method. The use of short-duration heat pulses provided a rapid, accurate assessment technique for determining dynamic and multi-directional flow patterns in the hyporheic zone and is a basis for improved understanding of biogeochemical processes at the water-streambed interface. © 2018 Author(s)."
2,10.3847/1538-4357/aaaf6b,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044731941&doi=10.3847%2f1538-4357%2faaaf6b&partnerID=40&md5=63e6a64d61c7941ad483349509b16d40,"We consider the possible observation of fast radio bursts (FRBs) with planned future radio telescopes, and investigate how well the dispersions and redshifts of these signals might constrain cosmological parameters. We construct mock catalogs of FRB dispersion measure (DM) data and employ Markov Chain Monte Carlo analysis, with which we forecast and compare with existing constraints in the flat ΛCDM model, as well as some popular extensions that include dark energy equation of state and curvature parameters. We find that the scatter in DM observations caused by inhomogeneities in the intergalactic medium (IGM) poses a big challenge to the utility of FRBs as a cosmic probe. Only in the most optimistic case, with a high number of events and low IGM variance, do FRBs aid in improving current constraints. In particular, when FRBs are combined with CMB+BAO+SNe+H 0 data, we find the biggest improvement comes in the constraint. Also, we find that the dark energy equation of state is poorly constrained, while the constraint on the curvature parameter, Ωk, shows some improvement when combined with current constraints. When FRBs are combined with future baryon acoustic oscillation (BAO) data from 21 cm Intensity Mapping, we find little improvement over the constraints from BAOs alone. However, the inclusion of FRBs introduces an additional parameter constraint, , which turns out to be comparable to existing constraints. This suggests that FRBs provide valuable information about the cosmological baryon density in the intermediate redshift universe, independent of high-redshift CMB data. © 2018. The American Astronomical Society. All rights reserved.."
,10.1007/s11222-018-9809-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044206001&doi=10.1007%2fs11222-018-9809-3&partnerID=40&md5=9a74d625736de1d0cf575cb272f68711,"Gibbs sampling is a widely used Markov chain Monte Carlo (MCMC) method for numerically approximating integrals of interest in Bayesian statistics and other mathematical sciences. Many implementations of MCMC methods do not extend easily to parallel computing environments, as their inherently sequential nature incurs a large synchronization cost. In the case study illustrated by this paper, we show how to do Gibbs sampling in a fully data-parallel manner on a graphics processing unit, for a large class of exchangeable models that admit latent variable representations. Our approach takes a systems perspective, with emphasis placed on efficient use of compute hardware. We demonstrate our method on a Horseshoe Probit regression model and find that our implementation scales effectively to thousands of predictors and millions of data points simultaneously. © 2018 The Author(s)"
,10.1088/1742-6596/973/1/012054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045641321&doi=10.1088%2f1742-6596%2f973%2f1%2f012054&partnerID=40&md5=ee3b7058e41fe0d4c9d676a7815c10e0,"We propose a new general numerical method aimed to solve integro-differential equations with variable coefficients. The problem under consideration arises in finance where in the context of pricing barrier options in a wide class of stochastic volatility models with jumps. To handle the effect of the correlation between the price and the variance, we use a suitable substitution for processes. Then we construct a Markov-chain approximation for the variation process on small time intervals and apply a maturity randomization technique. The result is a system of boundary problems for integro-differential equations with constant coefficients on the line in each vertex of the chain. We solve the arising problems using a numerical Wiener-Hopf factorization method. The approximate formulae for the factors are efficiently implemented by means of the Fast Fourier Transform. Finally, we use a recurrent procedure that moves backwards in time on the variance tree. We demonstrate the convergence of the method using Monte-Carlo simulations and compare our results with the results obtained by the Wiener-Hopf method with closed-form expressions of the factors. © Published under licence by IOP Publishing Ltd."
,10.1088/1475-7516/2018/03/033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045336973&doi=10.1088%2f1475-7516%2f2018%2f03%2f033&partnerID=40&md5=11888ddebfcabb744e3d99462547eba5,"We derive an observational constraint on a spherical inhomogeneity of the void centered at our position from the angular power spectrum of the cosmic microwave background (CMB) and local measurements of the Hubble parameter. The late time behaviour of the void is assumed to be well described by the so-called Λ-Lematre-Tolman-Bondi (ΛLTB) solution. Then, we restrict the models to the asymptotically homogeneous models each of which is approximated by a flat Friedmann-Lematre-Robertson-Walker model. The late time ΛLTB models are parametrized by four parameters including the value of the cosmological constant and the local Hubble parameter. The other two parameters are used to parametrize the observed distance-redshift relation. Then, the ΛLTB models are constructed so that they are compatible with the given distance-redshift relation. Including conventional parameters for the CMB analysis, we characterize our models by seven parameters in total. The local Hubble measurements are reflected in the prior distribution of the local Hubble parameter. As a result of a Markov-Chains-Monte-Carlo analysis for the CMB temperature and polarization anisotropies, we found that the inhomogeneous universe models with vanishing cosmological constant are ruled out as is expected. However, a significant under-density around us is still compatible with the angular power spectrum of CMB and the local Hubble parameter. © 2018 IOP Publishing Ltd and Sissa Medialab."
,10.1002/sim.7555,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037643262&doi=10.1002%2fsim.7555&partnerID=40&md5=606729101faa5281c959bbea20a65f0e,"When assessing association between a binary trait and some covariates, the binary response may be subject to unidirectional misclassification. Unidirectional misclassification can occur when revealing a particular level of the trait is associated with a type of cost, such as a social desirability or financial cost. The feasibility of addressing misclassification is commonly obscured by model identification issues. The current paper attempts to study the efficacy of inference when the binary response variable is subject to unidirectional misclassification. From a theoretical perspective, we demonstrate that the key model parameters possess identifiability, except for the case with a single binary covariate. From a practical standpoint, the logistic model with quantitative covariates can be weakly identified, in the sense that the Fisher information matrix may be near singular. This can make learning some parameters difficult under certain parameter settings, even with quite large samples. In other cases, the stronger identification enables the model to provide more effective adjustment for unidirectional misclassification. An extension to the Poisson approximation of the binomial model reveals the identifiability of the Poisson and zero-inflated Poisson models. For fully identified models, the proposed method adjusts for misclassification based on learning from data. For binary models where there is difficulty in identification, the method is useful for sensitivity analyses on the potential impact from unidirectional misclassification. Copyright © 2017 John Wiley & Sons, Ltd."
4,10.1016/j.vaccine.2018.02.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041898922&doi=10.1016%2fj.vaccine.2018.02.020&partnerID=40&md5=3273f0432b7d0d10acd4ac65341ec6b6,"Background: The expansion of childhood vaccination programs in low and middle income countries has been a substantial public health success story. Indicators of the performance of intervention programmes such as coverage levels and numbers covered are typically measured through national statistics or at the scale of large regions due to survey design, administrative convenience or operational limitations. These mask heterogeneities and ‘coldspots’ of low coverage that may allow diseases to persist, even if overall coverage is high. Hence, to decrease inequities and accelerate progress towards disease elimination goals, fine-scale variation in coverage should be better characterized. Methods: Using measles as an example, cluster-level Demographic and Health Surveys (DHS) data were used to map vaccination coverage at 1 km spatial resolution in Cambodia, Mozambique and Nigeria for varying age-group categories of children under five years, using Bayesian geostatistical techniques built on a suite of publicly available geospatial covariates and implemented via Markov Chain Monte Carlo (MCMC) methods. Results: Measles vaccination coverage was found to be strongly predicted by just 4–5 covariates in geostatistical models, with remoteness consistently selected as a key variable. The output 1 × 1 km maps revealed significant heterogeneities within the three countries that were not captured using province-level summaries. Integration with population data showed that at the time of the surveys, few districts attained the 80% coverage, that is one component of the WHO Global Vaccine Action Plan 2020 targets. Conclusion: The elimination of vaccine-preventable diseases requires a strong evidence base to guide strategies and inform efficient use of limited resources. The approaches outlined here provide a route to moving beyond large area summaries of vaccination coverage that mask epidemiologically-important heterogeneities to detailed maps that capture subnational vulnerabilities. The output datasets are built on open data and methods, and in flexible format that can be aggregated to more operationally-relevant administrative unit levels. © 2018 The Author(s)"
1,10.1016/j.ecolmodel.2018.01.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041440662&doi=10.1016%2fj.ecolmodel.2018.01.014&partnerID=40&md5=77ffa1511c931da5fee30464445736e3,"Forest models are increasingly being used to study ecosystem functioning, through simulation of carbon fluxes and productivity in different biomes and plant functional types all over the world. Several forest models based on the concept of Light Use Efficiency (LUE) rely mostly on a simplified mathematical structure and empirical parameters, require little amount of data to be run, and their computations are usually fast. However, possible calibration issues must be investigated in order to ensure reliable results. Here we addressed the important issue of delayed convergence when calibrating LUE models, characterized by a multiplicative structure, with a Bayesian approach. We tested two models (Prelued and the Horn and Schulz (2011a) model), applying three Markov Chain Monte Carlo-based algorithms with different number of iterations, and different sets of prior parameter distributions with increasing information content. The results showed that recently proposed algorithms for adaptive calibration did not confer a clear advantage over the Metropolis–Hastings Random Walk algorithm for the forest models used here, and that a high number of iterations is required to stabilize in the convergence region. This can be partly explained by the multiplicative mathematical structure of the models, with high correlations between parameters, and by the use of empirical parameters with neither ecological nor physiological meaning. The information content of the prior distributions of the parameters did not play a major role in reaching convergence with a lower number of iterations. We conclude that there is a need for a more careful approach to calibration to solve potential problems when applying models characterized by a multiplicative mathematical structure. Moreover, the calibration proved time consuming and mathematically difficult, so advantages of using a computationally fast and user-friendly model were lost due to the calibration process needed to obtain reliable results. © 2018 Elsevier B.V."
,10.1109/CAMSAP.2017.8313187,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050734368&doi=10.1109%2fCAMSAP.2017.8313187&partnerID=40&md5=b2d076e661b0ada3e48109a6296d2f00,"In this paper, we investigate a new imaging denoising algorithm for single-photon applications where the classical Poisson noise assumption does not hold. Precisely, we consider two different acquisition scenarios where the unknown intensity profile is to be recovered from subsampled measurements following binomial or geometric distributions, whose parameters are nonlinearly related to the intensities of interest. Adopting a Bayesian approach, a flexible prior model is assigned to the unknown intensity field and an adaptive Markov chain Monte Carlo methods is used to perform Bayesian inference. In particular, it allows us to automatically adjust the amount of regularisation required for satisfactory image inpainting/restoration. The performance of the proposed model/method is assessed quantitatively through a series of experiments conducted with controlled data and the results obtained are very promising for future analysis of multidimensional single-photon images. © 2017 IEEE."
1,10.1109/CAMSAP.2017.8313170,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050731405&doi=10.1109%2fCAMSAP.2017.8313170&partnerID=40&md5=1b4ec059f9cf565832ec81f0e6e89afb,"We report the results of a series of numerical studies examining the convergence rate for some approximate representations of α-stable distributions, which are a highly intractable class of distributions for inference purposes. Our proposed representation turns the intractable inference for an infinite-dimensional series of parameters into an (approximately) conditionally Gaussian representation, to which standard inference procedures such as Expectation-Maximization (EM), Markov chain Monte Carlo (MCMC) and Particle Filtering can be readily applied. While we have previously proved the asymptotic convergence of this representation, here we study the rate of this convergence for finite values of a truncation parameter, c. This allows the selection of appropriate truncations for different parameter configurations and for the accuracy required for the model. The convergence is examined directly in terms of cumulative distribution functions and densities, through the application of the Berry theorems and Parseval theorems. Our results indicate that the behaviour of our representations is significantly superior to that of representations that simply truncate the series with no Gaussian residual term. © 2017 IEEE."
,10.1109/CAMSAP.2017.8313132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050740015&doi=10.1109%2fCAMSAP.2017.8313132&partnerID=40&md5=4370665445a0a4144f094d46b7a71a6a,"Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. This is the motivation underlying the class of recurrent switching linear dynamical systems (rSLDS) [1], which build on the standard SLDS by introducing a model of how discrete transition probabilities depend on observations or continuous latent states. Previous work relied on Markov chain Monte Carlo algorithms and augmentation schemes for inference, but these methods only applied to a limited class of recurrent dependencies. Here we relax these constraints and consider recurrent dependencies specified by arbitrary parametric, nonlinear functions. We derive two structure-exploiting variational inference algorithms for these challenging models. Both leverage the conditionally linear Gaussian and Markovian nature of the models to perform efficient posterior inference. © 2017 IEEE."
,10.1146/annurev-statistics-031017-100141,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043485393&doi=10.1146%2fannurev-statistics-031017-100141&partnerID=40&md5=570f3fd996d5d126d9a2eb5c7351a093,"Markov chain Monte Carlo methods have revolutionized mathematical computation and enabled statistical inference within many previously intractable models. In this context, Hamiltonian dynamics have been proposed as an efficient way of building chains that can explore probability densities efficiently. The method emerges from physics and geometry, and these links have been extensively studied over the past thirty years. The aim of this review is to provide a comprehensive introduction to the geometric tools used in Hamiltonian Monte Carlo at a level accessible to statisticians, machine learners, and other users of the methodology with only a basic understanding of Monte Carlo methods. This will be complemented with some discussion of the most recent advances in the field, which we believe will become increasingly relevant to scientists. Copyright © 2018 by Annual Reviews. All rights reserved."
1,10.1109/AICCSA.2017.43,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046092320&doi=10.1109%2fAICCSA.2017.43&partnerID=40&md5=b1a124a2cee0767f685cde6f22ab86fa,"We propose a Bayesian approach to learn finite generalized inverted Dirichlet mixture models. The developed approach performs simultaneous parameters estimation, model complexity determination, and feature selection via a reversible jump Markov Chain Monte Carlo (RJMCMC) algorithm. A challenging application that concerns video forgery detection is deployed to validate our statistical framework and to show its merits. © 2017 IEEE."
2,10.1146/annurev-statistics-031017-100232,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043476178&doi=10.1146%2fannurev-statistics-031017-100232&partnerID=40&md5=a6458cda5d642ba57d3c57155dab5c50,"State-space models can be used to incorporate subject knowledge on the underlying dynamics of a time series by the introduction of a latent Markov state process. A user can specify the dynamics of this process together with how the state relates to partial and noisy observations that have been made. Inference and prediction then involve solving a challenging inverse problem: calculating the conditional distribution of quantities of interest given the observations. This article reviews Monte Carlo algorithms for solving this inverse problem, covering methods based on the particle filter and the ensemble Kalman filter. We discuss the challenges posed by models with high-dimensional states, joint estimation of parameters and the state, and inference for the history of the state process. We also point out some potential new developments that will be important for tackling cutting-edge filtering applications. Copyright © 2018 by Annual Reviews. All rights reserved."
,10.1109/TAC.2018.2813004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043402386&doi=10.1109%2fTAC.2018.2813004&partnerID=40&md5=6c7d8c265fa786f36e1447fd76585b46,"Bayesian nonlinear system identification for one of the major classes of dynamic model, the nonlinear autoregressive with exogenous input (NARX) model, has not been widely studied to date. Markov chain Monte Carlo (MCMC) methods have been developed, which tend to be accurate but can also be slow to converge. In this contribution, we present a novel, computationally efficient solution to sparse Bayesian identification of the NARX model using variational inference, which is orders of magnitude faster than MCMC methods. A sparsity-inducing hyper-prior is used to solve the structure detection problem. Key results include: 1. successful demonstration of the method on low signal-to-noise ratio signals (down to 2dB); 2. successful benchmarking in terms of speed and accuracy against a number of other algorithms: Bayesian LASSO, reversible jump MCMC, forward regression orthogonalisation, LASSO and simulation error minimisation with pruning; 3. accurate identification of a real world system, an electroactive polymer; and 4. demonstration for the first time of numerically propagating the estimated nonlinear time-domain model parameter uncertainty into the frequency-domain. IEEE"
1,10.1080/03610926.2018.1440306,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042944127&doi=10.1080%2f03610926.2018.1440306&partnerID=40&md5=823cf78d5994987cfa47aedb94615431,"A Bayesian approach based on the Markov Chain Monte Carlo technique is proposed for the non-homogeneous gamma process with power-law shape function. Vague and informative priors, formalized on some quantities having a “physical” meaning, are provided. Point and interval estimation of process parameters and some functions thereof are developed, as well as prediction on some observable quantities that are useful in defining the maintenance strategy is proposed. Some useful approximations are derived for the conditional and unconditional mean and median of the residual life to reduce computational time. Finally, the proposed approach is applied to a real dataset. © 2018 Taylor & Francis Group, LLC"
1,10.5194/npg-25-145-2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042903647&doi=10.5194%2fnpg-25-145-2018&partnerID=40&md5=3b86a555a741cec56d8bf026e758a964,"We develop a general framework for the frequency analysis of irregularly sampled time series. It is based on the Lomb-Scargle periodogram, but extended to algebraic operators accounting for the presence of a polynomial trend in the model for the data, in addition to a periodic component and a background noise. Special care is devoted to the correlation between the trend and the periodic component. This new periodogram is then cast into the Welch overlapping segment averaging (WOSA) method in order to reduce its variance. We also design a test of significance for the WOSA periodogram, against the background noise. The model for the background noise is a stationary Gaussian continuous autoregressive-moving-average (CARMA) process, more general than the classical Gaussian white or red noise processes. CARMA parameters are estimated following a Bayesian framework. We provide algorithms that compute the confidence levels for the WOSA periodogram and fully take into account the uncertainty in the CARMA noise parameters. Alternatively, a theory using point estimates of CARMA parameters provides analytical confidence levels for the WOSA periodogram, which are more accurate than Markov chain Monte Carlo (MCMC) confidence levels and, below some threshold for the number of data points, less costly in computing time. We then estimate the amplitude of the periodic component with least-squares methods, and derive an approximate proportionality between the squared amplitude and the periodogram. This proportionality leads to a new extension for the periodogram: the weighted WOSA periodogram, which we recommend for most frequency analyses with irregularly sampled data. The estimated signal amplitude also permits filtering in a frequency band. Our results generalise and unify methods developed in the fields of geosciences, engineering, astronomy and astrophysics. They also constitute the starting point for an extension to the continuous wavelet transform developed in a companion article (Lenoir and Crucifix, 2018). All the methods presented in this paper are available to the reader in the Python package WAVEPAL. © 2018 Author(s)."
1,10.5194/npg-25-175-2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042934130&doi=10.5194%2fnpg-25-175-2018&partnerID=40&md5=d123f5e42fa5f5685b9ebcbf49dc2062,"Geophysical time series are sometimes sampled irregularly along the time axis. The situation is particularly frequent in palaeoclimatology. Yet, there is so far no general framework for handling the continuous wavelet transform when the time sampling is irregular. <br><br> Here we provide such a framework. To this end, we define the scalogram as the continuous-wavelet-transform equivalent of the extended Lomb-Scargle periodogram defined in Part 1 of this study (Lenoir and Crucifix, 2018). The signal being analysed is modelled as the sum of a locally periodic component in the time-frequency plane, a polynomial trend, and a background noise. The mother wavelet adopted here is the Morlet wavelet classically used in geophysical applications. The background noise model is a stationary Gaussian continuous autoregressive-moving-average (CARMA) process, which is more general than the traditional Gaussian white and red noise processes. The scalogram is smoothed by averaging over neighbouring times in order to reduce its variance. The Shannon-Nyquist exclusion zone is however defined as the area corrupted by local aliasing issues. The local amplitude in the time-frequency plane is then estimated with least-squares methods. We also derive an approximate formula linking the squared amplitude and the scalogram. Based on this property, we define a new analysis tool: the weighted smoothed scalogram, which we recommend for most analyses. The estimated signal amplitude also gives access to band and ridge filtering. Finally, we design a test of significance for the weighted smoothed scalogram against the stationary Gaussian CARMA background noise, and provide algorithms for computing confidence levels, either analytically or with Monte Carlo Markov chain methods. All the analysis tools presented in this article are available to the reader in the Python package WAVEPAL. © 2018 Copernicus GmbH. All rights reserved."
,10.1080/03610926.2017.1316858,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029725923&doi=10.1080%2f03610926.2017.1316858&partnerID=40&md5=e1fb70118d0760f9c284d9159635b130,"In this article, we present the analysis of head and neck cancer data using generalized inverse Lindley stress–strength reliability model. We propose Bayes estimators for estimating P(X > Y), when X and Y represent survival times of two groups of cancer patients observed under different therapies. The X and Y are assumed to be independent generalized inverse Lindley random variables with common shape parameter. Bayes estimators are obtained under the considerations of symmetric and asymmetric loss functions assuming independent gamma priors. Since posterior becomes complex and does not possess closed form expressions for Bayes estimators, Lindley’s approximation and Markov Chain Monte Carlo techniques are utilized for Bayesian computation. An extensive simulation experiment is carried out to compare the performances of Bayes estimators with the maximum likelihood estimators on the basis of simulated risks. Asymptotic, bootstrap, and Bayesian credible intervals are also computed for the P(X > Y). © 2018 Taylor & Francis Group, LLC."
,10.1080/02331888.2017.1405419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034645929&doi=10.1080%2f02331888.2017.1405419&partnerID=40&md5=c1bf1ad40b48382dac7fd0e9514f7775,"This article is devoted to the development of product of spacings estimator for a Progressive hybrid Type-I censoring scheme with binomial removals. The experimental units are assumed to follow inverse Lindley distribution. We propose a Bayes estimator of associated scale parameter based on the product of spacings function and simultaneously compare it with that obtained under a usual Bayesian estimation procedure. The estimators are obtained under the squared error loss function along with corresponding HP intervals evaluated by using the Markov chain Monte-Carlo technique. The classical product of spacings estimator has also been derived and compared with the maximum likelihood estimator in addition to 95% average asymptotic confidence intervals. The applicability of the proposed methods is demonstrated by analysing a real data of guinea pigs affected with tuberculosis for the considered censoring scheme. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
5,10.1115/1.4037450,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042803982&doi=10.1115%2f1.4037450&partnerID=40&md5=bdebecbad569543db5d0aabf91367d80,"The transitional Markov chain Monte Carlo (TMCMC) is one of the efficient algorithms for performing Markov chain Monte Carlo (MCMC) in the context of Bayesian uncertainty quantification in parallel computing architectures. However, the features that are associated with its efficient sampling are also responsible for its introducing of bias in the sampling. We demonstrate that the Markov chains of each subsample in TMCMC may result in uneven chain lengths that distort the intermediate target distributions and introduce bias accumulation in each stage of the TMCMC algorithm. We remedy this drawback of TMCMC by proposing uniform chain lengths, with or without burn-in, so that the algorithm emphasizes sequential importance sampling (SIS) over MCMC. The proposed Bayesian annealed sequential importance sampling (BASIS) removes the bias of the original TMCMC and at the same time increases its parallel efficiency. We demonstrate the advantages and drawbacks of BASIS in modeling of bridge dynamics using finite elements and a disk-wall collision using discrete element methods. Copyright © 2018 by ASME."
,10.1016/j.ymssp.2017.09.035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032864707&doi=10.1016%2fj.ymssp.2017.09.035&partnerID=40&md5=38dc0f9f670f3e6a27bb7aa664c400c8,"The problem of combined state and parameter estimation in nonlinear state space models, based on Bayesian filtering methods, is considered. A novel approach, which combines Rao-Blackwellized particle filters for state estimation with Markov chain Monte Carlo (MCMC) simulations for parameter identification, is proposed. In order to ensure successful performance of the MCMC samplers, in situations involving large amount of dynamic measurement data and (or) low measurement noise, the study employs a modified measurement model combined with an importance sampling based correction. The parameters of the process noise covariance matrix are also included as quantities to be identified. The study employs the Rao-Blackwellization step at two stages: one, associated with the state estimation problem in the particle filtering step, and, secondly, in the evaluation of the ratio of likelihoods in the MCMC run. The satisfactory performance of the proposed method is illustrated on three dynamical systems: (a) a computational model of a nonlinear beam-moving oscillator system, (b) a laboratory scale beam traversed by a loaded trolley, and (c) an earthquake shake table study on a bending-torsion coupled nonlinear frame subjected to uniaxial support motion. © 2017 Elsevier Ltd"
,10.1007/s11222-017-9730-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014057596&doi=10.1007%2fs11222-017-9730-1&partnerID=40&md5=e7ad92a947011337b29626ce2ddd04c6,"We describe parallel Markov chain Monte Carlo methods that propagate a collective ensemble of paths, with local covariance information calculated from neighbouring replicas. The use of collective dynamics eliminates multiplicative noise and stabilizes the dynamics, thus providing a practical approach to difficult anisotropic sampling problems in high dimensions. Numerical experiments with model problems demonstrate that dramatic potential speedups, compared to various alternative schemes, are attainable. © 2017, The Author(s)."
1,10.1016/j.quageo.2017.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037355286&doi=10.1016%2fj.quageo.2017.11.003&partnerID=40&md5=13bde687616ebeec2da695fae047606a,"This study presents MCDoseE 2.0, a new fitting program for ESR dating dose response curve (DRC) fitting and dose calculation. The standalone software was specifically designed to remove assumed data weighting, and instead to obtain a full probabilistic solution of the DRC by propagating the uncertainties associated with the measured ESR intensities. It uses a non-linear Bayesian framework, specifically a Markov Chain Monte Carlo (MCMC) scheme based on the Metropolis-Hastings algorithm, where the solution is a probability distribution for the equivalent dose, according to the precision of the measurements. In this paper, we investigate the capabilities and limitations of MCDoseE 2.0 by comparing our results to those obtained with OriginPro 9.1®, a proven and commonly used commercial software package. The two programs were evaluated against both known-dose samples and random archaeological tooth enamel and quartz samples, using three commonly used DRC fitting functions. We found that both programs provide highly consistent results. When comparing the dose estimates obtained by both programs we found that 90% of the solutions are statistically indistinguishable regardless of the data weighting assumption used in OriginPro. We also found that MCDoseE 2.0 offers an increased precision on the ending results compared to the commercial software, as long as each measured ESR uncertainty remains within 2-sigma range of the mean error value of all measured ESR uncertainties of the dataset. The accuracy of the fitting results given by MCDoseE 2.0 are undeniably dependent on the measurement accuracy, and emphasises the need of a proper assessment of the experimental errors in the ESR intensities. A copy of the program is available in Supplementary information, and some basic instructions for its use are provided, as well as recommendations to ensure reliable and accurate fitting results. © 2017 Elsevier B.V."
,10.1016/j.dsp.2017.11.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037533509&doi=10.1016%2fj.dsp.2017.11.012&partnerID=40&md5=a9c2aea5be181929369e9448996c6725,"Monte Carlo methods are essential tools for Bayesian inference. Gibbs sampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively used in signal processing, machine learning, and statistics, employed to draw samples from complicated high-dimensional posterior distributions. The key point for the successful application of the Gibbs sampler is the ability to draw efficiently samples from the full-conditional probability density functions. Since in the general case this is not possible, in order to speed up the convergence of the chain, it is required to generate auxiliary samples whose information is eventually disregarded. In this work, we show that these auxiliary samples can be recycled within the Gibbs estimators, improving their efficiency with no extra cost. This novel scheme arises naturally after pointing out the relationship between the standard Gibbs sampler and the chain rule used for sampling purposes. Numerical simulations involving simple and real inference problems confirm the excellent performance of the proposed scheme in terms of accuracy and computational efficiency. In particular we give empirical evidence of performance in a toy example, inference of Gaussian processes hyperparameters, and learning dependence graphs through regression. © 2017 Elsevier Inc."
2,10.1016/j.jclepro.2017.11.246,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039419525&doi=10.1016%2fj.jclepro.2017.11.246&partnerID=40&md5=b41840b554b27a8b65df0739c825fca5,"Parameter uncertainty inherent in reservoir operation affects operation model robustness and has been considered in conventional operation focusing on improving hydropower generation. With more attention paid to ecological environment protection recently, riverine ecosystem protection requires environmental flow (e-flow) management to sustain a near-natural flow regime. Whether there is e-flow management in reservoir operation has an impact on the uncertainty of reservoir operation, but parameter uncertainty was rarely considered in reservoir operation with e-flow management. In this study, a framework is proposed for performing parameter uncertainty analysis in reservoir operation associated with e-flow management. Both e-flow requirements and hydropower generation are considered in reservoir operation to sustain the harmonious development between ecological environment and human society. To compare the effect of different e-flow managements on the uncertainty of reservoir operation, three e-flow management scenarios are set. The Metropolis-Hastings algorithm of Markov Chain Monte Carlo (MCMC) sampling approach was applied for parameter estimation and uncertainty quantification. We used this framework in a case study of Nuozhadu hydropower station on the Lancang River in southern China to test its effectiveness. The results demonstrated that parameter uncertainty greatly affects the robustness of reservoir operation model. The comparison of reservoir operation under different e-flow management scenarios shows that more detailed e-flow management can effectively reduce uncertainty in reservoir operation and sustain the near-natural flow regime in a river. © 2017 Elsevier Ltd"
3,10.1109/TITS.2017.2700481,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019887969&doi=10.1109%2fTITS.2017.2700481&partnerID=40&md5=5a211e21000125ed731da3d99d22382b,"Geo-location data from the check-ins made in online social media offers us information, in new ways, to understand activity-location choices of a large number of people. However, one of the major challenges of using check-in data is that it has missing activities, since users share their activities voluntarily. In this paper, we present a probabilistic modeling approach to reconstruct user activity-location sequences from this incomplete activity participation information. Specifically, we answer the question of how to predict an individual's next activity, its duration and location given the incomplete trajectory data. The model describes the dynamics of individual activity participation behavior evolving over continuous time. A semi-Markov modeling approach is used to capture the stochastic processes involved in the activity generation mechanism. We present a particle-based Markov chain Monte Carlo sampler to run inference over the model. We further develop an expectation-maximization algorithm to learn the unknown parameters of the model from incomplete trajectory data. Finally, the method is applied to synthetically generated activity-location sequences and a data set of Foursquare check-ins of the users from New York City. Our experiments show that this method can successfully extract the true transition and duration distributions given the incomplete trajectory information. The proposed approach can help building many intelligent transportation applications using check-in data. © 2000-2011 IEEE."
1,10.1016/j.ijar.2017.12.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041487626&doi=10.1016%2fj.ijar.2017.12.005&partnerID=40&md5=290420e36f6110d36db7af685083bf0a,"In official statistics, interest for data integration has been increasingly growing, due to the need of extracting information from different sources. However, the effects of these procedures on the validity of the resulting statistical analyses has been disregarded for a long time. In recent years, it has been largely recognized that linkage is not an error-free procedure and linkage errors, as false links and/or missed links, can invalidate the reliability of estimates in standard statistical models. In this paper we consider the general problem of making inference using data that have been probabilistically linked and we explore the effect of potential linkage errors on the production of small area estimates. We describe the existing methods and propose and compare new approaches both from a classical and from a Bayesian perspective. We perform a simulation study to assess pros and cons of each proposed method; our simulation scheme aims at reproducing a realistic context both for small area estimation and record linkage procedures. © 2018 Elsevier Inc."
1,10.1109/TCBB.2015.2485223,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044972648&doi=10.1109%2fTCBB.2015.2485223&partnerID=40&md5=dd5909b497410e5aeba6ecf494f3c221,"Differential gene expression testing is an analysis commonly applied to RNA-Seq data. These statistical tests identify genes that are significantly different across phenotypes. We extend this testing paradigm to multivariate gene interactions from a classification perspective with the goal to detect novel gene interactions for the phenotypes of interest. This is achieved through our novel computational framework comprised of a hierarchical statistical model of the RNA-Seq processing pipeline and the corresponding optimal Bayesian classifier. Through Markov Chain Monte Carlo sampling and Monte Carlo integration, we compute quantities where no analytical formulation exists. The performance is then illustrated on an expression dataset from a dietary intervention study where we identify gene pairs that have low classification error yet were not identified as differentially expressed. Additionally, we have released the software package to perform OBC classification on RNA-Seq data under an open source license and is available at http://bit.ly/obc-package. © 2004-2012 IEEE."
,10.1016/j.icheatmasstransfer.2018.02.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042868661&doi=10.1016%2fj.icheatmasstransfer.2018.02.001&partnerID=40&md5=c9979b57a0ba33b9ba847b6e481898ce,"This communication deals with the solution of an inverse parameter estimation problem with the dual-phase-lag heat conduction model. The case considered involves the heating of a metal-oxide-semiconductor field-effect transistor, for time and spatial scales where the validity of the classical heat conduction model based on Fourier's Law, which considers an infinite speed of propagation of thermal waves, has been questioned. The Markov chain Monte Carlo method is applied for the estimation of the parameters within the Bayesian framework of statistics, by using simulated transient temperature measurements. © 2018 Elsevier Ltd"
5,10.1111/jmi.12623,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041953179&doi=10.1111%2fjmi.12623&partnerID=40&md5=5e70ffa6dffea57d31b9fd73e25692df,A thresholded Gaussian random field model is developed for the microstructure of porous materials. Defining the random field as a solution to stochastic partial differential equation allows for flexible modelling of nonstationarities in the material and facilitates computationally efficient methods for simulation and model fitting. A Markov Chain Monte Carlo algorithm is developed and used to fit the model to three-dimensional confocal laser scanning microscopy images. The methods are applied to study a porous ethylcellulose/hydroxypropylcellulose polymer blend that is used as a coating to control drug release from pharmaceutical tablets. The aim is to investigate how mass transport through the material depends on the microstructure. We derive a number of goodness-of-fit measures based on numerically calculated diffusion through the material. These are used in combination with measures that characterize the geometry of the pore structure to assess model fit. The model is found to fit stationary parts of the material well. © 2017 The Authors Journal of Microscopy © 2017 Royal Microscopical Society
,10.1016/j.knosys.2017.12.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037722756&doi=10.1016%2fj.knosys.2017.12.005&partnerID=40&md5=a538c704204b84b23fdf9658e9e01ef5,"Traditionally, research on network theory focused on studying graphs with equivalent entities failing to deliberate the useful supplementary information related to the dynamic properties of the complex network interactions. This paper tries to study the evolution process of dynamic complex networks from a multilayer perspective by analyzing the properties of naturally multilayered web-based directed complex social networks of Google+ and Twitter, and undirected collaborative networks of DBLP and ASTRO-PH, thereby proposing a new non-parametric knowledge-based multilayer link recommendation approach. The paper investigates the layers’ evolution throughout the network evolution, inspects the evolution of each node's membership in different layers by an Infinite Factorial Hidden Markov Model, and finally formulates the intra-layer and inter-layer link generation process. Some Markov Chain Monte Carlo sampling strategies are driven to simulate parameters of the proposed multilayer model, using certain synthetic and real complex network datasets. Experimental results indicate great improvements in the performance of the proposed multilayer link recommendation approach in terms of certain analyzed performance measures. © 2017 Elsevier B.V."
,10.1061/AJRUA6.0000949,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045306299&doi=10.1061%2fAJRUA6.0000949&partnerID=40&md5=fb01de6861da968526a36be2bc81615a,"This study investigates the use of big data analytics in uncertainty quantification and applies the proposed framework to structural diagnosis and prognosis. With smart sensor technology making progress and low-cost online monitoring becoming increasingly possible, large quantities of data can be acquired during monitoring, thus exceeding the capacity of traditional data analytics techniques. The authors explore a software application technique to parallelize data analytics and efficiently handle the high volume, velocity, and variety of sensor data. Next, both forward and inverse problems in uncertainty quantification are investigated with this efficient computational approach. The authors use Bayesian methods for the inverse problem of diagnosis and parallelize numerical integration techniques such as Markov-chain Monte Carlo simulation and particle filter. To predict damage growth and the structure's remaining useful life (forward problem), Monte Carlo simulation is used to propagate the uncertainties (both aleatory and epistemic) to the future state. The software approach is again applied to drive the parallelization of multiple finite-element analysis (FEA) runs, thus greatly saving on the computational cost. The proposed techniques are illustrated for the efficient diagnosis and prognosis of alkali-silica reactions in a concrete structure. © 2018 American Society of Civil Engineers."
,10.1016/j.image.2018.01.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041452771&doi=10.1016%2fj.image.2018.01.003&partnerID=40&md5=25e9a64ab2db9364190c4c9db32a3b86,"Microscopic analysis of paper printing shows regularly spaced dots whose random shape depends on the printing technology, the configuration of the printer as well as the paper properties. The modelling and identification of paper and ink interactions are required for qualifying the printing quality, for controlling the printing process and for application in authentication as well. This paper proposes an approach to identify the authentic printer source using micro-tags consisting of microscopic printed dots embedded in the documents. These random shape features are modelled and extracted as a signature for a particular printer. In the paper, we propose a probabilistic model consisting of vector parameters using a spatial interaction binary model with inhomogeneous Markov chain. These parameters determine the location and describe the diverse micro random structures of microscopic printed dots. A Markov chain Monte Carlo (MCMC) algorithm is thus developed to approximate the Minimum Mean Squared Error estimator. The performance is assessed through numerical simulations. The real printed dots from the common printing technologies (conventional offset, waterless offset, inkjet, laser) are used to assess the effectiveness of the model. © 2018 Elsevier B.V."
1,10.1109/TPAMI.2017.2689007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041965843&doi=10.1109%2fTPAMI.2017.2689007&partnerID=40&md5=b9e67b1801fb14d8f11b8734e9bce95f,"In this paper, we present an attribute grammar for solving two coupled tasks: i) parsing a 2D image into semantic regions; and ii) recovering the 3D scene structures of all regions. The proposed grammar consists of a set of production rules, each describing a kind of spatial relation between planar surfaces in 3D scenes. These production rules are used to decompose an input image into a hierarchical parse graph representation where each graph node indicates a planar surface or a composite surface. Different from other stochastic image grammars, the proposed grammar augments each graph node with a set of attribute variables to depict scene-level global geometry, e.g., camera focal length, or local geometry, e.g., surface normal, contact lines between surfaces. These geometric attributes impose constraints between a node and its off-springs in the parse graph. Under a probabilistic framework, we develop a Markov Chain Monte Carlo method to construct a parse graph that optimizes the 2D image recognition and 3D scene reconstruction purposes simultaneously. We evaluated our method on both public benchmarks and newly collected datasets. Experiments demonstrate that the proposed method is capable of achieving state-of-The-Art scene reconstruction of a single image. © 1979-2012 IEEE."
,10.1115/1.4037557,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047053286&doi=10.1115%2f1.4037557&partnerID=40&md5=7397417ab97b0d2a74824bd8db07b3f2,"We demonstrate a statistical procedure for learning a high-order eddy viscosity model (EVM) from experimental data and using it to improve the predictive skill of a Reynoldsaveraged Navier-Stokes (RANS) simulator. The method is tested in a three-dimensional (3D), transonic jet-in-crossflow (JIC) configuration. The process starts with a cubic eddy viscosity model (CEVM) developed for incompressible flows. It is fitted to limited experimental JIC data using shrinkage regression. The shrinkage process removes all the terms from the model, except an intercept, a linear term, and a quadratic one involving the square of the vorticity. The shrunk eddy viscosity model is implemented in an RANS simulator and calibrated, using vorticity measurements, to infer three parameters. The calibration is Bayesian and is solved using a Markov chain Monte Carlo (MCMC) method. A 3D probability density distribution for the inferred parameters is constructed, thus quantifying the uncertainty in the estimate. The phenomenal cost of using a 3D flow simulator inside an MCMC loop is mitigated by using surrogate models (""curve-fits""). A support vector machine classifier (SVMC) is used to impose our prior belief regarding parameter values, specifically to exclude nonphysical parameter combinations. The calibrated model is compared, in terms of its predictive skill, to simulations using uncalibrated linear and CEVMs. We find that the calibrated model, with one quadratic term, is more accurate than the uncalibrated simulator. The model is also checked at a flow condition at which the model was not calibrated. Copyright © 2018 by ASME."
2,10.1016/j.ultras.2017.11.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037544171&doi=10.1016%2fj.ultras.2017.11.017&partnerID=40&md5=2af9083f3760fb379e6c14499fad5a8b,"This paper presents a study on model assessment for predicting structural fatigue life using Lamb waves. Lamb wave coupon testing is performed for model development. Three damage sensitive features, namely normalized energy, phase change, and correlation coefficient are extracted from Lamb wave data and are used to quantify the crack size. Four data-driven models are proposed. The average relative error and the probability of detection (POD) are proposed as two measures to evaluate the performance of the four models. To study the influence of model choice on the probabilistic fatigue life prediction, probability density functions of the actual crack size are obtained from the POD models given the Lamb wave data. Crack growth model parameters are statistically identified using Bayesian parameter estimation with Markov chain Monte Carlo simulations. The model assessment and the influence of model choice on fatigue life prediction are made using both coupon testing data with artificial cracks and realistic lap joint testing data with naturally developed cracks. © 2017 Elsevier B.V."
2,10.1109/TCST.2017.2672402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014865203&doi=10.1109%2fTCST.2017.2672402&partnerID=40&md5=d5884da22a87731add90615b523b7208,"Battery impedance spectroscopy models are given by fractional-order (FO) differential equations. In the discrete-time domain, they give rise to state-space models where the latent process is not Markovian. Parameter estimation for these models is, therefore, challenging, especially for noncommensurate FO models. In this paper, we propose a Bayesian approach to identify the parameters of generic FO systems. The computational challenge is tackled with particle Markov chain Monte Carlo methods, with an implementation specifically designed for the non-Markovian setting. Two examples are provided. In a first example, the approach is applied to identify a battery commensurate FO model with a single constant phase element (CPE) by using real data. We compare the proposed approach to an instrumental variable method. Then, we consider a noncommensurate FO model with more than one CPE and synthetic data sets, investigating how the proposed method enables the study of various effects on parameter identification, such as the data length, the magnitude of the input signal, the choice of prior, and the measurement noise. © 2017 IEEE."
,10.1002/cjs.11343,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030667645&doi=10.1002%2fcjs.11343&partnerID=40&md5=dc2fa6e491833b1e51df60e3f13bd38b,"Markov Chain Monte Carlo (MCMC) sampling from a posterior distribution corresponding to a massive data set can be computationally prohibitive as producing one sample requires a number of operations that is linear in the data size. In this article we introduce a new communication-free parallel method, the “Likelihood Inflating Sampling Algorithm (LISA),” that significantly reduces computational costs by randomly splitting the data set into smaller subsets and running MCMC methods “independently” in parallel on each subset using different processors. Each processor will be used to run an MCMC chain that samples sub-posterior distributions which are defined using an “inflated” likelihood function. We develop a strategy for combining the draws from different sub-posteriors to study the full posterior of the Bayesian Additive Regression Trees (BART) model. The performance of the method is tested using simulated data and a large socio-economic study. The Canadian Journal of Statistics 46: 147–175; 2018 © 2017 Statistical Society of Canada. © 2017 Statistical Society of Canada"
,10.1016/j.regsciurbeco.2018.01.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041464100&doi=10.1016%2fj.regsciurbeco.2018.01.001&partnerID=40&md5=38a6a44c1527899d9bc78628cc2f834c,"There is a great deal of literature regarding use of non-geographically based connectivity matrices or combinations of geographic and non-geographic structures in spatial econometrics models. We explore alternative approaches for constructing convex combinations of different types of dependence between observations. Pace and LeSage (2002) as well as Hazır et al. (2016) use convex combinations of different connectivity matrices to form a single weight matrix that can be used in conventional spatial regression estimation and inference. An example for the case of two weight matrices, W1,W2 reflecting different types of dependence between a cross-section of regions, firms, individuals etc., located in space would be: Wc=γ1W1+(1−γ1)W2,0≤γ1≤1. The matrix Wc reflects a convex combination of the two weight matrices, with the scalar parameter γ1 indicating the relative importance assigned to each type of dependence. We explore issues that arise in producing estimates and inferences from these more general cross-sectional regression relationships in a Bayesian framework. We propose two procedures to estimate such models and assess their finite sample properties through Monte Carlo experiments. We illustrate our methodology in an application to CEO salaries for a sample of nursing homes located in Texas. Two types of weights are considered, one reflecting spatial proximity of nursing homes and the other peer group proximity, which arise from the salary benchmarking literature. © 2018 Elsevier B.V."
,10.1093/icesjms/fsx175,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045131314&doi=10.1093%2ficesjms%2ffsx175&partnerID=40&md5=9b0f41461c97a4e964a315fa401f8cde,"Uncertainty coming from assessment models leads to risk in decision making and ignoring or misestimating it can result in an erroneous management action. Some parameters, such as selectivity or survey catchabilities, can present a wide range of shapes and the introduction of smooth functions, which up to now have not been widely used in assessment models, allows for more flexibility to capture underlying nonlinear structures. In this work a simulation study emulating a sardine population is carried out to compare three different methods for uncertainty estimation: multivariate normal distribution, bootstrap (without and with relative bias correction) and Markov chain Monte Carlo (MCMC). In order to study their performance depending on the model complexity, five different scenarios are defined depending on the shape of the smooth function of the fishing mortality. From 100 simulated datasets, performance is measured in terms of point estimation, coefficients of variation, bias, skewness, coverage probabilities, and correlation. In all approaches model fitting is carried out using the a4a framework. All three methods result in very similar performance. The main differences are found for observation variance parameters where the bootstrap and the multivariate normal approach result in underestimation of these parameters. In general, MCMC is considered to have better performance, being able to detect skewness, showing small relative bias and reaching expected coverage probabilities. It is also more efficient in terms of time consumption in comparison with bootstrapping. © The Author 2017. Published by Oxford University Press [on behalf of International Council for the Exploration of the Sea]. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com."
,10.1111/biom.12717,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019013185&doi=10.1111%2fbiom.12717&partnerID=40&md5=b6db9585507487920eb7cd444805a21e,"Advanced hepatocellular carcinoma (HCC) has limited treatment options and poor survival, therefore early detection is critical to improving the survival of patients with HCC. Current guidelines for high-risk patients include ultrasound screenings every six months, but ultrasounds are operator dependent and not sensitive for early HCC. Serum α-Fetoprotein (AFP) is a widely used diagnostic biomarker, but it has limited sensitivity and is not elevated in all HCC cases so, we incorporate a second blood-based biomarker, des’ γ carboxy-prothrombin (DCP), that has shown potential as a screening marker for HCC. The data from the Hepatitis C Antiviral Long-term Treatment against Cirrhosis (HALT-C) Trial is a valuable source of data to study biomarker screening for HCC. We assume the trajectories of AFP and DCP follow a joint hierarchical mixture model with random changepoints that allows for distinct changepoint times and subsequent trajectories of each biomarker. The changepoint indicators are jointly modeled with a Markov Random Field distribution to help detect borderline changepoints. Markov chain Monte Carlo methods are used to calculate posterior distributions, which are used in risk calculations among future patients and determine whether a patient has a positive screen. The screening algorithm was compared to alternatives in simulations studies under a range of possible scenarios and in the HALT-C Trial using cross-validation. © 2017, The International Biometric Society"
,10.1002/asmb.2276,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030463357&doi=10.1002%2fasmb.2276&partnerID=40&md5=48787ffbf82c58e1427e4ee11fb6d1a2,"One of the major challenges associated with the measurement of customer lifetime value is selecting an appropriate model for predicting customer future transactions. Among such models, the Pareto/negative binomial distribution (Pareto/NBD) is the most prevalent in noncontractual relationships characterized by latent customer defections; ie, defections are not observed by the firm when they happen. However, this model and its applications have some shortcomings. Firstly, a methodological shortcoming is that the Pareto/NBD, like all lifetime transaction models based on statistical distributions, assumes that the number of transactions by a customer follows a Poisson distribution. However, many applications have an empirical distribution that does not fit a Poisson model. Secondly, a computational concern is that the implementation of Pareto/NBD model presents some estimation challenges specifically related to the numerous evaluation of the Gaussian hypergeometric function. Finally, the model provides 4 parameters as output, which is insufficient to link the individual purchasing behavior to socio-demographic information and to predict the behavior of new customers. In this paper, we model a customer's lifetime transactions using the Conway-Maxwell-Poisson distribution, which is a generalization of the Poisson distribution, offering more flexibility and a better fit to real-world discrete data. To estimate parameters, we propose a Markov chain Monte Carlo algorithm, which is easy to implement. Use of this Bayesian paradigm provides individual customer estimates, which help link purchase behavior to socio-demographic characteristics and an opportunity to target individual customers. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1007/s00180-017-0747-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021864084&doi=10.1007%2fs00180-017-0747-x&partnerID=40&md5=dbf94a1172e4b7b3946b625ff292c719,"This paper introduces a new and computationally efficient Markov chain Monte Carlo (MCMC) estimation algorithm for the Bayesian analysis of zero, one, and zero and one inflated beta regression models. The algorithm is computationally efficient in the sense that it has low MCMC autocorrelations and computational time. A simulation study shows that the proposed algorithm outperforms the slice sampling and random walk Metropolis–Hastings algorithms in both small and large sample settings. An empirical illustration on a loss given default banking model demonstrates the usefulness of the proposed algorithm. © 2017, Springer-Verlag GmbH Germany (outside the USA)."
,10.1093/gji/ggx500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042069843&doi=10.1093%2fgji%2fggx500&partnerID=40&md5=0201eb664a5afaf2a55de609f0c2008f,"We apply a Bayesian Markov chain Monte Carlo (McMC) formalism to the inversion of refraction seismic, traveltime data sets to derive 2-D velocity models below linear arrays (i.e. profiles) of sources and seismic receivers. Typical refraction data sets, especially when using the far-offset observations, are known as having experimental geometries which are very poor, highly ill-posed and far from being ideal. As a consequence, the structural resolution quickly degrades with depth. Conventional inversion techniques, based on regularization, potentially suffer from the choice of appropriate inversion parameters (i.e. number and distribution of cells, starting velocity models, damping and smoothing constraints, data noise level, etc.) and only local model space exploration. McMC techniques are used for exhaustive sampling of the model space without the need of prior knowledge (or assumptions) of inversion parameters, resulting in a large number of models fitting the observations. Statistical analysis of these models allows to derive an average (reference) solution and its standard deviation, thus providing uncertainty estimates of the inversion result. The highly non-linear character of the inversion problem, mainly caused by the experiment geometry, does not allow to derive a reference solution and error map by a simply averaging procedure. We present a modified averaging technique, which excludes parts of the prior distribution in the posterior values due to poor ray coverage, thus providing reliable estimates of inversion model properties even in those parts of the models. The model is discretized by a set of Voronoi polygons (with constant slowness cells) or a triangulated mesh (with interpolation within the triangles). Forward traveltime calculations are performed by a fast, finite-difference-based eikonal solver. The method is applied to a data set from a refraction seismic survey from Northern Namibia and compared to conventional tomography. An inversion test for a synthetic data set from a known model is also presented. © The Author(s) 2017. Published by Oxford University Press on behalf of The Royal Astronomical Society."
1,10.1016/j.jmva.2017.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037036987&doi=10.1016%2fj.jmva.2017.11.003&partnerID=40&md5=732509e68ff41a8ea5677070cbb71a85,"This paper introduces a class of scale mixtures of normal selection factor (SMNSF) analysis models which are robust against departures from normality and designed to correct sample-selection bias. Various properties of this class of models are established, including a stochastic representation, a distributional hierarchy, and a quantification of sample-selection bias. A hierarchical Bayesian methodology is also developed for estimation purposes. It involves a simple and computationally feasible Markov Chain Monte Carlo algorithm that avoids analytical or numerical derivatives of the log-likelihood function. Results from simulation studies attest to the good finite-sample performance of the new model in terms of sample-selection bias reduction and robustness against outliers. A data illustration is included. © 2017 Elsevier Inc."
4,10.1007/s11222-017-9740-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016062293&doi=10.1007%2fs11222-017-9740-z&partnerID=40&md5=40535a72b9a1e395e57e4ef5a2ee1706,"Particle filters are a powerful and flexible tool for performing inference on state-space models. They involve a collection of samples evolving over time through a combination of sampling and re-sampling steps. The re-sampling step is necessary to ensure that weight degeneracy is avoided. In several situations of statistical interest, it is important to be able to compare the estimates produced by two different particle filters; consequently, being able to efficiently couple two particle filter trajectories is often of paramount importance. In this text, we propose several ways to do so. In particular, we leverage ideas from the optimal transportation literature. In general, though computing the optimal transport map is extremely computationally expensive, to deal with this, we introduce computationally tractable approximations to optimal transport couplings. We demonstrate that our resulting algorithms for coupling two particle filter trajectories often perform orders of magnitude more efficiently than more standard approaches. © 2017, Springer Science+Business Media New York."
1,10.1111/biom.12719,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018388561&doi=10.1111%2fbiom.12719&partnerID=40&md5=0a36002af90a09e28264bd9f151ee0ca,"Many studies of biomedical time series signals aim to measure the association between frequency-domain properties of time series and clinical and behavioral covariates. However, the time-varying dynamics of these associations are largely ignored due to a lack of methods that can assess the changing nature of the relationship through time. This article introduces a method for the simultaneous and automatic analysis of the association between the time-varying power spectrum and covariates, which we refer to as conditional adaptive Bayesian spectrum analysis (CABS). The procedure adaptively partitions the grid of time and covariate values into an unknown number of approximately stationary blocks and nonparametrically estimates local spectra within blocks through penalized splines. CABS is formulated in a fully Bayesian framework, in which the number and locations of partition points are random, and fit using reversible jump Markov chain Monte Carlo techniques. Estimation and inference averaged over the distribution of partitions allows for the accurate analysis of spectra with both smooth and abrupt changes. The proposed methodology is used to analyze the association between the time-varying spectrum of heart rate variability and self-reported sleep quality in a study of older adults serving as the primary caregiver for their ill spouse. © 2017, The International Biometric Society"
,10.1007/s11222-017-9735-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013177554&doi=10.1007%2fs11222-017-9735-9&partnerID=40&md5=9815e9ddce41726d15a64e6c2bdb3157,"In many research fields, scientific questions are investigated by analyzing data collected over space and time, usually at fixed spatial locations and time steps and resulting in geo-referenced time series. In this context, it is of interest to identify potential partitions of the space and study their evolution over time. A finite space-time mixture model is proposed to identify level-based clusters in spatio-temporal data and study their temporal evolution along the time frame. We anticipate space-time dependence by introducing spatio-temporally varying mixing weights to allocate observations at nearby locations and consecutive time points with similar cluster’s membership probabilities. As a result, a clustering varying over time and space is accomplished. Conditionally on the cluster’s membership, a state-space model is deployed to describe the temporal evolution of the sites belonging to each group. Fully posterior inference is provided under a Bayesian framework through Monte Carlo Markov chain algorithms. Also, a strategy to select the suitable number of clusters based upon the posterior temporal patterns of the clusters is offered. We evaluate our approach through simulation experiments, and we illustrate using air quality data collected across Europe from 2001 to 2012, showing the benefit of borrowing strength of information across space and time. © 2017, Springer Science+Business Media New York."
,10.21307/stattrans-2018-009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047770179&doi=10.21307%2fstattrans-2018-009&partnerID=40&md5=034a48960d6dfb2269c3820772cf814a,"There are presented in this report, seven case studies of the uses of statistics in the past and present. I do not intend these examples to be exhaustive. I intend them primarily as educational examples for readers who would like to know: What is statistics good for? Also, to encourage the readers to study detailed reports from the 13 International Year of Statistics given in the notes of this report. © 2018 Glowny Urzad Statystyczny. All rights reserved."
3,10.1093/sysbio/syx065,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043486392&doi=10.1093%2fsysbio%2fsyx065&partnerID=40&md5=f057f2cc0bb6cc0346ed605a3b1e497e,"Chromosome number is a key feature of the higher-order organization of the genome, and changes in chromosome number play a fundamental role in evolution. Dysploid gains and losses in chromosome number, as well as polyploidization events, may drive reproductive isolation and lineage diversification. The recent development of probabilistic models of chromosome number evolution in the groundbreaking work by Mayrose et al. (2010, ChromEvol) have enabled the inference of ancestral chromosome numbers over molecular phylogenies and generated new interest in studying the role of chromosome changes in evolution. However, the ChromEvol approach assumes all changes occur anagenetically (along branches), and does not model events that are specifically cladogenetic. Cladogenetic changes may be expected if chromosome changes result in reproductive isolation. Here we present a new class of models of chromosome number evolution (called ChromoSSE) that incorporate both anagenetic and cladogenetic change. The ChromoSSE models allow us to determine the mode of chromosome number evolution; is chromosome evolution occurring primarily within lineages, primarily at lineage splitting, or in clade-specific combinations of both? Furthermore, we can estimate the location and timing of possible chromosome speciation events over the phylogeny. We implemented ChromoSSE in a Bayesian statistical framework, specifically in the software RevBayes, to accommodate uncertainty in parameter estimates while leveraging the full power of likelihood based methods. We tested ChromoSSE's accuracy with simulations and re-examined chromosomal evolution in Aristolochia, Carex section Spirostachyae, Helianthus, Mimulus sensu lato (s.l.), and Primula section Aleuritia, finding evidence for clade-specific combinations of anagenetic and cladogenetic dysploid and polyploid modes of chromosome evolution. © 2017 The Author(s)."
,10.1111/2041-210X.12901,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043489725&doi=10.1111%2f2041-210X.12901&partnerID=40&md5=13c8bc4c30918ead87260e742af9588b,"Understanding and predicting how species traits are shaped by prevailing environmental conditions is an important yet challenging task in ecology. Functional trait-based approaches can replace potentially idiosyncratic species-specific response models in learning about community behaviour across environmental gradients. Customarily, models for traits given environment consider only trait means to predict species and functional diversity, as intra-taxon variability in traits is often thought to be negligible. A growing body of literature indicates that intra-taxon trait variability is substantial and critical in structuring plant communities and assessing ecosystem function. We propose flexible joint trait distribution models given environment and across species that incorporate intra-taxon variability as well as inter-site/plot variability. Using a Bayesian framework, our joint trait distribution models allow for mixed continuous, binary and ordinal trait variables and incorporate dependence among traits enabling both joint and conditional trait prediction at unobserved sites. The models can be used to inform about the well-known fourth-corner problem, which attempts to interpret trait-by-environment matrices. We demonstrate the utility of our methodology through joint predictive trait distributions for individual species as well as joint community-weighted trait distributions for environments while incorporating intra-taxon trait variability. Explicit details on the probabilistic interpretations of the random trait-by-environment matrices obtained arising under our model are also provided to address the fourth-corner problem. Finally, our joint trait distribution model is applied to simulated and real vegetation data collected from the Greater Cape Floristic Region of South Africa. The proposed methodology places a fully model-based foundation on explaining intra-taxon trait variation given environment. It extends the utility and interpretability of commonly applied techniques for investigating community-weighted traits and illuminates randomness in the fourth-corner problem. © 2017 The Authors. Methods in Ecology and Evolution © 2017 British Ecological Society"
,10.1016/j.urolonc.2017.10.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034608835&doi=10.1016%2fj.urolonc.2017.10.024&partnerID=40&md5=ccbd73f97f67adb51fdbc358a1db6d98,"Purpose: To estimate the health system costs of prostate cancer by disease risk category and treatment type over 2016 to 2025 and to identify potential strategies to contain the cost increase. Methods: A Markov cohort model was developed using clinical pathways from US prostate cancer guidelines and clinical expertise. Estimates of the probabilities of various treatments and outcomes and their unit costs were sourced from systematic reviews, meta-analyses, epidemiological publications and national cost reports. Estimated costs by stage of disease, by major treatments and by age at diagnosis were reported in 2016 US dollars. One-way and probabilistic sensitivity analyses assessed potential variation in the modeled costs. Results: Australia-wide costs of prostate cancer were estimated at US$270.9 million in 2016 rising to US$384.3 million in 2025, an expected increase of 42%. Of this total increase, newly diagnosed low risk cases will contribute US$32.9 million, intermediate-risk US$56.8 million, high-risk US$53.3 million and advanced US$12.6 million. For men diagnosed at age 65 with low-risk disease, lifetime costs per patient were US$14,497 for surgery, US$19,665 for radiation therapies to the primary lesion, and US$9,234 for active surveillance. For intermediate- or high-risk disease, mean costs per patient were US$34,941 for surgery plus radiation and US$31,790 for androgen deprivation therapy plus radiation while advanced cancer therapies were at US$31,574 per patient. Additional costs for managing iatrogenic disease secondary to these treatments were excluded. Conclusion: Strategies for identifying patients early before cancers have spread are critical to contain the estimated 42% increase in costs over the next decade. Increased uptake of active surveillance would also lead to substantial cost-savings in the management of low-risk prostate cancer. © 2018 Elsevier Inc."
,10.1007/s11269-017-1863-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034664137&doi=10.1007%2fs11269-017-1863-7&partnerID=40&md5=f7b5f0efc32b1254b1e52a5474c1958b,"Hydroclimatic drought conditions can affect the hydrological services offered by mountain river basins causing severe impacts on the population, becoming a challenge for water resource managers in Andean river basins. This study proposes an integrated methodological framework for assessing the risk of failure in water supply, incorporating probabilistic drought forecasts, which assists in making decisions regarding the satisfaction of consumptive, non-consumptive and environmental requirements under water scarcity conditions. Monte Carlo simulation was used to assess the risk of failure in multiple stochastic scenarios, which incorporate probabilistic forecasts of drought events based on a Markov chains (MC) model using a recently developed drought index (DI). This methodology was tested in the Machángara river basin located in the south of Ecuador. Results were grouped in integrated satisfaction indexes of the system (DSIG). They demonstrated that the incorporation of probabilistic drought forecasts could better target the projections of simulation scenarios, with a view of obtaining realistic situations instead of optimistic projections that would lead to riskier decisions. Moreover, they contribute to more effective results in order to propose multiple alternatives for prevention and/or mitigation under drought conditions. © 2017, Springer Science+Business Media B.V., part of Springer Nature."
,10.1371/journal.pcbi.1006046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044730670&doi=10.1371%2fjournal.pcbi.1006046&partnerID=40&md5=3a78ab60151366c93ad954a51e53adf6,"In the context of an ageing population, understanding the transmission of infectious diseases such as scabies through well-connected sub-units of the population, such as residential care homes, is particularly important for the design of efficient interventions to mitigate against the effects of those diseases. Here, we present a modelling methodology based on the efficient solution of a large-scale system of linear differential equations that allows statistical calibration of individual-based random models to real data on scabies in residential care homes. In particular, we review and benchmark different numerical methods for the integration of the differential equation system, and then select the most appropriate of these methods to perform inference using Markov chain Monte Carlo. We test the goodness-of-fit of this model using posterior predictive intervals and propagate forward the resulting parameter uncertainty in a Bayesian framework to consider the economic cost of delayed interventions against scabies, quantifying the benefits of prompt action in the event of detection. We also revisit the previous methodology used to assess the safety of treatments in small population sub-units—in this context ivermectin—and demonstrate that even a very slight relaxation of the implicit assumption of homogeneous death rates significantly increases the plausibility of the hypothesis that ivermectin does not cause excess mortality based upon the data of Barkwell and Shields [1]. © 2018 Kinyanjui et al."
,10.1053/j.semtcvs.2018.01.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042634455&doi=10.1053%2fj.semtcvs.2018.01.003&partnerID=40&md5=fe41a4409ec592b8784dd1caf2d83951,"We aimed to empirically derive an inotrope score to predict real-time outcomes using the doses of inotropes after pediatric cardiac surgery. The outcomes evaluated included in-hospital mortality, prolonged hospital length of stay, and composite poor outcome (mortality or prolonged hospital length of stay). The study population included patients <18 years of age undergoing heart operations (with or without cardiopulmonary bypass) of varying complexity. To create this novel pediatric cardiac inotrope score (PCIS), we collected the data on the highest doses of 4 commonly used inotropes (epinephrine, norepinephrine, dopamine, and milrinone) in the first 24 hours after heart operation. We employed a hierarchical framework by representing discrete probability models with continuous latent variables that depended on the dosage of drugs for a particular patient. We used Bayesian conditional probit regression to model the effects of the inotropes on the mean of the latent variables. We then used Markov chain Monte Carlo simulations for simulating posterior samples to create a score function for each of the study outcomes. The training dataset utilized 1030 patients to make the scientific model. An online calculator for the tool can be accessed at https://soipredictiontool.shinyapps.io/InotropeScoreApp. The newly proposed empiric PCIS demonstrated a high degree of discrimination for predicting study outcomes in children undergoing heart operations. The newly proposed empiric PCIS provides a novel measure to predict real-time outcomes using the doses of inotropes among children undergoing heart operations of varying complexity. © 2018 Elsevier Inc."
,10.1016/j.mbs.2018.01.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041489866&doi=10.1016%2fj.mbs.2018.01.002&partnerID=40&md5=902fc5e049b6fd7b2d0b6e33843d5346,"The spread of an infectious disease may depend on the structure of the network. To study the influence of the structure parameters of the network on the spread of the epidemic, we need to put these parameters into the epidemic model. The method of moment closure introduces structure parameters into the epidemic model. In this paper, we present a new moment closure epidemic model based on the approximation of third-order motifs in networks. The order of a motif defined in this paper is determined by the number of the edges in the motif, rather than by the number of nodes in the motif as defined in the literature. We provide a general approach to deriving a set of ordinary differential equations that describes, to a high degree of accuracy, the spread of an infectious disease. Using this method, we establish a susceptible-infected-recovered (SIR) model. We then calculate the basic reproduction number of the SIR model, and find that it decreases as the clustering coefficient increases. Finally, we perform some simulations using the proposed model to study the influence of the clustering coefficient on the final epidemic size, the maximum number of infected, and the peak time of the disease. The numerical simulations based on the SIR model in this paper fit the stochastic simulations based on the Monte Carlo method well at different levels of clustering. Our results show that the clustering coefficient poses impediments to the spread of disease under an SIR model. © 2018"
1,10.1371/journal.pone.0193974,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043448562&doi=10.1371%2fjournal.pone.0193974&partnerID=40&md5=0187055b8f07b43f68ac5cd09f74fa81,"Factor analysis is broadly used as a powerful unsupervised machine learning tool for reconstruction of hidden features in recorded mixtures of signals. In the case of a linear approximation, the mixtures can be decomposed by a variety of model-free Blind Source Separation (BSS) algorithms. Most of the available BSS algorithms consider an instantaneous mixing of signals, while the case when the mixtures are linear combinations of signals with delays is less explored. Especially difficult is the case when the number of sources of the signals with delays is unknown and has to be determined from the data as well. To address this problem, in this paper, we present a new method based on Nonnegative Matrix Factorization (NMF) that is capable of identifying: (a) the unknown number of the sources, (b) the delays and speed of propagation of the signals, and (c) the locations of the sources. Our method can be used to decompose records of mixtures of signals with delays emitted by an unknown number of sources in a nondispersive medium, based only on recorded data. This is the case, for example, when electromagnetic signals from multiple antennas are received asynchronously; or mixtures of acoustic or seismic signals recorded by sensors located at different positions; or when a shift in frequency is induced by the Doppler effect. By applying our method to synthetic datasets, we demonstrate its ability to identify the unknown number of sources as well as the waveforms, the delays, and the strengths of the signals. Using Bayesian analysis, we also evaluate estimation uncertainties and identify the region of likelihood where the positions of the sources can be found. This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication."
4,10.1016/j.epidem.2016.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009274885&doi=10.1016%2fj.epidem.2016.11.003&partnerID=40&md5=fab328d444e929b832082805cbd5768d,"Real-time forecasts of infectious diseases can help public health planning, especially during outbreaks. If forecasts are generated from mechanistic models, they can be further used to target resources or to compare the impact of possible interventions. However, paremeterising such models is often difficult in real time, when information on behavioural changes, interventions and routes of transmission are not readily available. Here, we present a semi-mechanistic model of infectious disease dynamics that was used in real time during the 2013–2016 West African Ebola epidemic, and show fits to a Ebola Forecasting Challenge conducted in late 2015 with simulated data mimicking the true epidemic. We assess the performance of the model in different situations and identify strengths and shortcomings of our approach. Models such as the one presented here which combine the power of mechanistic models with the flexibility to include uncertainty about the precise outbreak dynamics may be an important tool in combating future outbreaks. © 2016 The Author(s)"
1,10.1016/j.epidem.2017.02.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015326541&doi=10.1016%2fj.epidem.2017.02.011&partnerID=40&md5=cbb1d8e6caa13b10679a868607f2dd15,"We use two modelling approaches to forecast synthetic Ebola epidemics in the context of the RAPIDD Ebola Forecasting Challenge. The first approach is a standard stochastic compartmental model that aims to forecast incidence, hospitalization and deaths among both the general population and health care workers. The second is a model based on the renewal equation with latent variables that forecasts incidence in the whole population only. We describe fitting and forecasting procedures for each model and discuss their advantages and drawbacks. We did not find that one model was consistently better in forecasting than the other. © 2017 The Author(s)"
,10.1016/j.pocean.2018.02.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042703154&doi=10.1016%2fj.pocean.2018.02.013&partnerID=40&md5=49ab4e4a55da85e976ac11b6aaf8b819,"The study is the first attempt to (i) model spring food webs in three SW Mediterranean ecosystems which are under different anthropogenic pressures and (ii) to project the consequence of this stress on their function. Linear inverse models were built using the Monte Carlo method coupled with Markov Chains to characterize the food-web status of the Lagoon, the Channel (inshore waters under high eutrophication and chemical contamination) and the Bay of Bizerte (offshore waters under less anthropogenic pressure). Ecological network analysis was used for the description of structural and functional properties of each food web and for inter-ecosystem comparisons. Our results showed that more carbon was produced by phytoplankton in the inshore waters (966–1234 mg C m−2 d−1) compared to the Bay (727 mg C m−2 d−1). The total ecosystem carbon inputs into the three food webs was supported by high primary production, which was mainly due to &gt;10 µm algae. However, the three carbon pathways were characterized by low detritivory and a high herbivory which was mainly assigned to protozooplankton. This latter was efficient in channelling biogenic carbon. In the Lagoon and the Channel, foods webs acted almost as a multivorous structure with a tendency towards herbivorous one, whereas in the Bay the herbivorous pathway was more dominant. Ecological indices revealed that the Lagoon and the Channel food webs/systems had high total system throughput and thus were more active than the Bay. The Bay food web, which had a high relative ascendency value, was more organized and specialized. This inter–ecosystem difference could be due to the varying levels of anthropogenic impact among sites. Indeed, the low value of Finn's cycling index indicated that the three systems are disturbed, but the Lagoon and the Channel, with low average path lengths, appeared to be more stressed, as both sites have undergone higher chemical pollution and nutrient loading. This study shows that ecosystem models combined with ecological indices provide a powerful approach to detect change in environmental status and anthropogenic impacts. © 2018"
1,10.1016/j.autcon.2017.12.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038016515&doi=10.1016%2fj.autcon.2017.12.007&partnerID=40&md5=e8c7046fb7a958087e847c461456e9d4,"In this study, a methodology to model and predict the life-cycle performance of building façades based on Stochastic Petri Nets is proposed. The proposed model evaluates the performance of rendered façades over time, evaluating the uncertainty of the future performance of these coatings. The performance of rendered façades is evaluated based on a discrete qualitative scale composed of five condition levels, established according to the physical and visual degradation of these elements. In this study, the deterioration is modelled considering that the transition times between these condition states can be modelled as a random variable with different distributions. For that purpose, a Stochastic Petri Nets model is used, as a formal framework to describe this problem. The model's validation is based on probabilistic indicators of performance, computed using Monte-Carlo simulation and the probability distribution parameters leading to better fit are defined as those maximizing the likelihood, computed using Genetic Algorithm. In this study, a sample of 99 rendered façades, located in Portugal, is analysed, and the degradation condition of each case study is evaluated through in-situ visual inspections. The model proposed allows evaluating: i) the transition rate between degradation conditions; ii) the probability of belonging to a given degradation condition over time; and iii) the mean time of permanence in each degradation condition. The use of Petri Nets shows to be more accurate than a more traditional approach based on Markov Chains, but also allows developing future research to consider different environmental conditions, maintenance actions or inspections, amongst other aspects of life-cycle analysis of existing assets. © 2017"
1,10.1190/geo2017-0183.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042322707&doi=10.1190%2fgeo2017-0183.1&partnerID=40&md5=815406a52d4bb05c5527f06c63339aa8,"P-and S-wave inverse quality factors quantify seismic wave attenuation, which is related to several key reservoir parameters (porosity, saturation, and viscosity). Estimating the inverse quality factors from observed seismic data provides additional and useful information during gas-bearing reservoir prediction. First, we have developed an approximate reflection coefficient and attenuative elastic impedance (QEI) in terms of the inverse quality factors, and then we established an approach to estimate elastic properties (P- and S-wave impedances, and density) and attenuation (P- and S-wave inverse quality factors) from seismic data at different incidence angles and frequencies. The approach is implemented as a two-step inversion: a model-based and damped least-squares inversion for QEI, and a Bayesian Markov chain Monte Carlo inversion for the inverse quality factors. Synthetic data tests confirm that P- and S-wave impedances and inverse quality factors are reasonably estimated in the case of moderate data error or noise. Applying the established approach to a real data set is suggestive of the robustness of the approach, and furthermore that physically meaningful inverse quality factors can be estimated from seismic data acquired over a gas-bearing reservoir. © 2018 Society of Exploration Geophysicists."
,10.1002/bimj.201600225,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036577087&doi=10.1002%2fbimj.201600225&partnerID=40&md5=ab87682d4c5537ec7da9d3fa6c2b106c,"The deterministic inputs, noisy, “and” gate (DINA) model is a popular cognitive diagnosis model (CDM) in psychology and psychometrics used to identify test takers' profiles with respect to a set of latent attributes or skills. In this work, we propose an estimation method for the DINA model with the No-U-Turn Sampler (NUTS) algorithm, an extension to Hamiltonian Monte Carlo (HMC) method. We conduct a simulation study in order to evaluate the parameter recovery and efficiency of this new Markov chain Monte Carlo method and to compare it with two other Bayesian methods, the Metropolis Hastings and Gibbs sampling algorithms, and with a frequentist method, using the Expectation–Maximization (EM) algorithm. The results indicated that NUTS algorithm employed in the DINA model properly recovers all parameters and is accurate for all simulated scenarios. We apply this methodology in the mental health area in order to develop a new method of classification for respondents to the Beck Depression Inventory. The implementation of this method for the DINA model applied to other psychological tests has the potential to improve the medical diagnostic process. © 2017 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim"
,10.1007/s00180-017-0724-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016116530&doi=10.1007%2fs00180-017-0724-4&partnerID=40&md5=d6d01b5a38d20693faab76c1e8710270,"The optimal decision rule for testing hypothesis using observations or statistics on a two-dimensional lattice system is theoretically well-understood since Sun and Cai (J R Stat Soc Ser B (Stat Methodol) 71(2):393–424, 2009). However, its practical use still faces several difficulties that include the computation of the local index of significance (LIS). In this paper, we propose a peeling algorithm to compute the LIS, or equivalently the marginal posterior probability for the indicator of the true hypothesis for each site. We show that the proposed peeling algorithm has several advantages over the popular Markov chain Monte Carlo methods through an extensive numerical study. An application of the peeling algorithm to finding active voxels in a task-based fMRI experiment is also presented. © 2017, Springer-Verlag Berlin Heidelberg."
1,10.1109/TCBB.2015.2440244,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044921518&doi=10.1109%2fTCBB.2015.2440244&partnerID=40&md5=9aa89cd2184aae026b6890e885a6b1f4,"Ultra-high dimensional variable selection has become increasingly important in analysis of neuroimaging data. For example, in the Autism Brain Imaging Data Exchange (ABIDE) study, neuroscientists are interested in identifying important biomarkers for early detection of the autism spectrum disorder (ASD) using high resolution brain images that include hundreds of thousands voxels. However, most existing methods are not feasible for solving this problem due to their extensive computational costs. In this work, we propose a novel multiresolution variable selection procedure under a Bayesian probit regression framework. It recursively uses posterior samples for coarser-scale variable selection to guide the posterior inference on finer-scale variable selection, leading to very efficient Markov chain Monte Carlo (MCMC) algorithms. The proposed algorithms are computationally feasible for ultra-high dimensional data. Also, our model incorporates two levels of structural information into variable selection using Ising priors: the spatial dependence between voxels and the functional connectivity between anatomical brain regions. Applied to the resting state functional magnetic resonance imaging (R-fMRI) data in the ABIDE study, our methods identify voxel-level imaging biomarkers highly predictive of the ASD, which are biologically meaningful and interpretable. Extensive simulations also show that our methods achieve better performance in variable selection compared to existing methods. © 2004-2012 IEEE."
,10.1111/rurd.12072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044427675&doi=10.1111%2frurd.12072&partnerID=40&md5=2a2f006b2cee233cdeab74d10e217e0d,"This study examines the consistency and gaps in national and regional business cycles in Japan from a Bayesian point of view. The Tokyo monopolar system started in the mid-1970s, and recent descriptive statistics, such as migration and per capita income, show that the system continues, despite severe crises such as the burst of the 1990s economic bubble and the Lehman Brothers bankruptcy. We explore the relationship between national and regional business cycles in the system using a spatio-temporal Markov-switching model with the Markov chain Monte Carlo method. Our empirical results show that overall, the regional business cycle in the Kanto region, including Tokyo, is identical to the national business cycle. Moreover, we find that switches in the degree of spatial dependency occur around the turning points of business cycles, and that the degree of spatial dependency tends to be higher during a recession. © 2017 The Applied Regional Science Conference (ARSC) and John Wiley & Sons Australia, Ltd"
,10.1007/s40304-017-0123-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042527119&doi=10.1007%2fs40304-017-0123-8&partnerID=40&md5=07016c17af451e849161b32f1ad185f8,"In this paper, the optimum test plan and parameter estimation for 3-step step-stress accelerated life tests in the presence of modified progressive Type-I censoring are discussed. It is assumed that the lifetime of test units follows a Lomax distribution with log of characteristic life being quadratic function of stress level. The maximum likelihood and Bayesian method are used to obtain the point and interval estimators of the model parameters. The Bayes estimates are obtained using Markov chain Monte Carlo simulation based on Gibbs sampling. The optimum plan for 3-step step-stress test under modified progressive Type-I censoring is developed which minimizes the asymptotic variance of the maximum likelihood estimators of log of scale parameter at design stress. Finally, the numerical study with sensitivity analysis is presented to illustrate the proposed study. © 2018, School of Mathematical Sciences, University of Science and Technology of China and Springer-Verlag GmbH Germany, part of Springer Nature."
1,10.1007/s00362-016-0765-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961807634&doi=10.1007%2fs00362-016-0765-8&partnerID=40&md5=bfbbf962c449baefbf47b1144174cef9,"In this paper, we consider a system which has k statistically independent and identically distributed strength components and each component is constructed by a pair of statistically dependent elements. These elements (X1, Y1) , (X2, Y2) , … , (Xk, Yk) follow a bivariate Kumaraswamy distribution and each element is exposed to a common random stress T which follows a Kumaraswamy distribution. The system is regarded as operating only if at least s out of k(1 ≤ s≤ k) strength variables exceed the random stress. The multicomponent reliability of the system is given by Rs , k= P(at least s of the (Z1, … , Zk) exceed T) where Zi= min (Xi, Yi) , i= 1 , … , k. We estimate Rs , k by using frequentist and Bayesian approaches. The Bayes estimates of Rs , k have been developed by using Lindley’s approximation and the Markov Chain Monte Carlo methods due to the lack of explicit forms. The uniformly minimum variance unbiased and exact Bayes estimates of Rs , k are obtained analytically when the common second shape parameter is known. The asymptotic confidence interval and the highest probability density credible interval are constructed for Rs , k. The reliability estimators are compared by using the estimated risks through Monte Carlo simulations. Real data are analysed for an illustration of the findings. © 2016, Springer-Verlag Berlin Heidelberg."
,10.1093/jssam/smx004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044384650&doi=10.1093%2fjssam%2fsmx004&partnerID=40&md5=5f40c8bbece732843e4bac34ff1b3b88,"Skip patterns, bounds, and diverse measurement scales often exacerbate the problem of item nonresponse in the analysis of survey data. Sequential, or variable-by-variable imputation techniques have been quite successfully applied to overcome such problems. Most of these techniques have so far focused on relatively simple designs, and studies have demonstrated the consistency of these methods with techniques that draw from a joint posterior predictive distribution of missing data. Here we consider a sequential imputation technique based on a family of hierarchical regression models, extending the sequential approach to correlated data, (e.g., clustered data) and assess its performance. Each of the regression models is tailored to the variable being handled. Computational techniques used to approximate the posterior predictive distributions are based on Markov Chain Monte Carlo (MCMC) and numerical integration to overcome the problem of intractability. We present a simulation study assessing the compatibility of this approach with the joint data generation mechanism. In the scenarios studied, the sequential method leads to well-calibrated estimates and often performs better than methods that are currently available to practitioners. © The Author 2017. Published by Oxford University Press on behalf of the American Association for Public Opinion Research. All rights reserved."
,10.29220/CSAM.2018.25.2.131,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046888751&doi=10.29220%2fCSAM.2018.25.2.131&partnerID=40&md5=11b010c64822b79eaef70ba4fe526a2d,"We propose a robust event date model to estimate the date of a target event by a combination of individual dates obtained from archaeological artifacts assumed to be contemporaneous. These dates are affected by errors of different types: laboratory and calibration curve errors, irreducible errors related to contaminations, and taphonomic disturbances, hence the possible presence of outliers. Modeling based on a hierarchical Bayesian statistical approach provides a simple way to automatically penalize outlying data without having to remove them from the dataset. Prior information on individual irreducible errors is introduced using a uniform shrinkage density with minimal assumptions about Bayesian parameters. We show that the event date model is more robust than models implemented in BCal or OxCal, although it generally yields less precise credibility intervals. The model is extended in the case of stratigraphic sequences that involve several events with temporal order constraints (relative dating), or with duration, hiatus constraints. Calculations are based on Markov chain Monte Carlo (MCMC) numerical techniques and can be performed using ChronoModel software which is freeware, open source and cross-platform. Features of the software are presented in Vibet et al. (ChronoModel v1.5 user's manual, 2016). We finally compare our prior on event dates implemented in the ChronoModel with the prior in BCal and OxCal which involves supplementary parameters defined as boundaries to phases or sequences. © 2018 The Korean Statistical Society, and Korean International Statistical Society."
3,10.1093/mnras/stx2982,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040231469&doi=10.1093%2fmnras%2fstx2982&partnerID=40&md5=1713125146158201fdb2750a204fdb51,"In this paper, we study an anisotropic universe model with Bianchi-I metric using Joint lightcurve analysis (JLA) sample of Type Ia supernovae (SNe Ia). Because light-curve parameters of SNe Ia vary with different cosmological models and SNe Ia samples, we fit the SNe Ia light-curve parameters and cosmological parameters simultaneously employing Markov chain Monte Carlo method. Therefore, the results on the amount of deviation from isotropy of the dark energy equation of state (δ), and the level of anisotropy of the large-scale geometry (Σ0) at present, are totally model-independent. The constraints on the skewness and cosmic shear are -0.101 &lt; δ &lt;0.071 and -0.007 &lt; Σ0 &lt; 0.008. This result is consistent with a standard isotropic universe (δ = Σ0 = 0). However, a moderate level of anisotropy in the geometry of the Universe and the equation of state of dark energy, is allowed. Besides, there is no obvious evidence for a preferred direction of anisotropic axis in this model. © 2017 The Author(s). Published by Oxford University Press on behalf of the Royal Astronomical Society."
,10.1007/s00180-017-0752-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025427719&doi=10.1007%2fs00180-017-0752-0&partnerID=40&md5=008e47452d074e4c6f3ed694db8b7491,"A new computational strategy produces independent samples from the joint posterior distribution for a broad class of Bayesian spatial and spatiotemporal conditional autoregressive models. The method is based on reparameterization and marginalization of the posterior distribution and massive parallelization of rejection sampling using graphical processing units (GPUs) or other accelerators. It enables very fast sampling for small to moderate-sized datasets (up to approximately 10,000 observations) and feasible sampling for much larger datasets. Even using a mid-range GPU and a high-end CPU, the GPU-based implementation is up to 30 times faster than the same algorithm run serially on a single CPU, and the numbers of effective samples per second are orders of magnitude higher than those obtained with popular Markov chain Monte Carlo software. The method has been implemented in the R package CARrampsOcl. This work provides both a practical computing strategy for fitting a popular class of Bayesian models and a proof of concept that GPU acceleration can make independent sampling from Bayesian joint posterior densities feasible. © 2017, Springer-Verlag GmbH Germany."
4,10.1007/s11336-016-9525-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992727873&doi=10.1007%2fs11336-016-9525-x&partnerID=40&md5=47bc2c3d3f487c3aca7e6be54d991e45,"Statistical methods for identifying aberrances on psychological and educational tests are pivotal to detect flaws in the design of a test or irregular behavior of test takers. Two approaches have been taken in the past to address the challenge of aberrant behavior detection, which are (1) modeling aberrant behavior via mixture modeling methods, and (2) flagging aberrant behavior via residual based outlier detection methods. In this paper, we propose a two-stage method that is conceived of as a combination of both approaches. In the first stage, a mixture hierarchical model is fitted to the response and response time data to distinguish normal and aberrant behaviors using Markov chain Monte Carlo (MCMC) algorithm. In the second stage, a further distinction between rapid guessing and cheating behavior is made at a person level using a Bayesian residual index. Simulation results show that the two-stage method yields accurate item and person parameter estimates, as well as high true detection rate and low false detection rate, under different manipulated conditions mimicking NAEP parameters. A real data example is given in the end to illustrate the potential application of the proposed method. © 2016, The Psychometric Society."
1,10.1016/j.jcomm.2017.12.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044462264&doi=10.1016%2fj.jcomm.2017.12.003&partnerID=40&md5=f8352925e76eb3914e7a550ae26ae9d6,"In this paper we propose a model for oil price dynamics for which we provide an estimation method based on a recent technique named Particle Filtering. The model we are going to introduce extends a previous model proposed by Liu and Tang (2011), including a non constant volatility and jumps in the spot price dynamics. The estimation methodology we are going to adopt is similar to the Particle Markov Chain Monte Carlo (PMCMC) method proposed by Andrieu et al. (2010), and both spot and futures quotation data related to WTI (West Texas Intermediate) are analyzed in order to perform our inference procedure. The models considered allow to obtain explicit expressions for futures prices as functions of the model parameters and this in turn makes the calibration procedure fast and accurate at the same time. A comparison between the model considered and the model proposed by Liu and Tang is provided in terms of prices forecasting ability. The inference analysis shows that the introduction of both stochastic volatility and jumps improve significantly the ability of the model in capturing the oil price dynamics features. © 2018 Elsevier B.V."
2,10.1111/bcp.13470,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040327287&doi=10.1111%2fbcp.13470&partnerID=40&md5=f94974115fc9880b1c086c4cd09b944d,"Aims: Topical growth factors accelerate wound healing in patients with diabetic foot ulcers (DFU). Due to the absence of head-to-head comparisons, we carried out Bayesian network meta-analysis to compare the efficacy and safety of growth factors. Methods: Using an appropriate search strategy, randomized controlled trials on topical growth factors compared with standard of care in patients with DFU, were included. Proportion of patients with complete healing was the primary outcome. Odds ratio (95% confidence interval) was used as the effect estimate and random effects model was used for both direct and indirect comparisons. Markov Chain Monte Carlo simulation was used to obtain pooled estimates. Rankogram was generated based on surface under the cumulative ranking curve (SUCRA). Results: A total of 26 studies with 2088 participants and 1018 events were included. The pooled estimates for recombinant epidermal growth factor (rhEGF), autologous platelet rich plasma (PRP), recombinant human platelet-derived growth factor (rhPDGF) were 5.72 [3.34, 10.37], 2.65 [1.60, 4.54] and 1.97 [1.54, 2.55] respectively. SUCRA for rhEGF was 0.95. Sensitivity analyses did not reveal significant changes from the pooled estimates and rankogram. No differences were observed in the overall risk of adverse events between the growth factors. However, the growth factors were observed to lower the risk of lower limb amputation compared to standard of care. Conclusion: To conclude, rhEGF, rhPDGF and autologous PRP significantly improved the healing rate when used as adjuvants to standard of care, of which rhEGF may perform better than other growth factors. The strength of most of the outcomes assessed was low and the findings may not be applicable for DFU with infection or osteomyelitis. The findings of this study needs to be considered with caution as the results might change with findings from head-to-head studies. © 2017 The British Pharmacological Society"
3,10.1093/mnras/stx3026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040229345&doi=10.1093%2fmnras%2fstx3026&partnerID=40&md5=a7f5f88587e0da5791821dedb6865448,"We studied the central regions of the Galactic Centre to determine if the circumnuclear disc (CND) acts as an absorber or a barrier for the central X-rays diffuse emission. After reprocessing 4.6 Ms of Chandra observations, we were able to detect, for the first time, a depression in the X-ray luminosity of the diffuse emission whose size and location correspond to those of the CND. We extracted the X-ray spectra for various regions inside the CND footprint as well as for the region where the footprint is observed and for a region located outside the footprint. We simultaneously fitted these spectra as an optically thin plasma whose absorption by the interstellar medium (ISM) and by the local plasma were fitted independently using the Markov chain Monte Carlo method. The hydrogen column density of the ISM is 7.5 × 1022 cm-2. The X-ray diffuse emission inside the CND footprint is formed by a 2T plasma of 1 and 4 keV with slightly super-solar abundances except for the iron and carbon that are sub-solar. The plasma from the CND, in turn, is better described by a 1T model with abundances and local hydrogen column density that are very different from those of the innermost regions. The large iron abundance in this region confirms that the CND is dominated by the shock-heated ejecta of the Sgr A East supernova remnant. We deduced that the CND rather acts as a barrier for the Galactic Centre plasma and that the plasma located outside the CND may correspond to the collimated outflow possibly created by Sgr A* or the interaction between the wind of massive stars and the mini-spiral material. © 2017 The Author(s). Published by Oxford University Press on behalf of the Royal Astronomical Society."
1,10.1016/j.neuroimage.2017.03.039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016988802&doi=10.1016%2fj.neuroimage.2017.03.039&partnerID=40&md5=e24a3b5475b8428eebebd084e7c6eb95,"Several magnetic resonance imaging (MRI) contrasts are sensitive to myelin content in gray matter in vivo which has ignited ambitions of MRI-based in vivo cortical histology. Ultra-high field (UHF) MRI, at fields of 7 T and beyond, is crucial to provide the resolution and contrast needed to sample contrasts over the depth of the cortex and get closer to layer resolved imaging. Ex vivo MRI of human post mortem samples is an important stepping stone to investigate MRI contrast in the cortex, validate it against histology techniques applied in situ to the same tissue, and investigate the resolutions needed to translate ex vivo findings to in vivo UHF MRI. Here, we investigate key technology to extend such UHF studies to large human brain samples while maintaining high resolution, which allows investigation of the layered architecture of several cortical areas over their entire 3D extent and their complete borders where architecture changes. A 16 channel cylindrical phased array radiofrequency (RF) receive coil was constructed to image a large post mortem occipital lobe sample (~80×80×80 mm3) in a wide-bore 9.4 T human scanner with the aim of achieving high-resolution anatomical and quantitative MR images. Compared with a human head coil at 9.4 T, the maximum Signal-to-Noise ratio (SNR) was increased by a factor of about five in the peripheral cortex. Although the transmit profile with a circularly polarized transmit mode at 9.4 T is relatively inhomogeneous over the large sample, this challenge was successfully resolved with parallel transmit using the kT-points method. Using this setup, we achieved 60μm anatomical images for the entire occipital lobe showing increased spatial definition of cortical details compared to lower resolutions. In addition, we were able to achieve sufficient control over SNR, B0 and B1 homogeneity and multi-contrast sampling to perform quantitative T2* mapping over the same volume at 200 μm. Markov Chain Monte Carlo sampling provided maximum posterior estimates of quantitative T2* and their uncertainty, allowing delineation of the stria of Gennari over the entire length and width of the calcarine sulcus. We discuss how custom RF receive coil arrays built to specific large post mortem sample sizes can provide a platform for UHF cortical layer-specific quantitative MRI over large fields of view. © 2017 The Authors"
2,10.1093/mnras/stx3049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040235580&doi=10.1093%2fmnras%2fstx3049&partnerID=40&md5=3e43339ed2b7b8abbf2ce1b1fcb272bf,"The orbital parameters of binaries at intermediate periods (102-103 d) are difficult to measure with conventional methods and are very incomplete. We have undertaken a new survey, applying our pulsation timing method to Kepler light curves of 2224 main-sequence A/F stars and found 341 non-eclipsing binaries. We calculate the orbital parameters for 317 PB1 systems (single-pulsator binaries) and 24 PB2s (double-pulsators), tripling the number of intermediate-mass binaries with full orbital solutions. The method reaches down to small mass ratios q ≈ 0.02 and yields a highly homogeneous sample. We parametrize the massratio distribution using both inversion and Markov-Chain Monte Carlo forward-modelling techniques, and find it to be skewed towards low-mass companions, peaking at q ≈ 0.2. While solar-type primaries exhibit a brown dwarf desert across short and intermediate periods, we find a small but statistically significant (2.6σ) population of extreme-mass-ratio companions (q &lt; 0.1) to our intermediate-mass primaries. Across periods of 100-1500 d and at q &gt; 0.1, we measure the binary fraction of current A/F primaries to be 15.4 percent ± 1.4 per cent, though we find that a large fraction of the companions (21 percent ± 6 per cent) are white dwarfs in post-mass-transfer systems with primaries that are now blue stragglers, some of which are the progenitors of Type Ia supernovae, barium stars, symbiotics, and related phenomena. Excluding these white dwarfs, we determine the binary fraction of original A/F primaries to be 13.9 percent ± 2.1 per cent over the same parameter space. Combining our measurements with those in the literature, we find the binary fraction across these periods is a constant 5 per cent for primaries M1 &lt; 0.8M⊙, but then increases linearly with log M1, demonstrating that natal discs around more massive protostars M1 ≳ 1M⊙ become increasingly more prone to fragmentation. Finally, we find the eccentricity distribution of the main-sequence pairs to be much less eccentric than the thermal distribution. © 2017 The Author(s)."
,10.1002/sim.7541,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041329594&doi=10.1002%2fsim.7541&partnerID=40&md5=9a44fb520140e4b801a1178c182c4eaf,"In practice, count data may exhibit varying dispersion patterns and excessive zero values; additionally, they may appear in groups or clusters sharing a common source of variation. We present a novel Bayesian approach for analyzing such data. To model these features, we combine the Conway-Maxwell-Poisson distribution, which allows both overdispersion and underdispersion, with a hurdle component for the zeros and random effects for clustering. We propose an efficient Markov chain Monte Carlo sampling scheme to obtain posterior inference from our model. Through simulation studies, we compare our hurdle Conway-Maxwell-Poisson model with a hurdle Poisson model to demonstrate the effectiveness of our Conway-Maxwell-Poisson approach. Furthermore, we apply our model to analyze an illustrative dataset containing information on the number and types of carious lesions on each tooth in a population of 9-year-olds from the Iowa Fluoride Study, which is an ongoing longitudinal study on a cohort of Iowa children that began in 1991. Copyright © 2017 John Wiley & Sons, Ltd."
,10.23919/APCC.2017.8303984,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050610224&doi=10.23919%2fAPCC.2017.8303984&partnerID=40&md5=d36dc0347fc7c72aa3dc973b98fc4683,"The plasma sheath channel has a serious impact on the propagation of electromagnetic waves, resulting in the radio blackout problem in aerospace communications. In existing studies, the plasma sheath channel characteristics are analyzed based on computer simulations, lack of real-world experimental verification. Thereby, these studies failed to fully demonstrate the high-dynamics of the plasma sheath channel. In this paper, an experimental communication system based on the shock tube is proposed to investigate the plasma sheath channel. Then, the characteristics of the plasma sheath channel are analyzed based on the experimental results. In particular, the high-dynamic and fast time-varying channel characteristics are verified by analyzing the signal amplitude, signal phase shift and the coherence time. Finally, we show an example of using the presented channel characteristics. A non-stationary signal segmentation method is proposed based on the reversible jump Markov chain Monte Carlo algorithm, which is applicable to the plasma sheath channel signal segmentation. © 2017 IEEE."
,10.1080/01457632.2017.1305823,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020290839&doi=10.1080%2f01457632.2017.1305823&partnerID=40&md5=a1002e01c66c6d05e6d49f54be2c65c7,"This paper reports the use of Markov Chain Monte Carlo (MCMC) and Metropolis Hastings (MH) approach, to solve an inverse heat transfer problem. Three-dimensional, steady state, conjugate heat transfer from a Teflon cylinder of dimensions 100 mm diameter and 100 mm length with uniform volumetric internal heat generation is considered. The goal is to estimate volumetric heat generation and heat transfer coefficient, given the temperature data at certain fixed location on the surface of the cylinder. The internal volumetric heat generation is specified as input and the temperature and heat transfer coefficient values are obtained by a numerical solution to the governing equation. The temperature values also depend on heat transfer coefficient which is obtained by solving Navier–Stokes equation to obtain flow information. In order to reduce the computational cost, a neural network is trained from the computational fluid dynamics simulations. This is posed as an inverse problem wherein volumetric heat generation and heat transfer coefficient are unknown but the temperature data is known by conducting experiments. The novelty of the paper is the simultaneous determination of volumetric heat generation and heat transfer coefficient for the experimentally measured steady-state temperatures from a Teflon cylinder using MCMC-MH as an inverse model in a Bayesian framework and finally, the estimates are reported in terms of mean, maximum a posteriori, and the standard deviation which is the uncertainty associated with the estimated parameters. © 2018 Taylor & Francis Group, LLC."
1,10.1080/03610918.2018.1438619,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042367426&doi=10.1080%2f03610918.2018.1438619&partnerID=40&md5=29023f09dc780a011baec47166419b3d,"In the current study we develop the robust Bayesian inference for the generalized inverted family of distributions (GIFD) under an ε-contamination class of prior distributions for the shape parameter α, with different possibilities of known and unknown scale parameter. We used Type II censoring and Bartholomew sampling scheme (1963) for the following derivations under the squared-error loss function (SELF) and linear exponential (LINEX) loss function : ML-II Bayes estimators of the i) parameters; ii) Reliability function and; iii) Hazard function. We also present simulation study and analysis of a real data set. © 2018 Taylor & Francis Group, LLC"
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048123311&partnerID=40&md5=7bae440f17558de7de3a6a2386af1ce6,"Inverse problems arise in many fields of science focusing on the process that explores the causal factors from which a set of measurements are observed. Compared with deterministic methods, statistical inversion is more capable of finding global optima of nonlinear inverse problems. In this paper, we propose a statistical approach based on the Markov chain Monte Carlo (MCMC) method and its implementation with the scalable dataset under the parallel environment Message Passing Interface (MPI). Traditional MCMC method launches one chain at a time. Our approach can simultaneously launch multiple Markov chains for one inverse problem. By calculating the data misfit and applying proper clustering algorithms, we can get a good estimation of the formation parameters from different inversion results. Numerical experimental evidences show that our parallel approach has better performance than traditional approaches. Meanwhile, it requires much less computational time with appropriate resource allocation. © 2018 USNC-URSI."
,10.1109/ASPDAC.2018.8297393,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045312615&doi=10.1109%2fASPDAC.2018.8297393&partnerID=40&md5=537ca0c5a1be38c86ac3e6b7ed895c0b,"The effect of negative bias temperature instability (NBTI) varies significantly according to given workloads, and thus path delay degradation is strongly dependent on each use case. In this paper, we propose a subset simulation (SS) framework that efficiently and accurately finds the worst case workload and the failure probability covering various workloads. In the proposed method, workloads that yield worst aged delay are efficiently generated by the NBTI-aware Markov chain Monte Carlo method. Through numerical experiments using benchmark circuits, the proposed method achieves up to 36 times speedup compared to the naive Monte Carlo method. From the result of the SS, feasible workload that gives worst aged delay is obtained. © 2018 IEEE."
,10.1002/sim.7530,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040746153&doi=10.1002%2fsim.7530&partnerID=40&md5=ad2bd1d664fc4703b3ec5c6a18cd9e9d,"Many disease diagnoses involve subjective judgments by qualified raters. For example, through the inspection of a mammogram, MRI, or ultrasound image, the clinician himself becomes part of the measuring instrument. To reduce diagnostic errors and improve the quality of diagnoses, it is necessary to assess raters' diagnostic skills and to improve their skills over time. This paper focuses on a subjective binary classification process, proposing a hierarchical model linking data on rater opinions with patient true disease-development outcomes. The model allows for the quantification of the effects of rater diagnostic skills (bias and magnifier) and patient latent disease severity on the rating results. A Bayesian Markov chain Monte Carlo (MCMC) algorithm is developed to estimate these parameters. Linking to patient true disease outcomes, the rater-specific sensitivity and specificity can be estimated using MCMC samples. Cost theory is used to identify poor- and strong-performing raters and to guide adjustment of rater bias and diagnostic magnifier to improve the rating performance. Furthermore, diagnostic magnifier is shown as a key parameter to present a rater's diagnostic ability because a rater with a larger diagnostic magnifier has a uniformly better receiver operating characteristic (ROC) curve when varying the value of diagnostic bias. A simulation study is conducted to evaluate the proposed methods, and the methods are illustrated with a mammography example. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1103/PhysRevE.97.022413,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042234285&doi=10.1103%2fPhysRevE.97.022413&partnerID=40&md5=60ea6d1989408d2dff104a30dc9bc5d2,"The deterministic Hill function depends only on the average values of molecule numbers. To account for the fluctuations in the molecule numbers, the argument of the Hill function needs to contain the means, the standard deviations, and the correlations. Here we present a method that allows for stochastic Hill functions to be constructed from the dynamical evolution of stochastic biocircuits with specific topologies. These stochastic Hill functions are presented in a closed analytical form so that they can be easily incorporated in models for large genetic regulatory networks. Using a repressive biocircuit as an example, we show by Monte Carlo simulations that the traditional deterministic Hill function inaccurately predicts time of repression by an order of two magnitudes. However, the stochastic Hill function was able to capture the fluctuations and thus accurately predicted the time of repression. © 2018 American Physical Society."
2,10.1103/PhysRevD.97.043520,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043700554&doi=10.1103%2fPhysRevD.97.043520&partnerID=40&md5=7ba9db5a489dc02732df7e43bc4551ef,"The Higgs-dilaton model is a scale-invariant extension of the Standard Model nonminimally coupled to gravity and containing just one additional degree of freedom on top of the Standard Model particle content. This minimalistic scenario predicts a set of measurable consistency relations between the inflationary observables and the dark-energy equation-of-state parameter. We present an alternative derivation of these consistency relations that highlights the connections and differences with the α-attractor scenario. We study how far these constraints allow one to distinguish the Higgs-dilaton model from ΛCDM and wCDM cosmologies. To this end we first analyze existing data sets using a Markov chain Monte Carlo approach. Second, we perform forecasts for future galaxy surveys using a Fisher matrix approach, both for galaxy clustering and weak lensing probes. Assuming that the best fit values in the different models remain comparable to the present ones, we show that both Euclid- and SKA2-like missions will be able to discriminate a Higgs-dilaton cosmology from ΛCDM and wCDM. © 2018 American Physical Society."
2,10.1088/1475-7516/2018/02/043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043600145&doi=10.1088%2f1475-7516%2f2018%2f02%2f043&partnerID=40&md5=07d066e4b3914a0dfa7e003686fac9f6,"We revisit the constraints that Planck 2015 temperature, polarization and lensing data impose on the parameters of warm inflation. To this end, we study warm inflation driven by a single scalar field with a quartic self interaction potential in the weak dissipative regime. We analyse the effect of the parameters of warm inflation, namely, the inflaton self coupling λ and the inflaton dissipation parameter QP on the CMB angular power spectrum. We constrain λ and QP for 50 and 60 number of e-foldings with the full Planck 2015 data (TT, TE, EE + lowP and lensing) by performing a Markov-Chain Monte Carlo analysis using the publicly available code CosmoMC and obtain the joint as well as marginalized distributions of those parameters. We present our results in the form of mean and 68 % confidence limits on the parameters and also highlight the degeneracy between λ and QP in our analysis. From this analysis we show how warm inflation parameters can be well constrained using the Planck 2015 data. © 2018 IOP Publishing Ltd and Sissa Medialab."
9,10.3847/1538-4357/aaaa68,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042702449&doi=10.3847%2f1538-4357%2faaaa68&partnerID=40&md5=dc7f257e00adad290b59c0c85b614888,"We report a systematic search for an emission line around 3.5 keV in the spectrum of the cosmic X-ray background using a total of ∼10 Ms Chandra observations toward the COSMOS Legacy and Extended Chandra Deep Field South survey fields. We find marginal evidence of a feature at an energy of ∼3.51 keV with a significance of 2.5-3σ, depending on the choice of statistical treatment. The line intensity is best fit at (8.8 ±2.9) ×10-7 ph cm-2 s-1 when using a simple Δχ 2 or ph cm-2 s-1 when Markov chain Monte Carlo is used. Based on our knowledge of Chandra and the reported detection of the line by other instruments, an instrumental origin for the line remains unlikely. We cannot, however, rule out a statistical fluctuation, and in that case our results provide a 3σ upper limit at 1.85 ×10-6 ph cm-2 s-1. We discuss the interpretation of this observed line in terms of the iron line background, S xvi charge exchange, as well as potentially being from sterile neutrino decay. We note that our detection is consistent with previous measurements of this line toward the Galactic center and can be modeled as the result of sterile neutrino decay from the Milky Way for the dark matter distribution modeled as a Navarro-Frenk-White profile. For this case, we estimate a mass m ν ∼ 7.01 keV and a mixing angle sin2(2θ) = (0.83-2.75) ×10-10. These derived values are in agreement with independent estimates from galaxy clusters, the Galactic center, and M31. © 2018. The American Astronomical Society. All rights reserved.."
4,10.3847/1538-4357/aaa9be,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042688972&doi=10.3847%2f1538-4357%2faaa9be&partnerID=40&md5=9d2c07da174c199eb18d92342b8e438b,"The interstellar medium is crucial to understanding the physics of active galaxies and the coevolution between supermassive black holes and their host galaxies. However, direct gas measurements are limited by sensitivity and other uncertainties. Dust provides an efficient indirect probe of the total gas. We apply this technique to a large sample of quasars, whose total gas content would be prohibitively expensive to measure. We present a comprehensive study of the full (1 to 500 μm) infrared spectral energy distributions of 87 redshift <0.5 quasars selected from the Palomar-Green sample, using photometric measurements from 2MASS, WISE, and Herschel, combined with Spitzer mid-infrared (5-40 μm) spectra. With a newly developed Bayesian Markov Chain Monte Carlo fitting method, we decompose various overlapping contributions to the integrated spectral energy distribution, including starlight, warm dust from the torus, and cooler dust on galaxy scales. This procedure yields a robust dust mass, which we use to infer the gas mass, using a gas-to-dust ratio constrained by the host galaxy stellar mass. Most (90%) quasar hosts have gas fractions similar to those of massive, star-forming galaxies, although a minority (10%) seem genuinely gas-deficient, resembling present-day massive early-type galaxies. This result indicates that ""quasar mode"" feedback does not occur or is ineffective in the host galaxies of low-redshift quasars. We also find that quasars can boost the interstellar radiation field and heat dust on galactic scales. This cautions against the common practice of using the far-infrared luminosity to estimate the host galaxy star formation rate. © 2018. The American Astronomical Society. All rights reserved."
,10.1109/ACCESS.2018.2807807,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042173085&doi=10.1109%2fACCESS.2018.2807807&partnerID=40&md5=635ce434e9937076d1a01173600e0e58,"Ultrasound contrast imaging (UCI) aims to detect flow changes in the vascular bed that can help differentiate normal from diseased tissues thus providing an early screening tool for diagnosis or treatment monitoring. Ultrasound contrast agents (UCAs), used in UCI, are microbubbles that scatter ultrasound non-linearly. To date the signal processing research has successfully subtracted signals from the linear response of tissue (linear signals), but, in general, has not provided a sensitive detection that is specific to the UCA signal. This paper develops a method for the temporal and spectral estimation of linear and non-linear ultrasound echo signals. This technique is based on non-parametric methods for coarse estimation, followed by a parametric method within a Bayesian framework for estimation refinement. The results show that the pulse location can be estimated to within ±3 sample points accuracy for signals consisting of ≈ 80 sample points depending on the signal type, while the frequency content can be estimated to within 0.050 MHz deviations for frequencies in the 1 to 4 MHz range. This parametric spectral estimation achieved a 5-fold improvement in the frequency resolution compared with Fourier-based methods, and revealed previously unresolved frequency information that led to over 80% correct signal classification for linear and non-linear echo signals. © 2018 IEEE."
1,10.1080/02664763.2017.1288200,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012052974&doi=10.1080%2f02664763.2017.1288200&partnerID=40&md5=63ee40758c12a5dd71f5809de612f8f2,"In this paper we propose a novel Bayesian statistical methodology for spatial survival data. Our methodology broadens the definition of the survival, density and hazard functions by explicitly modeling the spatial dependency using direct derivations of these functions and their marginals and conditionals. We also derive spatially dependent likelihood functions. Finally we examine the applications of these derivations with geographically augmented survival distributions in the context of the Louisiana Surveillance, Epidemiology, and End Results registry prostate cancer data. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
1,10.1002/2017GL076429,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041899083&doi=10.1002%2f2017GL076429&partnerID=40&md5=9b7740fb43dc60f64b9a08aed7aa6516,"Accelerating rates of quasiperiodic “drumbeat” long-period earthquakes (LPs) are commonly reported before eruptions at andesite and dacite volcanoes, and promise insights into the nature of fundamental preeruptive processes and improved eruption forecasts. Here we apply a new Bayesian Markov chain Monte Carlo gamma point process methodology to investigate an exceptionally well-developed sequence of drumbeat LPs preceding a recent large vulcanian explosion at Tungurahua volcano, Ecuador. For more than 24 hr, LP rates increased according to the inverse power law trend predicted by material failure theory, and with a retrospectively forecast failure time that agrees with the eruption onset within error. LPs resulted from repeated activation of a single characteristic source driven by accelerating loading, rather than a distributed failure process, showing that similar precursory trends can emerge from quite different underlying physics. Nevertheless, such sequences have clear potential for improving forecasts of eruptions at Tungurahua and analogous volcanoes. ©2018. American Geophysical Union. All Rights Reserved."
3,10.1016/j.physa.2017.11.134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035357319&doi=10.1016%2fj.physa.2017.11.134&partnerID=40&md5=4b36e02f862739557157355748bf403c,"The traditional complex network theory is particularly focused on network models in which all network constituents are dealt with equivalently, while fail to consider the supplementary information related to the dynamic properties of the network interactions. This is a main constraint leading to incorrect descriptions of some real-world phenomena or incomplete capturing the details of certain real-life problems. To cope with the problem, this paper addresses the multilayer aspects of dynamic complex networks by analyzing the properties of intrinsically multilayered co-authorship networks, DBLP and Astro Physics, and presenting a novel multilayer model of dynamic complex networks. The model examines the layers evolution (layers birth/death process and lifetime) throughout the network evolution. Particularly, this paper models the evolution of each node's membership in different layers by an Infinite Factorial Hidden Markov Model considering feature cascade, and thereby formulates the link generation process for intra-layer and inter-layer links. Although adjacency matrixes are useful to describe the traditional single-layer networks, such a representation is not sufficient to describe and analyze the multilayer dynamic networks. This paper also extends a generalized mathematical infrastructure to address the problems issued by multilayer complex networks. The model inference is performed using some Markov Chain Monte Carlo sampling strategies, given synthetic and real complex networks data. Experimental results indicate a tremendous improvement in the performance of the proposed multilayer model in terms of sensitivity, specificity, positive and negative predictive values, positive and negative likelihood ratios, F1-score, Matthews correlation coefficient, and accuracy for two important applications of missing link prediction and future link forecasting. The experimental results also indicate the strong predictivepower of the proposed model for the application of cascade prediction in terms of accuracy. © 2017 Elsevier B.V."
,10.3969/j.issn.1001-8360.2018.02.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047099401&doi=10.3969%2fj.issn.1001-8360.2018.02.001&partnerID=40&md5=e13716fbf86f70a57cd1edcf7431ecd8,"The development of urban rail transit network provides more and more alternative routes to passengers.However, passengers usually only consider the routes(i.e.effective route set generation) in the threshold range of a certain factor(e.g.travel time), and then choose the best one from the set of effective routes.A semi-compensatory Mixed Logit route choice model for urban rail transit passengers was proposed, in which the above two sub-processes were combined based on Bayesian theory and the effective function coefficients and threshold parameters were regarded as random variables to express the differences among passengers'choice preferences and among the thresholds of the effective route set.A model estimation method was designed combining the data augmentation technique and Markov Chain Monte Carlo method to estimate effective function coefficients and threshold parameters endogenously.Based on the surveyed data in Guangzhou Metro, the estimations show that the thresholds are more reliable and the proposed model is better than traditional models.With respect to the scenario of the network structural change caused by the connection of Line 6 with Guangzhou Metro network, the transfer flow volumes of all transfer stations are estimated based on the proposed model, with the mean absolute error of 5.19%.The results demonstrate the adaptability of the proposed model to the travel demand forecasting under the condition of network structural change. © 2018, Department of Journal of the China Railway Society. All right reserved."
,10.1145/3174243.3174259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050192342&doi=10.1145%2f3174243.3174259&partnerID=40&md5=c10271fe00769a1de42e6e3f551d4d84,"This paper proposes CausaLearn, the first automated framework that enables real-time and scalable approximation of Probability Density Function (PDF) in the context of causal Bayesian graphical models. CausaLearn targets complex streaming scenarios in which the input data evolves over time and independence cannot be assumed between data samples (e.g., continuous time-varying data analysis). Our framework is devised using a HW/SW co-design approach. We provide the first implementation of Hamiltonian Markov Chain Monte Carlo on FPGA that can efficiently sample from the steady state probability distribution at scales while considering the correlation between the observed data. CausaLearn is customizable to the limits of the underlying resource provisioning in order to maximize the effective system throughput. It uses physical profiling to abstract high-level hardware characteristics. These characteristics are integrated into our automated customization unit in order to tile, schedule, and batch the PDF approximation workload corresponding to the pertinent platform resources and constraints. We benchmark the design performance for analyzing various massive time-series data on three FPGA platforms with different computational budgets. Our extensive evaluations demonstrate up to two orders-of-magnitude runtime and energy improvements compared to the best-known prior solution. We provide an accompanying API that can be leveraged by data scientists and practitioners to automate and abstract hardware design optimization. © 2018 Association for Computing Machinery."
1,10.1016/j.physa.2017.11.076,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035073299&doi=10.1016%2fj.physa.2017.11.076&partnerID=40&md5=ef590f19fe10efc547d1a4d57fc204b9,"It is increasingly recognized that understanding the complex interplay patterns between epidemic spreading and human behavioral is a key component of successful infection control efforts. In particular, individuals can obtain the information about epidemics and respond by altering their behaviors, which can affect the spreading dynamics as well. Besides, because the existence of herd-like behaviors, individuals are very easy to be influenced by the global awareness information. Here, in this paper, we propose a global awareness controlled spreading model (GACS) to explore the interplay between the coupled dynamical processes. Using the global microscopic Markov chain approach, we obtain the analytical results for the epidemic thresholds, which shows a high accuracy by comparison with lots of Monte Carlo simulations. Furthermore, considering other classical models used to describe the coupled dynamical processes, including the local awareness controlled contagion spreading (LACS) model, Susceptible–Infected–Susceptible–Unaware–Aware–Unaware (SIS–UAU) model and the single layer occasion, we make a detailed comparisons between the GACS with them. Although the comparisons and results depend on the parameters each model has, the GACS model always shows a strong restrain effects on epidemic spreading process. Our results give us a better understanding of the coupled dynamical processes and highlights the importance of considering the spreading of global awareness in the control of epidemics. © 2017 Elsevier B.V."
2,10.1093/bioinformatics/btx626,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042550998&doi=10.1093%2fbioinformatics%2fbtx626&partnerID=40&md5=f3d381ca52628e4dc034e4f220e4d609,"Summary Biological models contain many parameters whose values are difficult to measure directly via experimentation and therefore require calibration against experimental data. Markov chain Monte Carlo (MCMC) methods are suitable to estimate multivariate posterior model parameter distributions, but these methods may exhibit slow or premature convergence in high-dimensional search spaces. Here, we present PyDREAM, a Python implementation of the (Multiple-Try) Differential Evolution Adaptive Metropolis [DREAM (ZS) ] algorithm developed byVrugt and ter Braak (2008)andLaloy and Vrugt (2012). PyDREAM achieves excellent performance for complex, parameter-rich models and takes full advantage of distributed computing resources, facilitating parameter inference and uncertainty estimation of CPU-intensive biological models. Availability and implementation PyDREAM is freely available under the GNU GPLv3 license from the Lopez lab GitHub repository at http://github.com/LoLab-VU/PyDREAM. Contact c.lopez@vanderbilt.edu Supplementary informationSupplementary dataare available at Bioinformatics online. © The Author 2017. Published by Oxford University Press."
,10.1109/PIMRC.2017.8292505,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045275309&doi=10.1109%2fPIMRC.2017.8292505&partnerID=40&md5=7dd42b070a6205765197ef0dbee7c8f7,"Heterogenous networks (HetNets) using different size cells and several different networks with multiple wireless access technologies can provide large capacities while also improving the localization accuracy. In this paper, we propose a novel received signal strength (RSS) based inter-network cooperative localization framework based on a Metropolis-Hastings (MH) algorithm for twotier HetNets with unknown transmit powers. Through the MH based estimation methodology, the unknown position of user equipment and transmit powers of base stations (BSs) are jointly estimated. The validity of the proposed method is confirmed by simulation results. © 2017 IEEE."
,10.1109/TCBB.2018.2802911,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042111564&doi=10.1109%2fTCBB.2018.2802911&partnerID=40&md5=c63322ee112cf2fc41abbf9672d2e891,"The subtree prune-and-regraft (SPR) distance metric is a fundamental way of comparing evolutionary trees. It has wide-ranging applications, such as to study lateral genetic transfer, viral recombination, and Markov chain Monte Carlo phylogenetic inference. Although the rooted version of SPR distance can be computed relatively efficiently between rooted trees using fixed-parameter-tractable maximum agreement forest (MAF) algorithms, no MAF formulation is known for the unrooted case. Correspondingly, previous algorithms are unable to compute unrooted SPR distances larger than 7. CCBY"
,10.1002/sim.7533,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034573162&doi=10.1002%2fsim.7533&partnerID=40&md5=9d1aabba76c88940db240f4d100eadac,"Drug dilution (MIC) and disk diffusion (DIA) are the 2 most common antimicrobial susceptibility assays used by hospitals and clinics to determine an unknown pathogen's susceptibility to various antibiotics. Since only one assay is commonly used, it is important that the 2 assays give similar results. Calibration of the DIA assay to the MIC assay is typically done using the error-rate bounded method, which selects DIA breakpoints that minimize the observed discrepancies between the 2 assays. In 2000, Craig proposed a model-based approach that specifically models the measurement error and rounding processes of each assay, the underlying pathogen distribution, and the true monotonic relationship between the 2 assays. The 2 assays are then calibrated by focusing on matching the probabilities of correct classification (susceptible, indeterminant, and resistant). This approach results in greater precision and accuracy for estimating DIA breakpoints. In this paper, we expand the flexibility of the model-based method by introducing a Bayesian 4-parameter logistic model (extending Craig's original 3-parameter model) as well as a Bayesian nonparametric spline model to describe the relationship between the 2 assays. We propose 2 ways to handle spline knot selection, considering many equally spaced knots but restricting overfitting via a random walk prior and treating the number and location of knots as additional unknown parameters. We demonstrate the 2 approaches via a series of simulation studies and apply the methods to 2 real data sets. Copyright © 2017 John Wiley & Sons, Ltd."
4,10.3847/1538-4357/aaa9c2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042613883&doi=10.3847%2f1538-4357%2faaa9c2&partnerID=40&md5=433768bb91f97575469b6d6181998fcd,"We present deep spectroscopic observations of a Lyman break galaxy (LBG) candidate (hereafter MACS1149-JD) at z ∼ 9.5 with the Hubble Space Telescope (HST) WFC3/IR grisms. The grism observations were taken at four distinct position angles, totaling 34 orbits with the G141 grism, although only 19 of the orbits are relatively uncontaminated along the trace of MACS1149-JD. We fit a three-parameter (z, F160W mag, and Lyα equivalent width [EW]) LBG template to the three least contaminated grism position angles using a Markov chain Monte Carlo approach. The grism data alone are best fit with a redshift of (68% confidence), in good agreement with our photometric estimate of (68% confidence). Our analysis rules out Lyα emission from MACS1149-JD above a 3σ EW of 21 Å, consistent with a highly neutral IGM. We explore a scenario where the red Spitzer/IRAC [3.6]-[4.5] color of the galaxy previously pointed out in the literature is due to strong rest-frame optical emission lines from a very young stellar population rather than a 4000 Å break. We find that while this can provide an explanation for the observed IRAC color, it requires a lower redshift (z ≲ 9.1), which is less preferred by the HST imaging data. The grism data are consistent with both scenarios, indicating that the red IRAC color can still be explained by a 4000 Å break, characteristic of a relatively evolved stellar population. In this interpretation, the photometry indicates that a Myr stellar population is already present in this galaxy only ∼500 Myr after the big bang. © 2018. The American Astronomical Society. All rights reserved."
,10.3389/fncel.2018.00033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043597927&doi=10.3389%2ffncel.2018.00033&partnerID=40&md5=302c90ce4582e9a04bd187bc70f3d037,"Understanding the relationships between the rates and dynamics of current wave forms under voltage clamp conditions is essential for understanding phenomena such as state-dependence and use-dependence, which are fundamental for the action of drugs used as anti-epileptics, anti-arrhythmics, and anesthetics. In the present study, we mathematically analyze models of blocking mechanisms. In previous experimental studies of potassiumchannels we have shown that the effect of local anesthetics can be explained by binding to channels in the open state. We therefore here examine models that describe the effect of a blocking drug that binds to a non-inactivating channel in its open state. Such binding induces an inactivation-like current decay at higher potential steps. The amplitude of the induced peak depends on voltage and concentration of blocking drug. In the present study, using analytical methods, we (i) derive a criterion for the existence of a peak in the open probability time evolution for a model with an arbitrary number of closed states, (ii) derive formula for the relative height of the peak amplitude, and (iii) determine the voltage dependence of the relative peak height. Two findings are apparent: (1) the dissociation (unbinding) rate constant is important for the existence of a peak in the current waveform, while the association (binding) rate constant is not, and (2) for a peak to exist it suffices that the dissociation rate must be smaller than the absolute value of all eigenvalues to the kinetic matrix describing the model. © 2018 Zeberg, Nilsson and Århem."
,10.1186/s12966-018-0649-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041841279&doi=10.1186%2fs12966-018-0649-5&partnerID=40&md5=7802dd3d7209d07a0d9e0a28ee5d8dad,"Background: Guided by the Socialization Model of Child Behavior (SMCB), this cross-sectional study examined direct and indirect associations of parental cognitions and behavior, the home and neighborhood environment, and toddlers' personal attributes with toddlers' physical activity and screen time. Methods: Participants included 193 toddlers (1.6 ± 0.2 years) from the Parents' Role in Establishing healthy Physical activity and Sedentary behavior habits (PREPS) project. Toddlers' screen time and personal attributes, physical activity- or screen time-specific parental cognitions and behaviors, and the home and neighborhood environment were measured via parental-report using the PREPS questionnaire. Accelerometry-measured physical activity was available in 123 toddlers. Bayesian estimation in structural equation modeling (SEM) using the Markov Chain Monte Carlo algorithm was performed to test an SMCB hypothesized model. Covariates included toddlers' age, sex, race/ethnicity, main type of childcare, and family household income. Results: In the SMCB hypothesized screen time model, higher parental barrier self-efficacy for limiting toddlers' screen time was associated with higher parental screen time limiting practices (β=0.451), while higher parental negative outcome expectations for limiting toddlers' screen time was associated with lower parental screen time limiting practices (β = - 0.147). In turn, higher parental screen time limiting practices was associated with lower screen time among toddlers (β = - 0.179). Parental modeling of higher screen time was associated with higher screen time among toddlers directly (β=0.212) and indirectly through the home environment. Specifically, higher screen time among parents was associated with having at least one electronic device in toddlers' bedrooms (β=0.146) and, in turn, having electronics in the bedroom, compared to none, was associated with higher screen time among toddlers (β=0.250). Neighborhood safety was not associated with toddlers' screen time in the SEM analysis. No significant correlations were observed between the SMCB variables and toddlers' physical activity; thus, no further analyses were performed for physical activity. Conclusions: Parents and their interactions with the home environment may play an important role in shaping toddlers' screen time. Findings can inform family-based interventions aiming to minimize toddlers' screen time. Future research is needed to identify correlates of toddlers' physical activity. © 2018 The Author(s)."
,10.1103/PhysRevE.97.022112,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042082092&doi=10.1103%2fPhysRevE.97.022112&partnerID=40&md5=47fefa24d8d1e1aea640327293120708,"We study diffusion-controlled two-species annihilation with a finite number of particles. In this stochastic process, particles move diffusively, and when two particles of opposite type come into contact, the two annihilate. We focus on the behavior in three spatial dimensions and for initial conditions where particles are confined to a compact domain. Generally, one species outnumbers the other, and we find that the difference between the number of majority and minority species, which is a conserved quantity, controls the behavior. When the number difference exceeds a critical value, the minority becomes extinct and a finite number of majority particles survive, while below this critical difference, a finite number of particles of both species survive. The critical difference Δc grows algebraically with the total initial number of particles N, and when N 1, the critical difference scales as Δc∼N1/3. Furthermore, when the initial concentrations of the two species are equal, the average number of surviving majority and minority particles, M+ and M-, exhibit two distinct scaling behaviors, M+∼N1/2 and M-∼N1/6. In contrast, when the initial populations are equal, these two quantities are comparable M+∼M-∼N1/3. © 2018 American Physical Society."
,10.1109/ITNEC.2017.8284942,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046701177&doi=10.1109%2fITNEC.2017.8284942&partnerID=40&md5=ee5e1d804b632324d2323a53645aa350,"A blind estimation algorithm based on overlapping segment Markov Chain Monte Carlo-Unscented Kalman Filter(MCMC-UKF) is proposed for the problem of spread spectrum code and information sequence blind estimation of long code direct sequence spread spectrum(DSSS) signal. The algorithm is based on the Bayesian framework model, combined with the idea of overlapping segmentation, using the UKF algorithm to solve the nonlinear model, estimate the mean and variance of the posterior probability of each parameter, and finally use the MCMC method to iterate segment spread spectrum sequence, the sequence of splicing to complete the spread spectrum sequence and information sequence estimates. The algorithm can achieve effective estimation of short codes and long code signals, and is not limited by the type of spread spectrum sequences. The simulation results show that the proposed algorithm has better performance with low SNR. © 2017 IEEE."
,10.1080/03610918.2017.1288244,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020457131&doi=10.1080%2f03610918.2017.1288244&partnerID=40&md5=eba3473bce5a557af8d3a37c6b32a090,"Mixture models are frequently used for modeling complex data. An extension of the EM algorithm, here called ECME, is proposed to compute the maximum likelihood estimate of parameters of symmetric α-stable mixture model (SαSMM). Comprehensive simulation studies are performed to show the performance of the proposed ECME algorithm. The robustness of the SαSMM is investigated by simulations when it is used to model data generated from mixture of exponential power and t distributions. Both proposed ECME and Bayesian approaches are applied to three sets of real data, which shows that the proposed ECME algorithm outperforms the Bayesian paradigm for all three sets. Also, the SαSMM is compared with the mixture of normal, skew normal, t, and skew t distributions for modeling four sets of real data. It turns out that the SαSMM works as well as or better than above models. This can be considered as SαSMM capability in robust mixture modeling. © 2017 Taylor & Francis Group, LLC."
,10.1109/ICTUS.2017.8285989,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047124809&doi=10.1109%2fICTUS.2017.8285989&partnerID=40&md5=a73d1304e0c91f76d3133d107e1b8a64,"We discuss an interval estimation approach for parameters and software reliability assessment measures, which are derived from a discretized software reliability model. In our approach, we apply the Markov chain Monte Carlo (MCMC) method for conducing Bayesian interval estimations in software reliability assessment. Further, we shows numerical examples of our approach by using actual fault count data. © 2017 IEEE."
2,10.1063/1.5017031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041418662&doi=10.1063%2f1.5017031&partnerID=40&md5=febd53c9b296efe7e3e865d977fb578e,"We use Markov state models (MSMs) to analyze the dynamics of a β-hairpin-forming peptide in Monte Carlo (MC) simulations with interacting protein crowders, for two different types of crowder proteins [bovine pancreatic trypsin inhibitor (BPTI) and GB1]. In these systems, at the temperature used, the peptide can be folded or unfolded and bound or unbound to crowder molecules. Four or five major free-energy minima can be identified. To estimate the dominant MC relaxation times of the peptide, we build MSMs using a range of different time resolutions or lag times. We show that stable relaxation-time estimates can be obtained from the MSM eigenfunctions through fits to autocorrelation data. The eigenfunctions remain sufficiently accurate to permit stable relaxation-time estimation down to small lag times, at which point simple estimates based on the corresponding eigenvalues have large systematic uncertainties. The presence of the crowders has a stabilizing effect on the peptide, especially with BPTI crowders, which can be attributed to a reduced unfolding rate ku, while the folding rate kf is left largely unchanged. © 2018 Author(s)."
,10.1080/07474938.2015.1032166,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949544964&doi=10.1080%2f07474938.2015.1032166&partnerID=40&md5=971bc18e8b8703d9df3c0fe01a08a148,"Decreasing block rate pricing is a nonlinear price system often used for public utility services. Residential gas services in Japan and the United Kingdom are provided under this price schedule. The discrete/continuous choice approach is used to analyze the demand under decreasing block rate pricing. However, the nonlinearity problem, which has not been examined in previous studies, arises because a consumer’s budget set (a set of affordable consumption amounts) is nonconvex, and hence, the resulting model includes highly nonlinear functions. To address this problem, we propose a feasible, efficient method of demand estimation on the nonconvex budget. The advantages of our method are as follows: (i) the construction of an Markov chain Monte Carlo algorithm with an efficient blanket based on the Hermite–Hadamard integral inequality and the power-mean inequality, (ii) the explicit consideration of the (highly nonlinear) separability condition, which often makes numerical likelihood maximization difficult, and (iii) the introduction of normal disturbance into the discrete/continuous choice model on the nonconvex budget set. The proposed method is applied to estimate the Japanese residential gas demand function and evaluate the effect of price schedule changes as a policy experiment. © 2015 Taylor & Francis Group, LLC."
2,10.1137/16M1108340,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045631248&doi=10.1137%2f16M1108340&partnerID=40&md5=ec736c0102ccf1af9f2474b8669cfa64,"Modern imaging methods rely strongly on Bayesian inference techniques to solve challenging imaging problems. Currently, the predominant Bayesian computation approach is convex optimization, which scales very efficiently to high-dimensional image models and delivers accurate point estimation results. However, in order to perform more complex analyses, for example, image uncertainty quantification or model selection, it is necessary to use more computationally intensive Bayesian computation techniques such as Markov chain Monte Carlo methods. This paper presents a new and highly efficient Markov chain Monte Carlo methodology to perform Bayesian computation for high-dimensional models that are log-concave and nonsmooth, a class of models that is central in imaging sciences. The methodology is based on a regularized unadjusted Langevin algorithm that exploits tools from convex analysis, namely, Moreau–Yoshida envelopes and proximal operators, to construct Markov chains with favorable convergence properties. In addition to scaling efficiently to high-dimensions, the method is straightforward to apply to models that are currently solved by using proximal optimization algorithms. We provide a detailed theoretical analysis of the proposed methodology, including asymptotic and nonasymptotic convergence results with easily verifiable conditions, and explicit bounds on the convergence rates. The proposed methodology is demonstrated with four experiments related to image deconvolution and tomographic reconstruction with total-variation and `1 priors, where we conduct a range of challenging Bayesian analyses related to uncertainty quantification, hypothesis testing, and model selection in the absence of ground truth. © 2018 Society for Industrial and Applied Mathematics and by SIAM."
6,10.1073/pnas.1715640115,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041541402&doi=10.1073%2fpnas.1715640115&partnerID=40&md5=73443e8dc6a40b7360f2e1ff946f3b07,"Plague, caused by the bacterium Yersinia pestis, can spread through human populations by multiple transmission pathways. Today, most human plague cases are bubonic, caused by spillover of infected fleas from rodent epizootics, or pneumonic, caused by inhalation of infectious droplets. However, little is known about the historical spread of plague in Europe during the Second Pandemic (14-19th centuries), including the Black Death, which led to high mortality and recurrent epidemics for hundreds of years. Several studies have suggested that human ectoparasite vectors, such as human fleas (Pulex irritans) or body lice (Pediculus humanus humanus), caused the rapidly spreading epidemics. Here, we describe a compartmental model for plague transmission by a human ectoparasite vector. Using Bayesian inference, we found that this model fits mortality curves from nine outbreaks in Europe better than models for pneumonic or rodent transmission. Our results support that human ectoparasites were primary vectors for plague during the Second Pandemic, including the Black Death (1346-1353), ultimately challenging the assumption that plague in Europe was predominantly spread by rats."
,10.1016/j.compchemeng.2017.11.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037523482&doi=10.1016%2fj.compchemeng.2017.11.011&partnerID=40&md5=7bfd52e772d1a234c68b6d3a74a17d9a,"The inverse problem associated with fitting parameters of an ordinary differential equation (ODE) system to data is nonlinear and multimodal, which is of great challenge to gradient-based optimizers. Markov Chain Monte Carlo (MCMC) techniques provide an alternative approach to solving these problems and can escape local minima by design. APT-MCMC was created to allow users to setup ODE simulations in Python and run as compiled C++ code. It combines affine-invariant ensemble of samplers and parallel tempering MCMC techniques to improve the simulation efficiency. Simulations use Bayesian inference to provide probability distributions of parameters, which enable analysis of multiple minima and parameter correlation. Benchmark tests result in a 20×–60× speedup but 14% increase in memory usage against emcee, a similar MCMC package in Python. Several MCMC hyperparameters were analyzed: number of temperatures, ensemble size, step size, and swap attempt frequency. Heuristic tuning guidelines are provided for setting these hyperparameters. © 2017 Elsevier Ltd"
,10.1088/1748-0221/13/02/P02004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043583350&doi=10.1088%2f1748-0221%2f13%2f02%2fP02004&partnerID=40&md5=8a641be518c22890d2f9193b2d9b8f8a,"The Ricochet experiment seeks to measure Coherent (neutral-current) Elastic Neutrino-Nucleus Scattering (CEνNS) using dark-matter-style detectors with sub-keV thresholds placed near a neutrino source, such as the MIT (research) Reactor (MITR), which operates at 5.5 MW generating approximately 2.2 × 1018 ν/second in its core. Currently, Ricochet is characterizing the backgrounds at MITR, the main component of which comes in the form of neutrons emitted from the core simultaneous with the neutrino signal. To characterize this background, we wrapped Bonner cylinders around a 3 2He thermal neutron detector, whose data was then unfolded via a Markov Chain Monte Carlo (MCMC) to produce a neutron energy spectrum across several orders of magnitude. We discuss the resulting spectrum and its implications for deploying Ricochet at the MITR site as well as the feasibility of reducing this background level via the addition of polyethylene shielding around the detector setup. © 2018 IOP Publishing Ltd and Sissa Medialab."
,10.4230/LIPIcs.STACS.2018.18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044531120&doi=10.4230%2fLIPIcs.STACS.2018.18&partnerID=40&md5=69f6697ce284150bfc0258d63e2bdeb7,"Approximating the stationary probability of a state in a Markov chain through Markov chain Monte Carlo techniques is, in general, inefficient. Standard random walk approaches require O(t/p(v)) operations to approximate the probability p(v) of a state v in a chain with mixing time t, and even the best available techniques still have complexity O(t1.5/p(v)0.5); and since these complexities depend inversely on p(v), they can grow beyond any bound in the size of the chain or in its mixing time. In this paper we show that, for time-reversible Markov chains, there exists a simple randomized approximation algorithm that breaks this “small-p(v) barrier”. © Marco Bressan, Enoch Peserico, and Luca Pretto."
1,10.1088/1681-7575/aaa5be,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045694539&doi=10.1088%2f1681-7575%2faaa5be&partnerID=40&md5=bebc4274b7382dee3d6abc0f3c436c0f,"The Guide to the Expression of Uncertainty in Measurement (GUM) includes formulas that produce an estimate of a scalar output quantity that is a function of several input quantities, and an approximate evaluation of the associated standard uncertainty. This contribution presents approximate, Bayesian counterparts of those formulas for the case where the output quantity is a parameter of the joint probability distribution of the input quantities, also taking into account any information about the value of the output quantity available prior to measurement expressed in the form of a probability distribution on the set of possible values for the measurand. The approximate Bayesian estimates and uncertainty evaluations that we present have a long history and illustrious pedigree, and provide sufficiently accurate approximations in many applications, yet are very easy to implement in practice. Differently from exact Bayesian estimates, which involve either (analytical or numerical) integrations, or Markov Chain Monte Carlo sampling, the approximations that we describe involve only numerical optimization and simple algebra. Therefore, they make Bayesian methods widely accessible to metrologists. We illustrate the application of the proposed techniques in several instances of measurement: isotopic ratio of silver in a commercial silver nitrate; odds of cryptosporidiosis in AIDS patients; height of a manometer column; mass fraction of chromium in a reference material; and potential-difference in a Zener voltage standard. © 2018 European Physical Society."
,10.1093/gji/ggx461,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042149778&doi=10.1093%2fgji%2fggx461&partnerID=40&md5=ae4f88c23b1b8d66ebba85056a4da405,"Cycle skipping is a serious issue in fullwaveforminversion (FWI) since it leads to local minima. To date, most FWI algorithms depend on local gradient based optimization approaches, which cannot guarantee convergence towards the global minimum if the misfit function involves local minima and the starting model is far from the true solution. In this study, I propose a misfit function based on non-stationary time warping functions, which can be calculated by solving a seismogram registration problem. Considering the inherent cycle skipping and local minima issues of the registration problem, I use a Markov chain Monte Carlo (MCMC) method to solve it. With this global optimization approach, I am able to directly sample the global minimum and measure non-stationary traveltime differences between observed and predicted seismograms. Theapriori constraint about the sparsity of the localwarping functions is incorporated to eliminate unreasonable solutions. No window selections are required in this procedure. In comparison to other approaches for measuring traveltime differences, the proposed method enables us to align signals with different numbers of events. This property is a direct consequence of the usage of MCMC optimization and sparsity constraints. Several numerical examples demonstrate that the proposed misfit function allows us to tackle the cycle skipping problem and construct accurate long-wavelength velocity models even without low frequency data and good starting models. © The Authors 2017. Published by Oxford University Press on behalf of The Royal Astronomical Society."
1,10.21629/JSEE.2018.01.21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044430610&doi=10.21629%2fJSEE.2018.01.21&partnerID=40&md5=6675aacabbdcf138bf637cf67c81326b,"Most of the maintenance optimization models in condition-based maintenance (CBM) consider the cost-optimal criterion, but few papers have dealt with availability maximization for maintenance applications. A novel optimal Bayesian control approach is presented for maintenance decision making. The system deterioration evolves as a three-state continuous time hidden semi-Markov process. Considering the optimal maintenance policy, the multivariate Bayesian control scheme based on the hidden semi-Markov model (HSMM) is developed, the objective is to maximize the long-run expected average availability per unit time. The proposed approach can optimize the sampling interval and control limit jointly. A case study using Markov chain Monte Carlo (MCMC) simulation is provided and a comparison with the Bayesian control scheme based on hidden Markov model (HMM), the age-based replacement policy, Hotelling's T2, multivariate exponentially weihted moving average (MEWMA) and multivariate cumulative sum (MCUSUM) control charts is given, which illustrates the effectiveness of the proposed method. © 1990-2011 Beijing Institute of Aerospace Information."
,10.1007/s12204-018-1912-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042144086&doi=10.1007%2fs12204-018-1912-2&partnerID=40&md5=927f767889c88544f57301c390cb2e8f,"Performing arts and movies have become commercial products with high profit and great market potential. Previous research works have developed comprehensive models to forecast the demand for movies. However, they did not pay enough attention to the decision support for performing arts which is a special category unlike movies. For performing arts with high-dimensional categorical attributes and limit samples, determining ticket prices in different levels is still a challenge job faced by the producers and distributors. In terms of these difficulties, factorization machine (FM), which can handle huge sparse categorical attributes, is used in this work first. Adaptive stochastic gradient descent (ASGD) and Markov chain Monte Carlo (MCMC) are both explored to estimate the model parameters of FM. FM with ASGD (FM-ASGD) and FM with MCMC (FM-MCMC) both can achieve a better prediction accuracy, compared with a traditional algorithm. In addition, the multi-output model is proposed to determine the price in multiple price levels simultaneously, which avoids the trouble of the models’ repeating training. The results also confirm the prediction accuracy of the multi-output model, compared with those from the general single-output model. © 2018, Shanghai Jiaotong University and Springer-Verlag GmbH Germany, part of Springer Nature."
1,10.1007/s00477-017-1417-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018250315&doi=10.1007%2fs00477-017-1417-9&partnerID=40&md5=b2e9ba509e82d80b7e3b8976cd781e5c,"It is often of interest to model the incidence and duration of threshold exceedance events for an environmental variable over a set of monitoring locations. Such data arrive over continuous time and can be considered as observations of a two-state process yielding, sequentially, a length of time in the below threshold state followed by a length of time in the above threshold state, then returning to the below threshold state, etc. We have a two-state continuous time Markov process, often referred to as an alternating renewal process. The process is observed over a truncated time window and, within this window, duration in each state is modeled using a distinct cumulative intensity specification. Initially, we model each intensity over the window using a parametric regression specification. We extend the regression specification adding temporal random effects to enrich the model using a realization of a log Gaussian process over time. With only one type of renewal, this specification is referred to as a Gaussian process modulated renewal process. Here, we introduce Gaussian process modulation to the intensity for each state. Model fitting is done within a Bayesian framework. We clarify that fitting with a customary log Gaussian process specification over a lengthy time window is computationally infeasible. The nearest neighbor Gaussian process, which supplies sparse covariance structure, is adopted to enable tractable computation. We propose methods for both generating data under our models and for conducting model comparison. The model is applied to hourly ozone data for four monitoring sites at different locations across the United States for the ozone season of 2014. For each site, we obtain estimated profiles of up-crossing and down-crossing intensity functions through time. In addition, we obtain inference regarding the number of exceedances, the distribution of the duration of exceedance events, and the proportion of time in the above and below threshold state for any time interval. © 2017, Springer-Verlag Berlin Heidelberg."
4,10.1016/j.ress.2017.09.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032584070&doi=10.1016%2fj.ress.2017.09.020&partnerID=40&md5=3580eeea48d184165fd3de4f3ca66011,"This paper investigates the Bayesian melding method (BMM) for system reliability analysis by effectively integrating various available sources of expert knowledge and data at both subsystem and system levels. The integration of multiple priors is investigated under both linear and geometric pooling methods. The aggregated system prior distributions using various pooling methods including the BMM are evaluated and compared. Based on these integrated and updated prior distributions and three scenarios of data availability from a system and/or subsystems, methods for posterior system reliability inference are proposed. Computational challenges for posterior inferences using the sophisticated BMM are addressed using the adaptive sampling importance re-sampling (SIR) method. A numerical example with simulation results illustrates the applications of the proposed methods and provides insights for system reliability analysis using multilevel information. © 2017 Elsevier Ltd"
7,10.1017/S0956792517000079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017438057&doi=10.1017%2fS0956792517000079&partnerID=40&md5=0f4794330a5631523a3e12e32dbbf52a,"We propose a novel algorithm which allows to sample paths from an underlying price process in a local volatility model and to achieve a substantial variance reduction when pricing exotic options. The new algorithm relies on the construction of a discrete multinomial tree. The crucial feature of our approach is that - in a similar spirit to the Brownian Bridge - each random path runs backward from a terminal fixed point to the initial spot price. We characterize the tree in two alternative ways: (i) in terms of the optimal grids originating from the Recursive Marginal Quantization algorithm, (ii) following an approach inspired by the finite difference approximation of the diffusion's infinitesimal generator. We assess the reliability of the new methodology comparing the performance of both approaches and benchmarking them with competitor Monte Carlo methods. Copyright © Cambridge University Press 2017."
,10.1002/qre.2240,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040336014&doi=10.1002%2fqre.2240&partnerID=40&md5=969ddb2b05b351d2e6d1b561221314cd,"This article describes Bayes design of hybrid-censored life testing plans. A design criterion based on posterior variance of quantile of suitable order is proposed. The Weibull lifetime model with gamma prior distribution on model parameters is considered for illustration. Instead of using Markov chain Monte Carlo technique to compute the posterior quantities of interest, a large sample approximation is considered, which is easy to apply. Some life testing plans are presented. The effect of different prior information on the posterior quantity of interest is studied. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1016/j.csda.2017.09.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029819485&doi=10.1016%2fj.csda.2017.09.002&partnerID=40&md5=f2e59b39b1563750c77d639096881ec0,"The grouped independence Metropolis–Hastings (GIMH) and Markov chain within Metropolis (MCWM) algorithms are pseudo-marginal methods used to perform Bayesian inference in latent variable models. These methods replace intractable likelihood calculations with unbiased estimates within Markov chain Monte Carlo algorithms. The GIMH method has the posterior of interest as its limiting distribution, but suffers from poor mixing if it is too computationally intensive to obtain high-precision likelihood estimates. The MCWM algorithm has better mixing properties, but tends to give conservative approximations of the posterior and is still expensive. A new method is developed to accelerate the GIMH method by using a Gaussian process (GP) approximation to the log-likelihood and train this GP using a short pilot run of the MCWM algorithm. This new method called GP-GIMH is illustrated on simulated data from a stochastic volatility and a gene network model. The new approach produces reasonable posterior approximations in these examples with at least an order of magnitude improvement in computing time. Code to implement the method for the gene network example can be found at http://www.runmycode.org/companion/view/2663. © 2017 Elsevier B.V."
3,10.1002/stc.2089,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040967879&doi=10.1002%2fstc.2089&partnerID=40&md5=448bf4783bf8cab997e0b9e25a8d953f,"This paper presents a comprehensive study of the full-scale ambient vibration test, modal analysis, finite element (FE) modeling, and model updating of a coupled building in Hong Kong. The coupled building comprised a main part and a complementary part. To capture the dynamic properties of the building, a 21-setup ambient vibration test was designed and conducted. The modal parameters of each setup were identified following a fast Bayesian fast Fourier transform approach, and the partial mode shapes from the different setups were assembled following the least squares method. The identified modal parameters were analyzed and discussed in detail, revealing certain features of the coupling effects between the main and complementary parts. To determine the equivalent Young's moduli of various structural components, an FE model of the coupled building was developed and updated with the identified modal parameters. The Bayesian approach was followed to explicitly handle the uncertainties induced by modeling error and measurement noise. To ensure the model updating method is applicable even in unidentifiable cases, a Markov chain Monte Carlo simulation was employed in the proposed method to generate samples for approximating the posterior probability density functions of uncertain model parameters. The close match between the modal parameters calculated from the updated FE model and those identified from the measured time-domain data verified the validity of the proposed FE model. This study provides valuable experience and information for the development of structural model updating and structural health monitoring of building systems. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.1177/1045389X17704911,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041315232&doi=10.1177%2f1045389X17704911&partnerID=40&md5=1c39769f4cd3b2feafa07f65289eda8e,"In this article, a Bayesian inference approach is applied to conduct uncertainty quantification on notch damage in a beam structure using guided Lamb wave responses. The proposed methodology not only determines the notch damage characteristics but also quantifies associated uncertainties of these inferred values. The correlation between crack location and extent is investigated as well, because such information is essential for decision-making in the structural health monitoring applications. First, a spectral finite element model is used to characterize Lamb wave propagation responses in a beam under lead-zirconate-titanate actuation and sensing. Very few elements are required to accurately capture the wave propagation. The lead-zirconate-titanate sensor can pick up the reflected wave responses from both boundaries and damages. Total 18 simulation cases were generated by varying notch damage extent, damage location, and noise level. Second, the Markov Chain Monte Carlo techniques are employed to estimate the notch damage location and extent from guided Lamb wave responses, in which the random walk metropolis algorithm is used. Finally, both crack size/location and associated uncertainties are characterized. In summary, the proposed probabilistic damage detection is successfully demonstrated in beam structures using guided wave responses, which can be extended to other structural health monitoring applications. © 2017, © The Author(s) 2017."
,10.6038/cjg2018K0759,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052305487&doi=10.6038%2fcjg2018K0759&partnerID=40&md5=fe26abaa05f97e65c24a53411b4cbe37,"A single set of vertically aligned fractures embedded in a purely isotropic background medium may be considered to be a long-wavelength effective transversely isotropic medium with a horizontal symmetry axis (HTI). The wide-azimuth seismic data can be used to invert the elastic and anisotropic parameters on the subsurface by observing variation in subsurface seismic response along different azimuths, which also contains the abundant information of reservoir property parameters such as porosity. In this paper, a method of probabilistic seismic joint inversion for reservoir fracture and petrophysical parameters driven by rock-physics models is proposed. Firstly, the elastic and anisotropic parameters of fractured rocks are derived based on the amplitude variation with angles of incidence and azimuth (AVAZ). Then a statistical rock-physics model is built to characterize the interrelationship between the reservoir petrophysical parameters, such as porosity and fracture density, and the fractured parameters, and then we create a large number of samples of stochastic simulation using the Markov chain Monte Carlo (MCMC) method. The expectation maximization (EM) algorithm is used to estimate the a posteriori probabilistic density function (PDF), and finally the inversion results of the anisotropic reservoir parameters are found to be the place that the maximum a posteriori PDF is. Tests on well log data and seismic data validate that the method proposed in this paper can be used for stably and reasonably deriving the elastic and anisotropic parameters of fractured rocks, and provides a reliable method of probabilistic seismic inversion for reservoir fracture and petrophysical parameters, such as porosity and fracture density. © 2018, Science Press. All right reserved."
,10.1016/j.econlet.2017.11.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040776433&doi=10.1016%2fj.econlet.2017.11.031&partnerID=40&md5=6e735018be5ebf5add65441f70737ba8,"This paper extends the literature on the calculation and interpretation of impacts for spatial autoregressive models. Using a Bayesian framework, we show how the individual direct and indirect impacts associated with an exogenous variable introduced in a nonlinear way in such models can be computed, theoretically and empirically. Rather than averaging the individual impacts, we suggest to graphically analyze them along with their confidence intervals calculated from Markov chain Monte Carlo (MCMC). We also explicitly derive the form of the gap between individual impacts in the spatial autoregressive model and the corresponding model without a spatial lag and show, in our application on the Boston dataset, that it is higher for spatially highly connected observations. © 2017 Elsevier B.V."
,10.1016/j.jmp.2017.10.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035036338&doi=10.1016%2fj.jmp.2017.10.005&partnerID=40&md5=28c3a773711fe13948e734d7446f974c,"In a fully Bayesian framework, a novel slice–Gibbs algorithm is developed to estimate a multilevel item response theory (IRT) model. The advantage of this algorithm is that it can recover parameters well based on various types of prior distributions of the item parameters, including informative and non-informative priors. In contrast to the traditional Metropolis–Hastings (M–H) within Gibbs algorithm, the slice–Gibbs algorithm is faster and more efficient, due to its drawing the sample with acceptance probability as one, rather than tuning the proposal distributions to achieve the reasonable acceptance probabilities, especially for the logistic model without conjugate distribution. In addition, based on the Markov chain Monte Carlo (MCMC) output, two model assessment methods are investigated concerning the goodness of fit between models. The information criterion method on the basis of marginal likelihood is proposed to assess the different structural multilevel models, and the cross-validation method is used to evaluate the overall multilevel IRT models. The feasibility and effectiveness of the slice–Gibbs algorithm are investigated in simulation studies. An application using a real data involving students’ mathematics test achievements is reported. © 2017 Elsevier Inc."
1,10.1016/j.jqsrt.2017.10.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033606744&doi=10.1016%2fj.jqsrt.2017.10.020&partnerID=40&md5=24b6196e67f3d7783188902e1a197f3c,"Particle transport in Markov mixtures can be addressed by the so-called Chord Length Sampling (CLS) methods, a family of Monte Carlo algorithms taking into account the effects of stochastic media on particle propagation by generating on-the-fly the material interfaces crossed by the random walkers during their trajectories. Such methods enable a significant reduction of computational resources as opposed to reference solutions obtained by solving the Boltzmann equation for a large number of realizations of random media. CLS solutions, which neglect correlations induced by the spatial disorder, are faster albeit approximate, and might thus show discrepancies with respect to reference solutions. In this work we propose a new family of algorithms (called ’Poisson Box Sampling’ PBS) aimed at improving the accuracy of the CLS approach for transport in d-dimensional binary Markov mixtures. In order to probe the features of PBS methods, we will focus on three-dimensional Markov media and revisit the benchmark problem originally proposed by Adams, Larsen and Pomraning [1] and extended by Brantley [2]: for these configurations we will compare reference solutions, standard CLS solutions and the new PBS solutions for scalar particle flux, transmission and reflection coefficients. PBS will be shown to perform better than CLS at the expense of a reasonable increase in computational time. © 2017 Elsevier Ltd"
,10.1111/rssc.12238,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032905501&doi=10.1111%2frssc.12238&partnerID=40&md5=5c27194915581ce25f7ca3eb4da38429,"Central banks have long used dynamic stochastic general equilibrium models, which are typically estimated by using Bayesian techniques, to inform key policy decisions. This paper offers an empirical strategy that quantifies the information content of the data relative to that of the prior distribution. Using an off-the-shelf dynamic stochastic general equilibrium model applied to quarterly euro area data from 1970, quarter 3, to 2009, quarter 4, we show how Monte Carlo simulations can reveal parameters for which the model's structure obscures identification. By integrating out components of the likelihood function and conducting a Bayesian sensitivity analysis, we uncover parameters that are weakly informed by the data. The weak identification of some key structural parameters in our comparatively simple model should raise a red flag to researchers trying to draw valid inferences from, and to base policy on, complex large-scale models featuring many parameters. © 2017 Royal Statistical Society"
,10.1016/j.sste.2017.11.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039866817&doi=10.1016%2fj.sste.2017.11.002&partnerID=40&md5=c5efe604f284bf032e79d39a109f7a9c,"The purpose of this study is to identify regions with diabetes health-service shortage. American Diabetes Association (ADA)-accredited diabetes self-management education (DSME) is recommended for all those with diabetes. In this study, we focus on demographic patterns and geographic regionalization of the disease by including accessibility and availability of diabetes education resources as a critical component in understanding and confronting differences in diabetes prevalence, as well as addressing regional or sub-regional differences in awareness, treatment and control. We conducted an ecological county-level study utilizing publicly available secondary data on 3,109 counties in the continental U.S. We used a Bayesian spatial cluster model that enabled spatial heterogeneities across the continental U.S. to be addressed. We used the American Diabetes Association (ADA) website to identify 2012 DSME locations and national 2010 county-level diabetes rates estimated by the Centers for Disease Control and Prevention and identified regions with low DSME program availability relative to their diabetes rates and population density. Only 39.8% of the U.S. counties had at least one ADA-accredited DSME program location. Based on our 95% credible intervals, age-adjusted diabetes rates and DSME program locations were associated in only seven out of thirty five identified clusters. Out of these seven, only two clusters had a positive association. We identified clusters that were above the 75th percentile of average diabetes rates, but below the 25th percentile of average DSME location counts and found that these clusters were all located in the Southeast portion of the country. Overall, there was a lack of relationship between diabetes rates and DSME center locations in the U.S., suggesting resources could be more efficiently placed according to need. Clusters that were high in diabetes rates and low in DSME placements, all in the southeast, should particularly be considered for additional DSME programming. © 2017 Elsevier Ltd"
,10.1016/j.jbankfin.2017.10.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033558780&doi=10.1016%2fj.jbankfin.2017.10.017&partnerID=40&md5=0053cbf1aeeb76328e2dbefe7698bc89,"The existence of a self-financing trading strategy that replicates the money market account at a fixed future date at a lower cost than the current value of this account constitutes a money market bubble (MMB). Understanding whether a market exhibits an MMB is crucial, in particular, for derivative pricing. An MMB precludes the existence of a risk-neutral probability measure. The benchmark approach allows to study MMBs and is formulated under the real world probability measure. It does not require the existence of a risk neutral probability measure. Using a range of well-known stochastic volatility models, we study the existence of an MMB in the US economy, and find that the US market exhibits an MMB for all models considered that allow it. This suggests that for derivative pricing and hedging care should be taken when making assumptions pertaining to the existence of a risk-neutral probability measure. Less expensive portfolios are likely to exist for a wide range of long-term derivatives, as typical for pensions. © 2017 Elsevier B.V."
,10.1109/TVT.2017.2754552,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030645271&doi=10.1109%2fTVT.2017.2754552&partnerID=40&md5=71133e1e07fe42e3ca28825f0b096cf7,"With the requirement for growth of massive connections in the fifth-generation (5G) system, there is an increasing challenge for traditional multiple access techniques to meet the needs of the exponentially increased number of terminals for the resource constrained networks. The sparse code multiple access (SCMA) technology has been proposed for the 5G communication systems to supply stronger connectivity with limited resources. However, a low-complexity decoding algorithm is required by the SCMA decoder for the high computation complexity of decoding nonorthogonal signals. In this paper, we propose a high-performance and low-cost decoding algorithm based on a Bayesian program learning method, Monte Carlo Markov Chain (MCMC). We also propose a new MCMC sampling method to generate samples from a joint update parallel (JUP) MCMC sampler. The simulation results show that the JUP-based MCMC SCMA decoder can save 60$\%$ computation complexity compared to the existing decoding method with a codebook size 16, which only has 0.5-dB performance loss compared to the maximum-likelihood-like decoding algorithm. © 1967-2012 IEEE."
1,10.1177/0962280216628903,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041951244&doi=10.1177%2f0962280216628903&partnerID=40&md5=90b0697a75b379c0db56908940effe27,"The problem of multiple hypothesis testing can be represented as a Markov process where a new alternative hypothesis is accepted in accordance with its relative evidence to the currently accepted one. This virtual and not formally observed process provides the most probable set of non null hypotheses given the data; it plays the same role as Markov Chain Monte Carlo in approximating a posterior distribution. To apply this representation and obtain the posterior probabilities over all alternative hypotheses, it is enough to have, for each test, barely defined Bayes Factors, e.g. Bayes Factors obtained up to an unknown constant. Such Bayes Factors may either arise from using default and improper priors or from calibrating p-values with respect to their corresponding Bayes Factor lower bound. Both sources of evidence are used to form a Markov transition kernel on the space of hypotheses. The approach leads to easy interpretable results and involves very simple formulas suitable to analyze large datasets as those arising from gene expression data (microarray or RNA-seq experiments). © 2016, © The Author(s) 2016."
2,10.1371/journal.pgen.1007139,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043333278&doi=10.1371%2fjournal.pgen.1007139&partnerID=40&md5=db272dd0ebc66bf016e58341255469c2,"Simultaneous analysis of genetic associations with multiple phenotypes may reveal shared genetic susceptibility across traits (pleiotropy). For a locus exhibiting overall pleiotropy, it is important to identify which specific traits underlie this association. We propose a Bayesian meta-analysis approach (termed CPBayes) that uses summary-level data across multiple phenotypes to simultaneously measure the evidence of aggregate-level pleiotropic association and estimate an optimal subset of traits associated with the risk locus. This method uses a unified Bayesian statistical framework based on a spike and slab prior. CPBayes performs a fully Bayesian analysis by employing the Markov Chain Monte Carlo (MCMC) technique Gibbs sampling. It takes into account heterogeneity in the size and direction of the genetic effects across traits. It can be applied to both cohort data and separate studies of multiple traits having overlapping or non-overlapping subjects. Simulations show that CPBayes can produce higher accuracy in the selection of associated traits underlying a pleiotropic signal than the subset-based meta-analysis ASSET. We used CPBayes to undertake a genome-wide pleiotropic association study of 22 traits in the large Kaiser GERA cohort and detected six independent pleiotropic loci associated with at least two phenotypes. This includes a locus at chromosomal region 1q24.2 which exhibits an association simultaneously with the risk of five different diseases: Dermatophytosis, Hemorrhoids, Iron Deficiency, Osteoporosis and Peripheral Vascular Disease. We provide an R-package ‘CPBayes’ implementing the proposed method. © 2018 Majumdar et al."
4,10.3758/s13428-017-0869-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016936952&doi=10.3758%2fs13428-017-0869-7&partnerID=40&md5=b639522ae25ccd5abd0df49e32dc5f39,"Multinomial processing tree (MPT) models are a class of measurement models that account for categorical data by assuming a finite number of underlying cognitive processes. Traditionally, data are aggregated across participants and analyzed under the assumption of independently and identically distributed observations. Hierarchical Bayesian extensions of MPT models explicitly account for participant heterogeneity by assuming that the individual parameters follow a continuous hierarchical distribution. We provide an accessible introduction to hierarchical MPT modeling and present the user-friendly and comprehensive R package TreeBUGS, which implements the two most important hierarchical MPT approaches for participant heterogeneity—the beta-MPT approach (Smith & Batchelder, Journal of Mathematical Psychology 54:167-183, 2010) and the latent-trait MPT approach (Klauer, Psychometrika 75:70-98, 2010). TreeBUGS reads standard MPT model files and obtains Markov-chain Monte Carlo samples that approximate the posterior distribution. The functionality and output are tailored to the specific needs of MPT modelers and provide tests for the homogeneity of items and participants, individual and group parameter estimates, fit statistics, and within- and between-subjects comparisons, as well as goodness-of-fit and summary plots. We also propose and implement novel statistical extensions to include continuous and discrete predictors (as either fixed or random effects) in the latent-trait MPT model. © 2017, The Author(s)."
,10.1371/journal.pone.0191768,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041366157&doi=10.1371%2fjournal.pone.0191768&partnerID=40&md5=36b56f1df1092455f502ab8ef8256d3b,"Many coupled human-natural systems have the potential to exhibit a highly nonlinear threshold response to external forcings resulting in fast transitions to undesirable states (such as eutrophication in a lake). Often, there are considerable uncertainties that make identifying the threshold challenging. Thus, rapid learning is critical for guiding management actions to avoid abrupt transitions. Here, we adopt the shallow lake problem as a test case to compare the performance of four common data assimilation schemes to predict an approaching transition. In order to demonstrate the complex interactions between management strategies and the ability of the data assimilation schemes to predict eutrophication, we also analyze our results across two different management strategies governing phosphorus emissions into the shallow lake. The compared data assimilation schemes are: ensemble Kalman filtering (EnKF), particle filtering (PF), pre-calibration (PC), and Markov Chain Monte Carlo (MCMC) estimation. While differing in their core assumptions, each data assimilation scheme is based on Bayes’ theorem and updates prior beliefs about a system based on new information. For large computational investments, EnKF, PF and MCMC show similar skill in capturing the observed phosphorus in the lake (measured as expected root mean squared prediction error). EnKF, followed by PF, displays the highest learning rates at low computational cost, thus providing a more reliable signal of an impending transition. MCMC approaches the true probability of eutrophication only after a strong signal of an impending transition emerges from the observations. Overall, we find that learning rates are greatest near regions of abrupt transitions, posing a challenge to early learning and preemptive management of systems with such abrupt transitions. © 2018 Singh et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
2,10.1002/qre.2241,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033550199&doi=10.1002%2fqre.2241&partnerID=40&md5=5a47e29acd729f4bc5cbe6ced548f5cb,"In recent years, the need for a more accurate dependability modelling (encompassing reliability, availability, maintenance, and safety) has favoured the emergence of novel dynamic dependability techniques able to account for temporal and stochastic dependencies of a system. One of the most successful and widely used methods is Dynamic Fault Tree that, with the introduction of the dynamic gates, enables the analysis of dynamic failure logic systems such as fault-tolerant or reconfigurable systems. Among the dynamic gates, Priority-AND (PAND) is one of the most frequently used gates for the specification and analysis of event sequences. Despite the numerous modelling contributions addressing the resolution of the PAND gate, its failure logic and the consequences for the coherence behaviour of the system need to be examined to understand its effects for engineering decision-making scenarios including design optimization and sensitivity analysis. Accordingly, the aim of this short communication is to analyse the coherence region of the PAND gate so as to determine the coherence bounds and improve the efficacy of the dynamic dependability modelling process. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1109/TIT.2017.2742509,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028507815&doi=10.1109%2fTIT.2017.2742509&partnerID=40&md5=ea274843e051ca13c2dee75899e95cc6,"Sampling from the lattice Gaussian distribution has emerged as an important problem in coding, decoding, and cryptography. In this paper, the classic Metropolis-Hastings (MH) algorithm in Markov chain Monte Carlo methods is adopted for lattice Gaussian sampling. Two MH-based algorithms are proposed, which overcome the limitation of Klein's algorithm. The first one, referred to as the independent Metropolis-Hastings-Klein (MHK) algorithm, establishes a Markov chain via an independent proposal distribution. We show that the Markov chain arising from this independent MHK algorithm is uniformly ergodic, namely, it converges to the stationary distribution exponentially fast regardless of the initial state. Moreover, the rate of convergence is analyzed in terms of the theta series, leading to predictable mixing time. A symmetric Metropolis-Klein algorithm is also proposed, which is proven to be geometrically ergodic. © 2017 IEEE."
,10.1016/j.sste.2017.11.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036509613&doi=10.1016%2fj.sste.2017.11.001&partnerID=40&md5=c718993f371883e0b5dc1ff8253ca2ce,"Approximate Bayesia n Computation (ABC) provides an attractive approach to estimation in complex Bayesian inferential problems for which evaluation of the kernel of the posterior distribution is impossible or computationally expensive. These highly parallelizable techniques have been successfully applied to many fields, particularly in cases where more traditional approaches such as Markov chain Monte Carlo (MCMC) are impractical. In this work, we demonstrate the application of approximate Bayesian inference to spatially heterogeneous Susceptible-Exposed-Infectious-Removed (SEIR) stochastic epidemic models. These models have a tractable posterior distribution, however MCMC techniques nevertheless become computationally infeasible for moderately sized problems. We discuss the practical implementation of these techniques via the open source ABSEIR package for R. The performance of ABC relative to traditional MCMC methods in a small problem is explored under simulation, as well as in the spatially heterogeneous context of the 2014 epidemic of Chikungunya in the Americas. © 2017 Elsevier Ltd"
,10.1016/j.powtec.2017.11.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035799642&doi=10.1016%2fj.powtec.2017.11.031&partnerID=40&md5=044102a0dd3fce320ee3643282c7591e,"This paper investigates the layer formation in spray coating processes. Based on a Monte-Carlo simulation, a stochastic model of the coating layer thickness distribution was derived. It couples the stochastic process of droplet deposition on the particle surface with the droplet shape constructed from a spherical cap model and the droplets wetting properties (contact angle). The model was successfully shown to be able to replace the simulation. A parameter study revealed recommendations for designing a coating process, which were in agreement with the works from other authors. The model was then used to investigate the influence of overspray on the coating quality in comparison with experiments. It was found that the presence of overspray not only reduces the process efficiency but also increases the coefficient of variation of the resulting layer thickness distribution. This was caused by an increase in droplet size due to a predominant drying of small drops. It was also found, that a higher solid content of the spray solution increases the coefficient of variation, not only due to a decreased number of droplets, but also due to a greater variability in the layer thickness each droplet introduces. © 2017 Elsevier B.V."
3,10.1097/ALN.0000000000001981,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046574081&doi=10.1097%2fALN.0000000000001981&partnerID=40&md5=fc72be83d0dbcf7dc3af7f9646ccf2ff,"Background: Cost-effectiveness analyses on cell salvage for cesarean delivery to inform national and societal guidelines on obstetric blood management are lacking. This study examined the cost-effectiveness of cell salvage strategies in obstetric hemorrhage from a societal perspective. Methods: Markov decision analysis modeling compared the cost-effectiveness of three strategies: use of cell salvage for every cesarean delivery, cell salvage use for high-risk cases, and no cell salvage. A societal perspective and lifetime horizon was assumed for the base case of a 26-yr-old primiparous woman presenting for cesarean delivery. Each strategy integrated probabilities of hemorrhage, hysterectomy, transfusion reactions, emergency procedures, and cell salvage utilization; utilities for quality of life; and costs at the societal level. One-way and Monte Carlo probabilistic sensitivity analyses were performed. A threshold of $100,000 per quality-Adjusted life-year gained was used as a cost-effectiveness criterion. Results: Cell salvage use for cases at high risk for hemorrhage was cost-effective (incremental cost-effectiveness ratio, $34,881 per quality-Adjusted life-year gained). Routine cell salvage use for all cesarean deliveries was not cost-effective, costing $415,488 per quality-Adjusted life-year gained. Results were not sensitive to individual variation of other model parameters. The probabilistic sensitivity analysis showed that at the $100,000 per quality-Adjusted life-year gained threshold, there is more than 85% likelihood that cell salvage use for cases at high risk for hemorrhage is favorable. Conclusions: The use of cell salvage for cases at high risk for obstetric hemorrhage is economically reasonable; routine cell salvage use for all cesarean deliveries is not. These findings can inform the development of public policies such as guidelines on management of obstetric hemorrhage. Visual Abstract: An online visual overview is available for this article at http://links.lww.com/ALN/B631. © 2018 Lippincott Williams and Wilkins. All rights reserved."
,10.3934/mbe.2018007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041583214&doi=10.3934%2fmbe.2018007&partnerID=40&md5=535d4db60dabf78a53a78b522d7a98b8,"When mathematical models of infectious diseases are used to inform health policy, an important first step is often to calibrate a model to disease surveillance data for a specific setting (or multiple settings). It is increasingly common to also perform sensitivity analyses to demonstrate the robustness, or lack thereof, of the modeling results. Doing so requires the modeler to find multiple parameter sets for which the model produces behavior that is consistent with the surveillance data. While frequently overlooked, the calibration process is nontrivial at best and can be inefficient, poorly communicated and a major hurdle to the overall reproducibility of modeling results. In this work, we describe a general approach to calibrating infectious disease models to surveillance data. The technique is able to match surveillance data to high accuracy in a very efficient manner as it is based on the Newton-Raphson method for solving nonlinear systems. To demonstrate its robustness, we use the calibration technique on multiple models for the interacting dynamics of HIV and HSV-2."
1,10.1016/j.prevetmed.2017.11.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034856291&doi=10.1016%2fj.prevetmed.2017.11.010&partnerID=40&md5=b2c36fc22c55b84f662c55bc0a3900d8,"Using imperfect tests may lead to biased estimates of disease frequency and of associations between risk factors and disease. For instance in longitudinal udder health studies, both quarters at risk and incident intramammary infections (IMI) can be wrongly identified, resulting in selection and misclassification bias, respectively. Diagnostic accuracy can possibly be improved by using duplicate or triplicate samples for identifying quarters at risk and, subsequently, incident IMI. The objectives of this study were to evaluate the relative impact of selection and misclassification biases resulting from IMI misclassification on measures of disease frequency (incidence) and of association with hypothetical exposures. The effect of improving the sampling strategy by collecting duplicate or triplicate samples at first or second sampling was also assessed. Data sets from a hypothetical cohort study were simulated and analyzed based on a separate scenario for two common mastitis pathogens representing two distinct prevailing patterns. Staphylococcus aureus, a relatively uncommon pathogen with a low incidence, is identified with excellent sensitivity and almost perfect specificity. Coagulase negative staphylococci (CNS) are more prevalent, with a high incidence, and with milk bacteriological culture having fair Se but excellent Sp. The generated data sets for each scenario were emulating a longitudinal cohort study with two milk samples collected one month apart from each quarter of a random sample of 30 cows/herd, from 100 herds, with a herd-level exposure having a known strength of association. Incidence of IMI and measure of association with exposure (odds ratio; OR) were estimated using Markov Chain Monte Carlo (MCMC) for each data set and using different sampling strategies (single, duplicate, triplicate samples with series or parallel interpretation) for identifying quarters at risk and incident IMI. For S. aureus biases were small with an observed incidence of 0.29 versus a true incidence of 0.25 IMI/100 quarter-month. In the CNS scenario, diagnostic errors in the two samples led to important selection (40 IMI/100 quarter-month) and misclassification (23 IMI/100 quarter-month) biases for estimation of IMI incidence, respectively. These biases were in opposite direction and therefore the incidence measure obtained using single sampling on both the first and second test (29 IMI/100 quarter-month) was exactly the true value. In the S. aureus scenario the OR for association with exposure showed little bias (observed OR of 3.1 versus true OR of 3.2). The CNS scenario revealed the presence of a large misclassification bias moving the association towards the null value (OR of 1.7 versus true OR of 2.6). Little improvement could be brought using different sampling strategies aiming at improving Se and/or Sp on first and/or second sampling or using a two out of three interpretation for IMI definition. Increasing number of samples or tests can prevent bias in some situations but efforts can be spared by holding to a single sampling approach in others. When designing longitudinal studies, evaluating potential biases and best sampling strategy is as critical as the choice of test. © 2017 Elsevier B.V."
,10.1007/s11265-016-1147-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042639035&doi=10.1007%2fs11265-016-1147-0&partnerID=40&md5=c397b6eaa3e110033c908914b8a8be49,"We propose computationally highly efficient Neyman-Pearson (NP) tests for anomaly detection over birth-death type discrete time Markov chains. Instead of relying on extensive Monte Carlo simulations (as in the case of the baseline NP), we directly approximate the log-likelihood density to match the desired false alarm rate; and therefore obtain our efficient implementations. The proposed algorithms are appropriate for processing large scale data in online applications with real time false alarm rate controllability. Since we do not require parameter tuning, our algorithms are also adaptive to non-stationarity in the data source. In our experiments, the proposed tests demonstrate superior detection power compared to the baseline NP while nearly achieving the desired rates with negligible computational resources. © 2016, Springer Science+Business Media New York."
,10.1016/j.jhydrol.2017.12.071,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044861434&doi=10.1016%2fj.jhydrol.2017.12.071&partnerID=40&md5=e64d32d7320bbb2588e040bad4600e45,"Bayesian inference using Markov Chain Monte Carlo (MCMC) provides an explicit framework for stochastic calibration of hydrogeologic models accounting for uncertainties; however, the MCMC sampling entails a large number of model calls, and could easily become computationally unwieldy if the high-fidelity hydrogeologic model simulation is time consuming. This study proposes a surrogate-based Bayesian framework to address this notorious issue, and illustrates the methodology by inverse modeling a regional MODFLOW model. The high-fidelity groundwater model is approximated by a fast statistical model using Bagging Multivariate Adaptive Regression Spline (BMARS) algorithm, and hence the MCMC sampling can be efficiently performed. In this study, the MODFLOW model is developed to simulate the groundwater flow in an arid region of Oman consisting of mountain-coast aquifers, and used to run representative simulations to generate training dataset for BMARS model construction. A BMARS-based Sobol’ method is also employed to efficiently calculate input parameter sensitivities, which are used to evaluate and rank their importance for the groundwater flow model system. According to sensitivity analysis, insensitive parameters are screened out of Bayesian inversion of the MODFLOW model, further saving computing efforts. The posterior probability distribution of input parameters is efficiently inferred from the prescribed prior distribution using observed head data, demonstrating that the presented BMARS-based Bayesian framework is an efficient tool to reduce parameter uncertainties of a groundwater system. © 2018 Elsevier B.V."
,10.1007/s11771-018-3747-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042086817&doi=10.1007%2fs11771-018-3747-2&partnerID=40&md5=ede81f4e3f458e95326bb6818f086296,"Reliability and remaining useful life (RUL) estimation for a satellite rechargeable lithium battery (RLB) are significant for prognostic and health management (PHM). A novel Bayesian framework is proposed to do reliability analysis by synthesizing multisource data, including bivariate degradation data and lifetime data. Bivariate degradation means that there are two degraded performance characteristics leading to the failure of the system. First, linear Wiener process and Frank Copula function are used to model the dependent degradation processes of the RLB’s temperature and discharge voltage. Next, the Bayesian method, in combination with Markov Chain Monte Carlo (MCMC) simulations, is provided to integrate limited bivariate degradation data with other congeneric RLBs’ lifetime data. Then reliability evaluation and RUL prediction are carried out for PHM. A simulation study demonstrates that due to the data fusion, parameter estimations and predicted RUL obtained from our model are more precise than models only using degradation data or ignoring the dependency of different degradation processes. Finally, a practical case study of a satellite RLB verifies the usability of the model. © 2018, Central South University Press and Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1016/j.trb.2017.12.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040592261&doi=10.1016%2fj.trb.2017.12.011&partnerID=40&md5=4f8976c6bf687be21f9d8597418a5e85,"Many-to-many matching relationship in a two-sided market has been widely observed in today's transportation activities. Observation of such matching relationship raises some interesting questions: what factors drive the matching of two agents? Is the formation of matching relationship related with joint behavior which may lead to different understandings of planning and operation? To answer these questions, econometric models may be the best methodology. However, to the authors’ best knowledge, there lacks a well-established econometric model to explain the observed data that contains matching relationship in a two-sided transportation market. Therefore, this paper proposes an innovative ordinal joint response model to bridge the gap. The proposed model consists of two regression equations: the first uses a latent dependent variable to disentangle the many-to-many matching relationship; the second specifies an ordered probit equation to investigate the ordinal outcome of joint behavior. Error terms of the two equations are assumed correlated to capture the correlation of the matching process and joint behavior. An example of airline-airport matching is used to demonstrate the proposed model. © 2017 Elsevier Ltd"
3,10.1371/journal.pcbi.1005965,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042713565&doi=10.1371%2fjournal.pcbi.1005965&partnerID=40&md5=662889f8ee94ccfb8953efa7370d5ed5,"A key constraint in genomic testing in oncology is that matched normal specimens are not commonly obtained in clinical practice. Thus, while well-characterized genomic alterations do not require normal tissue for interpretation, a significant number of alterations will be unknown in whether they are germline or somatic, in the absence of a matched normal control. We introduce SGZ (somatic-germline-zygosity), a computational method for predicting somatic vs. germline origin and homozygous vs. heterozygous or sub-clonal state of variants identified from deep massively parallel sequencing (MPS) of cancer specimens. The method does not require a patient matched normal control, enabling broad application in clinical research. SGZ predicts the somatic vs. germline status of each alteration identified by modeling the alteration’s allele frequency (AF), taking into account the tumor content, tumor ploidy, and the local copy number. Accuracy of the prediction depends on the depth of sequencing and copy number model fit, which are achieved in our clinical assay by sequencing to high depth (>500x) using MPS, covering 394 cancer-related genes and over 3,500 genome-wide single nucleotide polymorphisms (SNPs). Calls are made using a statistic based on read depth and local variability of SNP AF. To validate the method, we first evaluated performance on samples from 30 lung and colon cancer patients, where we sequenced tumors and matched normal tissue. We examined predictions for 17 somatic hotspot mutations and 20 common germline SNPs in 20,182 clinical cancer specimens. To assess the impact of stromal admixture, we examined three cell lines, which were titrated with their matched normal to six levels (10–75%). Overall, predictions were made in 85% of cases, with 95–99% of variants predicted correctly, a significantly superior performance compared to a basic approach based on AF alone. We then applied the SGZ method to the COSMIC database of known somatic variants in cancer and found >50 that are in fact more likely to be germline. © 2018 Sun et al."
2,10.1371/journal.pgen.1007198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043344129&doi=10.1371%2fjournal.pgen.1007198&partnerID=40&md5=c401732fa47dfe868dca40cc6035a239,"Pericentrin is a conserved centrosomal protein whose dysfunction has been linked to several human diseases. It has been implicated in many aspects of centrosome and cilia function, but its precise role is unclear. Here, we examine Drosophila Pericentrin-like-protein (PLP) function in vivo in tissues that form both centrosomes and cilia. Plp mutant centrioles exhibit four major defects: (1) They are short and have subtle structural abnormalities; (2) They disengage prematurely, and so overduplicate; (3) They organise fewer cytoplasmic MTs during interphase; (4) When forming cilia, they fail to establish and/or maintain a proper connection to the plasma membrane—although, surprisingly, they can still form an axoneme-like structure that can recruit transition zone (TZ) proteins. We show that PLP helps assemble “pericentriolar clouds” of electron-dense material that emanate from the central cartwheel spokes and spread outward to surround the mother centriole. We propose that the partial loss of these structures may largely explain the complex centriole, centrosome and cilium defects we observe in Plp mutant cells. © 2018 Roque et al."
,10.1007/s10886-017-0916-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040071025&doi=10.1007%2fs10886-017-0916-y&partnerID=40&md5=148348c44afbf5910b0b7fc82b80c947,"Gas-chromatography-electroantennographic detection (GC-EAD) is a technique used in the identification of volatile organic compounds (VOCs), such as pheromones and plant host odors, which are physiologically relevant to insects. Although pheromones often elicit large EAD responses, other behaviorally relevant odors may elicit responses that are difficult to discern from noise. Lock-in amplification has long been used to reduce noise in a wide range of applications. Its utility when incorporated with GC-EAD was demonstrated previosuly by chopping (or pulsing) effluent-laden air that flowed over an insect antenna. This method had the disadvantage that it stimulated noise-inducing mechanoreceptors and, in some cases, disturbed the electrochemical interfaces in a preparation, limiting its performance. Here, the chopping function necessary for lock-in amplification was implemented directly on the GC effluent using a simple Deans switch. The technique was applied to excised antennae from female Heliothis virescens responding to phenethyl alcohol, a common VOC emitted by plants. Phenethyl alcohol was always visible and quantifiable on the flame ionization detector (FID) chromatogram, allowing the timing and amount of stimulus delivered to the antennal preparation to be measured. In our new chopper EAG configuration, the antennal preparation was shielded from air currents in the room, further reducing noise. A dose-response model in combination with a Markov-chain monte-carlo (MCMC) method for Bayesian inference was used to estimate and compare performance in terms of error rates involved in the detection of insect responses to GC peaks visible on an FID detector. Our experiments showed that the predicted single-trial phenethyl alcohol detection limit on female H. virescens antennae (at a 5.0% expected error rate) was 140,330 pg using traditional EAG recording methods, compared to 2.6–6.3 pg (5th to the 95th percentile) using Deans switch-enabled lock-in amplification, corresponding to a 10.4–12.7 dB increase in signal-to-noise ratio. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
1,10.1051/0004-6361/201731345,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042068411&doi=10.1051%2f0004-6361%2f201731345&partnerID=40&md5=1c821c5a2842b2e91170607526b1536b,"Several studies have shown that stellar activity features, such as occulted and non-occulted starspots, can affect the measurement of transit parameters biasing studies of transit timing variations and transmission spectra. We present PyTranSpot, which we designed to model multiband transit light curves showing starspot anomalies, inferring both transit and spot parameters. The code follows a pixellation approach to model the star with its corresponding limb darkening, spots, and transiting planet on a two dimensional Cartesian coordinate grid. We combine PyTranSpot with a Markov chain Monte Carlo framework to study and derive exoplanet transmission spectra, which provides statistically robust values for the physical properties and uncertainties of a transiting star-planet system. We validate PyTranSpot's performance by analyzing eleven synthetic light curves of four different star-planet systems and 20 transit light curves of the well-studied WASP-41b system. We also investigate the impact of starspots on transit parameters and derive wavelength dependent transit depth values for WASP-41b covering a range of 6200-9200 Å, indicating a flat transmission spectrum. © ESO, 2018."
4,10.1088/1361-6420/aaa34d,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040994880&doi=10.1088%2f1361-6420%2faaa34d&partnerID=40&md5=2813a02cca8f1aa1ba313665bc92a83f,"In computational inverse problems, it is common that a detailed and accurate forward model is approximated by a computationally less challenging substitute. The model reduction may be necessary to meet constraints in computing time when optimization algorithms are used to find a single estimate, or to speed up Markov chain Monte Carlo (MCMC) calculations in the Bayesian framework. The use of an approximate model introduces a discrepancy, or modeling error, that may have a detrimental effect on the solution of the ill-posed inverse problem, or it may severely distort the estimate of the posterior distribution. In the Bayesian paradigm, the modeling error can be considered as a random variable, and by using an estimate of the probability distribution of the unknown, one may estimate the probability distribution of the modeling error and incorporate it into the inversion. We introduce an algorithm which iterates this idea to update the distribution of the model error, leading to a sequence of posterior distributions that are demonstrated empirically to capture the underlying truth with increasing accuracy. Since the algorithm is not based on rejections, it requires only limited full model evaluations. We show analytically that, in the linear Gaussian case, the algorithm converges geometrically fast with respect to the number of iterations when the data is finite dimensional. For more general models, we introduce particle approximations of the iteratively generated sequence of distributions; we also prove that each element of the sequence converges in the large particle limit under a simplifying assumption. We show numerically that, as in the linear case, rapid convergence occurs with respect to the number of iterations. Additionally, we show through computed examples that point estimates obtained from this iterative algorithm are superior to those obtained by neglecting the model error. © 2018 IOP Publishing Ltd."
,10.1111/stan.12115,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038933910&doi=10.1111%2fstan.12115&partnerID=40&md5=4a44cdfda92d0c7ab83c5b99ec08b2d3,"Modeling the correlation structure of returns is essential in many financial applications. Considerable evidence from empirical studies has shown that the correlation among asset returns is not stable over time. A recent development in the multivariate stochastic volatility literature is the application of inverse Wishart processes to characterize the evolution of return correlation matrices. Within the inverse Wishart multivariate stochastic volatility framework, we propose a flexible correlated latent factor model to achieve dimension reduction and capture the stylized fact of ‘correlation breakdown’ simultaneously. The parameter estimation is based on existing Markov chain Monte Carlo methods. We illustrate the proposed model with several empirical studies. In particular, we use high-dimensional stock return data to compare our model with competing models based on multiple performance metrics and tests. The results show that the proposed model not only describes historic stylized facts reasonably but also provides the best overall performance. © 2017 The Authors. Statistica Neerlandica © 2017 VVS."
,10.1177/1471082X17719633,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040771524&doi=10.1177%2f1471082X17719633&partnerID=40&md5=41f2b660a7e79318ea5278ff7d7600f3,"In this work, we propose a Bayesian quantile regression method to response variables with mixed discrete-continuous distribution with a point mass at zero, where these observations are believed to be left censored or true zeros. We combine the information provided by the quantile regression analysis to present a more complete description of the probability of being censored given that the observed value is equal to zero, while also studying the conditional quantiles of the continuous part. We build up a Markov Chain Monte Carlo method from related models in the literature to obtain samples from the posterior distribution. We demonstrate the suitability of the model to analyse this censoring probability with a simulated example and two applications with real data. The first is a well-known dataset from the econometrics literature about women labour in Britain, and the second considers the statistical analysis of expenditures with durable goods, considering information from Brazil. © 2018, SAGE Publications."
,10.1111/obes.12187,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019881635&doi=10.1111%2fobes.12187&partnerID=40&md5=8dab6ae30e473fb2f7141576140f2c36,"In this study, we consider Bayesian methods for the estimation of a sample selection model with spatially correlated disturbance terms. We design a set of Markov chain Monte Carlo algorithms based on the method of data augmentation. The natural parameterization for the covariance structure of our model involves an unidentified parameter that complicates posterior analysis. The unidentified parameter – the variance of the disturbance term in the selection equation – is handled in different ways in these algorithms to achieve identification for other parameters. The Bayesian estimator based on these algorithms can account for the selection bias and the full covariance structure implied by the spatial correlation. We illustrate the implementation of these algorithms through a simulation study and an empirical application. © 2017 The Department of Economics, University of Oxford and John Wiley & Sons Ltd"
,10.1002/2017JB014833,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044425922&doi=10.1002%2f2017JB014833&partnerID=40&md5=e9bb178620bc2a10afc9a564fef9e4c9,"Multichannel seismic data acquired recently over Shatsky Rise in the northwest Pacific have the potential to provide important constraints on structure and rock properties from this oceanic plateau. We apply a reversible jump Markov Chain Monte Carlo sampling technique to invert for acoustic impedance structure from portions of the processed, poststack seismic lines. This approach, which applies a Bayesian inversion formulation, also allows a quantification of uncertainty in inversion results. It also allows an automatic estimation of the number of model parameters, such as the number of layers, required to fit the data, greatly simplifying the inversion. We use it to infer the shallow acoustic impedance structure of the Tamu and Ori volcanoes at the Shatsky Rise oceanic plateau. Since acoustic impedance depends on the type of basalt present, the results allow estimation of lithology and, therefore, insight into the late-stage evolution of both the volcanoes. Specifically, results from the shallow crust (∼1 km) suggest a higher percentage of massive flow basalts at Tamu Massif compared to Ori Massif. The percentage of pillow basalts and geochemically altered basalts is higher at the summit compared to that at the flanks of the volcanoes. ©2018. American Geophysical Union. All Rights Reserved."
1,10.1093/mnras/stx2810,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046163630&doi=10.1093%2fmnras%2fstx2810&partnerID=40&md5=fae4611c4991c396d1609ecb9a246fc8,"We test the possible deviation of the cosmic distance duality relation DA(z)(1 + z)2/DL(z) ≡ 1 using the standard candles/rulers in a fully model-independent manner. Type Ia supernovae are used as the standard candles to derive the luminosity distance DL(z), and ultracompact radio sources are used as the standard rulers to obtain the angular diameter distance DA(z).We write the deviation of distance duality relation as DA(z)(1 + z)2/DL(z) = η(z). Specifically, we use two parametrizations of η(z), i.e. η1(z) = 1 + η0z and η2(z) = 1 + η0z/(1 + z). The parameter η0 is obtained using the Markov chain Monte Carlo methods by comparing DL(z) and DA(z) at the same redshift. The best-fitting results are η0 =-0.06 ± 0.05 and -0.18 ± 0.16 for the first and second parametrizations, respectively. Our results depend on neither the cosmological models nor the matter contents or the curvature of the Universe. © 2018 The Author(s)."
,10.3847/1538-4357/aaa3e5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041894542&doi=10.3847%2f1538-4357%2faaa3e5&partnerID=40&md5=e19cb44e0e59086fe7c864223e719743,"We modified the broadband photometric reverberation mapping (PRM) code, JAVELIN, and tested the availability to get broad-line region time delays that are consistent with the spectroscopic reverberation mapping (SRM) project SDSS-RM. The broadband light curves of SDSS-RM quasars produced by convolution with the system transmission curves were used in the test. We found that under similar sampling conditions (evenly and frequently sampled), the key factor determining whether the broadband PRM code can yield lags consistent with the SRM project is the flux ratio of the broad emission line to the reference continuum, which is in line with the previous findings. We further found a critical line-to-continuum flux ratio, about 6%, above which the mean of the ratios between the lags from PRM and SRM becomes closer to unity, and the scatter is pronouncedly reduced. We also tested our code on a subset of SDSS Stripe 82 quasars, and found that our program tends to give biased lag estimations due to the observation gaps when the R-L relation prior in Markov Chain Monte Carlo is discarded. The performance of the damped random walk (DRW) model and the power-law (PL) structure function model on broadband PRM were compared. We found that given both SDSS-RM-like or Stripe 82-like light curves, the DRW model performs better in carrying out broadband PRM than the PL model. © 2018. The American Astronomical Society. All rights reserved.."
,10.1007/s00606-017-1466-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031941082&doi=10.1007%2fs00606-017-1466-z&partnerID=40&md5=893c162a4db2075830e2d1cde71f02b7,"Capparis (Capparaceae) has been used as a medicinal plant since ancient time. Capparis species were divided into Old World and New World taxa as described by the sectional division of Capparis. However, plastid DNA sequence data of Indian Capparis species were not analyzed in previous phylogenetic studies. Here, we have added Indian Capparis data in previous phylogeny and analyzed the relationship of Indian Capparis with Old World and New World taxa. The plastid phylogeny presented here includes Capparis taxa from its major distribution areas, New World and African capparoids. The presented phylogeny is used for the determination of biogeographic history of Capparis and recently segregated genera. Phylogenetic analyses of the combined plastid data revealed that the Indian Capparis are more closely related to Old World taxa and have connections with African, Australian and Eastern Asian species. Sectional classification of Old World and Indian Capparis considered in this study is reflected from the presented plastid phylogeny. The ancestral area reconstruction using Bayesian Binary Markov Chain Monte Carlo method strongly supports for the Africa as the ancestral region for both Old World and New World Capparis. Molecular marker-based genetic diversity studies on Indian Capparis are scarce. This work also includes the genetic diversity study of Indian Capparis species. Utility and efficacy of ISSR markers to study inter- and intraspecies variation in Capparis is evident from the AMOVA results. © 2017, Springer-Verlag GmbH Austria."
,10.1109/ITW.2017.8278001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046342290&doi=10.1109%2fITW.2017.8278001&partnerID=40&md5=26c370d20a05f72d9c205bf202ab5040,"Sampling from the lattice Gaussian distribution is emerging as an important problem in coding and cryptography. In this paper, the conventional Gibbs sampling algorithm is demonstrated to be geometrically ergodic in tackling with lattice Gaussian sampling, which means its induced Markov chain converges exponentially fast to the stationary distribution. Moreover, as the exponential convergence rate is dominated by the spectral radius of the forward operator of the Markov chain, a comprehensive analysis is given and we show that the convergence performance can be further enhanced by usages of blocked sampling strategy and choices of selection probabilities. © 2017 IEEE."
,10.1007/s11263-018-1064-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045133041&doi=10.1007%2fs11263-018-1064-8&partnerID=40&md5=e3a694af76662a8dcdd13b6574331358,"Faces in natural images are often occluded by a variety of objects. We propose a fully automated, probabilistic and occlusion-aware 3D morphable face model adaptation framework following an analysis-by-synthesis setup. The key idea is to segment the image into regions explained by separate models. Our framework includes a 3D morphable face model, a prototype-based beard model and a simple model for occlusions and background regions. The segmentation and all the model parameters have to be inferred from the single target image. Face model adaptation and segmentation are solved jointly using an expectation–maximization-like procedure. During the E-step, we update the segmentation and in the M-step the face model parameters are updated. For face model adaptation we apply a stochastic sampling strategy based on the Metropolis–Hastings algorithm. For segmentation, we apply loopy belief propagation for inference in a Markov random field. Illumination estimation is critical for occlusion handling. Our combined segmentation and model adaptation needs a proper initialization of the illumination parameters. We propose a RANSAC-based robust illumination estimation technique. By applying this method to a large face image database we obtain a first empirical distribution of real-world illumination conditions. The obtained empirical distribution is made publicly available and can be used as prior in probabilistic frameworks, for regularization or to synthesize data for deep learning methods. © 2018 Springer Science+Business Media, LLC, part of Springer Nature"
,10.1016/j.tecto.2017.12.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044641766&doi=10.1016%2fj.tecto.2017.12.021&partnerID=40&md5=ddbcfcda1469b3e200322ddba3720363,"Marine terraces on growing fault-propagation folds provide valuable insight into the relationship between fold kinematics and uplift rates, providing a means to distinguish among otherwise non-unique kinematic model solutions. Here, we investigate this relationship at two locations in North Canterbury, New Zealand: the Kate anticline and Haumuri Bluff, at the northern end of the Hawkswood anticline. At both locations, we calculate uplift rates of previously dated marine terraces, using DGPS surveys to estimate terrace inner edge elevations. We then use Markov chain Monte Carlo methods to fit fault-propagation fold kinematic models to structural geologic data, and we incorporate marine terrace uplift into the models as an additional constraint. At Haumuri Bluff, we find that marine terraces, when restored to originally horizontal surfaces, can help to eliminate certain trishear models that would fit the geologic data alone. At Kate anticline, we compare uplift rates at different structural positions and find that the spatial pattern of uplift rates is more consistent with trishear than with a parallel-fault propagation fold kink-band model. Finally, we use our model results to compute new estimates for fault slip rates (~ 1–2 m/ka at Kate anticline and 1–4 m/ka at Haumuri Bluff) and ages of the folds (~ 1 Ma), which are consistent with previous estimates for the onset of folding in this region. These results are consistent with previous work on the age of onset of folding in this region, provide revised estimates of fault slip rates necessary to understand the seismic hazard posed by these faults, and demonstrate the value of incorporating marine terraces in inverse fold kinematic models as a means to distinguish among non-unique solutions. © 2017 Elsevier B.V."
2,10.1523/JNEUROSCI.2988-17.2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041317580&doi=10.1523%2fJNEUROSCI.2988-17.2017&partnerID=40&md5=ce975845183c037f760dc089b349c344,"Gonadotrop in-releasing hormone (GnRH) neurons produce the central output controlling fertility and are regulated by steroid feedback. A switch from estradiol negative to positive feedback initiates the GnRH surge, ultimately triggering ovulation. This occurs on a daily basis in ovariectomized, estradiol-treated (OVX+E) mice; GnRH neurons are suppressed in the morning and activated in the afternoon. To test the hypotheses that estradiol and time of day signals alter GnRH neuron responsiveness to stimuli, GFP-identified GnRH neurons in brain slices from OVX+E or OVX female mice were recorded during the morning or afternoon. No differences were observed in baseline membrane potential. Current-clamp revealed GnRH neurons fired more action potentials in response to current injection during positive feedback relative to all other groups, which were not different from each other despite reports of differing ionic conductances. Kisspeptin increased GnRH neuron response in cells from OVX and OVX+E mice in the morning but not afternoon. Paradoxically, excitability in kisspeptin knock-out mice was similar to the maximum observed in control mice but was unchanged by time of day or estradiol. A mathematical model applying a Markov Chain Monte Carlo method to estimate probability distributions for estradiol-and time of day-dependent parameters was used to predict intrinsic properties underlying excitability changes. A single identifiable distribution of solutions accounted for similar GnRH neuron excitability in all groups other than positive feedback despite different underlying conductance properties; this was attributable to interdependence of voltage-gated potassium channel properties. In contrast, redundant solutions may explain positive feedback, perhaps indicative of the importance of this state for species survival. © 2018 the authors."
,10.1021/acs.iecr.7b04415,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041449955&doi=10.1021%2facs.iecr.7b04415&partnerID=40&md5=d19c0da63cac501ff5dd24db63653891,"Discrete modeling is a concept to establish thermodynamics on Shannon entropy expressed by variables that characterize discrete states of individual molecules in terms of their interacting neighbors in a mixture. To apply this method to condensed-phase lattice fluids, this paper further develops an approach proposed by Vinograd which features discrete Markov-chains for the sequential lattice construction and rigorous use of Shannon information as thermodynamic entropy, providing an in-depth discussion of the modeling concept evolved. The development comprises (1) improved accuracy compared to Monte Carlo data and (2) an extension from a two-dimensional to a three-dimensional simple lattice. The resulting model outperforms the quasichemical approximation proposed by Guggenheim, a frequently used reference model for the simple case of spherical molecules with uniform energetic surface properties. To illustrate its potential as a starting point for developing gE-models in chemical engineering applications, the proposed modeling methodology is extended, using the example of a simple approach for dicelike lattice molecules with multiple interaction sites on their surfaces, to address more realistic substances. A comparison with Monte Carlo simulations shows the model's capability to distinguish between isomeric configurations, which is a promising basis for future gE-model development in view of activity coefficients for liquid mixtures. © 2017 American Chemical Society."
,10.1007/s11063-017-9755-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041199962&doi=10.1007%2fs11063-017-9755-7&partnerID=40&md5=d67223f87427744a4a670436f00b0022,"Hand-craft features and clustering algorithms constitute the main parts of the unsupervised clustering system. Performance of the clustering deteriorates when the assumed probabilistic distribution of the data differs from the true one. This paper introduces a novel method that combines systematically the deep Boltzmann machine (DBM) with the Dirichlet process based Gaussian mixture model (DP-GMM) to bypass the problem of distribution mismatch. DBM is firstly used to extract the deep complex data features. By tactfully designing the distributions of different layers in DBM to make them compatible to that of the DP-GMM, we build a distribution consistent clustering system. The system is then jointly optimized by Markov chain Monte Carlo method with succinct updating formulations. The experimental results on two real databases of underwater acoustical target show the effectiveness and the robustness of the proposed clustering method. © 2018 Springer Science+Business Media, LLC, part of Springer Nature"
,10.1109/ICSRS.2017.8272818,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046662814&doi=10.1109%2fICSRS.2017.8272818&partnerID=40&md5=8f41d3a58c0d0f2871361e7919662acc,"Reactor Protection System (RPS) is one of the most important systems in instrumentation and control systems of Nuclear Power Plants (NPP). Research of RPS reliability is a hot topic in the field of NPP safety and reliability. Since there are lots of equipment in a RPS and there are complex relationships among the equipment, traditional reliability methods, such as fault tree analysis and Markov chain theory, have many limitations in the research of RPS reliability. This paper takes a preliminary research of RPS reliability based on Monte Carlo Methods. To simulate the behavior of every equipment in a RPS, Monte Carlo Method would get system reliability with considering all static and dynamic characters of the RPS. Standard Monte Carlo Method needs huge calculation power to simulate a high reliable system, and we study biased techniques to reduce the variance and improve the simulation efficiency during the research of RPS reliability. © 2017 IEEE."
,10.1109/PESGM.2017.8273971,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046359231&doi=10.1109%2fPESGM.2017.8273971&partnerID=40&md5=af43713e5b56f6346892d824b2573650,"This paper investigates modeling a heterogeneous group of Thermostatically Controlled Loads (TCLs) as a discrete-time discrete-state Markov model. Details of the Markov model development process using a statistical learning technique with full parameter heterogeneity are described and evaluated. The learning process is based on a training data sets which are obtained via Monte Carlo simulation of individual end-users' devices. The performance of various Markov models is evaluated under different initial conditions to determine representative models that can approximates the TCLs dynamics for the short-term and long-term applications. A detailed analysis of the model's eigenvalues is used to derive an accurate distribution of the TCL devices during steady-state conditions. This distribution is validated and tested against the actual devices' simulation. The study is applied to a group of 10,000 heterogeneous air-conditioning devices and the simulations are performed using MATLAB (R2015a). © 2017 IEEE."
,10.1109/PESGM.2017.8274181,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046337363&doi=10.1109%2fPESGM.2017.8274181&partnerID=40&md5=0c2039f19adf5443f228ceecd9b17965,"To enhance the availability of utility-scale PV plants generations, energy storage systems (ESS) have been applied. In both planning and operation stages, it is important to evaluate the credible capacity of optimally controlled PV-ESS plants. In this paper, a credible capacity evaluation method is proposed, which is suitable for PV farms with battery devices. A sequence Monte-Carlo simulation is introduced to evaluate the effective load-carrying capability of the PV-ESS plant. Then, a Markov chain is presented for modeling state transition of the energy storage system, considering fluctuation of both PV power and load. The optimized dispatch of energy storages is formulated as a Markov decision process, which generates the optimal control strategy of the PV-ESS plant. A real power system named as Qinghai Grid is adopted to validate the effectiveness and practicality of the proposed method. © 2017 IEEE."
,10.1109/ICONDA.2017.8270406,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050584680&doi=10.1109%2fICONDA.2017.8270406&partnerID=40&md5=e1870d119d3cc96e00845db7687c2523,"The recent terror incidents make us question if the existing surveillance systems are still the best solution. CCTVs have proven to be a solution for large scale surveillance but when it comes to solving crimes, CCTVs have played a very minimal role. The concept that is proposed in this paper is an idea that is set to overcome these shortcomings and revolutionize the surveillance systems. Based on the framework of a quadcopter with autonomous flight capabilities and auto-tracking feature, the drone uses image processing algorithm of Probability Hypothesis Density (PHD) filtering using a Markov Chain Monte Carlo (MCMC) implementation. To efficiently control the swarm of quadcopters we use an Energy Efficient Coverage Path Planning (EECPP) problem. The concept explained in this paper integrates a swarm of drones which can act autonomously with Image processing and can be the key for the future of public monitoring and security when made into a full scale device, saving precious lives at times. © 2017 IEEE."
,10.1109/JSYST.2018.2792307,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041324365&doi=10.1109%2fJSYST.2018.2792307&partnerID=40&md5=ac909457ba5b6dba96828fbd928d1169,"Among different subjects related to the natural gas and electricity market, this paper is particularly focused on the analysis of different scenarios for exporting natural gas. Iran has the second largest reservoirs of natural gas in the world and exports it to different countries. On the other hand, in the restructured power system, the role of natural gas is growing in the electricity generation due to its lower pollution. This paper analyzes different scenarios for exporting the natural gas including direct transfer at forward and hub prices, exporting at citygate price, conversion into electricity for exporting at forward price, and transferring via the power market. In this regard, system dynamics is applied for long-term analysis of the considered scenarios. The natural gas price is modeled by Markov chain Monte Carlo for a long-term period, and the models are analyzed using the data published by the energy information administration. The results show that exporting the natural gas or the electricity in suitable forward price are the most profitable scenarios. While exporting the natural gas at the demand price is profitable, transferring the electricity via the power market cannot meet the economic goals, even by expanding the renewable resources. IEEE"
2,10.1103/PhysRevC.97.014907,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041172885&doi=10.1103%2fPhysRevC.97.014907&partnerID=40&md5=e973332ed814e0c293c3e3c20a52cbdd,"By applying a Bayesian model-to-data analysis, we estimate the temperature and momentum dependence of the heavy quark diffusion coefficient in an improved Langevin framework. The posterior range of the diffusion coefficient is obtained by performing a Markov chain Monte Carlo random walk and calibrating on the experimental data of D-meson RAA and v2 in three different collision systems at the Relativistic Heavy-Ion Collidaer (RHIC) and the Large Hadron Collider (LHC): Au-Au collisions at 200 GeV and Pb-Pb collisions at 2.76 and 5.02 TeV. The spatial diffusion coefficient is found to be consistent with lattice QCD calculations and comparable with other models' estimation. We demonstrate the capability of our improved Langevin model to simultaneously describe the RAA and v2 at both RHIC and the LHC energies, as well as the higher order flow coefficient such as D meson v3. We show that by applying a Bayesian analysis, we are able to quantitatively and systematically study the heavy flavor dynamics in heavy-ion collisions. © 2018 American Physical Society."
,10.1080/00949655.2017.1381845,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030183876&doi=10.1080%2f00949655.2017.1381845&partnerID=40&md5=6e07f40463d2c066dc470dabc48b8d51,"In most practical applications, the quality of count data is often compromised due to errors-in-variables (EIVs). In this paper, we apply Bayesian approach to reduce bias in estimating the parameters of count data regression models that have mismeasured independent variables. Furthermore, the exposure model is misspecified with a flexible distribution, hence our approach remains robust against any departures from normality in its true underlying exposure distribution. The proposed method is also useful in realistic situations as the variance of EIVs is estimated instead of assumed as known, in contrast with other methods of correcting bias especially in count data EIVs regression models. We conduct simulation studies on synthetic data sets using Markov chain Monte Carlo simulation techniques to investigate the performance of our approach. Our findings show that the flexible Bayesian approach is able to estimate the values of the true regression parameters consistently and accurately. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
4,10.3847/1538-4357/aaa3fc,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041098276&doi=10.3847%2f1538-4357%2faaa3fc&partnerID=40&md5=4b0c0e73040fbe8238ef3f031bd7f9e8,"Bulge globular clusters (GCs) with metallicities [Fe/H] ≲ -1.0 and blue horizontal branches are candidates to harbor the oldest populations in the Galaxy. Based on the analysis of HST proper-motion-cleaned color-magnitude diagrams in filters F435W and F625W, we determine physical parameters for the old bulge GCs NGC 6522 and NGC 6626 (M28), both with well-defined blue horizontal branches. We compare these results with similar data for the inner halo cluster NGC 6362. These clusters have similar metallicities (-1.3 ≤ [Fe/H] ≤ -1.0) obtained from high-resolution spectroscopy. We derive ages, distance moduli, and reddening values by means of statistical comparisons between observed and synthetic fiducial lines employing likelihood statistics and the Markov chain Monte Carlo method. The synthetic fiducial lines were generated using α-enhanced BaSTI and Dartmouth stellar evolutionary models, adopting both canonical (Y ∼ 0.25) and enhanced (Y ∼ 0.30-0.33) helium abundances. RR Lyrae stars were employed to determine the HB magnitude level, providing an independent indicator to constrain the apparent distance modulus and the helium enhancement. The shape of the observed fiducial line could be compatible with some helium enhancement for NGC 6522 and NGC 6626, but the average magnitudes of RR Lyrae stars tend to rule out this hypothesis. Assuming canonical helium abundances, BaSTI and Dartmouth models indicate that all three clusters are coeval, with ages between ∼12.5 and 13.0 Gyr. The present study also reveals that NGC 6522 has at least two stellar populations, since its CMD shows a significantly wide subgiant branch compatible with 14% ± 2% and 86% ± 5% for first and second generations, respectively. © 2018. The American Astronomical Society. All rights reserved.."
,10.1109/ICCVW.2017.76,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046271970&doi=10.1109%2fICCVW.2017.76&partnerID=40&md5=49e50b04ab7bb116049f26095a5af650,"Hand pose is emerging as an important interface for human-computer interaction. The problem of hand pose estimation from passive stereo inputs has received less attention in the literature compared to active depth sensors. This paper seeks to address this gap by presenting a data-driven method to estimate a hand pose from a stereoscopic camera input, by introducing a stochastic approach to propose potential depth solutions to the observed stereo capture and evaluate these proposals using two convolutional neural networks (CNNs). The first CNN, configured in a Siamese network architecture, evaluates how consistent the proposed depth solution is to the observed stereo capture. The second CNN estimates a hand pose given the proposed depth. Unlike sequential approaches that reconstruct pose from a known depth, our method jointly optimizes the hand pose and depth estimation through Markov-chain Monte Carlo (MCMC) sampling. This way, pose estimation can correct for errors in depth estimation, and vice versa. Experimental results using an inexpensive stereo camera show that the proposed system more accurately measures pose better than competing methods. © 2017 IEEE."
,10.1109/CDC.2017.8263872,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046251029&doi=10.1109%2fCDC.2017.8263872&partnerID=40&md5=e02ff086d82b3af9e2e69bf958cf8cc2,"In this paper we introduce the a fortiori expectation-maximization (AFEM) algorithm for computing the parameters of a distribution from which unlabeled, correlated point sets are presumed to be generated. Each unlabeled point is assumed to correspond to a target with independent probability of appearance but correlated positions. We propose replacing the expectation phase of the algorithm with a Kalman filter modified within a Bayesian framework to account for the unknown point labels which manifest as uncertain measurement matrices. We also propose a mechanism to reorder the measurements in order to improve parameter estimates. In addition, we use a state-of-the-art Markov chain Monte Carlo sampler to efficiently sample measurement matrices. In the process, we indirectly propose a constrained k-means clustering algorithm. Simulations verify the utility of AFEM against a traditional expectation-maximization algorithm in a variety of scenarios. © 2017 IEEE."
,10.1109/CDC.2017.8263890,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046294187&doi=10.1109%2fCDC.2017.8263890&partnerID=40&md5=1bd0741fe4fa6db756b0452536e384c7,"The objective of this paper is to evaluate the transient performance of the conventional Monte Carlo method (called FMC in this paper) with a fixed number of samples. The following question is asked: under what conditions could the propagated FMC ensemble have been generated by a direct sampling of the unknown true state-pdf? To answer this question, the propagated ensemble is viewed as the realization of a Markov chain and the FMC process as the evolution of the associated transition kernel. An equation governing the evolution of FMC transition kernel is derived. It is shown that for systems with 'zero-divergence' in their force field (D · f = 0), the true evolved state pdf is the invariant distribution of the propagated FMC transition kernel at all times. No such equivalence is guaranteed for systems with non-zero divergence. Numerical simulations are provided to support theoretical claims about ensemble quality for both types of dynamic systems. © 2017 IEEE."
,10.1080/03610918.2017.1400053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041016797&doi=10.1080%2f03610918.2017.1400053&partnerID=40&md5=4bcb81bbda90d2cdd33a42787f29d71f,"The combined model accounts for different forms of extra-variability and has traditionally been applied in the likelihood framework, or in the Bayesian setting via Markov chain Monte Carlo. In this article, integrated nested Laplace approximation is investigated as an alternative estimation method for the combined model for count data, and compared with the former estimation techniques. Longitudinal, spatial, and multi-hierarchical data scenarios are investigated in three case studies as well as a simulation study. As a conclusion, integrated nested Laplace approximation provides fast and precise estimation, while avoiding convergence problems often seen when using Markov chain Monte Carlo. © 2017 Taylor & Francis Group, LLC"
,10.1080/03610926.2017.1303732,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029406140&doi=10.1080%2f03610926.2017.1303732&partnerID=40&md5=4f05ef3e40e9e865e98b66840135df79,"Markov-switching (MS) models are becoming increasingly popular as efficient tools of modeling various phenomena in different disciplines, in particular for non Gaussian time series. In this articlept"", we propose a broad class of Markov-switching BILINEAR–GARCH processes (MS − BLGARCH hereafter) obtained by adding to a MS − GARCH model one or more interaction components between the observed series and its volatility process. This parameterization offers remarkably rich dynamics and complex behavior for modeling and forecasting financial time-series data which exhibit structural changes. In these models, the parameters of conditional variance are allowed to vary according to some latent time-homogeneous Markov chain with finite state space or “regimes.” The main aim of this new model is to capture asymmetric and hence purported to be able to capture leverage effect characterized by the negativity of the correlation between returns shocks and subsequent shocks in volatility patterns in different regimes. So, first, some basic structural properties of this new model including sufficient conditions ensuring the existence of stationary, causal, ergodic solutions, and moments properties are given. Second, since the second-order structure provides a useful information to identify an appropriate time-series model, we derive the expression of the covariance function of for MS − BLGARCH and for its powers. As a consequence, we find that the second (resp. higher)-order structure is similar to some linear processes, and hence MS − BLGARCH (resp. its powers) admit an ARMA representation. This finding allows us for parameter estimation via GMM procedure proved by a Monte Carlo study and applied to foreign exchange rate of the Algerian Dinar against the single European currency. © 2018 Taylor & Francis Group, LLC."
,10.1021/acs.est.7b04906,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041443933&doi=10.1021%2facs.est.7b04906&partnerID=40&md5=e9bf73c03a4235404d6fd172eb86820c,"Metal contamination is a major problem in many estuaries. Toxicokinetic models are useful tools for predicting metal accumulation in estuarine organisms and managing the associated ecological risks. However, obtaining toxicokinetic parameter values with sufficient predictive power is challenging for dynamic estuarine waters. In this study, we determined the toxicokinetics of multiple metals in the oyster Crassostrea hongkongensis in a dynamic estuary polluted by metals using a 48 day transplant experiment. During the experiment, metal concentrations in oysters, water, and suspended particles were intensively monitored at 3 day intervals. The toxicokinetic parameters were then estimated using the Markov chain Monte Carlo (MCMC) method. The calibrated model was capable of successfully simulating the time-course of metal bioaccumulation in oysters and was further validated by predicting the bioaccumulation at another site in the estuary. Furthermore, the model was used to assess the relative importance of different pathways in metal bioaccumulation. With the MCMC method, distributions instead of single values were assigned to model parameters. This method makes the model predictions probabilistic with clearly defined uncertainties, and they are thus particularly useful for the risk assessment of metals in aquatic systems. © 2017 American Chemical Society."
,10.1109/ICSIIT.2017.71,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049327768&doi=10.1109%2fICSIIT.2017.71&partnerID=40&md5=599adea32cda01770b78395934570d60,"A simple independent generalized extreme value (GEV) model and a three-stage hierarchical model were applied to regional climate model outputs for temperature extremes over Tasmania, Australia. The parameters of each model were estimated using a maximum likelihood and a hybrid Markov chain Monte Carlo (MCMC) approach respectively. The two models were compared based on how well the models could predict extremes for 50 randomly selected locations that were withheld from fitting, using root mean squared prediction error (RMSPE), ten times. The RMSPEs of the two models show that the three-stage hierarchical model outperformed the simple model. We showed that the spatial hierarchical model has successfully smoothed the shape parameters. The high values tend to be pulled down, the low values to be pushed up. © 2017 IEEE."
,10.1080/03610918.2017.1414248,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041098761&doi=10.1080%2f03610918.2017.1414248&partnerID=40&md5=2cbbc8dd858a8df76363bcf929f09593,"Partial linear single-index model (PLSIM) has both the flexibility of nonparametric treatment and interpretability of linear term, yet existing literatures about it mainly focused on mean regression, and quantile regression analysis is scarce. Based on free knot spline approximation, we apply asymmetric Laplace distribution to implement Bayesian quantile regression, and perform variable selection in linear term and index vector via binary indicators. Our approach is exempt from regularity conditions in frequentist method, and could execute variable selection and quantile regression under mutual posterior correction, which is also the first work to implement them jointly for PLSIM in fully Bayesian framework. The numerical simulation manifests the superiority of our approach to previous methods, which embodied in better efficiency of variable selection, index vector estimates and link function approximation with different error distributions. For illustration of its application, we build a power consumption model of A2/O process in wastewater treatment and emphatically analyze the impact of water quality factors. © 2017 Taylor & Francis Group, LLC"
,10.1109/ISGTEurope.2017.8260306,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046245594&doi=10.1109%2fISGTEurope.2017.8260306&partnerID=40&md5=eb5cbd572e9e01f29530b2a9d01756f5,"In this paper, operation management of microgrids is performed. To do so, some contingencies including outage of distributed generators (DG), energy storage (ES) and the upstream network are considered. Since the microgrids have suitable capabilities in terms of control and communication, demand response reserve can be applied to improve the operation management. Using Monte Carlo simulation method and Markov chain, several scenarios are generated to show the possible contingencies in various hours. Then, a scenario reduction method is used for reducing the number of scenarios. Finally, a two-stage stochastic model is applied to solve a day-ahead scheduling problem in mixed-integer linear programming by GAMS. Consequently, the effect of demand response in the reduction of operation cost is demonstrated. © 2017 IEEE."
1,10.1016/j.ces.2017.10.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032854262&doi=10.1016%2fj.ces.2017.10.003&partnerID=40&md5=f4aecfeed82b3ce621f5f252bfd182dc,"The phase equilibrium in the loading and stripping stages of the liquid–liquid extraction of iron (III) with hydroxyoxime extractant in kerosene was studied over a wide range of hydroxyoxime extractant (Acorga M5640, 3–25 vol–%) and iron (1.5–45 g/L) concentrations. A mechanistic mathematical model explaining the phase equilibrium in the loading stage was developed and validated with new experimental data. The model accounts for the non-ideality of both the aqueous and the organic phases. The composition of the aqueous sulfate solution was calculated through speciation of the electrolytes with an extended Debye–Hückel model. The organic phase non-ideality in the loading stage was described with an empirical correlation that accounts for the effect of extractant concentration on the extraction equilibrium constant. The model parameters were fitted against measured experimental data using nonlinear regression analysis. A mechanistic mathematical model explaining the co-extraction of iron in copper liquid–liquid extraction was developed and validated using D-optimal experiment design and the Markov chain Monte Carlo algorithm. The model facilitates the optimization of copper liquid–liquid extraction circuits, as iron is the most common impurity in industrial systems, making process operations challenging. © 2017 Elsevier Ltd"
,10.1016/j.patrec.2017.11.022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037543059&doi=10.1016%2fj.patrec.2017.11.022&partnerID=40&md5=f278214ff81b0d210c3a6b5c067c4d15,"Estimating the log-likelihood gradient with respect to the parameters of a Restricted Boltzmann Machine (RBM) typically requires sampling using Markov Chain Monte Carlo (MCMC) techniques. To save computation time, the Markov chains are only run for a small number of steps, which leads to a biased estimate. This bias can cause RBM training algorithms such as Contrastive Divergence (CD) learning to deteriorate. We adopt the idea behind Population Monte Carlo (PMC) methods to devise a new RBM training algorithm termed Population-Contrastive-Divergence (pop-CD). Compared to CD, it leads to a consistent estimate and may have a significantly lower bias. Its computational overhead is negligible compared to CD, but the variance of the gradient estimate increases. We experimentally show that pop-CD can significantly outperform CD. In many cases, we observed a smaller bias and achieved higher log-likelihood values. However, when the RBM distribution has many hidden neurons, the consistent estimate of pop-CD may still have a considerable bias and the variance of the gradient estimate requires a smaller learning rate. Thus, despite its superior theoretical properties, it is not advisable to use pop-CD in its current form on large problems. © 2017"
1,10.1016/j.energy.2017.11.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037356109&doi=10.1016%2fj.energy.2017.11.005&partnerID=40&md5=806604e5373f76daa11f9bd8d10df30a,"The massive deployment of plug-in electric vehicles (PEVs), renewable energy resources (RES), and distributed energy storage systems (DESS) has gained significant interest under the smart grid vision. However, their special features and operational characteristics have created a paradigm shift in distribution network resource allocation studies. This paper presents a combined model formulation for the concurrent optimal resource allocation of PEVs charging stations, RES and DESS in distribution networks. The formulation employs a general objective function that optimizes the total Annual Cost of Energy (ACOE). The decision variables in the formulation are the locations and capacities of PEVs charging stations, RES, and DESS units. A Markov Chain Monte Carlo (MCMC) simulation model is utilized to account for the uncertainties of PEVs charging demand and output generation of RES units. Also, in order to enhance the accuracy of the resource allocation problem, the coordinated control of PEVs charging, RES output power, and DESS charging/discharging are incorporated in the formulated model. The formulation is decomposed into two interdependent sub-problems and solved using a combination of metaheuristic and deterministic optimization techniques. A sample case study is presented to illustrate the performance of the algorithm. © 2017 Elsevier Ltd"
,10.1109/BigData.2017.8258417,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047755652&doi=10.1109%2fBigData.2017.8258417&partnerID=40&md5=b3476b9c6fe5d0ba3220a1fc2a7d155a,"E-commerce websites such as Yelp.com, allow users to write online reviews of products and services, so new customers can have quick access to user experiences, covering everything from auto-repair to hospitals. However, a typical user may find it difficult to identify a topic of interest due to the overwhelming amount of review information. To deal with this issue, the Latent Dirichlet Allocation (LDA) model can be used to associate meaningful terms with text-based reviews, permitting keyword retrieval of individual documents. LDA is a powerful unsupervised learning approach, which has been widely used for topic modeling as well as in other related fields. A conventional implementation of LDA is through the Markov-Chain Monte-Carlo methodology, called Collapsed Gibbs Sampling (CGS). However, due to the usage of random numbers in the CGS approach, results from multiple trials on the same data are usually inconsistent. To avoid this tendency, we revise the conventional LDA approach using Variational Gibbs Sampling (VGS). VGS eliminates random numbers, and thus leads to consistent results as well as better performance. Our case study shows that our improved LDA can be used to automatically identify keywords and topics in online hospital reviews. Due to the usage of VGS, the accuracy of topic identification has been consistently improved. © 2017 IEEE."
,10.1007/s11222-017-9796-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041797150&doi=10.1007%2fs11222-017-9796-9&partnerID=40&md5=08c6585cbf3bef4b46cd5f4a181e6be5,"We present a new Bayesian nonparametric approach to estimating the spectral density of a stationary time series. A nonparametric prior based on a mixture of B-spline distributions is specified and can be regarded as a generalization of the Bernstein polynomial prior of Petrone (Scand J Stat 26:373–393, 1999a; Can J Stat 27:105–126, 1999b) and Choudhuri et al. (J Am Stat Assoc 99(468):1050–1059, 2004). Whittle’s likelihood approximation is used to obtain the pseudo-posterior distribution. This method allows for a data-driven choice of the number of mixture components and the location of knots. Posterior samples are obtained using a Metropolis-within-Gibbs Markov chain Monte Carlo algorithm, and mixing is improved using parallel tempering. We conduct a simulation study to demonstrate that for complicated spectral densities, the B-spline prior provides more accurate Monte Carlo estimates in terms of (Formula presented.)-error and uniform coverage probabilities than the Bernstein polynomial prior. We apply the algorithm to annual mean sunspot data to estimate the solar cycle. Finally, we demonstrate the algorithm’s ability to estimate a spectral density with sharp features, using real gravitational wave detector data from LIGO’s sixth science run, recoloured to match the Advanced LIGO target sensitivity. © 2018 Springer Science+Business Media, LLC, part of Springer Nature"
,10.1103/PhysRevE.97.012113,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040743973&doi=10.1103%2fPhysRevE.97.012113&partnerID=40&md5=ac557dc6fc6d874d23118c5d9facc3dd,"A scalar Langevin-type process X(t) that is driven by Ornstein-Uhlenbeck noise η(t) is non-Markovian. However, the joint dynamics of X and η is described by a Markov process in two dimensions. But even though there exists a variety of techniques for the analysis of Markov processes, it is still a challenge to estimate the process parameters solely based on a given time series of X. Such a partially observed 2D process could, e.g., be analyzed in a Bayesian framework using Markov chain Monte Carlo methods. Alternatively, an embedding strategy can be applied, where first the joint dynamics of X and its temporal derivative Ẋ is analyzed. Subsequently, the results can be used to determine the process parameters of X and η. In this paper, we propose a more direct approach that is purely based on the moments of the increments of X, which can be estimated for different time-increments τ from a given time series. From a stochastic Taylor expansion of X, analytic expressions for these moments can be derived, which can be used to estimate the process parameters by a regression strategy. © 2018 American Physical Society."
1,10.5194/bg-15-187-2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045831896&doi=10.5194%2fbg-15-187-2018&partnerID=40&md5=1880292d6f26f7aa4da24b205d6c57b0,"Modeling net ecosystem exchange (NEE) at the regional scale with land surface models (LSMs) is relevant for the estimation of regional carbon balances, but studies on it are very limited. Furthermore, it is essential to better understand and quantify the uncertainty of LSMs in order to improve them. An important key variable in this respect is the prognostic leaf area index (LAI), which is very sensitive to forcing data and strongly affects the modeled NEE. We applied the Community Land Model (CLM4.5-BGC) to the Rur catchment in western Germany and compared estimated and default ecological key parameters for modeling carbon fluxes and LAI. The parameter estimates were previously estimated with the Markov chain Monte Carlo (MCMC) approach DREAM(zs) for four of the most widespread plant functional types in the catchment. It was found that the catchment-scale annual NEE was strongly positive with default parameter values but negative (and closer to observations) with the estimated values. Thus, the estimation of CLM parameters with local NEE observations can be highly relevant when determining regional carbon balances. To obtain a more comprehensive picture of model uncertainty, CLM ensembles were set up with perturbed meteorological input and uncertain initial states in addition to uncertain parameters. C3 grass and C3 crops were particularly sensitive to the perturbed meteorological input, which resulted in a strong increase in the standard deviation of the annual NEE sum (δΣNEE) for the different ensemble members from ∼ 2 to 3 g C m-2 yr-1 (with uncertain parameters) to ∼ 45 g C m-2 yr-1 (C3 grass) and ∼ 75 g C m-2 yr-1 (C3 crops) with perturbed forcings. This increase in uncertainty is related to the impact of the meteorological forcings on leaf onset and senescence, and enhanced/reduced drought stress related to perturbation of precipitation. The NEE uncertainty for the forest plant functional type (PFT) was considerably lower (σ NEE ∼ 4.0-13.5 g C m-2 yr-1 with perturbed parameters, meteorological forcings and initial states). We conclude that LAI and NEE uncertainty with CLM is clearly underestimated if uncertain meteorological forcings and initial states are not taken into account. © 2018 Wilson Ornithological Society. All rights reserved."
,10.1051/epjconf/201817006002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041029123&doi=10.1051%2fepjconf%2f201817006002&partnerID=40&md5=abf5ef732c8be8f6a59990a5c6ad4c91,"Gamma spectrometry is a passive non-destructive assay used to quantify radionuclides present in more or less complex objects. Basic methods using empirical calibration with a standard in order to quantify the activity of nuclear materials by determining the calibration coefficient are useless on non-reproducible, complex and single nuclear objects such as waste packages. Package specifications as composition or geometry change from one package to another and involve a high variability of objects. Current quantification process uses numerical modelling of the measured scene with few available data such as geometry or composition. These data are density, material, screen, geometric shape, matrix composition, matrix and source distribution. Some of them are strongly dependent on package data knowledge and operator backgrounds. The French Commissariat à l'Energie Atomique (CEA) is developing a new methodology to quantify nuclear materials in waste packages and waste drums without operator adjustment and internal package configuration knowledge. This method suggests combining a global stochastic approach which uses, among others, surrogate models available to simulate the gamma attenuation behaviour, a Bayesian approach which considers conditional probability densities of problem inputs, and Markov Chains Monte Carlo algorithms (MCMC) which solve inverse problems, with gamma ray emission radionuclide spectrum, and outside dimensions of interest objects. The methodology is testing to quantify actinide activity in different kind of matrix, composition, and configuration of sources standard in terms of actinide masses, locations and distributions. Activity uncertainties are taken into account by this adjustment methodology. © The Authors, published by EDP Sciences, 2018."
,10.1186/s12913-017-2775-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040369374&doi=10.1186%2fs12913-017-2775-1&partnerID=40&md5=6765e5deff64fb609decad1f73f9eeb1,"Background: Drug markets are very complex and, while many new drugs are registered each year, little is known about what drives the prescription of these new drugs. This study attempts to lift the veil from this important subject by analyzing simultaneously the impact of several variables on the prescription of novelty. Methods: Data provided by four Swiss sickness funds were analyzed. These data included information about more than 470,000 insured, notably their drug intake. Outcome variable that captured novelty was the age of the drug prescribed. The overall variance in novelty was partitioned across five levels (substitutable drug market, patient, physician, region, and prescription) and the influence of several variables measured at each of these levels was assessed using a non-hierarchical multilevel model estimated by Bayesian Markov Chain Monte Carlo methods. Results: More than 92% of the variation in novelty was explained at the substitutable drug market-level and at the prescription-level. Newer drugs were prescribed in markets that were costlier, less concentrated, included more insured, provided more drugs and included more active substances. Over-the-counter drugs were on average 12.5 years older while generic drugs were more than 15 years older than non-generics. Regional disparities in terms of age of prescribed drugs could reach 2.8 years. Conclusions: Regulation of the demand has low impact, with little variation explained at the patient-level and physician-level. In contrary, the market structure (e.g. end of patent with generic apparition, concurrence among producers) had a strong contribution to the variation of drugs ages. © 2018 The Author(s)."
2,10.1103/PhysRevE.97.012106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040732698&doi=10.1103%2fPhysRevE.97.012106&partnerID=40&md5=b1a5fafb6f026e9fb516261051a4b199,"The Totally Asymmetric Exclusion Process (TASEP) is a classical stochastic model for describing the transport of interacting particles, such as ribosomes moving along the messenger ribonucleic acid (mRNA) during translation. Although this model has been widely studied in the past, the extent of collision between particles and the average distance between a particle to its nearest neighbor have not been quantified explicitly. We provide here a theoretical analysis of such quantities via the distribution of isolated particles. In the classical form of the model in which each particle occupies only a single site, we obtain an exact analytic solution using the matrix ansatz. We then employ a refined mean-field approach to extend the analysis to a generalized TASEP with particles of an arbitrary size. Our theoretical study has direct applications in mRNA translation and the interpretation of experimental ribosome profiling data. In particular, our analysis of data from Saccharomyces cerevisiae suggests a potential bias against the detection of nearby ribosomes with a gap distance of less than approximately three codons, which leads to some ambiguity in estimating the initiation rate and protein production flux for a substantial fraction of genes. Despite such ambiguity, however, we demonstrate theoretically that the interference rate associated with collisions can be robustly estimated and show that approximately 1% of the translating ribosomes get obstructed. © 2018 American Physical Society."
,10.1051/epjconf/201816801008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040970232&doi=10.1051%2fepjconf%2f201816801008&partnerID=40&md5=d4899fd48e1823d82a22202738413db1,"The gravitational waves from compact binary systems are viewed as a standard siren to probe the evolution of the universe. This paper summarizes the potential and ability to use the gravitational waves to constrain the cosmological parameters and the dark sector interaction in the Gaussian process methodology. After briefly introducing the method to reconstruct the dark sector interaction by the Gaussian process, the concept of standard sirens and the analysis of reconstructing the dark sector interaction with LISA are outlined. Furthermore, we estimate the constraint ability of the gravitational waves on cosmological parameters with ET. The numerical methods we use are Gaussian process and the Markov-Chain Monte-Carlo. Finally, we also forecast the improvements of the abilities to constrain the cosmological parameters with ET and LISA combined with the Planck. © The Authors, published by EDP Sciences, 2018."
,10.1109/ICUWB.2017.8251009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046036316&doi=10.1109%2fICUWB.2017.8251009&partnerID=40&md5=3001f33735ab0a3829c7aec780c21235,"One of the major challenges with the increase in wind power generation is the uncertain nature of wind speed. So far the uncertainty about wind speed has been presented through probability distributions. Also the existing models that consider the uncertainty of the wind speed primarily view the distributions of the wind speed over a wind farm as being homogeneous. However, the uncertainty about these wind speed models has not yet been considered. In this paper the Bayesian approach to taking into account the uncertainty inherent in the wind speed model has been presented. The proposed Bayesian predictive model of the wind speed aggregates the non-homogeneous distributions into a single continuous distribution. Therefore, the result is able to capture the variation among the probability distributions of the wind speeds at the turbines' locations in a wind farm. More specifically, instead of using a wind speed distribution whose parameters are known or estimated, the parameters are considered as random whose variations are according to probability distributions. The Bayesian predictive model for a Rayleigh which only has a single model scale parameter has been proposed. Also closed-form posterior and predictive inferences under different reasonable choices of prior distribution in sensitivity analysis have been presented. © 2017 IEEE."
2,10.23919/EURAD.2017.8249147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040868249&doi=10.23919%2fEURAD.2017.8249147&partnerID=40&md5=155380c98f660f863739e769ed0515c5,"In this paper, we tackle the task of multi-target tracking of humans in an indoor setting using a low power 77 GHz MIMO CMOS radar. A drawback of such a highresolution and low-power device is the higher sensitivity to noise, which makes the analysis of signals more challenging. Therefore, a pipeline is proposed to address both pre-processing of the radar signal and multi-target tracking. In the pre-processing phase, we focus on handling the low Signal-to-Noise Ratio (SNR) and eliminating so-called ghost targets. The tracking method we propose is based on Markov Chain Monte Carlo Data Association (MCMCDA), thus taking a combinatorial approach towards the task of tracking. The pipeline is tested on a number of real-world scenarios and shows promising results, overcoming the significant amount of noise associated with embedded radar devices. © 2017 European Microwave Association."
,10.1109/WSC.2017.8247926,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044543652&doi=10.1109%2fWSC.2017.8247926&partnerID=40&md5=4b8dd7dc3a47fb468e9b0bed3d32dc46,"Simulation from the tail of the multivariate normal density has numerous applications in statistics and operations research. Unfortunately, there is no simple formula for the cumulative distribution function of the multivariate normal law, and simulation from its tail can frequently only be approximate. In this article we present an asymptotically efficient Monte Carlo estimator for quantities related to the tail of the multivariate normal distribution. The estimator leverages upon known asymptotic approximations. In addition, we generalize the notion of asymptotic efficiency of Monte Carlo estimators of rare-event probabilities to the sampling properties of Markov chain Monte Carlo algorithms. Regarding these new notions, we propose a simple and practical Markov chain sampler for the normal tail that is asymptotically optimal. We then give a numerical example from finance that illustrates the benefits of an asymptotically efficient Markov chain Monte Carlo sampler. © 2017 IEEE."
,10.1007/s10614-017-9784-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040079132&doi=10.1007%2fs10614-017-9784-3&partnerID=40&md5=9b9c46644992b66f8800a402c1ad922a,"Stochastic volatility models have been widely appreciated to model the time-varying volatility in empirical finance. In practice, whether or not there is leverage effect in asset time series is one of important stylized facts. In this paper, in the context of the stochastic volatility models, the main purpose is to develop a Bayesian approach for testing the leverage effect. The performance of the developed procedure is illustrated by the simulation studies and two empirical examples. © 2018 Springer Science+Business Media, LLC, part of Springer Nature"
4,10.1109/ETCM.2017.8247445,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043355705&doi=10.1109%2fETCM.2017.8247445&partnerID=40&md5=5839bcb808801329c1cd837144d5e124,"This paper presents a reliability model of a Static Var Compensator (SVC) using an innovative algorithm based on sequential Monte Carlo simulation and Markov chains. The method employs the equivalent circuit of a SVC and takes the failure rate and repair time of each component as input in order to compute the failure rate and repair time of the whole SVC system. The specific contribution of this investigation is that it presents a mathematical pathway to model operating conditions of a SVC subject to individual operating states of its components, resulting in a comprehensive reliability model. © 2017 IEEE."
,10.1109/EI2.2017.8245412,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049209947&doi=10.1109%2fEI2.2017.8245412&partnerID=40&md5=eb71a647c1158557eb1a7e8aa08111bf,"Distributed integrated energy system (DIES) is conductive to alleviate the energy shortage and environment pollution. As a basis for planning and operation, reliability evaluation is important for the development of DIES. Firstly, this paper established a reliability evaluation model of DIES based on an energy hub model, which can describe the complex interconnection between different power supply subsystems. Then, the impacts of dynamic behavior of thermostatically controlled load on the reliability evaluation of DIES is analyzed based on an electric water heater (EWH) model. And the reliability evaluation of DIES is carried out based on a Markov chain Monte Carlo (MCMC) simulation. The feasibility of the proposed method is validated by extensive case studies. © 2017 IEEE."
1,10.1080/10705511.2017.1364968,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030175725&doi=10.1080%2f10705511.2017.1364968&partnerID=40&md5=08726e67a521ed23fb2607dc14b894fb,"In psychological, social, behavioral, and medical studies, hidden Markov models (HMMs) have been extensively applied to the simultaneous modeling of heterogeneous observation and hidden transition in the analysis of longitudinal data. However, the majority of the existing HMMs are developed in a parametric framework without latent variables. This study considers a novel semiparametric HMM, which comprises a semiparametric latent variable model to investigate the complex interrelationships among latent variables and a nonparametric transition model to examine the linear and nonlinear effects of potential predictors on hidden transition. The Bayesian P-splines approach and Markov chain Monte Carlo methods are developed to estimate the unknown, a Bayesian model comparison statistic, is employed to conduct model comparison. The empirical performance of the proposed methodology is evaluated through simulation studies. An application to a data set derived from the National Longitudinal Survey of Youth is presented. Copyright © Taylor & Francis Group, LLC."
,10.1080/10705511.2017.1372688,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031788731&doi=10.1080%2f10705511.2017.1372688&partnerID=40&md5=e9669382bbd9b7708ca691207e81ed2c,"Multivariate heterogenous data with latent variables are common in many fields such as biological, medical, behavioral, and social-psychological sciences. Mixture structural equation models are multivariate techniques used to examine heterogeneous interrelationships among latent variables. In the analysis of mixture models, determination of the number of mixture components is always an important and challenging issue. This article aims to develop a full Bayesian approach with the use of reversible jump Markov chain Monte Carlo method to analyze mixture structural equation models with an unknown number of components. The proposed procedure can simultaneously and efficiently select the number of mixture components and conduct parameter estimation. Simulation studies show the satisfactory empirical performance of the method. The proposed method is applied to study risk factors of osteoporotic fractures in older people. Copyright © Taylor & Francis Group, LLC."
,10.1080/10618600.2017.1317263,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026430416&doi=10.1080%2f10618600.2017.1317263&partnerID=40&md5=178a066587b4a9c63a6eed305259e88a,"The pseudo likelihood method of Besag (1974) has remained a popular method for estimating Markov random field on a very large lattice, despite various documented deficiencies. This is partly because it remains the only computationally tractable method for large lattices. We introduce a novel method to estimate Markov random fields defined on a regular lattice. The method takes advantage of conditional independence structures and recursively decomposes a large lattice into smaller sublattices. An approximation is made at each decomposition. Doing so completely avoids the need to compute the troublesome normalizing constant. The computational complexity is O(N), where N is the number of pixels in the lattice, making it computationally attractive for very large lattices. We show through simulations, that the proposed method performs well, even when compared with methods using exact likelihoods. Supplementary material for this article is available online. © 2018 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
,10.1080/10618600.2017.1307117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026419157&doi=10.1080%2f10618600.2017.1307117&partnerID=40&md5=424d6a247a9b369b6b5456c0cf569c64,"The complexity of the Metropolis–Hastings (MH) algorithm arises from the requirement of a likelihood evaluation for the full dataset in each iteration. One solution has been proposed to speed up the algorithm by a delayed acceptance approach where the acceptance decision proceeds in two stages. In the first stage, an estimate of the likelihood based on a random subsample determines if it is likely that the draw will be accepted and, if so, the second stage uses the full data likelihood to decide upon final acceptance. Evaluating the full data likelihood is thus avoided for draws that are unlikely to be accepted. We propose a more precise likelihood estimator that incorporates auxiliary information about the full data likelihood while only operating on a sparse set of the data. We prove that the resulting delayed acceptance MH is more efficient. The caveat of this approach is that the full dataset needs to be evaluated in the second stage. We therefore propose to substitute this evaluation by an estimate and construct a state-dependent approximation thereof to use in the first stage. This results in an algorithm that (i) can use a smaller subsample m by leveraging on recent advances in Pseudo-Marginal MH (PMMH) and (ii) is provably within O(m− 2) of the true posterior. © 2018 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
5,10.1080/01621459.2017.1328358,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047367446&doi=10.1080%2f01621459.2017.1328358&partnerID=40&md5=b4c9a0b0002c3e75e6043e709406d8ea,"We perform differential expression analysis of high-throughput sequencing count data under a Bayesian nonparametric framework, removing sophisticated ad hoc pre-processing steps commonly required in existing algorithms. We propose to use the gamma (beta) negative binomial process, which takes into account different sequencing depths using sample-specific negative binomial probability (dispersion) parameters, to detect differentially expressed genes by comparing the posterior distributions of gene-specific negative binomial dispersion (probability) parameters. These model parameters are inferred by borrowing statistical strength across both the genes and samples. Extensive experiments on both simulated and real-world RNA sequencing count data show that the proposed differential expression analysis algorithms clearly outperform previously proposed ones in terms of the areas under both the receiver operating characteristic and precision-recall curves. Supplementary materials for this article are available online. © 2018 American Statistical Association."
3,10.1080/10618600.2017.1316280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026410934&doi=10.1080%2f10618600.2017.1316280&partnerID=40&md5=b2ef485df779e4b378cc09cb5f5a34b4,"Univariate or multivariate ordinal responses are often assumed to arise from a latent continuous parametric distribution, with covariate effects that enter linearly. We introduce a Bayesian nonparametric modeling approach for univariate and multivariate ordinal regression, which is based on mixture modeling for the joint distribution of latent responses and covariates. The modeling framework enables highly flexible inference for ordinal regression relationships, avoiding assumptions of linearity or additivity in the covariate effects. In standard parametric ordinal regression models, computational challenges arise from identifiability constraints and estimation of parameters requiring nonstandard inferential techniques. A key feature of the nonparametric model is that it achieves inferential flexibility, while avoiding these difficulties. In particular, we establish full support of the nonparametric mixture model under fixed cut-off points that relate through discretization the latent continuous responses with the ordinal responses. The practical utility of the modeling approach is illustrated through application to two datasets from econometrics, an example involving regression relationships for ozone concentration, and a multirater agreement problem. Supplementary materials with technical details on theoretical results and on computation are available online. © 2018 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
1,10.1080/07350015.2015.1137757,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018178230&doi=10.1080%2f07350015.2015.1137757&partnerID=40&md5=28be38edd9adc4f1aa26cc86e9324560,"This article develops a new Markov-switching vector autoregressive (VAR) model with stochastic correlation for contagion analysis on financial markets. The correlation and the log-volatility dynamics are driven by two independent Markov chains, thus allowing for different effects such as volatility spill-overs and correlation shifts with various degrees of intensity. We outline a suitable Bayesian inference procedure based on Markov chain Monte Carlo algorithms. We then apply the model to some major and Asian-Pacific cross rates against the U.S. dollar and find strong evidence supporting the existence of contagion effects and correlation drops during crises, closely in line with the stylized facts outlined in the contagion literature. A comparison of this model with its closest competitors, such as a time-varying parameter VAR, reveals that our model has a better predictive ability. Supplementary materials for this article are available online. © 2018 American Statistical Association."
,10.1080/03610918.2017.1280830,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020163044&doi=10.1080%2f03610918.2017.1280830&partnerID=40&md5=5c76cf7851607e6b1ac3e652fff81afd,"A number of nonstationary models have been developed to estimate extreme events as function of covariates. A quantile regression (QR) model is a statistical approach intended to estimate and conduct inference about the conditional quantile functions. In this article, we focus on the simultaneous variable selection and parameter estimation through penalized quantile regression. We conducted a comparison of regularized Quantile Regression model with B-Splines in Bayesian framework. Regularization is based on penalty and aims to favor parsimonious model, especially in the case of large dimension space. The prior distributions related to the penalties are detailed. Five penalties (Lasso, Ridge, SCAD0, SCAD1 and SCAD2) are considered with their equivalent expressions in Bayesian framework. The regularized quantile estimates are then compared to the maximum likelihood estimates with respect to the sample size. A Markov Chain Monte Carlo (MCMC) algorithms are developed for each hierarchical model to simulate the conditional posterior distribution of the quantiles. Results indicate that the SCAD0 and Lasso have the best performance for quantile estimation according to Relative Mean Biais (RMB) and the Relative Mean-Error (RME) criteria, especially in the case of heavy distributed errors. A case study of the annual maximum precipitation at Charlo, Eastern Canada, with the Pacific North Atlantic climate index as covariate is presented. © 2018 Taylor & Francis Group, LLC."
,10.1080/10618600.2017.1330206,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031090874&doi=10.1080%2f10618600.2017.1330206&partnerID=40&md5=90b1a441461d36eda1fd87d4eea0053f,"The multiset sampler has been shown to be an effective algorithm to sample from complex multimodal distributions, but the multiset sampler requires that the parameters in the target distribution can be divided into two parts: the parameters of interest and the nuisance parameters. We propose a new self-multiset sampler (SMSS), which extends the multiset sampler to distributions without nuisance parameters. We also generalize our method to distributions with unbounded or infinite support. Numerical results show that the SMSS and its generalization have a substantial advantage in sampling multimodal distributions compared to the ordinary Markov chain Monte Carlo algorithm and some popular variants. Supplemental materials for the article are available online. © 2018 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
2,10.1080/15598608.2017.1299058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017408331&doi=10.1080%2f15598608.2017.1299058&partnerID=40&md5=38f9784b668c591c4edc1d9a6d5f5c4d,"In medical studies, the monotone partial likelihood is frequently encountered in the analysis of time-to-event data using the Cox model. For example, with a binary covariate, the subjects can be classified into two groups. If the event of interest does not occur (zero event) for all the subjects in one of the groups, the resulting partial likelihood is monotone and consequently, the covariate effects are difficult to estimate. In this article, we develop both Bayesian and frequentist approaches using a data-dependent Jeffreys-type prior to handle the monotone partial likelihood problem. We first carry out an in-depth examination of the conditions of the monotone partial likelihood and then characterize sufficient and necessary conditions for the propriety of the Jeffreys-type prior. We further study several theoretical properties of the Jeffreys-type prior for the Cox model. In addition, we propose two variations of the Jeffreys-type prior: the shifted Jeffreys-type prior and the Jeffreys-type prior based on the first risk set. An efficient Markov-chain Monte Carlo algorithm is developed to carry out posterior computation. We perform extensive simulations to examine the performance of parameter estimates and demonstrate the applicability of the proposed method by analyzing real data from the SEER prostate cancer study. © 2018 Grace Scientific Publishing, LLC."
1,10.1080/01621459.2016.1255636,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033691180&doi=10.1080%2f01621459.2016.1255636&partnerID=40&md5=b72ab08a1f69d064b7da2c2ebae38343,"A natural Bayesian approach for mixture models with an unknown number of components is to take the usual finite mixture model with symmetric Dirichlet weights, and put a prior on the number of components—that is, to use a mixture of finite mixtures (MFM). The most commonly used method of inference for MFMs is reversible jump Markov chain Monte Carlo, but it can be nontrivial to design good reversible jump moves, especially in high-dimensional spaces. Meanwhile, there are samplers for Dirichlet process mixture (DPM) models that are relatively simple and are easily adapted to new applications. It turns out that, in fact, many of the essential properties of DPMs are also exhibited by MFMs—an exchangeable partition distribution, restaurant process, random measure representation, and stick-breaking representation—and crucially, the MFM analogues are simple enough that they can be used much like the corresponding DPM properties. Consequently, many of the powerful methods developed for inference in DPMs can be directly applied to MFMs as well; this simplifies the implementation of MFMs and can substantially improve mixing. We illustrate with real and simulated data, including high-dimensional gene expression data used to discriminate cancer subtypes. Supplementary materials for this article are available online. © 2018 American Statistical Association."
,10.1080/00480169.2017.1391723,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032833544&doi=10.1080%2f00480169.2017.1391723&partnerID=40&md5=f8b1895c3437346a2350fdc50869eb9a,"AIMS: To determine the sensitivity (Se) and specificity (Sp) of pregnancy diagnosis using transrectal ultrasonography and an ELISA for pregnancy-associated glycoprotein (PAG) in milk, in lactating dairy cows in seasonally calving herds approximately 85–100 days after the start of the herd’s breeding period. METHODS: Paired results were used from pregnancy diagnosis using transrectal ultrasonography and ELISA for PAG in milk carried out approximately 85 and 100 days after the start of the breeding period, respectively, from 879 cows from four herds in Victoria, Australia. A Bayesian latent class model was used to estimate the proportion of cows pregnant, the Se and Sp of each test, and covariances between test results in pregnant and non-pregnant cows. Prior probability estimates were defined using beta distributions for the expected proportion of cows pregnant, Se and Sp for each test, and covariances between tests. Markov Chain Monte Carlo iterations identified posterior distributions for each of the unknown variables. Posterior distributions for each parameter were described using medians and 95% probability (i.e. credible) intervals (PrI). The posterior median estimates for Se and Sp for each test were used to estimate positive predictive and negative predictive values across a range of pregnancy proportions. RESULTS: The estimate for proportion pregnant was 0.524 (95% PrI = 0.485–0.562). For pregnancy diagnosis using transrectal ultrasonography, Se and Sp were 0.939 (95% PrI = 0.890–0.974) and 0.943 (95% PrI = 0.885–0.984), respectively; for ELISA, Se and Sp were 0.963 (95% PrI = 0.919–0.990) and 0.870 (95% PrI = 0.806–0.931), respectively. The estimated covariance between test results was 0.033 (95% PrI = 0.008–0.046) and 0.035 (95% PrI = 0.018–0.078) for pregnant and non-pregnant cows, respectively. Pregnancy diagnosis results using transrectal ultrasonography had a higher positive predictive value but lower negative predictive value than results from the ELISA across the range of pregnancy proportions assessed. CONCLUSIONS AND CLINICAL RELEVANCE: Pregnancy diagnosis using transrectal ultrasonography and ELISA for PAG in milk had similar Se but differed in predictive values. Pregnancy diagnosis in seasonally calving herds around 85–100 days after the start of the breeding period using the ELISA is expected to result in a higher negative predictive value but lower positive predictive value than pregnancy diagnosis using transrectal ultrasonography. Thus, with the ELISA, a higher proportion of the cows with negative results will be non-pregnant, relative to results from transrectal ultrasonography, but a lower proportion of cows with positive results will be pregnant. © 2017 New Zealand Veterinary Association."
,10.1002/andp.201700214,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044290374&doi=10.1002%2fandp.201700214&partnerID=40&md5=78bc02cafc30816c7346ff319173b744,"From its inception in the 1950s to the modern frontiers of applied statistics, Markov chain Monte Carlo has been one of the most ubiquitous and successful methods in statistical computing. The development of the method in that time has been fueled by not only increasingly difficult problems but also novel techniques adopted from physics. Here, the history of Markov chain Monte Carlo is reviewed from its inception with the Metropolis method to the contemporary state-of-the-art in Hamiltonian Monte Carlo, focusing on the evolving interplay between the statistical and physical perspectives of the method. © 2018 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim."
,10.1117/12.2293588,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047350507&doi=10.1117%2f12.2293588&partnerID=40&md5=a7fcb2dfd5ea643b6475db7a3b13f54c,"Image guided diagnostics and therapy comprise decisions concerning treatment and intervention based on registered image data of patients in many clinical settings. Therefore, knowledge about the reliability of the registration result is crucial. In this paper, we tackle this issue by estimating the registration uncertainty based on Bayesian analysis, i.e. examining the posterior distribution of parameters describing the underlying transformations. The intractability of posterior distributions allows only an approximation, usually realized by Monte Carlo sampling methods. Conventional Markov Chain Monte Carlo (MCMC) algorithms require a large number of posterior samples to ensure robust estimates, which inflicts a high computational burden. The contribution of this work is the embedding of the MCMC approach into a cost reducing multilevel framework. Multilevel MCMC fits into the multi-resolution framework usually applied for image registration. In this work we evaluate the performance of our method using a B-spline transformation framework, i.e. the B-spline coefficients are the parameters to estimate. We demonstrate its correctness by comparison with a ground-truth of the posterior distribution, evaluate the efficiency through examination of the cost reduction and show the reliability as uncertainty estimator on brain MRI images. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only."
3,10.1016/j.advwatres.2017.11.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034570705&doi=10.1016%2fj.advwatres.2017.11.011&partnerID=40&md5=b74719038d48c02654016e286330a727,"Particle Filters (PFs) have received increasing attention by researchers from different disciplines including the hydro-geosciences, as an effective tool to improve model predictions in nonlinear and non-Gaussian dynamical systems. The implication of dual state and parameter estimation using the PFs in hydrology has evolved since 2005 from the PF-SIR (sampling importance resampling) to PF-MCMC (Markov Chain Monte Carlo), and now to the most effective and robust framework through evolutionary PF approach based on Genetic Algorithm (GA) and MCMC, the so-called EPFM. In this framework, the prior distribution undergoes an evolutionary process based on the designed mutation and crossover operators of GA. The merit of this approach is that the particles move to an appropriate position by using the GA optimization and then the number of effective particles is increased by means of MCMC, whereby the particle degeneracy is avoided and the particle diversity is improved. In this study, the usefulness and effectiveness of the proposed EPFM is investigated by applying the technique on a conceptual and highly nonlinear hydrologic model over four river basins located in different climate and geographical regions of the United States. Both synthetic and real case studies demonstrate that the EPFM improves both the state and parameter estimation more effectively and reliably as compared with the PF-MCMC. © 2017 Elsevier Ltd"
,10.1007/978-981-10-8108-8_38,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042080306&doi=10.1007%2f978-981-10-8108-8_38&partnerID=40&md5=0e60ad43d93bd2324912623802b4b4db,"A novel framework of particle filter, named bidirectional Markov chain Monte Carlo particle filter (BMCMCPF), has been proposed to estimate articulated human movement state and action category jointly. Owing to the reason that we regard action category as the estimated state in our framework, firstly the motion models for every possible action are built via autoregressive modeling for the captured motion data with minimum distance. Meanwhile, the dynamic model and observation model also get coupled so that tracking and recognition can achieve synchronously. Then, the state estimation is completed by using the bidirectional Marko chain Monte Carlo sampling. BMCMCPF can not only improve the tracking performance as its global optimization property, but also smooth the joint’s movement trajectories to ensure the motion coordination. The experimental results on HumanEva datasets show that the effectiveness of BMCMCPF with unknown motion modality in solving the tracking problem. © 2018, Springer Nature Singapore Pte Ltd."
,10.1007/978-3-319-68385-0_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032705319&doi=10.1007%2f978-3-319-68385-0_12&partnerID=40&md5=023f8dfa60faf8bbec4c6dad9cdfc584,"In this paper, we present three different approaches for feature selection, starting from a naïve Markov Chain Monte Carlo random walk algorithm to more refined methods like simulated annealing and genetic algorithms. It is typical for textual data to have thousands of dimensions in their feature space which makes feature selection a crucial phase before the final classification. Classification of legal documents into eight categories was performed via a simple document similarity measure based on term frequency and the nearest neighbour concept. With an average success rate of 76.4%, the random walk algorithm not only performed better than the simulated annealing and genetic algorithms but also matched the accuracy of support vector machines. Although these methods have commonly been used for selecting appropriate features in other fields, their use in text categorisation have not been satisfactorily investigated. And, to our knowledge, this is the first work which investigates their use in the legal domain. This generic text classification framework can further be enhanced by using an active learning methodology for the selection of training samples rather than following a passive learning approach. © Springer International Publishing AG 2018."
,10.1137/17M1144301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053525284&doi=10.1137%2f17M1144301&partnerID=40&md5=8516088afd463b46279151fde6b98371,"Markov chain Monte Carlo (MCMC) has been widely used to approximate the expectation of the statistic of a given probability measure \pi on a finite set, and the asymptotic variance is a typical approach to evaluating the performance of MCMC methods. In this paper, we provide a lower bound of the worst-case analysis of the asymptotic variance over general Markov chains with invariant probability \pi , reversible as well as nonreversible ones, and construct an optimal transition matrix that achieves this lower bound. In fact, for any statistic f to be evaluated, an MCMC with the constructed optimal transition matrix produces a smaller asymptotic variance than independent sampling. © 2018 Society for Industrial and Applied Mathematics."
,10.2514/1.J055947,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043253515&doi=10.2514%2f1.J055947&partnerID=40&md5=a9ffe682af2d74fe1f34fa8711e225e2,"Tail modelingis anefficient method used in reliability estimationofhighly safe structures. Classical tail modeling is basedonperforming limit-state functionevaluations throughasampling scheme, selectingathreshold valuetospecify the tail part of the cumulative distribution function, fitting a proper model to the tail part, and estimating the reliability. In this approach, limit-state function calculations that do not belong to the tail part are mostly discarded, and so majority of limit-state evaluations are wasted. In this paper, Markov chain Monte Carlo method with Metropolis-Hastings algorithm is used to draw samples from the tail part only so that a more accurate reliability index prediction is achieved. A commonly used proposal distribution formula is modified by using a scale parameter. The optimal value of this scale parameter is obtained for various numerical example problems with a varying number of random variables, and an approximate relationship is obtained between the optimal value of the scale parameter and the number of random variables. The approximate relationship is tested on the reliability prediction of a horizontal axis wind turbine and observed to work well. It is also found that the proposed approach is more accurate than the classical tail modeling when the number of variables is less than or equal to four. For a larger number of random variables, none of the two approaches are found to be superior to another. Copyright © 2017 by Gamze Bayrak and Erdem Acar. Published by the American Instituteof Aeronautics and Astronautics, Inc., with permission."
,10.1504/IJCSM.2018.091733,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047367230&doi=10.1504%2fIJCSM.2018.091733&partnerID=40&md5=a7f0cae6a2d6e2aa632c51dc51d06373,"Sequential Monte Carlo (SMC) methods (also known as particle filter) provide a way to solve the state estimation problem in nonlinear non-Gaussian state space models (SSM) through numerical approximation. Particle smoothing is one retrospective state estimation method based on particle filtering. In this paper, we propose a new particle smoother. The basic idea is easy and leads to a forward-backward procedure, where the Metropolis-Hastings algorithm is used to resample the filtering particles. The goodness of the new scheme is assessed using a nonlinear SSM. It is concluded that this new particle smoother is suitable for state estimation in complicated dynamical systems. Copyright © 2018 Inderscience Enterprises Ltd."
1,10.2514/1.G002296,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039853757&doi=10.2514%2f1.G002296&partnerID=40&md5=8963d27af91fef88acdb712ff9d96374,"This paper presents a new approach to estimate an observed space object's shape, while also inferring other attributes, such as its inertial attitude and surface parameters. An adaptive Hamiltonian Markov chain Monte Carlo estimation approach is employed, which uses light-curve data and process inversion to estimate the shape and other attributes. The main advantage of this approach over previous ones is that it can estimate these attributes simultaneously, whereas previous approaches typically rely on a priori knowledge of one or more of them to estimate a particular attribute. Also, unlike previous approaches, the new approach is shown to work well for relatively high dimensions and non-Gaussian distributions of the light-curve-inversion problem. Simulation results involving singleand multiple-faceted objects are shown. Good results are obtained for all cases. © Copyright 2017 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051540699&partnerID=40&md5=4e78e977526d995c1046cada2f83a8aa,"Performance test is an important part of the commissioning procedure after natural gas transmission pipeline construction. It is common that the natural gas pipeline always experience unsteady injection and withdrawal situations at different meter stations. This makes the estimation of pipeline parameters very difficult due to the uncertainty and dynamics natural of gas flow. A Markov Chain Monte Carlo (MCMC) method is introduced in this paper to find pipeline parameters that enable the best match to the performance testing data. The parameters to be determined using this method include the specific gravity of the gas, the effective roughness of the pipeline, black powder mass concentration and average particle size. The influence of black powder on gas pipeline pressure loss is included based the experience of a commissioned gas pipeline. The method presented in this paper consists of three major steps: data collection, data uncertainty modeling, and the MCMC simulation to determine the most likelihood parameter. Synchronized pressure, temperature, and flow rate data from compressor stations, mainline block valves, and metering stations are collected from the DCS as the basis for input and benchmarking. The variation of gas pressure, temperature, and flow rate at all meter stations are statistically analyzed to derive the probability distribution for fluctuations. In addition the probability distribution for uncertainty (and reliability) of flow meters, pressure and temperature transmitters are modeled based on published data. The specific gravity, black powder concentration and average size variation are then added with uncertainties. The final step is the estimate of the specific gravity, effective pipeline roughness, and black power concentration and sizes based on the MCMC simulations. A real long distance gas pipeline performance test result is presented as an example to show the method is used. Instead of identifying a single parameter such as pipeline roughness the proposed method helps to provide estimate of multiple parameters based on the data. The results show better matching between the measurement data and hydraulics model prediction because of the consideration of more physical aspects. © Copyright 2018, PSIG, Inc."
,10.1615/Int.J.UncertaintyQuantification.2018021551,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048193047&doi=10.1615%2fInt.J.UncertaintyQuantification.2018021551&partnerID=40&md5=1abcebae8b076d3145e02f24224a778b,"In this paper, we consider computing expectations with respect to probability laws associated with a certain class of stochastic systems. In order to achieve such a task, one must not only resort to numerical approximation of the expectation but also to a biased discretization of the associated probability. We are concerned with the situation for which the discretization is required in multiple dimensions, for instance in space-time. In such contexts, it is known that the multi-index Monte Carlo (MIMC) method of Haji-Ali, Nobile, and Tempone, (Numer. Math., 132, pp. 767– 806, 2016) can improve on independent identically distributed (i.i.d.) sampling from the most accurate approximation of the probability law. Through a nontrivial modification of the multilevel Monte Carlo (MLMC) method, this method can reduce the work to obtain a given level of error, relative to i.i.d. sampling and even to MLMC. In this paper, we consider the case when such probability laws are too complex to be sampled independently, for example a Bayesian inverse problem where evaluation of the likelihood requires solution of a partial differential equation model, which needs to be approximated at finite resolution. We develop a modification of the MIMC method, which allows one to use standard Markov chain Monte Carlo (MCMC) algorithms to replace independent and coupled sampling, in certain contexts. We prove a variance theorem for a simplified estimator that shows that using our MIMCMC method is preferable, in the sense above, to i.i.d. sampling from the most accurate approximation, under appropriate assumptions. The method is numerically illustrated on a Bayesian inverse problem associated to a stochastic partial differential equation, where the path measure is conditioned on some observations. © 2018 by Begell House, Inc."
,10.3233/978-1-61499-843-3-159,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043576869&doi=10.3233%2f978-1-61499-843-3-159&partnerID=40&md5=7d3b3c8c1dfea9a4ed94ffc7ff4de6e9,"Calibration of individual based models (IBMs), successful in modeling complex ecological dynamical systems, is often performed only ad-hoc. Bayesian inference can be used for both parameter estimation and uncertainty quantification, but its successful application to realistic scenarios has been hindered by the complex stochastic nature of IBMs. Computationally expensive techniques such as Particle Filter (PF) provide marginal likelihood estimates, where multiple model simulations (particles) are required to get a sample from the state distribution conditional on the observed data. Particle ensembles are re-sampled at each data observation time, requiring particle destruction and replication, which lead to an increase in algorithmic complexity. We present SPUX, a Python implementation of parallel Particle Markov Chain Monte Carlo (PMCMC) algorithm, which mitigates high computational costs by distributing particles over multiple computational units. Adaptive load re-balancing techniques are used to mitigate computational work imbalances introduced by re-sampling. Framework performance is investigated and significant speed-ups are observed for a simple predator-prey IBM model. © 2018 The authors and IOS Press."
,10.1111/gean.12135,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028765512&doi=10.1111%2fgean.12135&partnerID=40&md5=716fe9f5cd617ec4e06a59206ad76699,"Spatial econometric specifications pose unique computational challenges to Bayesian analysis, making it difficult to estimate models efficiently. In the literature, the main focus has been on extending Bayesian analysis to increasingly complex spatial models. The stochastic efficiency of commonly used Markov Chain Monte Carlo (MCMC) samplers has received less attention by comparison. Specifically, Bayesian methods to analyze effective sample size and samplers that provide large effective size have not been thoroughly considered in the literature. Thus, we compare three MCMC techniques: the familiar Metropolis-within-Gibbs sampling, Slice-within-Gibbs sampling, and Hamiltonian Monte Carlo. The latter two methods, while common in other domains, are not as widely encountered in Bayesian spatial econometrics. We assess these methods across four different scenarios in which we estimate the spatial autoregressive parameter in a mixed regressive, spatial autoregressive specification (or, spatial lag model). We find that off-the-shelf implementations of the newer high-yield simulation techniques require significant adaptation to be viable. We further find that the effective sizes are often significantly smaller than nominal sizes. In addition, we find that stopping simulation early may understate posterior credible interval widths when effective sample size is small. More broadly, we suggest that sample information and stopping rules deserve more attention in both applied and basic Bayesian spatial econometric research. © 2017 The Ohio State University"
,10.1186/s12859-017-2003-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042509162&doi=10.1186%2fs12859-017-2003-3&partnerID=40&md5=7ffb2a1c1ed33de451750d2b5c692ecb,"Background: Running multiple-chain Markov Chain Monte Carlo (MCMC) provides an efficient parallel computing method for complex Bayesian models, although the efficiency of the approach critically depends on the length of the non-parallelizable burn-in period, for which all simulated data are discarded. In practice, this burn-in period is set arbitrarily and often leads to the performance of far more iterations than required. In addition, the accuracy of genomic predictions does not improve after the MCMC reaches equilibrium. Results: Automatic tuning of the burn-in length for running multiple-chain MCMC was proposed in the context of genomic predictions using BayesA and BayesCπ models. The performance of parallel computing versus sequential computing and tunable burn-in MCMC versus fixed burn-in MCMC was assessed using simulation data sets as well by applying these methods to genomic predictions of a Chinese Simmental beef cattle population. The results showed that tunable burn-in parallel MCMC had greater speedups than fixed burn-in parallel MCMC, and both had greater speedups relative to sequential (single-chain) MCMC. Nevertheless, genomic estimated breeding values (GEBVs) and genomic prediction accuracies were highly comparable between the various computing approaches. When applied to the genomic predictions of four quantitative traits in a Chinese Simmental population of 1217 beef cattle genotyped by an Illumina Bovine 770 K SNP BeadChip, tunable burn-in multiple-chain BayesCπ (TBM-BayesCπ) outperformed tunable burn-in multiple-chain BayesCπ (TBM-BayesA) and Genomic Best Linear Unbiased Prediction (GBLUP) in terms of the prediction accuracy, although the differences were not necessarily caused by computational factors and could have been intrinsic to the statistical models per se. Conclusions: Automatically tunable burn-in multiple-chain MCMC provides an accurate and cost-effective tool for high-performance computing of Bayesian genomic prediction models, and this algorithm is generally applicable to high-performance computing of any complex Bayesian statistical model. © The Author(s). 2018."
,10.2298/TSCI1804673T,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052440492&doi=10.2298%2fTSCI1804673T&partnerID=40&md5=46f5c8bfbafc3fca7d0c28fef22e9288,A thermal problem can be always modeled using an integral equation. This paper uses the Monte Carlo method based on the simulation of a continuous Markov chain to solve Fredholm integral equations of the second kind. Some examples are given to show the efficiency of the present work. © 2018 Society of Thermal Engineers of Serbia.
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054593969&partnerID=40&md5=69b01ecb1b99d2abb99d7e660ea86b14,"Ensemble Kalman filter (EnKF) and Markov chain Monte Carlo (MCMC) are popular methods to obtain the posterior distribution of unknown parameters in the reservoir model. However, millions of simulation runs may be required in MCMC for accurate sampling of posterior as subsurface flow problems are highly nonlinear and non-Gaussian. Similarly, EnKF formulated on the basis of linear and Gaussian assumptions may also require a large number of realizations to correctly map the solution space of the unknown model parameters, ultimately resulting in the high computational cost. Data-driven meta/surrogate/proxy models provide an alternative solution to alleviate the issue of high computational cost. Since these models are not as accurate as numerical solutions of partial differential equations (PDE), their implementation may add an uncertainty in the forecast model. In literature, the effect of uncertainty in forecast model on data assimilation is not well studied, especially with field-scale reservoir models. In this work, we propose the robust assisted history matching workflow using polynomial chaos expansion (PCE) based forecast model. Proposed forecast model relies on reducing parameter space using Karhunen-Loeve (KL) expansion which preserves the two-point statistics of the field. Random variables from KL expansion and orthogonal polynomials corresponding to the prior probability density function (pdf) form the set of input parameters in PCE. Further, non-intrusive probabilistic collocation method (PCM) is used to compute PCE coefficients. PCE forecast model is then used in EnKF and MCMC to calculate the likelihood of the samples in place of high fidelity full physics simulation runs. A case study is performed using a 3D field scale model of a reservoir located near Fort McMurray in northern Alberta, Canada. Performance of EnKF and MCMC are assessed under forecast model uncertainty using rigorous qualitative and quantitative analysis and posterior distribution characterization. Results clearly depict that, although EnKF provided reliable mean and variance estimates of model parameters, MCMC outperformed the former even under the uncertainty associated with PCE metamodel. Inaccurate initial assumptions of model parameters were successfully handled by MCMC, although, with a longer burn-in period. Furthermore, characterization of posterior demonstrated reduced uncertainty in the estimation of model parameters using MCMC as compared to EnKF. Practical implications of the proposed approach and performance assessment under forecast model uncertainty will be consequential in designing accurate and computationally efficient reservoir characterization and optimization workflows and hence, improved decision-making in reservoir management. © 2018 European Association of Geoscientists and Engineers EAGE. All rights reserved."
,10.1007/s11222-018-9828-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051651404&doi=10.1007%2fs11222-018-9828-0&partnerID=40&md5=108b936ccb89ddf08fd165afda6518ce,"Bayesian analysis often concerns an evaluation of models with different dimensionality as is necessary in, for example, model selection or mixture models. To facilitate this evaluation, transdimensional Markov chain Monte Carlo (MCMC) relies on sampling a discrete indexing variable to estimate the posterior model probabilities. However, little attention has been paid to the precision of these estimates. If only few switches occur between the models in the transdimensional MCMC output, precision may be low and assessment based on the assumption of independent samples misleading. Here, we propose a new method to estimate the precision based on the observed transition matrix of the model-indexing variable. Assuming a first-order Markov model, the method samples from the posterior of the stationary distribution. This allows assessment of the uncertainty in the estimated posterior model probabilities, model ranks, and Bayes factors. Moreover, the method provides an estimate for the effective sample size of the MCMC output. In two model selection examples, we show that the proposed approach provides a good assessment of the uncertainty associated with the estimated posterior model probabilities. © 2018, The Author(s)."
,10.1137/17M1134640,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049791282&doi=10.1137%2f17M1134640&partnerID=40&md5=e71098878079ee5690b78353eaa207cf,"We introduce a new framework for efficient sampling from complex probability distributions, using a combination of transport maps and the Metropolis-Hastings rule. The core idea is to use deterministic couplings to transform typical Metropolis proposal mechanisms (e.g., random walks, Langevin methods) into non-Gaussian proposal distributions that can more effectively explore the target density. Our approach adaptively constructs a lower triangular transport map-an approximation of the Knothe-Rosenblatt rearrangement-using information from previous Markov chain Monte Carlo (MCMC) states, via the solution of an optimization problem. This optimization problem is convex regardless of the form of the target distribution and can be solved efficiently without gradient information from the target probability distribution; the target distribution is instead represented via samples. Sequential updates enable efficient and parallelizable adaptation of the map even for large numbers of samples. We show that this approach uses inexact or truncated maps to produce an adaptive MCMC algorithm that is ergodic for the exact target distribution. Numerical demonstrations on a range of parameter inference problems show order-of-magnitude speedups over standard MCMC techniques, measured by the number of effectively independent samples produced per target density evaluation and per unit of wallclock time. © 2018 U.S. Government."
,10.1093/pasj/psx150,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042215259&doi=10.1093%2fpasj%2fpsx150&partnerID=40&md5=b7ad1a2cc70bbdc2021be536187710b7,"We fit the spectral energy distributions (SEDs) of 46 GeV-TeV BL Lac objects in the frame of leptonic one-zone synchrotron self-Compton (SSC) model and investigate the physical properties of these objects. We use the Markov chain Monte Carlo (MCMC) method to obtain the basic parameters, such as magnetic field (B), the break energy of the relativistic electron distribution (γ′b), and the electron energy spectral index. Based on the modeling results, we support the following scenarios for GeV-TeV BL Lac objects. (1) Some sources have large Doppler factors, implying other radiation mechanism should be considered. (2) Compared with flat spectrum quasars (FSRQs), GeV-TeV BL Lac objects have weaker magnetic fields and larger Doppler factors, which cause the ineffective cooling and shift the SEDs to higher bands. Their jet powers are around 4.0 × 1045 erg s-1, compared with radiation power, 5.0 × 1042 erg s-1, indicating that only a small fraction of jet power is transformed into the emission power. (3) For some BL Lacs with large Doppler factors, their jet components could have two substructures, e.g., the fast core and the slow sheath. For most GeV-TeV BL Lacs, Kelvin. Helmholtz instabilities are suppressed by their higher magnetic fields, leading to micro-variability or intro-day variability in the optical bands. (4) Combined with a sample of FSRQs, an anti-correlation between the peak luminosity, Lpk, and the peak frequency, νpk, is obtained, favoring the blazar sequence scenario. In addition, an anti-correlation between the jet power, Pjet, and the break Lorentz factor, γb, also supports the blazar sequence. © The Author(s) 2018."
1,10.1214/17-BA1084,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054672211&doi=10.1214%2f17-BA1084&partnerID=40&md5=310215d0a97e9bdbb2c2749f9f1c761d,"We discuss a few principles to guide the design of efficient Metropolis- Hastings proposals for well-behaved target distributions without deeply divided modes. We illustrate them by developing and evaluating novel proposal kernels using a variety of target distributions. Here, efficiency is measured by the variance ratio relative to the independent sampler. The first principle is to introduce negative correlation in the MCMC sample or to reduce positive correlation: to propose something new, propose something different. This explains why singlemoded proposals such as the Gaussian random-walk is poorer than the uniform random walk, which is in turn poorer than the bimodal proposals that avoid values very close to the current value. We evaluate three new bimodal proposals called Box, Airplane and StrawHat, and find that they have similar performance to the earlier Bactrian kernels, suggesting that the general shape of the proposal matters, but not the specific distributional form. We propose the ""Mirror"" kernel, which generates new values around the mirror image of the current value on the other side of the target distribution (effectively the ""opposite"" of the current value). This introduces negative correlations, leading in many cases to efficiency of > 100%. The second principle, applicable to multidimensional targets, is that a sequence of well-designed one-dimensional proposals can be more efficient than a single d-dimensional proposal. Thirdly, we suggest that variable transformation be explored as a general strategy for designing efficient MCMC kernels. We apply these principles to a high-dimensional Gaussian target with strong correlations, a logistic regression problem and a molecular clock dating problem to illustrate their practical utility. © 2018 International Society for Bayesian Analysis."
,10.2514/6.2018-1975,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044351687&doi=10.2514%2f6.2018-1975&partnerID=40&md5=061066afa67f3e2c6ee6385919defd1e,"The lack of observability inherent in the linearized dynamics model for angles-only relative navigation between two satellites in close proximity has been well established by numerous studies, showing that an infinite set of possible relative orbits satisfy the observations. This work seeks a probabilistic method of angles-only orbit determination and to study this problem using the full nonlinear formulation. The lack of range observability in the problem makes Gaussian approximations a poor representation of the solution probability density, and motivates higher fidelity approaches than typical Markov Chain Monte Carlo approaches for probability distribution sampling. In order to achieve this, Hamiltonian Monte Carlo sampling of a theoretical probability distribution of possible solutions is explored, which is known to be more successful for high dimensional problems. The technique is performed on several angles-only measurement cases with increasingly difficult observability, including close proximity and coplanar cases. It is observed that when tuned correctly, Hamiltonian Monte Carlo sampling can successfully resolve the probability distributions of the possible deputy states, showing increasingly non-Gaussian behavior as observability is limited. Additionally, Hamiltonian Monte Carlo achieves this much more efficiently than traditional Markov Chain Monte Carlo techniques. © 2018, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved."
1,10.1137/17M1112595,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046769002&doi=10.1137%2f17M1112595&partnerID=40&md5=01af8c5d8d22360fd798c9c4d6afa53a,"In this article we consider static Bayesian parameter estimation for partially observed diffusions that are discretely observed. We work under the assumption that one must resort to discretizing the underlying diffusion process, for instance, using the Euler–Maruyama method. Given this assumption, we show how one can use Markov chain Monte Carlo (MCMC) and particularly particle MCMC [C. Andrieu, A. Doucet, and R. Holenstein, J. R. Stat. Soc. Ser. B Stat. Methodol., 72 (2010), 269–342] to implement a new approximation of the multilevel (ML) Monte Carlo (MC) collapsing sum identity. Our approach comprises constructing an approximate coupling of the posterior density of the joint distribution over parameter and hidden variables at two different discretization levels and then correcting by an importance sampling method. The variance of the weights are independent of the length of the observed data set. The utility of such a method is that, for a prescribed level of mean square error, the cost of this MLMC method is provably less than i.i.d. sampling from the posterior associated to the most precise discretization. However the method here comprises using only known and efficient simulation methodologies. The theoretical results are illustrated by inference of the parameters of two prototypical processes given noisy partial observations of the process: the first is an Ornstein–Uhlenbeck process and the second is a more general Langevin equation. © 2018 Society for Industrial and Applied Mathematics."
,10.1016/j.automatica.2017.09.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034062676&doi=10.1016%2fj.automatica.2017.09.025&partnerID=40&md5=9ea92170f72cd244bfa7b5ec7f8ff495,"This paper considers the problem of initial uncertainty forecasting in deterministic nonlinear continuous-time dynamical systems via particle ensembles. The popular Monte Carlo method, which while simple to implement, faces fundamental issues. In particular, it is not clear how well the propagated particles continue to represent the true state-uncertainty at future times. This paper evaluates the performance of Monte Carlo forecasting by analyzing it in the context of Markov chain Monte Carlo (MCMC) theory. The propagated ensemble is viewed as the realization of a Markov chain at each time instant, generated by an associated instantaneous transition kernel. It is shown that for a special class of nonlinear systems that have zero divergence, the propagated kernel is in detailed balance with the true state probability density function. This guarantees statistical consistency of the Monte Carlo ensemble with the truth at all times for such systems. On the other hand, no such guarantee is possible for systems with non-zero divergence. © 2017 Elsevier Ltd"
1,10.1137/17M1149250,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049488912&doi=10.1137%2f17M1149250&partnerID=40&md5=9b511a0dbfb0b0e57057e46d166992f7,"The point spread function (PSF) of a translation invariant imaging system is its impulse response, which cannot always be measured directly. This is the case in high-energy X-ray radiography, and the PSF must be estimated from images of calibration objects indirectly related to the impulse response. When the PSF is assumed to have radial symmetry, it can be estimated from an image of an opaque straight edge. We use a nonparametric Bayesian approach, where the prior probability density for the PSF is modeled as a Gaussian Markov random field and radial symmetry is incorporated in a novel way. Markov chain Monte Carlo posterior estimation is carried out by adapting a recently developed improvement to the Gibbs sampling algorithm, referred to as partially collapsed Gibbs sampling. Moreover, the algorithm we present is proven to satisfy invariance with respect to the target density. Finally, we demonstrate the efficacy of these methods on radiographic data obtained from a high-energy X-ray diagnostic system at the U. S. Department of Energy's Nevada National Security Site. © 2018 Mission Support and Test Services, LLC contractor to US DOE."
1,10.1016/j.mineng.2017.10.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032302270&doi=10.1016%2fj.mineng.2017.10.017&partnerID=40&md5=a80d0ead7d82549825c0d79da714319b,"Gold dissolution was investigated in ferric chloride solution, being one alternative cyanide-free leaching media of increasing interest. The effect of process variables ([Fe3+] = 0.02–1.0 M, [Cl−] = 2–5 M, pH = 0–1.0, T = 25–95 °C) on reaction mechanism and kinetics were studied electrochemically using rotating disk electrode with ωcyc = 100–2500 RPM and Tafel method. The highest gold dissolution rate (7.3 · 10−4 mol m−2 s−1) was achieved at 95 °C with [Fe3+] = 0.5 M, [Cl−] = 4 M, pH =1.0 and ωcyc = 2500 RPM. Increase in gold dissolution rate was observed with increase in temperature, ferric ion concentration and chloride concentration, but gold dissolution rate did not have a clear dependency on pH. Redox potential was found to vary between 636 and 741 mV vs. SCE during experiments. According to the calculated equilibrium and measured open circuit potentials, gold was suggested to dissolve as aurous ion Au+ and form AuCl2 −, rather than auric ion Au3+ and form AuCl4 −. Further, it is suggested that AuCl2 − does not oxidize to AuCl4 − under the investigated conditions. Levich plot and the calculated activation energies suggested that gold dissolution was limited by mass and electron transfer. According to a mechanistic kinetic model developed in the current work, intrinsic surface reaction mainly controls gold dissolution, especially at higher rotational speeds (&gt;1000 RPM). Uncertainties in the model parameters of the mechanistic kinetic model were studied with Markov chain Monte Carlo methods. © 2017 The Authors"
,10.1007/s00521-018-3718-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053494204&doi=10.1007%2fs00521-018-3718-4&partnerID=40&md5=966746dfa4791306426d49bb90d7d7af,"Due to the risk in outsourcing, supplier selection is a critical issue for companies. Considerable evidence shows that among the criteria for selecting a supplier, quality is the most critical factor and the process yield index is an efficient tool for assessing the process quality of the supplier. Although the frequentist approach has been adopted to discriminate the degrees of two yield indices to solve the supplier selection problem, unknown parameters must be estimated from samples, which potentially introduce uncertainty into the statistical testing process. Instead of the frequentist inference, this study proposes using the Bayesian inference to derive the posterior distribution of the ratio of two yield indices. Furthermore, a Markov chain Monte Carlo technique is applied to discern the empirical posterior distribution of the ratio with the aim of discriminating the degrees of two yield indices. The simulations show that the proposed method is not only of reasonable empirical size but tests well in terms of power. © 2018, The Natural Computing Applications Forum."
,10.1108/IMDS-10-2016-0423,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041132502&doi=10.1108%2fIMDS-10-2016-0423&partnerID=40&md5=ae0ccd1fbc17f88c6557d7e926755310,"Purpose – The purpose of this paper is to examine the effects of atmospherics and affective state on shoppers’ in-store behaviour using the two approaches in structural equation modelling (SEM), i.e. Frequentist and Bayesian approaches. Shoppers’ affective state was tested for its mediating effect on in-store shopping behaviour. Design/methodology/approach – The final sample consists of 382 respondents who were drawn from shoppers at selected apparel stores in six of the most popular shopping malls around Kuala Lumpur (Malaysia). A frequentist approach to SEM is common among researchers and offers generally an analysis of the relationships between multiple latent variables and constructs. Alternatively, the Bayesian SEM (BSEM) approach stems from the diffusion of the model’s posterior distributions using the Markov Chain Monte Carlo technique. More specifically, this technique is inherently more flexible and substantive in determining parameter estimates as compared to the more conventional, the frequentist approach to SEM. Findings – The results show the mixed effects of atmospheric cues in retail setting on shoppers’ affective state. More specifically, the positive direct effect of atmospheric cues (music) on in-store behaviour was confirmed while other atmospheric cues (colour and store layout) were found to be fully mediated by affective state. The Bayesian approach was able to offer more distinctive results complementing the frequentist approach. Research limitations/implications – Although the current sample size is adequate, it will be interesting to examine how a bigger sample size and different antecedents of in-store behaviour in retailing can affect the comparison between the frequentist approach in SEM and BSEM. Practical implications – The authors found that a combination of well-designed store atmospherics and layout store can produce pleasurable effects on shoppers resulting in positive affective state. This study found that results from both frequentist and Bayesian approaches complement each other and it may be beneficial for future studies to utilise both approaches in SEM. Originality/value – This paper met the aim to compare the approaches in SEM and the need to consider both approaches on in-store shopping environment. Overall, the authors contend that the Bayesian approach to SEM is a potentially viable alternative to frequentist SEM, especially when studies are conducted under dynamic conditions such as apparel retailing. © Emerald Publishing Limited."
1,10.2514/1.T5094,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039730712&doi=10.2514%2f1.T5094&partnerID=40&md5=414907d33e67d941b809a14335bde8fa,"This work investigates the sensitivity of direct simulation Monte Carlo input parameters for a high-temperature hypersonic flow scenario. The Computation of Hypersonic Ionizing Particles in Shocks (CHIPS) direct simulation Monte Carlo code simulates a rarefied hypersonic shock tube experiment in air.Aprevious model has been improved to include 11-species air with charged species collisions and reactions in order to model high-temperature interactions appropriately. CHIPS is used to simulate NASA Electric Arc Shock Tube data for a peak radiative heating lunar return scenario. A global Monte Carlo sensitivity analysis is conducted on this scenario to investigate which reaction rates have the greatest effect on the simulation results. The sensitivity of each reaction rate was measured by calculating the square of the Pearson correlation coefficient and the mutual information for a certain quantity of interest. The most sensitive parameters are identified in preparation for future Markov chain Monte Carlo calibrations with the Electric Arc Shock Tube data. Copyright © 2017 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved."
,10.1155/2018/5148085,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051326931&doi=10.1155%2f2018%2f5148085&partnerID=40&md5=f05a240ccd29334c01a78a79c62eac23,"Despite the wide application of Floating Car Data (FCD) in urban link travel time and congestion estimation, the sparsity of observations from a low penetration rate of GPS-equipped floating cars make it difficult to estimate travel time distribution (TTD), especially when the travel times may have multimodal distributions that are associated with the underlying traffic states. In this case, the study develops a Bayesian approach based on particle filter framework for link TTD estimation using real-time and historical travel time observations from FCD. First, link travel times are classified by different traffic states according to the levels of vehicle delays. Then, a state-transition function is represented as a Transition Probability Matrix of the Markov chain between upstream and current links with historical observations. Using the state-transition function, an importance distribution is constructed as the summation of historical link TTDs conditional on states weighted by the current link state probabilities. Further, a sampling strategy is developed to address the sparsity problem of observations by selecting the particles with larger weights in terms of the importance distribution and a Gaussian likelihood function. Finally, the current link TTD can be reconstructed by a generic Markov Chain Monte Carlo algorithm incorporating high weighted particles. The proposed approach is evaluated using real-world FCD. The results indicate that the proposed approach provides good accurate estimations, which are very close to the empirical distributions. In addition, the approach with different percentage of floating cars is tested. The results are encouraging, even when multimodal distributions and very few or no observations exist. © 2018 Wenwen Qin and Meiping Yun."
,10.1137/17M1151900,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049477025&doi=10.1137%2f17M1151900&partnerID=40&md5=7bd9ff98f5b5b7a5566f5e1095010bb1,"We consider a nonlinear filtering problem whereby the signal obeys the stochastic Navier–Stokes equations and is observed through a linear mapping with additive noise. The setup is relevant to data assimilation for numerical weather prediction and climate modeling, where similar models are used for unknown ocean or wind velocities. We present a particle filtering methodology that uses likelihood-informed importance proposals, adaptive tempering, and a small number of appropriate Markov chain Monte Carlo steps. We provide a detailed design for each of these steps and show in our numerical examples that they are all crucial in terms of achieving good performance and efficiency. © 2018 Society for Industrial and Applied Mathematics."
,10.1137/16M1087667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053896504&doi=10.1137%2f16M1087667&partnerID=40&md5=26e0523216ed2dfa7eec5d5cea07e2aa,"The rotor-router model is a deterministic process analogous to a simple random walk on a graph, and the discrepancy of token configurations between the rotor-router model and its corresponding random walk has been investigated in some contexts. Motivated by general Markov chains beyond simple random walks, this paper investigates a generalized model which imitates a Markov chain (of multiple tokens) possibly containing irrational transition probabilities. We are concerned with the vertexwise discrepancy of the numbers of tokens between the generalized model and its corresponding Markov chain, and present an upper bound of the discrepancy in terms of the mixing time of the Markov chain. © 2018 Society for Industrial and Applied Mathematics."
1,10.1016/j.cja.2017.08.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039935372&doi=10.1016%2fj.cja.2017.08.020&partnerID=40&md5=437000deeb0801fdf6c4c5ed74fb110d,This paper investigates Bayesian methods for aerospace system reliability analysis using various sources of test data and expert knowledge at both subsystem and system levels. Four scenarios based on available information for the priors and test data of a system and/or subsystems are studied using specific Bayesian inference techniques. This paper proposes the Bayesian melding method for integrating subsystem-level priors with system-level priors for both system- and subsystem-level reliability analysis. System and subsystem reliability outcomes are compared under different scenarios. Computational challenges for posterior inferences using the sophisticated Bayesian melding method are addressed using Markov Chain Monte Carlo (MCMC) and adaptive Sampling Importance Re-sampling (SIR) methods. A case study with simulation results illustrates the applications of the proposed methods and provides insights for aerospace system reliability analysis using available multilevel information. © 2017 Chinese Society of Aeronautics and Astronautics.
,10.1016/j.eng.2018.06.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054458087&doi=10.1016%2fj.eng.2018.06.006&partnerID=40&md5=2360346aca164d960a3aad1d79270559,"This study develops a multivariate eco-hydrological risk-assessment framework based on the multivariate copula method in order to evaluate the occurrence of extreme eco-hydrological events for the Xiangxi River within the Three Gorges Reservoir (TGR) area in China. Parameter uncertainties in marginal distributions and dependence structure are quantified by a Markov chain Monte Carlo (MCMC) algorithm. Uncertainties in the joint return periods are evaluated based on the posterior distributions. The probabilistic features of bivariate and multivariate hydrological risk are also characterized. The results show that the obtained predictive intervals bracketed the observations well, especially for flood duration. The uncertainty for the joint return period in “AND” case increases with an increase in the return period for univariate flood variables. Furthermore, a low design discharge and high service time may lead to high bivariate hydrological risk with great uncertainty. © 2018 THE AUTHORS"
,10.1007/978-3-319-99978-4_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053603288&doi=10.1007%2f978-3-319-99978-4_17&partnerID=40&md5=c2bda6db5f3d1043f63d94a130db9899,"Bounded rationality investigates utility-optimizing decision-makers with limited information-processing power. In particular, information theoretic bounded rationality models formalize resource constraints abstractly in terms of relative Shannon information, namely the Kullback-Leibler Divergence between the agents’ prior and posterior policy. Between prior and posterior lies an anytime deliberation process that can be instantiated by sample-based evaluations of the utility function through Markov Chain Monte Carlo (MCMC) optimization. The most simple model assumes a fixed prior and can relate abstract information-theoretic processing costs to the number of sample evaluations. However, more advanced models would also address the question of learning, that is how the prior is adapted over time such that generated prior proposals become more efficient. In this work we investigate generative neural networks as priors that are optimized concurrently with anytime sample-based decision-making processes such as MCMC. We evaluate this approach on toy examples. © 2018, The Author(s)."
,10.1016/j.envsoft.2018.03.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046668997&doi=10.1016%2fj.envsoft.2018.03.008&partnerID=40&md5=f52c576e98ca07d0d6d0fa3ef7c2b1c8,"When studying the empirical phenomenon of wildfires, we can distinguish between the occurrence at a specific location and time and the burnt area measured. This study proposes using structured additive regression models based on zero-one-inflated beta distribution for studying wildfire occurrence and burnt area simultaneously. Beta distribution affords a convenient way of studying the percentage of burnt area in cases where such percentages are bounded away from zero and one. Inflation with zeros and ones enables observations without wildfires or with 100% burnt areas to be treated as special cases. Structured additive regression allows one to include a variety of covariates, while simultaneously exploring spatial and temporal correlations. Our inferences are based on an efficient Markov chain Monte Carlo simulation algorithm utilizing iteratively weighted least squares approximations as proposal densities. Application of the proposed methodology to a large wildfire database covering Galicia (Spain) provides essential information for improved wildfire management. © 2018 Elsevier Ltd"
,10.1137/15M1021751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049428905&doi=10.1137%2f15M1021751&partnerID=40&md5=98baeb8ecef2cd678cd3e61067432e45,"Many scientific and engineering problems require one to perform Bayesian inferences in function spaces, in which the unknowns are of infinite dimension. In such problems, many standard Markov chain Monte Carlo (MCMC) algorithms become arbitrarily slow under the mesh refinement, which is referred to as being dimension dependent. In this work we develop an independence sampler based MCMC method for the Bayesian inferences of functions. We represent the proposal distribution as a mixture of a finite number of specially parametrized Gaussian measures. We also design an efficient adaptive algorithm to adjust the parameter values of the mixtures from the previous samples. Finally we provide numerical examples to demonstrate the efficiency and robustness of the proposed method, even for problems with multimodal posterior distributions. © 2018 Society for Industrial and Applied Mathematics."
,10.1098/rsif.2018.0283,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053038382&doi=10.1098%2frsif.2018.0283&partnerID=40&md5=e74b8b48ba7ab8adc51d626839b72953,"Methods from stochastic dynamical systems theory have been instrumental in understanding the behaviours of chemical reaction networks (CRNs) arising in natural systems. However, considerably less attention has been given to the inverse problem of synthesizing CRNs with a specified behaviour, which is important for the forward engineering of biological systems. Here, we present a method for generating discrete-state stochastic CRNs from functional specifications, which combines synthesis of reactions using satisfiability modulo theories and parameter optimization using Markov chain Monte Carlo. First, we identify candidate CRNs that have the possibility to produce correct computations for a given finite set of inputs. We then optimize the parameters of each CRN, using a combination of stochastic search techniques applied to the chemical master equation, to improve the probability of correct behaviour and rule out spurious solutions. In addition, we use techniques from continuous-time Markov chain theory to analyse the expected termination time for each CRN. We illustrate our approach by synthesizing CRNs for probabilistically computing majority, maximum and division, producing both known and previously unknown networks, including a novel CRN for probabilistically computing the maximum of two species. In future, synthesis techniques such as these could be used to automate the design of engineered biological circuits and chemical systems. © 2018 The Authors."
,10.1007/978-3-030-01234-2_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055096002&doi=10.1007%2f978-3-030-01234-2_12&partnerID=40&md5=08d69e089f3629f0a76a95dd621e5f83,"We propose a computational framework to jointly parse a single RGB image and reconstruct a holistic 3D configuration composed by a set of CAD models using a stochastic grammar model. Specifically, we introduce a Holistic Scene Grammar (HSG) to represent the 3D scene structure, which characterizes a joint distribution over the functional and geometric space of indoor scenes. The proposed HSG captures three essential and often latent dimensions of the indoor scenes: (i) latent human context, describing the affordance and the functionality of a room arrangement, (ii) geometric constraints over the scene configurations, and (iii) physical constraints that guarantee physically plausible parsing and reconstruction. We solve this joint parsing and reconstruction problem in an analysis-by-synthesis fashion, seeking to minimize the differences between the input image and the rendered images generated by our 3D representation, over the space of depth, surface normal, and object segmentation map. The optimal configuration, represented by a parse graph, is inferred using Markov chain Monte Carlo (MCMC), which efficiently traverses through the non-differentiable solution space, jointly optimizing object localization, 3D layout, and hidden human context. Experimental results demonstrate that the proposed algorithm improves the generalization ability and significantly outperforms prior methods on 3D layout estimation, 3D object detection, and holistic scene understanding. © Springer Nature Switzerland AG 2018."
,10.1002/asmb.2395,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053444779&doi=10.1002%2fasmb.2395&partnerID=40&md5=215ce3861fff93bc6c525c6ab9e0e718,"This study proposes a threshold realized generalized autoregressive conditional heteroscedastic (GARCH) model that jointly models daily returns and realized volatility, thereby taking into account the bias and asymmetry of realized volatility. We incorporate this threshold realized GARCH model with skew Student-t innovations as the observation equation, view this model as a sharp transition model, and treat the realized volatility as a proxy for volatility under this nonlinear structure. Through the Bayesian Markov chain Monte Carlo method, the model can jointly estimate the parameters in the return equation, the volatility equation, and the measurement equation. As an illustration, we conduct a simulation study and apply the proposed method to the US and Japan stock markets. Based on quantile forecasting and volatility estimation, we find that the threshold heteroskedastic framework with realized volatility successfully models the asymmetric dynamic structure. We also investigate the predictive ability of volatility by comparing the proposed model with the traditional GARCH model as well as some popular asymmetric GARCH and realized GARCH models. This threshold realized GARCH model with skew Student-t innovations outperforms the competing risk models in out-of-sample volatility and Value-at-Risk forecasting. © 2018 John Wiley & Sons, Ltd."
,10.1007/978-3-030-00111-7_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054484981&doi=10.1007%2f978-3-030-00111-7_7&partnerID=40&md5=6320c794272381e69a9d844dff338693,"Probabilistic parallel multiset rewriting systems (PPMRS) model probabilistic, dynamic systems consisting of multiple, (inter-) acting agents and objects (entities), where multiple individual actions can be performed in parallel. The main computational challenge in these approaches is computing the distribution of parallel actions (compound actions), that can be formulated as a constraint satisfaction problem (CSP). Unfortunately, computing the partition function for this distribution exactly is infeasible, as it requires to enumerate all solutions of the CSP, which are subject to a combinatorial explosion. The central technical contribution of this paper is an efficient Markov Chain Monte Carlo (MCMC)-based algorithm to approximate the partition function, and thus the compound action distribution. The proposal function works by performing backtracking in the CSP search tree, and then sampling a solution of the remaining, partially solved CSP. We demonstrate our approach on a Lotka-Volterra system with PPMRS semantics, where exact compound action computation is infeasible. Our approach allows to perform simulation studies and Bayesian filtering with PPMRS semantics in scenarios where this was previously infeasible. © Springer Nature Switzerland AG 2018."
2,10.1016/j.mbs.2017.10.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032966411&doi=10.1016%2fj.mbs.2017.10.005&partnerID=40&md5=34116b1ec50bb668a0fbd9d78ba7edc9,"The efficiency of spatial repellents and long-lasting insecticide-treated nets (LLINs) is a key research topic in malaria control. Insecticidal nets reduce the mosquito-human contact rate and simultaneously decrease mosquito populations. However, LLINs demonstrate dissimilar efficiency against different species of malaria mosquitoes. Various factors have been proposed as an explanation, including differences in insecticide-induced mortality, flight characteristics, or persistence of attack. Here we present a discrete agent-based approach that enables the efficiency of LLINs, baited traps and Insecticide Residual Sprays (IRS) to be examined. The model is calibrated with hut-level experimental data to compare the efficiency of protection against two mosquito species: Anopheles gambiae and Anopheles arabiensis. We show that while such data does not allow an unambiguous identification of the details of how LLINs alter the vector behavior, the model calibrations quantify the overall impact of LLINs for the two different mosquito species. The simulations are generalized to community-scale scenarios that systematically demonstrate the lower efficiency of the LLINs in control of An. arabiensis compared to An. gambiae. © 2017 The Author(s)"
,10.1109/TCOMM.2018.2873393,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054519829&doi=10.1109%2fTCOMM.2018.2873393&partnerID=40&md5=afd1fd3afcfdd4f83c7b3fe6d7f967e5,"Random access has been considered for machinetype communication (MTC). In particular, a random access scheme with a pool of preambles is widely studied, which is similar to multichannel ALOHA, to support a number of MTC devices. In this paper, we study optimal approaches for user activity detection in random access with preambles over fading channels. Since the computational complexity grows exponentially with the number of preambles, a low-complexity detector is derived using Markov chain Monte Carlo (MCMC) approaches that can approximately solve an optimal maximum a posteriori (MAP) detection problem. The resulting MCMC detector can enjoy a trade-off between performance and complexity, while its complexity to obtain a sample is linearly proportional to the number of preambles. A performance analysis for optimal detection is also studied to see the optimal performance. Simulation results confirm that the MCMC detector performs better than compressive sensing (CS) based approaches and can provide a near optimal performance under certain conditions with a reasonable computational complexity. IEEE"
,10.1007/978-3-319-75996-8_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046257506&doi=10.1007%2f978-3-319-75996-8_15&partnerID=40&md5=94aa0c285140dfc4f3a1b15ad5a1a158,"The error scaling for Markov chain Monte Carlo (MCMC) techniques with N samples behaves like 1/√N. This scaling makes it often very time intensive to reduce the error of calculated observables, in particular for applications in 4-dimensional lattice quantum chromodynamics as our theory of the interaction between quarks and gluons. Even more, for certain cases, where the infamous sign problem appears, MCMC methods fail to provide results with a reliable error estimate. It is therefore highly desirable to have alternative methods at hand which show an improved error scaling and have the potential to overcome the sign problem. One candidate for such an alternative integration technique we used is based on a new class of polynomially exact integration rules on U(N) and SU(N) which are derived from polynomially exact rules on spheres. We applied these rules successfully to a non-trivial, zero-dimensional model with a sign problem and obtained arbitrary precision results. In this article we test a possible way to apply the integration rules for spheres to the case of a one-dimensional U(1) model, the topological rotor, which already leads to a problem of very high dimensionality. © 2018, Springer International Publishing AG, part of Springer Nature."
,10.1137/16M1060625,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044499487&doi=10.1137%2f16M1060625&partnerID=40&md5=4743515692ddc5dc2bb072b3410a8235,"We present a randomized maximum a posteriori (rMAP) method for generating approximate samples of posteriors in high dimensional Bayesian inverse problems governed by large- scale forward problems. We derive the rMAP approach by (1) casting the problem of computing the MAP point as a stochastic optimization problem; (2) interchanging optimization and expectation; and (3) approximating the expectation with a Monte Carlo method. For a specific randomized data and prior mean, rMAP reduces to the randomized maximum likelihood (RML) approach. It can also be viewed as an iterative stochastic Newton method. An analysis of the convergence of the rMAP samples is carried out for both linear and nonlinear inverse problems. Each rMAP sample requires solution of a PDE-constrained optimization problem; to solve these problems, we employ a state-of-the-art trust region inexact Newton conjugate gradient method with sensitivity-based warm starts. An approximate Metropolization approach is presented to reduce the bias in rMAP samples. Various numerical methods will be presented to demonstrate the potential of the rMAP approach in posterior sampling of nonlinear Bayesian inverse problems in high dimensions. © 2018 Society for Industrial and Applied Mathematics."
,10.1177/1475921716683360,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037730771&doi=10.1177%2f1475921716683360&partnerID=40&md5=6f848427066e1c668d6adc2eea2da3d7,"Crack identification in engineering structures has been widely investigated by researchers. Most of the literature on multiple crack identification, however, has focused on rather simple structures like beams and it is often assumed that the number of cracks is known while this is not a practical assumption. In this article, multiple crack identification in frame structures is investigated based on experimental vibration data using the Bayesian model class selection and swarm-based optimization methods to identify both the number of cracks and their characteristics. To this end, first, the numerical model of the intact frame is updated based on the natural frequencies of the intact state using the particle swarm inspired multi-elitist artificial bee colony algorithm. After updating the intact model of the structure, a set of numerical models of the cracked frame with different numbers of cracks is constructed. Since the number of cracks is not known a priori, the Bayesian model class selection is employed to find the most plausible model class in order to predict the number of cracks. Then, the parameters of the cracks are identified using the particle swarm inspired multi-elitist artificial bee colony algorithm. Instead of pinpointing to one optimal solution obtained after a large number of function evaluations, a set of best solutions whose objective values are less than 10−5 are recorded and the regions where the best solutions are concentrated are identified to see how the solution would differ if less number of function evaluations is employed. To fully assess the effectiveness of this approach, both numerical and experimental examples are utilized. The results confirm the effectiveness of the proposed method for identifying multiple cracks in the frames using a few experimental natural frequencies of the structure. The effect of using more natural frequencies on the accuracy of the location and depth of the cracks is also studied. © 2017, © The Author(s) 2017."
,10.1007/978-3-030-01449-0_22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054878308&doi=10.1007%2f978-3-030-01449-0_22&partnerID=40&md5=8d60f7b37cbc9eab36ff2996e422b710,"Automatic object detection is a widely investigated problem in different fields such as military and urban surveillance. The availability of Very High Resolution (VHR) optical remotely sensed data, has motivated the design of new object detection methods that allow recognizing small objects like ships, buildings and vehicles. However, the challenge always remains in increasing the accuracy and speed of these object detection methods. This can be difficult due to the complex background. Therefore, the development of robust and flexible models that analyze remotely sensed data for vehicle detection is needed. We propose in this paper a hierarchical Bayesian model for automatic vehicle detection. Experiments performed using real data indicate the benefit that can be drawn from our approach. © 2018, Springer Nature Switzerland AG."
2,10.1016/j.jqsrt.2017.09.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032487842&doi=10.1016%2fj.jqsrt.2017.09.014&partnerID=40&md5=4f4424910305c8c52e57b67d2a419e1c,"The Chord Length Sampling (CLS) algorithm is a powerful Monte Carlo method that models the effects of stochastic media on particle transport by generating on-the-fly the material interfaces seen by the random walkers during their trajectories. This annealed disorder approach, which formally consists of solving the approximate Levermore–Pomraning equations for linear particle transport, enables a considerable speed-up with respect to transport in quenched disorder, where ensemble-averaging of the Boltzmann equation with respect to all possible realizations is needed. However, CLS intrinsically neglects the correlations induced by the spatial disorder, so that the accuracy of the solutions obtained by using this algorithm must be carefully verified with respect to reference solutions based on quenched disorder realizations. When the disorder is described by Markov mixing statistics, such comparisons have been attempted so far only for one-dimensional geometries, of the rod or slab type. In this work we extend these results to Markov media in two-dimensional (extruded) and three-dimensional geometries, by revisiting the classical set of benchmark configurations originally proposed by Adams, Larsen and Pomraning [1] and extended by Brantley [2]. In particular, we examine the discrepancies between CLS and reference solutions for scalar particle flux and transmission/reflection coefficients as a function of the material properties of the benchmark specifications and of the system dimensionality. © 2017 Elsevier Ltd"
,10.1007/978-3-030-00350-0_29,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054066393&doi=10.1007%2f978-3-030-00350-0_29&partnerID=40&md5=b5ed4573a6aaf2649bfa5e9fd3cf4333,"The identification of properties and land destinations are key factors in urban planning decisions, especially in rapid-growing urbanized cities. This information is vital for cadaster matters, property taxes calculations, and therefore for the financial sustainability of a city. In this work we present a Markov-Monte Carlo simulation model to predict changes in land destinations. First, a Markov chain is established to identify the transition finite-state matrix of property destinations, and then a Monte Carlo simulation model is used to predict the changes. We present a case study for the city of Medellín, Colombia, using historical information from the cadaster office from 2004 to 2016. Results obtained allow identifying the urban areas with the larger number of changes. Moreover, these results provide support for urban planning decisions related to workforce sizing and visits sequences to the identified areas. © 2018, Springer Nature Switzerland AG."
,10.1177/0954409718777834,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047664983&doi=10.1177%2f0954409718777834&partnerID=40&md5=2276a254f35d1575d790e74cc3fe1288,"Track-related failures are a major factor contributing to train derailments in the United States. Therefore, determining the failure time is critical for safety purposes. Traditionally, failure time in track geometry has been modeled using defect data. However, unless it is an accident due to extreme events, track geometry fails as a result of an underlying degradation process. The first hitting time is referred to the probability distribution of the time at which the degradation path first reaches a safety threshold. This paper presents the formulation and implementation of the first hitting time in railway track geometry degradation using track geometry inspection data. The underlying degradation path is modeled as a Wiener process with drift, and the first hitting time follows an inverse Gaussian distribution. The results provide a more robust representation of the failure time in track geometry using degradation data. © 2018, IMechE 2018."
,10.2991/ijcis.11.1.49,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045665219&doi=10.2991%2fijcis.11.1.49&partnerID=40&md5=5c491f19e99c993b627f89c947f6b8d6,"As the rapid booming of reviews, a valid sentiment analysis model will significantly boost the review recommendation system’s capability, and present more constructive information for consumers. Topic probabilistic models have already shown many advantages for detecting potential structure of topics and sentiments in reviews corpus. However, most reviews are presented through time-dependent data streams and some respects of the potential structure are unfixed and time-varying, such as topic number and word probability distribution. In this paper, a novel probabilistic topic modelling framework is proposed, called on-line evolutionary sentiment/topic modeling (OESTM), which has the capacity for achieving the optimization of the aforementioned aspects. Firstly, OESTM depends on an improved non-parametric Bayesian model for estimating the best number of topics that can perfectly explain the current time-slice, and analyzes these latent topics and sentiment polarities simultaneously. Secondly, OESTM implements the birth, death and inheritance for detected topics through the transfer of parameters from previous time slices to the updated time slice. The experiments show that significant improvements have been achieved by the proposed model with respect to other state-of-the-art models. © 2018, the Authors."
,10.6220/joq.201808_25(4).0002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053402518&doi=10.6220%2fjoq.201808_25%284%29.0002&partnerID=40&md5=26a09e81648d90a5ff8d7346e0e0ebd6,"In this paper, we proposed Bayesian regression, four commonly used priors, and a test-bed methodology for evaluating empirical modeling techniques. This method was applied to evaluate and fine-tune several of the most popular Bayesian regression formulations. It provides a systematic analysis of the robustness of alternative Bayesian regression priors. Based on the result, we concluded that stochastic search variable selection (SSVS), which relies on a mixture of normal priors and Gibbs sampling, performed better than other priors, and handled regression problems such as bias, multicollinearity, and design moment better than other methods. To illustrate the proposed methods, we applied a tuned Bayesian regression formulation to minimize the surface roughness of a wooden plate. © 2018, Chinese Society for Quality. All rights reserved."
1,10.1137/16M1087175,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046685956&doi=10.1137%2f16M1087175&partnerID=40&md5=3b7c505bc0b35200a483277ac0b3ab85,"Random graph null models have found widespread application in diverse research communities analyzing network datasets, including social, information, and economic networks, as well as food webs, protein-protein interactions, and neuronal networks. The most popular random graph null models, called configuration models, are defined as uniform distributions over a space of graphs with a fixed degree sequence. Commonly, properties of an empirical network are compared to properties of an ensemble of graphs from a configuration model in order to quantify whether empirical network properties are meaningful or whether they are instead a common consequence of the particular degree sequence. In this work we study the subtle but important decisions underlying the specification of a configuration model, and we investigate the role these choices play in graph sampling procedures and a suite of applications. We place particular emphasis on the importance of specifying the appropriate graph labeling-stub-labeled or vertex-labeled-under which to consider a null model, a choice that closely connects the study of random graphs to the study of random contingency tables. We show that the choice of graph labeling is inconsequential for studies of simple graphs, but can have a significant impact on analyses of multigraphs or graphs with self-loops. The importance of these choices is demonstrated through a series of three in-depth vignettes, analyzing three different network datasets under many different configuration models and observing substantial differences in study conclusions under different models. We argue that in each case, only one of the possible configuration models is appropriate. While our work focuses on undirected static networks, it aims to guide the study of directed networks, dynamic networks, and all other network contexts that are suitably studied through the lens of random graph null models. © 2018 Society for Industrial and Applied Mathematics."
,10.1007/978-3-319-94211-7_25,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049664044&doi=10.1007%2f978-3-319-94211-7_25&partnerID=40&md5=f6d759ba7fc4e94a79a4ee0ae2ad316c,"In this paper, we focus on constructing new flexible and powerful parametric framework for visual data modeling and reconstruction. In particular, we propose a Bayesian density estimation method based upon mixtures of scaled Dirichlet distributions. The consideration of Bayesian learning is interesting in several respects. It allows simultaneous parameters estimation and model selection, it permits also taking uncertainty into account by introducing prior information about the parameters and it allows overcoming learning problems related to over- or under-fitting. In this work, three key issues related to the Bayesian mixture learning are addressed which are the choice of prior distributions, the estimation of the parameters, and the selection of the number of components. Moreover, a principled Metropolis-within-Gibbs sampler algorithm for scaled Dirichlet mixtures is developed. Finally, the proposed Bayesian framework is tested on a challenging real-life application namely visual scene reconstruction. © Springer International Publishing AG, part of Springer Nature 2018."
,10.1007/978-3-319-77249-3_25,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045988218&doi=10.1007%2f978-3-319-77249-3_25&partnerID=40&md5=23b1b68c81d984f1d266b6318a76c494,"Differential item functioning (DIF) occurs when individuals from different groups with the same level of ability have different probabilities of answering an item correctly. In this paper, we develop a Bayesian approach to detect DIF based on the credible intervals within the framework of item response theory models. Our method performed well for both uniform and non-uniform DIF conditions in the two-parameter logistic model. The efficacy of the proposed approach is demonstrated through simulation studies and a real data application. © Springer International Publishing AG, part of Springer Nature 2018."
,10.1109/TNSRE.2017.2769701,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033722631&doi=10.1109%2fTNSRE.2017.2769701&partnerID=40&md5=18b0de42fbe290793c29bd68270b7b65,"Improving balance performance among the elderly is of utmost importance because of the increasing number of injuries and fatalities caused by fall incidences. Digital games controlled by body movements (exergames) have been proposed as a way to improve balance among older people. However, the assessment of balance performance in real-time during exergaming remains a challenging task. This assessment could be used to provide instantaneous feedback and automatically adjust the exergame difficulty. Such features could potentially increase the motivation of the player, thus augmenting the effectiveness of exergames. As clear differences in balance performance have been identified between older and younger people, distinguishing between older and younger adults can help identifying measures of balance performance. We used generalized linear models to investigate whether the assessment of balance performance based on movement speed can be improved by incorporating curvature of the movement trajectory into the analysis. Indeed, our results indicated that curvature improves the performance of the models. Five-fold cross validation indicated that our method is promising for the assessment of balance performance in real-time by showing more than 90% classification accuracy. Finally, this method could be valuable not only for exergaming, but also for real-time assessment of body movements in sports, rehabilitation, and medicine. © 2017 IEEE."
,10.1002/asmb.2410,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053731638&doi=10.1002%2fasmb.2410&partnerID=40&md5=489b64d82cdac711e3de91a034bb4dd2,"This paper proposes an efficient estimation method for some elliptical copula regression models by expressing both copula density and marginal density functions as scale mixtures of normals (SMN). Implementing these models using the SMN is novel and allows efficient estimation via Bayesian methods. An innovative algorithm for the case of complex semicontinuous margins is also presented. We utilize the facts that copulas are invariant to the location and scale of the margins; all elliptical distributions have the same correlation structure; and some densities can be represented by the SMN. Two simulation studies, one on continuous margins and the other on semicontinuous margins, highlight the favorable performance of the proposed methods. Two empirical studies, one on the US excess returns and one on the Thai wage earnings, further illustrate the applicability of the proposals. © 2018 John Wiley & Sons, Ltd."
1,10.1093/gji/ggx428,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042152317&doi=10.1093%2fgji%2fggx428&partnerID=40&md5=9264b98d41ae6a0213be8ddbf3750df5,"Limited illumination, insufficient offset, noisy data and poor starting models can pose challenges for seismic full waveform inversion.We present an application of a tree based Bayesian inversion scheme which attempts to mitigate these problems by accounting for data uncertainty while using a mildly informative prior about subsurface structure. We sample the resulting posterior model distribution of compressional velocity using a trans-dimensional (trans-D) or Reversible Jump Markov chain Monte Carlo method in the wavelet transform domain of velocity. This allows us to attain rapid convergence to a stationary distribution of posterior models while requiring a limited number of wavelet coefficients to define a sampled model. Two synthetic, low frequency, noisy data examples are provided. The first example is a simple reflection + transmission inverse problem, and the second uses a scaled version of the Marmousi velocity model, dominated by reflections. Both examples are initially started from a semi-infinite half-space with incorrect background velocity. We find that the trans-D tree based approach together with parallel tempering for navigating rugged likelihood (i.e. misfit) topography provides a promising, easily generalized method for solving large-scale geophysical inverse problems which are difficult to optimize, but where the true model contains a hierarchy of features at multiple scales. © The Authors 2017. Published by Oxford University Press on behalf of The Royal Astronomical Society."
,10.1002/2017WR022148,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040987952&doi=10.1002%2f2017WR022148&partnerID=40&md5=bed897d409d8c77b28ecb869b1c613c2,"Probabilistic inversion within a multiple-point statistics framework is often computationally prohibitive for high-dimensional problems. To partly address this, we introduce and evaluate a new training-image based inversion approach for complex geologic media. Our approach relies on a deep neural network of the generative adversarial network (GAN) type. After training using a training image (TI), our proposed spatial GAN (SGAN) can quickly generate 2-D and 3-D unconditional realizations. A key characteristic of our SGAN is that it defines a (very) low-dimensional parameterization, thereby allowing for efficient probabilistic inversion using state-of-the-art Markov chain Monte Carlo (MCMC) methods. In addition, available direct conditioning data can be incorporated within the inversion. Several 2-D and 3-D categorical TIs are first used to analyze the performance of our SGAN for unconditional geostatistical simulation. Training our deep network can take several hours. After training, realizations containing a few millions of pixels/voxels can be produced in a matter of seconds. This makes it especially useful for simulating many thousands of realizations (e.g., for MCMC inversion) as the relative cost of the training per realization diminishes with the considered number of realizations. Synthetic inversion case studies involving 2-D steady state flow and 3-D transient hydraulic tomography with and without direct conditioning data are used to illustrate the effectiveness of our proposed SGAN-based inversion. For the 2-D case, the inversion rapidly explores the posterior model distribution. For the 3-D case, the inversion recovers model realizations that fit the data close to the target level and visually resemble the true model well. © 2018. American Geophysical Union. All Rights Reserved."
,10.1175/JTECH-D-17-0116.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041682127&doi=10.1175%2fJTECH-D-17-0116.1&partnerID=40&md5=b4a134cd16d2442d26ee616d50d71db9,"This study focuses on merging MODIS-mapped SSTs with 4-km spatial resolution and AMSR-E optimally interpolated SSTs at 25-km resolution. A new data fusion method was developed-the Spatiotemporal Hierarchical Bayesian Model (STHBM). This method, which is implemented through the Markov chain Monte Carlo technique utilized to extract inferential results, is specified hierarchically by decomposing the SST spatiotemporal process into three subprocesses, that is, the spatial trend process, the seasonal cycle process, and the spatiotemporal random effect process. Spatial-scale transformation and spatiotemporal variation are introduced into the fusion model through the data model and model parameters, respectively, with suitably selected link functions. Compared with two modern spatiotemporal statistical methods-the Bayesian maximum entropy and the robust fixed rank kriging-STHBM has the following strength: it can simultaneously meet the expression of uncertainties from data and model, seamless scale transformation, and SST spatiotemporal process simulation. Utilizing multisensors' complementation, merged data with complete spatial coverage, high resolution (4 km), and fine spatial pattern lying in MODIS SSTs can be obtained through STHBM. The merged data are assessed for local spatial structure, overall accuracy, and local accuracy. The evaluation results illustrate that STHBM can provide spatially complete SST fields with reasonably good data values and acceptable errors, and that the merged SSTs collect fine spatial patterns lying in MODIS SSTs with fine resolution. The accuracy of merged SSTs is between MODIS and AMSR-E SSTs. The contribution to the accuracy and the spatial pattern of the merged SSTs from the original MODIS SSTs is stronger than that of the original AMSR-E SSTs. © 2018 American Meteorological Society."
1,10.1016/j.jhydrol.2017.07.040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034116066&doi=10.1016%2fj.jhydrol.2017.07.040&partnerID=40&md5=704a0558298774d368c643676e81ea13,"Recent studies have identified the importance of vegetation processes in terrestrial hydrologic systems. Process-based ecohydrological models combine hydrological, physical, biochemical and ecological processes of the catchments, and as such are generally more complex and parametric than conceptual hydrological models. Thus, appropriate calibration objectives and model uncertainty analysis are essential for ecohydrological modeling. In recent years, Bayesian inference has become one of the most popular tools for quantifying the uncertainties in hydrological modeling with the development of Markov chain Monte Carlo (MCMC) techniques. The Bayesian approach offers an appealing alternative to traditional multi-objective hydrologic model calibrations by defining proper prior distributions that can be considered analogous to the ad-hoc weighting often prescribed in multi-objective calibration. Our study aims to develop appropriate prior distributions and likelihood functions that minimize the model uncertainties and bias within a Bayesian ecohydrological modeling framework based on a traditional Pareto-based model calibration technique. In our study, a Pareto-based multi-objective optimization and a formal Bayesian framework are implemented in a conceptual ecohydrological model that combines a hydrological model (HYMOD) and a modified Bucket Grassland Model (BGM). Simulations focused on one objective (streamflow/LAI) and multiple objectives (streamflow and LAI) with different emphasis defined via the prior distribution of the model error parameters. Results show more reliable outputs for both predicted streamflow and LAI using Bayesian multi-objective calibration with specified prior distributions for error parameters based on results from the Pareto front in the ecohydrological modeling. The methodology implemented here provides insight into the usefulness of multiobjective Bayesian calibration for ecohydrologic systems and the importance of appropriate prior distributions in such approaches. © 2017 Elsevier B.V."
2,10.1016/j.neuroimage.2017.08.077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038077897&doi=10.1016%2fj.neuroimage.2017.08.077&partnerID=40&md5=d325cb913446b80dd2622052b9822316,"Previous attempts at characterizing the spatial specificity of the blood oxygenation level dependent functional MRI (BOLD fMRI) response by estimating its point-spread function (PSF) have conventionally relied on retinotopic spatial representations of visual stimuli in area V1. Consequently, their estimates were confounded by the width and scatter of receptive fields of V1 neurons. Here, we circumvent these limits by instead using the inherent cortical spatial organization of ocular dominance columns (ODCs) to determine the PSF for both Gradient Echo (GE) and Spin Echo (SE) BOLD imaging at 7 Tesla. By applying Markov chain Monte Carlo sampling on a probabilistic generative model of imaging ODCs, we quantified the PSFs that best predict the spatial structure and magnitude of differential ODCs’ responses. Prior distributions for the ODC model parameters were determined by analyzing published data of cytochrome oxidase patterns from post-mortem histology of human V1 and of neurophysiological ocular dominance indices. The average PSF full-widths at half-maximum obtained from differential ODCs’ responses following the removal of voxels influenced by contributions from macroscopic blood vessels were 0.86 mm (SE) and 0.99 mm (GE). Our results provide a quantitative basis for the spatial specificity of BOLD fMRI at ultra-high fields, which can be used for planning and interpretation of high-resolution differential fMRI of fine-scale cortical organizations. © 2017"
2,10.1016/j.sigpro.2017.07.030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026450257&doi=10.1016%2fj.sigpro.2017.07.030&partnerID=40&md5=a526611c768c07cc7971f17135754b47,"The Bayesian estimation of the unknown parameters of state-space (dynamical) systems has received considerable attention over the past decade, with a handful of powerful algorithms being introduced. In this paper we tackle the theoretical analysis of the recently proposed nonlinear population Monte Carlo (NPMC). This is an iterative importance sampling scheme whose key features, compared to conventional importance samplers, are (i) the approximate computation of the importance weights (IWs) assigned to the Monte Carlo samples and (ii) the nonlinear transformation of these IWs in order to prevent the degeneracy problem that flaws the performance of conventional importance samplers. The contribution of the present paper is a rigorous proof of convergence of the nonlinear IS (NIS) scheme as the number of Monte Carlo samples, M, increases. Our analysis reveals that the NIS approximation errors converge to 0 almost surely and with the optimal Monte Carlo rate of M−1/2. Moreover, we prove that this is achieved even when the mean estimation error of the IWs remains constant, a property that has been termed exact approximation in the Markov chain Monte Carlo literature. We illustrate these theoretical results by means of a computer simulation example involving the estimation of the parameters of a state-space model typically used for target tracking. © 2017 Elsevier B.V."
1,10.1364/JOSAA.35.000088,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041720471&doi=10.1364%2fJOSAA.35.000088&partnerID=40&md5=239062c345dee1676a6a89cd4e4bb1cf,"Characterization of nanoparticle aggregates from observed scattered light leads to a highly complex inverse problem. Even the forward model is so complex that it prohibits the use of classical likelihood-based inference methods. In this study, we compare four so-called likelihood-free methods based on approximate Bayesian computation (ABC) that requires only numeric simulation of the forward model without the need of evaluating a likelihood. In particular, rejection, Markov chain Monte Carlo, population Monte Carlo, and adaptive population Monte Carlo (APMC) are compared in terms of accuracy. In the current model, we assume that the nanoparticle aggregates are mutually well separated and made up of particles of same size. Filippov's particle-cluster algorithm is used to generate aggregates, and discrete dipole approximation is used to estimate scattering behavior. It is found that the APMC algorithm is superior to others in terms of time and acceptance rates, although all algorithms produce similar posterior distributions. Using ABC techniques and utilizing unpolarized light experiments at 266 nm wavelength, characterization of soot aggregates is performed with less than 2 nm deviation in nanoparticle radius and 3-4 deviation in number of nanoparticles forming the monodisperse aggregates. Promising results are also observed for the polydisperse aggregate with log-normal particle size distribution. © 2017 Optical Society of America."
2,10.1190/GEO2017-0009.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037056636&doi=10.1190%2fGEO2017-0009.1&partnerID=40&md5=2834d110fc88de93d4962b694fd99dbc,"The principle of equivalence is known to cause nonuniqueness in interpretations of direct current (DC) resistivity data. Low-or high-resistivity equivalences arise when a thin geologic layer with a low/high resistivity is embedded in a relative high-/low-resistivity background formation causing strong resistivity-thickness correlations. The equivalences often make it impossible to resolve embedded layers. We found that the equivalence problem could be significantly reduced by combining the DC data with full-decay time-domain induced polarization (IP) measurements. We applied a 1D Markov chain Monte Carlo algorithm to invert synthetic DC data of models with low- and high-resistivity equivalences. By applying this inversion method, it is possible to study the space of equivalent models that have an acceptable fit to the observed data, and to make a full sensitivity analysis of the model parameters. Then, we include a contrast in chargeability into the model, modeled in terms of spectral Cole-Cole IP parameters, and invert the DC and IP data in combination. The results show that the addition of IP data largely resolves the DC equivalences. Furthermore, we present a field example in which DC and IP data were measured on a sand formation with an embedded clay layer known from a borehole drilling. Inversion results show that the DC data alone do not resolve the clay layer due to equivalence problems, but by adding the IP data to the inversion, the layer is resolved. © 2018 Society of Exploration Geophysicists. All rights reserved."
,10.1007/978-3-319-69814-4_43,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034615923&doi=10.1007%2f978-3-319-69814-4_43&partnerID=40&md5=4763843bed5e6c9d7dd9305720daa1cb,Recently refined Markov Chain Monte Carlo techniques for Bayesian inference are combined with the elegant and computationally advantageous specification of state space models to develop and evaluate an approach for the clustering of time series of fixed-income financial instruments. This approach is based upon the specification and estimation of a finite mixture model where each mixture component is represented by a time series generative model that is specified in linear state-space form. © Springer International Publishing AG 2018.
1,10.1016/j.ultras.2017.09.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029480823&doi=10.1016%2fj.ultras.2017.09.002&partnerID=40&md5=fe6c5baa94ec44a06e160f06c99cdd06,"Ultrasonic damage detection and characterization is commonly used in nondestructive evaluation (NDE) of aerospace composite components. In recent years there has been an increased development of guided wave based methods. In real materials and structures, these dispersive waves result in complicated behavior in the presence of complex damage scenarios. Model-based characterization methods utilize accurate three dimensional finite element models (FEMs) of guided wave interaction with realistic damage scenarios to aid in defect identification and classification. This work describes an inverse solution for realistic composite damage characterization by comparing the wavenumber-frequency spectra of experimental and simulated ultrasonic inspections. The composite laminate material properties are first verified through a Bayesian solution (Markov chain Monte Carlo), enabling uncertainty quantification surrounding the characterization. A study is undertaken to assess the efficacy of the proposed damage model and comparative metrics between the experimental and simulated output. The FEM is then parameterized with a damage model capable of describing the typical complex damage created by impact events in composites. The damage is characterized through a transdimensional Markov chain Monte Carlo solution, enabling a flexible damage model capable of adapting to the complex damage geometry investigated here. The posterior probability distributions of the individual delamination petals as well as the overall envelope of the damage site are determined. © 2017 Elsevier B.V."
3,10.1016/j.csda.2017.07.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028697379&doi=10.1016%2fj.csda.2017.07.009&partnerID=40&md5=afadc0259858681e323702c3d3a06088,"A novel distributed particle filter algorithm is presented, called drift homotopy likelihood bridging particle filter (DHLB-PF). The DHLB-PF is designed to surmount the degeneracy problem by employing a multilevel Markov chain Monte Carlo (MCMC) procedure after the resampling step of particle filtering. DHLB-PF considers a sequence of pertinent stationary distributions which facilitates the MCMC step as well as explores the state space with a higher degree of freedom. The proposed algorithm is tested in a multi-target tracking problem using a wireless sensor network where no fusion center is required for data processing. The observations are gathered only from the informative sensors, which are sensing useful observations of the nearby moving targets. The detection of those informative sensors, which are typically a small portion of the sensor network, is taking place by using a sparsity-aware matrix decomposition technique. Simulation results showcase that the DHLB-PF outperforms current popular tracking algorithms. © 2017 Elsevier B.V."
,10.1080/17415977.2018.1516767,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053273598&doi=10.1080%2f17415977.2018.1516767&partnerID=40&md5=cb9b692e574bec5813acad0cc2864eb9,"Non-invasive monitoring of tissues’ temperatures is necessary for some diagnostic and therapeutic applications. Photoacoustic is a new hybrid biomedical imaging technique, combining the high-contrast of optical properties with the high spatial resolution of ultrasound. The estimation of model parameters that are temperature dependent was used in this work to indirectly measure the temperatures of tissues, as the solution of an inverse problem within the Bayesian framework of statistics. A two-dimensional case was examined, which is related to the hyperthermia treatment of cancer with laser heating in the near infrared range. Simulated measurements were used in the inverse analysis. The Markov Chain Monte Carlo method provided accurate estimation of the spatial distribution of the Gruneisen parameter and the temperature distribution in the region of interest could be recovered with discrepancies smaller than 0.03°C. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1016/j.ifacol.2018.09.205,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054391788&doi=10.1016%2fj.ifacol.2018.09.205&partnerID=40&md5=57af4530d6501dae1467bd12daa5d24a,The identification of static parameters in jump Markov nonlinear models (JMNMs) poses a key challenge in explaining nonlinear and abruptly changing behavior of dynamical systems. This paper introduces a stochastic approximation expectation maximization algorithm to facilitate offline maximum likelihood parameter estimation in JMNMs. The method relies on the construction of a particle Gibbs kernel that takes advantage of the inherent structure of the model to increase the efficiency through Rao-Blackwellization. Numerical examples illustrate that the proposed solution outperforms related approaches. © 2018
,10.1002/wics.1452,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053215779&doi=10.1002%2fwics.1452&partnerID=40&md5=40211a74347415ca7f147a5d7ed1e82e,"Algorithms for computing the nonparametric maximum likelihood estimate of a univariate log-concave density are briefly described, and some relevant issues discussed. The fast few algorithms are further numerically compared in a small-scale simulation study. This article is categorized under: Statistical and Graphical Methods of Data Analysis > Markov Chain Monte Carlo (MCMC) Statistical and Graphical Methods of Data Analysis > Density Estimation Statistical and Graphical Methods of Data Analysis > Nonparametric Methods Algorithms and Computational Methods > Algorithms. © 2018 Wiley Periodicals, Inc."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048311195&partnerID=40&md5=778bd3267b9af167374d5eecc9188a3b,"Many systems, including space satellites, cannot be up-graded or repaired easily during their missions. Simulation-based design techniques are often used to check conditions that can induce critical malfunctions in them, to ensure sufficient credibility and reliability during operation. However, critical conditions with a very low probability of occurring (e.g., 10-8 per trial) rarely appear within a tractable number of simulations. We propose herein a multicanonical Markov Chain Monte Carlo (MCMC) technique extended for the efficient search of rare but critical conditions, to significantly enhance simulation efficiency. Furthermore, we demonstrate an application of our proposed technique to an efficient search of ""stray light"" in a space telescope satellite. © 2018 by SIAM."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054244591&partnerID=40&md5=02217d03c9e91443aed8e6be370bf8c3,"Online Word-of-Mouth (WOM) is an important aspect of consumer-firm relationship and is a leading indicator of product performance. However, prior research focuses considerably on the static view of online WOM. This paper attempts to explicate the dynamics of the spillover effects in online WOM in the U.S. automobile industry. I employed the Bayesian approach using Markov Chain Monte Carlo (MCMC) methods for model estimation. The results suggest that there is a pressing need for extending to the dynamic view of online WOM by examining the spillover effects. © 2018 Association for Information Systems. All rights reserved."
1,10.1016/j.petrol.2017.10.055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033498685&doi=10.1016%2fj.petrol.2017.10.055&partnerID=40&md5=15d846784ec1e1238ea89ca2090486aa,"In this study, we established a more efficient approach for fractured shale reservoir modeling with an emphasis on simplifying and automating the workflow for assisted history matching and uncertainty quantification. The improvement is especially notable for the process of history matching since the fracture geometry and properties can be directly set as parameters to be history matched. The resultant approach not only shows a significant reduction in the computational time while maintaining model accuracy, but also provides an automatic method for modifying the fracture related parameters - a laborious process in the traditional workflow. In the forward reservoir model, we implemented and extended the Embedded Discrete Fracture Model (Embedded DFM) approach for fractures with arbitrary strike and dip angle to a multiple porosity/permeability setting. The fractures are naturally discretized by the boundary of parent matrix grid blocks. Control volumes of fracture segments are generated according to the specific geometry of each of the segments. Three types of non-neighbor connections are then generated, namely the connection between the fracture segment and its parent matrix grid blocks, the connection between two intersecting fracture segments from different fractures, and the connection between two neighbor fracture segments from the same fracture. For each of the non-neighbor connections, transmissibility can be calculated honoring the physics of the flow. In our approach with Embedded Discrete Fracture Multiple-Porosity Model, the matrix is sub-divided into three porosity types, namely organic matrix (kerogen), inorganic matrix and natural fractures, with the necessary physics included for each of the porosity types. The macro fractures are explicitly represented with Embedded DFM. The proposed model provides a coherent method for characterizing the organic matrix, inorganic matrix, micro fractures as well as the hydraulic fractures of shale reservoirs. It offers a computationally efficient approach for modeling the severe heterogeneity due to hydraulic and natural fractures. Compared with traditional discrete fracture models, fewer grid blocks and lower levels of refinement are required. Compared with multiple porosity method, the proposed model has desirable accuracy for the simulation of reservoirs with large scale fractures. In the history matching and uncertainty quantification stage, due to the low efficiency of traditional Markov Chain Monte Carlo (MCMC) method when applied to reservoir history matching, a more advanced algorithm of two-stage MCMC is employed to evaluate the uncertainty for all the parameters. Since no upscaling of the fracture related parameters is required, the reservoir model can be generated by a pre-processor based on the proposed parameter, which maintains the adequacy of a Gaussian distribution assumption. Therefore, the workflow can be completely automated. By incorporating Embedded DFM and multiple porosity/permeability approaches, the improved model facilitates the history matching of fractured shale reservoirs by cutting the total amount of grid blocks, reducing the complexity of the gridding process, as well as improving the accuracy of fluid transportation within and among different porosity types. © 2017"
,10.1016/j.cviu.2018.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053720331&doi=10.1016%2fj.cviu.2018.07.001&partnerID=40&md5=c379241e9a4b2912abad1c71b9ccd7c0,"This paper approaches the problem of geometric multi-model fitting as a data segmentation problem. The proposed solution is based on a sequence of sampling hyperedges from a hypergraph, model selection and hypergraph clustering steps. We developed a sampling method that significantly facilitates solving the segmentation problem using a new form of the Markov-Chain-Monte-Carlo (MCMC) method to effectively sample from hyperedge distribution. To sample from this distribution effectively, our proposed Markov Chain includes new ways of long and short jumps to perform exploration and exploitation of all structures. To enhance the quality of samples, a greedy algorithm is used to exploit nearby structure based on the minimization of the Least kth Order Statistics cost function. Unlike common sampling methods, ours does not require any specific prior knowledge about the distribution of models. The output set of samples leads to a clustering solution by which the final model parameters for each segment are obtained. The method competes favorably with the state-of-the-art both in terms of computation power and segmentation accuracy. © 2018"
,10.12011/1000-6788(2018)01-0164-13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046818325&doi=10.12011%2f1000-6788%282018%2901-0164-13&partnerID=40&md5=899a48dbbae066b33c6ace4a21774ccd,"Random turnover of R&D staff influences new product R&D project portfolio scheduling. Using discrete Markov chain to describe staff's turnover processes with multi-skilled R&D staff as the scheduling object, we proposed a stochastic multi-objective constraint optimization model for the new product R&D project portfolio scheduling. Specifically, three objectives are strategic gains for talent cultivation, R&D cycle and R&D costs. The proposed model is solved by an adaptive Pareto sampling algorithm which utilizes the sampling method of Markov chain Monte Carlo, we calculate objective values for the deterministic model by serial schedule generation scheme, and obtain the Pareto set by the nondominated sorting genetic algorithmII for the multi-objective expected value model. Both model and algorithm were tested by a real-world case of staff scheduling for a new electric energy-saving product R&D project portfolio in a Chinese company. Since the algorithm converged well and obtained the Pareto set effectively, results indicated that the stochastic model is more suitable to reflect company's reality than the deterministic model. Practically, enterprises can use our model and algorithm to make an effective decision on multi-skilled staff scheduling scheme for a new product R&D project portfolio under the stochastic turnover scenario. © 2018, Editorial Board of Journal of Systems Engineering Society of China. All right reserved."
,10.1016/j.proci.2018.06.190,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050754171&doi=10.1016%2fj.proci.2018.06.190&partnerID=40&md5=29bfd1e0e29ecc098e16ad3028fc1879,"A procedure for determining the joint uncertainty of Arrhenius parameters across multiple combustion reactions of interest is demonstrated. This approach is capable of constructing the joint distribution of the Arrhenius parameters arising from the uncertain measurements performed in specific target experiments without having direct access to the underlying experimental data. The method involves constructing an ensemble of hypothetical data sets with summary statistics consistent with the available information reported by the experimentalists, followed by a fitting procedure that learns the structure of the joint parameter density across reactions using this consistent hypothetical data as evidence. The procedure is formalized in a Bayesian statistical framework, employing maximum-entropy and approximate Bayesian computation methods and utilizing efficient Markov chain Monte Carlo techniques to explore data and parameter spaces in a nested algorithm. We demonstrate the application of the method in the context of experiments designed to measure the rates of selected chain reactions in the H2-O2 system and highlight the utility of this approach for revealing the critical correlations between the parameters within a single reaction and across reactions, as well as for maximizing consistency when utilizing rate parameter information in predictive combustion modeling of systems of interest. © 2018 The Combustion Institute."
,10.1007/978-3-319-73441-5_26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041746321&doi=10.1007%2f978-3-319-73441-5_26&partnerID=40&md5=410a00a6a6a5cd6dc9fb850312afa3d5,This paper focuses on minimizing further the communications in Monte Carlo methods for Linear Algebra and thus improving the overall performance. The focus is on producing set of small number of covering Markov chains which are much longer that the usually produced ones. This approach allows a very efficient communication pattern that enables to transmit the sampled portion of the matrix in parallel case. The approach is further applied to quasi-Monte Carlo. A comparison of the efficiency of the new approach in case of Sparse Approximate Matrix Inversion and hybrid Monte Carlo and quasi-Monte Carlo methods for solving Systems of Linear Algebraic Equations is carried out. Experimental results showing the efficiency of our approach on a set of test matrices are presented. The numerical experiments have been executed on the MareNostrum III supercomputer at the Barcelona Supercomputing Center (BSC) and on the Avitohol supercomputer at the Institute of Information and Communication Technologies (IICT). © Springer International Publishing AG 2018.
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054570352&partnerID=40&md5=62c509c84769c628de25a1fb48e1c9aa,"We present a methodology to obtain a correct sampling of the posterior probability density function (pdf) conditional to observations where this posterior pdf can be formally expressed using Bayes' theorem. Generating a correct sampling of a multimodal posterior pdf is a challenging task which can only be achieved with Markov chain Monte Carlo (MCMC) methods. In standard MCMC such as random-walk MCMC, evaluation of acceptance probability for a proposed state requires a forward model run (a reservoir simulation run). When the forward model run is computationally expensive, we cannot afford to generate a long Markov chains with tens of thousands or more states. Therefore, it is critically important to design the MCMC such that it converges to the posterior pdf after generating a few thousand or less states. Here, a two-level MCMC procedure which can sample multimodal posteriors relatively efficiently is developed and applied. In the first step, we use the distributed Gauss-Newton (DGN) method to generate many modes of the posterior pdf in parallel; this procedure estimates sensitivity matrices without the need of an adjoint solution. A Gaussian mixture model (GMM) is then constructed based on the distinct modes that we find in the first step. In the second step, the constructed GMM is used as the proposal distribution for our MCMC algorithm. Because the proposal distribution is constructed as a direct approximation of the target pdf (without the normalizing constant), the Markov chain(s) constructed should converge relatively quickly to the posterior distribution and applications of the two-level MCMC algorithm to test problems show that our proposed two-level MCMC is far more efficient than the random-walk MCMC. © 2018 European Association of Geoscientists and Engineers EAGE. All rights reserved."
,10.1007/978-3-319-91143-4_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050315489&doi=10.1007%2f978-3-319-91143-4_12&partnerID=40&md5=907d258e94e308ac53026e3f4198e5cb,"In recent years, many statistical inference problems have been solved by using Markov Chain Monte Carlo (MCMC) techniques. However, it is necessary to derivate the analytical form for the likelihood function. Although the level of computing has increased steadily, there is a limitation caused by the difficulty or the misunderstanding of how computing the likelihood function. The Approximate Bayesian Computation (ABC) method dispenses the use of the likelihood function by simulating candidates of posterior distributions and using an algorithm to accept or reject the proposed candidates. This work presents an alternative nonparametric estimation method of smoothing empirical distributions with random Bernstein polynomials via ABC method. The Bernstein prior is obtained by rewriting the Bernstein polynomial in terms of k mixtures / m mixtures of beta densities and mixing weights. A study of simulation and a real example are presented to illustrate the method proposed. © 2018, Springer International Publishing AG, part of Springer Nature."
2,10.1016/j.radmeas.2017.10.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038883443&doi=10.1016%2fj.radmeas.2017.10.007&partnerID=40&md5=1b94d4a0a1878e1dead78fa93db63c4e,"Optically Stimulated Luminescence (OSL) is commonly used to date the last exposure of grains extracted from sediments to sunlight. However, it is frequent that some of the measured grains were not sufficiently exposed to light before burial. Such samples are said to be poorly bleached. We propose a new statistical model based on a Bayesian approach to analyse OSL measurements performed on poorly bleached sediment samples. For such data, we propose a mixture model of Gaussian distributions to analyse equivalent doses (De) distributions. This model can either be applied directly to the observed De values, or after a log-transformation. Bayesian analysis requires numerical approximation, to do this we use the JAGS (Just Another Gibbs Sampler) programme to run models using Markov Chain Monte Carlo simulations. We apply the model to synthetic datasets and real samples. © 2017"
1,10.1016/j.jsg.2018.05.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047982819&doi=10.1016%2fj.jsg.2018.05.026&partnerID=40&md5=93d9458ce451034a98c7f48b52bb6b07,"Despite the implementation of several kinematic and mechanical models for fault-related folding, and the incorporation of structural uncertainty in geological models, structural modeling is still too deterministic. The current focus is on forward modeling of a unique fit. We show the application of trishear inverse modeling, global optimization and Markov chain Monte Carlo (MCMC) methods, to a clay model of basement-involved compressional faulting, from which we know both total and incremental deformation. Global optimization and MCMC methods provide a range of possible models rather than a unique fit. Total inversions give an average of the model's deformation, while incremental inversions are more physically related to the model's evolution. Global optimization identifies the full range of possible models, while MCMC characterizes expected parameter values and their uncertainties. Structural inversions without sound geology are meaningless. A robust stratigraphy, geomorphic markers, mesoscopic structures, analogue and mechanical modeling can all greatly improve the inversions. © 2018 Elsevier Ltd"
,10.1007/978-981-10-7989-4_35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045317537&doi=10.1007%2f978-981-10-7989-4_35&partnerID=40&md5=2add9d54cec8b31030c68ea78375745c,"This paper presents a Bayesian-MCMC model to assess collector shoes slider degradation under different materials. A Markov Chain Monte Carlo (MCMC) method, based on the Bayesian decision model, is put forward and built up a framework in case of the life cycle of collector shoes under different materials forecast. All of inspection data is gathered from Beijing metro lines, and WinBUGS software are used to predict the slider’s wear rate. Result shows that the difference between the predicted value and the real one is less than 10% of the later one. Consequently, in case of new metro equipment parts, the newest method is able to ensure the safety operation in the metro by providing a valid device to the equipment manufacturers, the maintenance department as well as the purchasing department of the metro equipment. © 2018, Springer Nature Singapore Pte Ltd."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050470798&partnerID=40&md5=c207afa75abe68c5825c875ad3f41e28,"This study investigates factors that significantly contribute to pedestrians red light-running violation at signalized intersections. Data was collected at eight crosswalks located at two signalized intersections in Nanjing, China. A random parameters logistic regression was developed to explore the potential unobserved heterogeneous effects across observations. The Markov chain Monte Carlo (MCMC) simulation-based full Bayesian approach is employed to estimate the model parameters. Both parameter estimates and the odds ratio (OR) are developed and used to interpret the model. The estimation results show that contributing factors significantly influence pedestrians' red light running behavior, though they differ across observations. Modeling results show that age, gender, group size, cell phone use, pedestrian signal type, pedestrian volume, and pedestrian signal green ratio are statistically significant in the model. The variables of gender, pedestrian signal type, and pedestrian volume were found to have heterogeneous effects, appearing in the form of random parameters in the statistical model. © 2018 American Society of Civil Engineers."
,10.1117/12.2293772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047847289&doi=10.1117%2f12.2293772&partnerID=40&md5=cbd46280fd91a424db37252c8f6bff18,"It has been advocated that task-based measures of image quality (IQ) should be employed to evaluate and optimize imaging systems. Task-based measures of IQ quantify the performance of an observer on a medically relevant task. The Bayesian Ideal Observer (IO), which employs complete statistical information of the object and noise, achieves the upper limit of the performance for a binary signal classification task. However, computing the IO performance is generally analytically intractable and can be computationally burdensome when Markov-chain Monte Carlo (MCMC) techniques are employed. In this paper, supervised learning with convolutional neural networks (CNNs) is employed to approximate the IO test statistics for a signal-known-exactly and background-known-exactly (SKE/BKE) binary detection task. The receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC) are compared to those produced by the analytically computed IO. The advantages of the proposed supervised learning approach for approximating the IO are demonstrated. © 2018 SPIE."
,10.7498/aps.67.20172246,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046884481&doi=10.7498%2faps.67.20172246&partnerID=40&md5=998d4d197bd21c254e9e86075accf9b8,"The relationship between the sequential and structural features of intrinsically disordered peptides (IDPs) has attracted much attention during the recent decade. One essential problem relating to sequence-structure relationship is how the distribution of charged residues affects the structure of IDP. In this work, we address this problem with simulations on a series of random peptides composed of arginine and aspartic acids. With the ABSINTH implicit solvation model, the structural ensembles are generated with Markov Chain Monte Carlo method and replica-exchange sampling. The relations between various structural features (including the gyration radius, the tail distance, the distance between residues, and asphericity) and the distribution of charged residues are analyzed. Several limit cases (with parts of interactions switched off) are also calculated for comparison. The conversion from extended conformations to compact structures is observed, following the demixing of negatively and positively charged residues along the sequence. For the cases with well-mixed charges, the intra-chain electrostatic repulsions and attractions are balanced, which results in a generic Flory random coil-like conformation. Differently, for the case with well-separated charged residues, the electrostatic attraction between residues distant along the sequence induces a semi-compact hairpin-like conformation. This is consistent with the observations of Pappu group. Our results suggest that the structural dependence on charge distribution would not be sensitive to the selection of amino acid, and is determined by the patterns of charges, which demonstrates the robustness of the mechanism that the charge distribution modulates the structural features in the IDP system. Our results may broaden our understanding of the sequence-structure relation of IDP system. © 2018 Chinese Physical Society."
,10.1007/978-3-319-59315-9_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034233721&doi=10.1007%2f978-3-319-59315-9_7&partnerID=40&md5=ac8c340e0fe32434d939d37dcd3f1dbe,"Since the advent of the space-based photometric missions such as CoRoT and NASA’s Kepler, asteroseismology has acquired a central role in our understanding about stellar physics. The Kepler spacecraft, especially, is still releasing excellent photometric observations that contain a large amount of information not yet investigated. For exploiting the full potential of these data, sophisticated and robust analysis tools are now essential, so that further constraining of stellar structure and evolutionary models can be obtained. In addition, extracting detailed asteroseismic properties for many stars can yield new insights on their correlations to fundamental stellar properties and dynamics. After a brief introduction to the Bayesian notion of probability, I describe the code Diamonds for Bayesian parameter estimation and model comparison by means of the nested sampling Monte Carlo (NSMC) algorithm. NSMC constitutes an efficient and powerful method, in replacement to standard Markov chain Monte Carlo, very suitable for high-dimensional and multimodal problems that are typical of detailed asteroseismic analyses, such as the fitting and mode identification of individual oscillation modes in stars (known as peak-bagging). Diamonds is able to provide robust results for statistical inferences involving tens of individual oscillation modes, while at the same time preserving a considerable computational efficiency for identifying the solution. In the tutorial, I will present the fitting of the stellar background signal and the peak-bagging analysis of the oscillation modes in a red-giant star, providing an example to use Bayesian evidence for assessing the peak significance of the fitted oscillation peaks. © 2018, Springer International Publishing AG."
,10.1016/j.asr.2017.12.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038836362&doi=10.1016%2fj.asr.2017.12.009&partnerID=40&md5=1fa827a43a3b80fd590ef305be5cfa7a,"We model regular and irregular variation of ionospheric total electron content as stationary and non-stationary processes, respectively. We apply the method developed to SCINDA GPS data set observed at Bahir Dar, Ethiopia 11.6°N,37.4°E. We use hierarchical Bayesian inversion with Gaussian Markov random process priors, and we model the prior parameters in the hyperprior. We use Matérn priors via stochastic partial differential equations, and use scaled Inv-χ2 hyperpriors for the hyperparameters. For drawing posterior estimates, we use Markov Chain Monte Carlo methods: Gibbs sampling and Metropolis-within-Gibbs for parameter and hyperparameter estimations, respectively. This allows us to quantify model parameter estimation uncertainties as well. We demonstrate the applicability of the method proposed using a synthetic test case. Finally, we apply the method to real GPS data set, which we decompose to regular and irregular variation components. The result shows that the approach can be used as an accurate ionospheric disturbance characterization technique that quantifies the total electron content variability with corresponding error uncertainties. © 2017 COSPAR"
,10.1007/978-3-319-73441-5_30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041710703&doi=10.1007%2f978-3-319-73441-5_30&partnerID=40&md5=3005789f806e8fbc7c7f4a65e7291735,"We consider diffusion problems with partially reflecting boundaries that can be formulated in terms of an elliptic equation. To solve boundary value problems with the Robin condition, we propose a Monte Carlo method based on a randomization of an integral representation. The algorithm behaviour is analysed in its application for solving a model problem. © Springer International Publishing AG 2018."
,10.1002/asmb.2232,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013392773&doi=10.1002%2fasmb.2232&partnerID=40&md5=52409fd7f24dea965f5a1862ad5f549a,"This article describes statistical analyses pertaining to marketing data from a large multinational pharmaceutical firm. We describe models for monthly new prescription counts that are written by physicians for the firm's focal drug and for competing drugs, as functions of physician-specific and time-varying predictors. Modeling patterns in discrete-valued time series, and specifically time series of counts, based on large datasets, is the focus of much recent research attention. We first provide a brief overview of Bayesian approaches we have employed for modeling multivariate count time series using Markov Chain Monte Carlo methods. We then discuss a flexible level correlated model framework, which enables us to combine different marginal count distributions and to build a hierarchical model for the vector time series of counts, while accounting for the association among the components of the response vector, as well as possible overdispersion. We employ the integrated nested Laplace approximation (INLA) for fast approximate Bayesian modeling using the R-INLA package (r-inla.org). To enhance computational speed, we first build a model for each physician, use features of the estimated trends in the time-varying parameters in order to cluster the physicians into groups, and fit aggregate models for all physicians within each cluster. Our three-stage analysis can provide useful guidance to the pharmaceutical firm on their marketing actions. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.1007/978-3-319-91947-8_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048050201&doi=10.1007%2f978-3-319-91947-8_18&partnerID=40&md5=d32536d687ca8eff9713fe3807bd622e,"Template-based information extraction generalizes over standard token-level binary relation extraction in the sense that it attempts to fill a complex template comprising multiple slots on the basis of information given in a text. In the approach presented in this paper, templates and possible fillers are defined by a given ontology. The information extraction task consists in filling these slots within a template with previously recognized entities or literal values. We cast the task as a structure prediction problem and propose a joint probabilistic model based on factor graphs to account for the interdependence in slot assignments. Inference is implemented as a heuristic building on Markov chain Monte Carlo sampling. As our main contribution, we investigate the impact of soft constraints modeled as single slot factors which measure preferences of individual slots for ranges of fillers, as well as pairwise slot factors modeling the compatibility between fillers of two slots. Instead of relying on expert knowledge to acquire such soft constraints, in our approach they are directly captured in the model and learned from training data. We show that both types of factors are effective in improving information extraction on a real-world data set of full-text papers from the biomedical domain. Pairwise factors are shown to particularly improve the performance of our extraction model by up to +0.43 points in precision, leading to an F1 score of 0.90 for individual templates. © 2018, Springer International Publishing AG, part of Springer Nature."
,10.1117/12.2309466,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046436250&doi=10.1117%2f12.2309466&partnerID=40&md5=2834ef760661170a6cd631cb81d00c79,"In machine vision typical heuristic methods to extract parameterized objects out of raw data points are the Hough transform and RANSAC. Bayesian models carry the promise to optimally extract such parameterized objects given a correct definition of the model and the type of noise at hand. A category of solvers for Bayesian models are Markov chain Monte Carlo methods. Naive implementations of MCMC methods suffer from slow convergence in machine vision due to the complexity of the parameter space. Towards this blocked Gibbs and split-merge samplers have been developed that assign multiple data points to clusters at once. In this paper we introduce a new split-merge sampler, the triadic split-merge sampler, that perform steps between two and three randomly chosen clusters. This has two advantages. First, it reduces the asymmetry between the split and merge steps. Second, it is able to propose a new cluster that is composed out of data points from two different clusters. Both advantages speed up convergence which we demonstrate on a line extraction problem. We show that the triadic split-merge sampler outperforms the conventional split-merge sampler. Although this new MCMC sampler is demonstrated in this machine vision context, its application extend to the very general domain of statistical inference. © 2018 Copyright SPIE."
,10.1111/risa.12988,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044323201&doi=10.1111%2frisa.12988&partnerID=40&md5=1b8b46a381cf423708c076ea5bb56e53,"Harbor seals in Iliamna Lake, Alaska, are a small, isolated population, and one of only two freshwater populations of harbor seals in the world, yet little is known about their abundance or risk for extinction. Bayesian hierarchical models were used to estimate abundance and trend of this population. Observational models were developed from aerial survey and harvest data, and they included effects for time of year and time of day on survey counts. Underlying models of abundance and trend were based on a Leslie matrix model that used prior information on vital rates from the literature. We developed three scenarios for variability in the priors and used them as part of a sensitivity analysis. The models were fitted using Markov chain Monte Carlo methods. The population production rate implied by the vital rate estimates was about 5% per year, very similar to the average annual harvest rate. After a period of growth in the 1980s, the population appears to be relatively stable at around 400 individuals. A population viability analysis assessing the risk of quasi-extinction, defined as any reduction to 50 animals or below in the next 100 years, ranged from 1% to 3%, depending on the prior scenario. Although this is moderately low risk, it does not include genetic or catastrophic environmental events, which may have occurred to the population in the past, so our results should be applied cautiously. © 2018 Society for Risk Analysis."
,10.3788/OPE.20182601.0161,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044670447&doi=10.3788%2fOPE.20182601.0161&partnerID=40&md5=47bc4cf9f53bad53b2a3fee0355b6f12,"To decompose asymmetric full-waveform LiDAR data with unknown number of components, a full-waveform LiDAR decomposition method was proposed based on skew-normal distribution and reversible-jump Markov Chain Monte Carlo (RJMCMC) algorithm, which can automatically determine the numbers of components. First, the energy function was used to describe the differences between the actual waveform and the ideal waveform that obeyed the skew-normal distribution, and the likelihood function was defined by Gibbs distribution. Second, the parameter models of the ideal waveform were established using the prior distribution. Then the Bayesian paradigm was followed to build the ideal waveform model. Third, an RJMCMC algorithm was designed to determine the numbers of components and decompose the waveform. The proposed algorithm was used to decompose ICESat-GLAS waveform data in various typical regions. Experimental results indicate that the cross-correlation of the true data and the result is up to 98.9%. The proposed method can not only fit the skewed waveform data and normal waveform data, but also more accurately determine the number of components in comparison to other methods. It can realize the accurate decomposition of full-waveform LiDAR data, and the decomposition result is consistent with the corresponding elevation information. © 2018, Science Press. All right reserved."
,10.1002/qre.2409,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055116954&doi=10.1002%2fqre.2409&partnerID=40&md5=56289270a400caa3dceb0a08a233a6fb,"We consider change-point detection and estimation in sequences of functional observations. This setting often arises when the quality of a process is characterized by such observations, called profiles, and monitoring profiles for changes in structure can be used to ensure the stability of the process over time. While interest in phase II profile monitoring has grown, few methods approach the problem from a Bayesian perspective. We propose a wavelet-based Bayesian methodology that bases inference on the posterior distribution of the change point without placing restrictive assumptions on the form of profiles. By obtaining an analytic form of this posterior distribution, we allow the proposed method to run online without using Markov chain Monte Carlo (MCMC) approximation. Wavelets, an effective tool for estimating nonlinear signals from noise-contaminated observations, enable us to flexibly distinguish between sustained changes in profiles and the inherent variability of the process. We analyze observed profiles in the wavelet domain and consider two possible prior distributions for coefficients corresponding to the unknown change in the sequence. These priors, previously applied in the nonparametric regression setting, yield tuning-free choices of hyperparameters. We present additional considerations for controlling computational complexity over time and their effects on performance. The proposed method significantly outperforms a relevant frequentist competitor on simulated data. © 2018 John Wiley & Sons, Ltd."
,10.1117/12.2320450,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053888142&doi=10.1117%2f12.2320450&partnerID=40&md5=eba1cef4d32cee9c9839dfcb8c90ec26,"Night target tracking usually fails due to various reasons such as insufficient light, appearance change, motion blur, illumination variation, and deformation. Because infrared (IR) and visible video data provides comple- mentary information that can be utilized suitably and efficiently, we explore a novel framework by combining correlation filter-based visible tracking and Markov chain Monte Carlo (MCMC)-based IR tracking to overcome these challenges. In this framework, the two types of videos are asynchronous, and the frame rate of visible video is several times faster than that of IR video. Visible video is first used for location and scale estimation by solving a ridge regression problem efficiently in the correlation filter domain. When recording IR data, we use a uniquely designed feature shape context descriptor for the best location and scale estimation of an IR video target by using the MCMC particle filter. Then, we use candidate region location-scale fusion rules for the final target tracking update. Meanwhile, we build an accurately labeled IR and visible target tracking dataset for experiments. The result shows that the performance of our proposed approach is better than the state-of-the-art trackers for night target tracking, and our approach can significantly improve re-tracking performance when there is the drift. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only."
,10.1177/1748006X17751494,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042380381&doi=10.1177%2f1748006X17751494&partnerID=40&md5=05b3867306e0b87af3a4c257b01cec34,"The lifetime evolution of mechanical equipment with complicated structure and the harsh operating environment cannot be accurately expressed due to the dynamics of the failure mechanism. However, the performance monitoring of equipment, with the information characterizing the failure process from the sensed data, can be used to assess the failure time and then the online remaining useful life. Because of the existence of nonlinearity and non-Gaussian for most real systems, for online assessment, unscented Kalman filter combined with particle filter is studied, instead of the standard particle filter with importance sampling, which is modified to update the states iteratively. Meanwhile, Markov chain Monte Carlo is performed after resampling to improve the prediction accuracy. In the modeling, state–space model is developed to quantify the relationship between the information from online observation and underlying degradation, and the unscented particle filter is investigated to realize the assessment of remaining useful life. In particular, the sufficient statistic method is presented to obtain a joint recursive estimation on both the system state and model parameters for those state–space model with unknown time-invariant ones. At the end of this article, the acoustic emission signals of a milling cutter are illustrated as a case study for cutter online remaining useful life estimate. The milling cutter example demonstrates the effectiveness of the proposed method for online estimate and provides useful insights regarding the necessity of online updating and the assessment. © 2017, IMechE 2018."
,10.1177/0954410018781464,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048781175&doi=10.1177%2f0954410018781464&partnerID=40&md5=ced2eaeb04b05b3782dc1db337fcee88,"This paper presents an approach to solve the combined size and shape design optimization problems using recently developed subset simulation optimization for both continuous and discrete design variables. Except for the componentwise Metropolis–Hasting algorithm, a recently developed adaptive conditional sampling algorithm is also employed as an alternative approach for generating new conditional samples (candidate designs) for each simulation level, which enhances the accuracy and stability of the optimization process. Besides, a double-criterion sorting algorithm is used to handle the design constraints and integrate them in the generation of conditional samples during the Markov Chain Monte Carlo simulation, and the inverse transform method is employed to deal with the discrete design variables. Totally, four numerical examples are considered, including a 15-bar 2D truss, an 18-bar 2D truss, a 39-bar 3D truss and a truss-type landing gear of an unmanned aerial vehicle. The optimal designs obtained from subset simulation optimization using either the componentwise Metropolis–Hasting algorithm or the adaptive conditional sampling algorithm succeed in substantially reducing the weights of the truss-type structures under design constraints in terms of the member stress, the Euler buckling and the nodal displacement. The computational results indicate the proposed method can be taken as an alternative tool for structural optimization design on truss structures when involving the combined size and shape design. © 2018, © IMechE 2018."
,10.1080/00295639.2018.1512790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053869379&doi=10.1080%2f00295639.2018.1512790&partnerID=40&md5=00ac7d98ca375289292dd7be34bd5df4,"Two-fluid model-based multiphase computational fluid dynamics (MCFD) has been considered one of the most promising tools to investigate a two-phase flow and boiling system for engineering purposes. The MCFD solver requires closure relations to make the conservation equations solvable. The wall boiling closure relations, for example, provide predictions on wall superheat and heat partitioning. The accuracy of these closure relations significantly influences the predictive capability of the solver. In this paper, a study of validation and uncertainty quantification (VUQ) for the wall boiling closure relations in the MCFD solver is performed. The work has three purposes: (1) to identify influential parameters to the quantities of interest (QoIs) of the boiling system through sensitivity analysis (SA), (2) to evaluate the parameter uncertainty through Bayesian inference with the support of multiple data sets, and (3) to quantitatively measure the agreement between solver predictions and data sets. The widely used Kurul-Podowski wall boiling closure relation is studied in this paper. Several statistical methods are used, including the Morris Screening method for global SA, Markov Chain Monte Carlo for inverse Bayesian inference, and confidence interval as the validation metric. The VUQ results indicate that the current empirical correlations-based wall boiling closure relations achieved satisfactory agreement on wall superheat predictions. However, the closure relations also demonstrate intrinsic inconsistency and fail to give consistently accurate predictions for all QoIs over the well-developed nucleate boiling regime. ©, © American Nuclear Society."
5,10.1016/j.ress.2017.09.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030669828&doi=10.1016%2fj.ress.2017.09.029&partnerID=40&md5=b01c53d0339a78a276391261568464f6,"In nuclear reactor fuel performance simulation, fission gas release (FGR) and swelling involve treatment of several complicated and interrelated physical processes, which inevitably depend on uncertain input parameters. However, the uncertainties associated with these input parameters are only known by “expert judgment”. In this paper, inverse Uncertainty Quantification (UQ) under the Bayesian framework is applied to BISON code FGR model based on Risø-AN3 time series experimental data. Inverse UQ seeks statistical descriptions of the uncertain input parameters that are consistent with the available measurement data. It always captures the uncertainties in its estimates rather than merely determining the best-fit values. Kriging metamodel is applied to greatly reduce the computational cost during Markov Chain Monte Carlo sampling. We performed a dimension reduction for the FGR time series data using Principal Component Analysis. We also projected the original FGR time series measurement data onto the PC subspace as “transformed experiment data”. A forward uncertainty propagation based on the posterior distributions shows that the agreement between BISON simulation and Risø-AN3 time series measurement data is greatly improved. The posterior distributions for the uncertain input factors can be used to replace the expert specifications for future uncertainty/sensitivity analysis. © 2017 Elsevier Ltd"
,10.2514/6.2018-3775,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051289751&doi=10.2514%2f6.2018-3775&partnerID=40&md5=968b4d028f334adad0149548a5adc8fa,"This paper presents a statistical inference method for impedance eduction in a flow duct facility. The impedance is recast into a random variable, and Bayes’ theorem is used to obtain the posterior probability density of both its real and imaginary parts, thus expressing the knowledge/uncertainty one has on the impedance value, given a certain experimental data uncertainty. An evolutionary Markov-Chain Monte Carlo technique is selected to explore the probability space, and a surrogate model based on the method of snapshots is employed to speed up the calculations. The Linearized Euler Equations are solved using a two-dimensional Discontinuous Galerkin scheme, accounting for the presence of a grazing flow. The inference process is first validated on published NASA GIT results, where acoustic pressure measurements on the wall opposite the liner are used as inputs. Then, the same procedure is applied to educe the impedance of a conventional SDOF liner in ONERA’s B2A acoustic bench, where a Laser Doppler Velocimetry technique is used to measure the two components of the acoustic velocity fields above the liner. © 2018 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved."
,10.1016/j.ifacol.2018.09.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053821995&doi=10.1016%2fj.ifacol.2018.09.010&partnerID=40&md5=2659501294664123e6f18d69e11550ec,"A distinguishing feature of systems biology is the interrogation of models as a means of making predictions or generating deeper understanding of the systems under study. However, when using a given data set to address a specific question, a unique and provably correct model formulation to apply is rarely known. Instead, a large selection of alternative formulations of varying scopes ensues from combinatorial composition of entities. In this scenario, computational methods that allow us to make statistically valid inferences and predictions, while accounting for the uncertainty in model formulation are desired. We investigate into Bayesian Model Averaging (BMA), which accounts for model uncertainty by considering an ensemble of candidate models instead of a single model instance. To show the computational tractability of BMA, we perform model uncertainty analysis for a realistically sized reaction network from the domain of metabolic flux analysis, featuring an ensemble of millions of models. This is made possible using a Markov Chain Monte Carlo (MCMC) method, tailored to handle parameter and model structure uncertainty simultaneously. To investigate the computational burden of solving the multi-model problem, a super-model is created that includes the reactions of all models in the multi-model problem. The computational burden of the multi-model problem is compared to that of conventional MCMC inference on the single super-model. The comparison yields the surprising insight that the multi-model problem is computationally less expensive than the single super-model problem. Furthermore, we demonstrate, with the example at hand, that BMA yields valid structural network inferences. © 2018"
,10.1155/2018/1450683,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049110001&doi=10.1155%2f2018%2f1450683&partnerID=40&md5=62c9fc7e278274692ecc3f9b962c8c4d,"A key issue in assessment on tunnel face stability is a reliable evaluation of required support pressure on the tunnel face and its variations during tunnel excavation. In this paper, a Bayesian framework involving Markov Chain Monte Carlo (MCMC) simulation is implemented to estimate the uncertainties of limit support pressure. The probabilistic analysis for the three-dimensional face stability of tunnel below river is presented. The friction angle and cohesion are considered as random variables. The uncertainties of friction angle and cohesion and their effects on tunnel face stability prediction are evaluated using the Bayesian method. The three-dimensional model of tunnel face stability below river is based on the limit equilibrium theory and is adopted for the probabilistic analysis. The results show that the posterior uncertainty bounds of friction angle and cohesion are much narrower than the prior ones, implying that the reduction of uncertainty in cohesion and friction significantly reduces the uncertainty of limit support pressure. The uncertainty encompassed in strength parameters are greatly reduced by the MCMC simulation. By conducting uncertainty analysis, MCMC simulation exhibits powerful capability for improving the reliability and accuracy of computational time and calculations. © 2018 Weiping Liu et al."
,10.1016/j.swevo.2018.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053310178&doi=10.1016%2fj.swevo.2018.09.001&partnerID=40&md5=e91094e1480d60690818b39ee6b1b525,"State-of-the-art global optimization techniques adapt the search range and direction to the objective function. Differential Evolution algorithms perform this adaptation implicitly, leading to a contour fitting property which has been empirically observed, but lacks theoretical grounding. In this paper, we formalize the contour fitting notion and derive an analytical model that links the differential mutation operator with the adaptation of the range and direction of search. Our analysis uses the Differential Mutation Evolutionary Algorithm (DMEA), which optimizes the multidimensional Gaussian objective function. Through our analysis, we are able to make several observations. Firstly, a normally distributed population remains normal in consecutive iterations. Moreover, parameters of the population distribution can be updated with explicit algebraic formulas. Furthermore, for a scaling factor below a critical value, the population reaches a stable state. Finally, the covariance matrix of a population in a stable state is proportional to the covariance matrix of the Gaussian objective function. The analytical results explaining the contour fitting property are confirmed with a simulation study. Although this work focuses on theoretical analyses, the proposed DE/prop/1 mutation and DMEA algorithm maintain population diversity and therefore, can lead to optimization by means of saddle-crossing and can become a basis for adaptive Markov Chain Monte Carlo sampling schemes or an element of strategy adaptive differential evolution variants. The latter approach was tested on the CEC 2017 benchmark and found to significantly improve the performance of the SaDE algorithm. © 2018 Elsevier B.V."
,10.2436/20.8080.02.66,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049174062&doi=10.2436%2f20.8080.02.66&partnerID=40&md5=f08b9d29866ad327589e002e840258f9,"In this paper, it is proposed a Bayesian analysis of a time series in the presence of a random change-point and autoregressive terms. The development of this model was motivated by a data set related to the monthly number of asthma medications dispensed by the public health services of Ribeirão Preto, Southeast Brazil, from 1999 to 2011. A pronounced increase trend has been observed from 1999 to a specific change-point, with a posterior decrease until the end of the series. In order to obtain estimates for the parameters of interest, a Bayesian Markov Chain Monte Carlo (MCMC) simulation procedure using the Gibbs sampler algorithm was developed. The Bayesian model with autoregressive terms of order 1 fits well to the data, allowing to estimate the change-point at July 2007, and probably reflecting the results of the new health policies and previously adopted programs directed toward patients with asthma. The results imply that the present model is useful to analyse the monthly number of dispensed asthma medications and it can be used to describe a broad range of epidemiological time series data where a change-point is present. © 2018 Institut d'Estadistica de Catalunya. All Rights Reserved."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041237875&partnerID=40&md5=090d4df9946a00c288a954f9f966852e,"Complex approach to risk assessment is crucial especially if we deal with an offshore greenfield which development plan is hovering on the verge of profitability. The paper describes the algorithm and some outputs of application of the modern simulation technologies for assessment of the risks associated with the development of one of the Caspian offshore fields. The carried out risk assessment is based on multi-realization calculations including geomodeling, reservoir simulation and integrated modeling of fluid flow in wells and gathering system. Several types of variables affecting geovolumes and fluid flow are used to estimate uncertainties. The neighboring hydrocarbon field, where pre-Fasila formation has been under development for more than 20 years, is included into the model because its hydrodynamic connection with the studied field is in question. Markov Chain Monte Carlo technique is applied to get a posterior distribution of the future reservoir production. The calculation resulted in the conclusion that two offshore platforms should be installed at the middle and south-eastern parts of the field. This conclusion respects the risk defined by two main sources of uncertainty, the impact of which varies in different parts of the field. The applied approach enables quantitative risk assessment to be made in acceptable time, thus helps to make transparent and informed decisions and improves the project management efficiency. Copyright 2017, Society of Petroleum Engineers."
,10.1007/978-3-319-93713-7_16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049009241&doi=10.1007%2f978-3-319-93713-7_16&partnerID=40&md5=f8ff547a1461ab2a1fe1e0b1abc93654,"A new Monte Carlo algorithm for solving singular linear systems of equations is introduced. In fact, we consider the convergence of resolvent operator Rλ and we construct an algorithm based on the mapping of the spectral parameter λ. The approach is applied to systems with singular matrices. For such matrices we show that fairly high accuracy can be obtained. © 2018, Springer International Publishing AG, part of Springer Nature."
2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046441194&partnerID=40&md5=a48956caa81193874d73bf2740b9ec5c,"To develop the opposing left-turn traffic conflict model of the signalized intersections, the traffic conflict data and traffic volume data were extracted from 101 hours video data at 12 signalized intersections in Vancouver by the computer vision techniques. Given the impacts of traffic flow status on the traffic conflicts, the traffic flow status was divided into 4 scenarios based on the v/c indicator. The traffic conflict model based on multivariate Poisson-lognormal distribution under multiple scenarios and the traffic conflict model based on Poisson-lognormal distribution under the single scenario were developed. The posterior distribution of the models parameters were derived by the Bayesian estimation method. Based on the Markov Chain Monte Carlo simulation, model parameters were estimated. Using the deviance information criterion (DIC) and the models' expectation variance, the goodness-of-fit and the precision of the models were compared. The results show that the goodness-of-fit of the multivariate Poisson-lognormal based traffic conflict model is superior to the single Poisson-lognormal based traffic conflict model. The precision of multivariate Poisson-lognormal based traffic conflict model under the 4 scenarios are twice, 1.5 times, twice, and 1.4 times larger than that of the single Poisson-lognormal based traffic conflict model. The conflicting volumes have different impacts on traffic conflicts under different traffic flow status. Results of elasticity analysis show that a 1% increase in traffic flow of through movement may increase the opposing left-turn conflict frequency by 0.36%, 0.56%, 0.17%, and 0.78% for traffic scenario 1, 2, 3, and 4, respectively, given that the left-turn traffic flow remains the same. Accordingly, a 1% increase of the traffic flow in the left-turn may increase the opposing left-turn conflict frequency by 0.40%, 0.67%, 0.40%, and 0.51% for traffic scenario 1, 2, 3, and 4, respectively, given that the through traffic flow remains the same. © 2018, Editorial Department of China Journal of Highway and Transport. All right reserved."
,10.1080/00295639.2018.1499279,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051918828&doi=10.1080%2f00295639.2018.1499279&partnerID=40&md5=03a22656f6ef2d87047cf1ad661b9dfa,"In the framework of Best Estimate Plus Uncertainty methodology, the uncertainties involved in model predictions must be quantified to prove that the investigated design is reasonable and acceptable. The uncertainties in predictions are usually calculated by propagating input uncertainties through the simulation model, which requires knowledge of the model or code input uncertainties, for example, the means, variances, distribution types, etc. However, in best-estimate system thermal-hydraulic codes such as TRACE, some parameters in empirical correlations may have large uncertainties that are unknown to code users, and their uncertainties are therefore simply ignored or described by expert opinion. In this paper, the issue of missing uncertainty information for physical model parameters in the thermal-hydraulic code TRACE is addressed with inverse uncertainty quantification (IUQ), using the steady-state void fraction experimental data in the Organisation for Economic Co-operation and Development/Nuclear Energy Agency PSBT (Pressurized water reactor Sub-channel and Bundle Tests benchmark. The IUQ process is formulated through a Bayesian perspective, which can yield the posterior distributions of the uncertain inputs. A Gaussian process emulator is employed to significantly reduce the computational burden involved in sampling the posteriors using the Markov Chain Monte Carlo method. The posterior distributions are further used in forward uncertainty quantification and sensitivity analysis to quantify the influences of those parameters on the quantities of interest. The results demonstrate the effectiveness of the IUQ framework with a practical nuclear engineering example and show the necessity of quantifying and reducing uncertainty of physical model parameters in future work. ©, © American Nuclear Society."
,10.4271/2018-01-1103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045512323&doi=10.4271%2f2018-01-1103&partnerID=40&md5=eca0765104e2c4839d98092e9378a52f,"This paper presents an approach for comparing alternative repairable systems and calculating the value of information obtained by testing a specified number of such systems. More specifically, an approach is presented to determine the value of information that comes from field testing a specified number of systems in order to appropriately estimate the reliability metric associated with each of the respective repairable systems. Here the reliability of a repairable system will be measured by its failure rate. In support of the decision making effort, the failure rate is translated into an expected utility based on a utility curve that represents the risk tolerance of the decision maker. The algorithm calculates the change of the expected value of the decision with the sample size. The change in the value of the decision represents the value of information obtained from testing. The approach uses a Bayesian probability model, which allows the decision maker to incorporate subjective priors on the reliability performance of the design alternatives. The dependency is modeled using Copulas to couple the marginal prior distributions of the alternatives to a single, joint prior. The procedure being presented in this paper uses Markov Chain Monte Carlo simulation (MCMC) to determine the posterior probability density and the resulting expected utility of the decision. The approach considers design alternatives based on failure rate metric, e.g. the number of failures per unit (FPU) or the number of failures per unit time, and utilizes Archimedean Copulas to couple the dependent marginals that describe the priors for each design alternative's failure per unit behavior. This paper is an extension the paper ""Assessing the Value of Information for Multiple, Correlated Design Alternatives"" (Capser and Nikolaidis, 2017), which presented an approach for determining optimal sample sizes for assessing correlated non-repairable design alternatives based on the prior estimate of their joint failure probability. © 2018 SAE International. All Rights Reserved."
,10.1093/GJI/GGY302,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054872776&doi=10.1093%2fGJI%2fGGY302&partnerID=40&md5=8229ce801c53e0dffc36e58fb4b98461,"While single point source is an oversimplified representation of medium to large earthquakes, finite fault models in many cases over parameterize the inversion due to the lack of sufficient near field data. Multiple point source solutions can fill in the gap in-between these two representations. Here, we propose a Markov-Chain-Monte-Carlo multiple point source inversion scheme, in combination with the advantage of the cut-and-paste technique, which cuts the seismogram into Pnl and surface portions and allows different time-shifts for each segment to align data with the synthetics. We apply the approach to the Mw 6.2 foreshock in the 2016 Kumamoto earthquake sequence by using the strong-motion observations within 100 km. We are able to perform the inversion at relatively high-frequency ranges (0.02-0.28 Hz for Pnl and 0.02-0.20 Hz for surface waves) with confidence on the velocity model built on the Mw 5.4 path calibration event. Our results show that the rupture was mainly composed of three subevents, with total duration of about 12 s and total Mw of 6.2. The strikes of three subevents agree well with surface fault mapping where the Futagawa Fault intersects the Hinagu Fault with ~30° difference in strike. Our solution shows that the first subevent dips to the southeast, while the second and the third subevents, located ~3 km the north and ~4 km to the southwest of the first subevent, dip to the northwest. The focal mechanism of the first subevent shows remarkable agreement with the first-motion solution. The fault geometry also shows well consistence with the relocated aftershocks, which delineate a SE-dipping fault around the first subevent and two NW-dipping faults to the north and south, respectively, corresponding to the second and the third subevent. The sum of moment tensor of subevents that have not only different geometry but also rake angles shows strong Compensated Linear Vector Dipole (CLVD) component (48-50 per cent). With a local 1D crustal model, a full-moment-tensor inversion using regional long-period waveform data also detects strong CLVD component of this earthquake. In contrast, using the PREM model results in an almost pure double-couple solution. In short, we have precisely resolved the rupture process, the intricate fault geometry, and the strong CLVD component with strong-motion data. This highlights the importance of extracting the relatively high-frequency information from the waveformdata and with accurate velocity model in seismic source analyses of large earthquakes. © The Author(s) 2018. Published by Oxford University Press on behalf of The Royal Astronomical Society."
1,10.1016/j.radphyschem.2018.06.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049553832&doi=10.1016%2fj.radphyschem.2018.06.024&partnerID=40&md5=9c964a1113932015957447a1606bff7a,"We apply Bayesian techniques to determine the location and intensity of a gamma radiation source in an urban environment using count rates taken from a distributed detector network. A simplified model of the radiation transport process is used to construct a statistical model for the detector count rates in the presence of a randomly varying background. Markov Chain Monte Carlo is used to generate samples from the Bayesian posterior density, which can be used to inform search and interdiction efforts. We also present a modification of the traditional Metropolis sampling algorithm that allows us to incorporate fixed parameter uncertainties in building macroscopic cross sections and account for their effects on the posterior distribution. This method is then applied to a test problem based on a real urban geometry with different levels of uncertainty in the building cross sections. The results show that the uncertainty in the estimated source location is modest, even with a large degree of uncertainty in the building cross sections. © 2018"
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051627635&partnerID=40&md5=82e194be90611ab16f870ede95f7ce07,"Since LWD deep azimuthal resistivity service was first introduced a decade ago followed by the ultra-deep azimuthal resistivity a few years ago, the new service has been under spotlight and drawn great attention from operators. The azimuthal propagation resistivity tools all use the concept of multi-spacings, multi-frequencies, and multi-components. The measurements acquired by the tool are much richer than that of the conventional omni-directional propagation resistivity. The applications of the new service are widely ranged from the well placement, the reservoir mapping, the geo-stopping, the landing, the fault detection to the salt edge detection, etc. However, due to the complexity of the measurement physics, the tool response characteristics and the data processing/inversion, without the well understanding the uncertainty of the service, operators do not have sufficient confidence to use the service as much as expected. To promote the understanding of the technology and clear many questions surfing around the industry, in this paper, we systematically study the sensitivity and quantify the uncertainty of the azimuthal propagation resistivity technology in various formation model. The sensitivity of the measurements to the dip angle, the anisotropy, the layer boundaries, and the formation resistivity is essential to assess the capability of the technology for practical applications such as the reservoir boundary mapping, the formation evaluation and the well placement. A group of studies is conducted to evaluate the sensitivity under several common situations including homogeneous isotropy formation, homogeneous anisotropy formation, and layered formation. The information content of the measurements and the proper use of the measurements is clearly demonstrated. The depth of detection (DoD) in a two-layer formation presented in the format of “Picasso” plot is studied. The common practice to produce “Picasso” plot is based on the noise threshold of the measurement which is not always realistic. It only reflects the quality of the measurements rather than the quality (error bar) of the distance to boundary (D2B) resulting from the inversion processing. D2B-error-based DoD is investigated in the paper. The comparison between these two methods reveals the commonly used, noise-based DoD is considerably overestimated. A set of 1D formation models, proposed by SPWLA Resistivity Special Interest Group (RtSIG) chapter, is used to quantify the uncertainty of the bed boundary position, the formation resistivity, the dip angle through a novel statistical analysis, the trans-dimensional Markov Chain Monte Carlo (tMCMC) method. The probability maps of the boundary interface and the distribution of the resistivity profile can be extracted from the statistical characteristics of the posterior probability distribution (PPD). The exercise of the statistical solver on the formation models recommended by SPWLA RtSIG demonstrates that the uncertainty quantification techniques can be crucial to assess the azimuthal propagation resistivity technology. A field example from a subsea gas well of Wheatstone liquefied-natural-gas project in Western Australia is used to confirm the importance of the uncertainty quantification in evaluating the capacity of the azimuthal propagation resistivity measurements. Copyright 2018, held jointly by the Society of Petrophysicists and Well Log Analysts (SPWLA) and the submitting authors."
,10.1007/978-3-319-95165-2_22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049978878&doi=10.1007%2f978-3-319-95165-2_22&partnerID=40&md5=dcf2f36f727d836cbb08bcdbaa2149d3,"In subsurface characterization using a history matching algorithm subsurface properties are reconstructed with a set of limited data. Here we focus on the characterization of the permeability field in an aquifer using Markov Chain Monte Carlo (MCMC) algorithms, which are reliable procedures for such reconstruction. The MCMC method is serial in nature due to its Markovian property. Moreover, the calculation of the likelihood information in the MCMC is computationally expensive for subsurface flow problems. Running a long MCMC chain for a very long period makes the method less attractive for the characterization of subsurface. In contrast, several shorter MCMC chains can substantially reduce computation time and can make the framework more suitable to subsurface flows. However, the convergence of such MCMC chains should be carefully studied. In this paper, we consider multi-MCMC chains for a single–phase flow problem and analyze the chains aiming at a reliable characterization. © Springer International Publishing AG, part of Springer Nature 2018."
,10.1049/iet-rsn.2017.0235,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040184706&doi=10.1049%2fiet-rsn.2017.0235&partnerID=40&md5=63e446552a2f83d30229167f5910cbee,"This study deals with the problem of joint delay-Doppler estimation in a practically motivated scenario of passive bistatic radar, where the surveillance channel is polluted by the direct-path signal residual. A new joint delay-Doppler maximum-likelihood estimator (MLE) based on Markov chain Monte Carlo (MCMC) is proposed. The MCMC method allows one to compute the MLE in a computationally efficient manner. The proposed estimator is based upon generating random variates using a Markov Chain whose stationary distribution approximates the likelihood function and guarantees convergence to the global maximum. In contrast to the recently proposed modified cross-correlation estimator, and the expectation-maximisation-based MLE, it avoids grid search which may lead to a straddle loss or initialisation-dependent iteration which may lead to convergence problems. Simulation results indicate that the proposed estimator achieves a significant performance improvement over existing methods. © 2017. The Institution of Engineering and Technology."
,10.1007/s11222-017-9722-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010735053&doi=10.1007%2fs11222-017-9722-1&partnerID=40&md5=42d16fd03b90b903224cb9ef19cb701b,"We present a sequential Monte Carlo algorithm for Markov chain trajectories with proposals constructed in reverse time, which is advantageous when paths are conditioned to end in a rare set. The reverse time proposal distribution is constructed by approximating the ratio of Green’s functions in Nagasawa’s formula. Conditioning arguments can be used to interpret these ratios as low-dimensional conditional sampling distributions of some coordinates of the process given the others. Hence, the difficulty in designing SMC proposals in high dimension is greatly reduced. Empirically, our method outperforms an adaptive multilevel splitting algorithm in three examples: estimating an overflow probability in a queueing model, the probability that a diffusion follows a narrowing corridor, and the initial location of an infection in an epidemic model on a network. © 2017, Springer Science+Business Media New York."
1,10.1093/COMNET/CNX024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044219998&doi=10.1093%2fCOMNET%2fCNX024&partnerID=40&md5=e3750053658fa0dc5aa95aec3d005012,"Spreading processes are ubiquitous in natural and artificial systems. They can be studied via a plethora of models, depending on the specific details of the phenomena under study. Disease contagion and rumour spreading are among the most important of these processes due to their practical relevance. However, despite the similarities between them, current models address both spreading dynamics separately. In this article, we propose a general spreading model that is based on discrete time Markov chains. The model includes all the transitions that are plausible for both a disease contagion process and rumour propagation. We show that our model not only covers the traditional spreading schemes but that it also contains some features relevant in social dynamics, such as apathy, forgetting, and lost/recovering of interest. The model is evaluated analytically to obtain the spreading thresholds and the early time dynamical behaviour for the contact and reactive processes in several scenarios. Comparison with Monte Carlo simulations shows that the Markov chain formalism is highly accurate while it excels in computational efficiency. We round off our work by showing how the proposed framework can be applied to the study of spreading processes occurring on social networks. © The authors 2017. Published by Oxford University Press. All rights reserved."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050285932&partnerID=40&md5=5c20f7014880203ade4fd2f491a8bfb7,"In this study, we discuss effects of an observational error of phase velocity on a shallow phase velocity profile from a numerical case. Uncertainty in S-wave velocity profile is obtained from inversion of the phase velocity using the Markov-Chain Monte Carlo. The uncertainty of S-wave velocity profile is used to estimate the variability of nonlinear soil amplification. Variability of linear amplification is also estimated as the comparison to the nonlinear one. We find that the nonlinear amplification has less variability than linear response because of decrease of shear modulus and the increase of damping in nonlinear amplification. © 2018 European Association of Geoscientists and Engineers EAGE. All rights reserved."
,10.1007/978-3-319-77404-6_63,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045409316&doi=10.1007%2f978-3-319-77404-6_63&partnerID=40&md5=98958ea74b0b27e4044f370a904d8358,"Counting perfect matchings has played a central role in the theory of counting problems. The permanent, corresponding to bipartite graphs, was shown to be #P-complete to compute exactly by Valiant (1979), and a fully polynomial randomized approximation scheme (FPRAS) was presented by Jerrum, Sinclair, and Vigoda (2004) using a Markov chain Monte Carlo (MCMC) approach. However, it has remained an open question whether there exists an FPRAS for counting perfect matchings in general graphs. In fact, it was unresolved whether the same Markov chain defined by JSV is rapidly mixing in general. In this paper, we show that it is not. We prove torpid mixing for any weighting scheme on hole patterns in the JSV chain. As a first step toward overcoming this obstacle, we introduce a new algorithm for counting matchings based on the Gallai−Edmonds decomposition of a graph, and give an FPRAS for counting matchings in graphs that are sufficiently close to bipartite. In particular, we obtain a fixed-parameter tractable algorithm for counting matchings in general graphs, parameterized by the greatest “order” of a factor-critical subgraph. © Springer International Publishing AG, part of Springer Nature 2018."
3,10.1111/geb.12666,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038123927&doi=10.1111%2fgeb.12666&partnerID=40&md5=c834d376b900f36051824a78e2f87dd0,"Aim: Species distribution models are important tools used to study the distribution and abundance of organisms relative to abiotic variables. Dynamic local interactions among species in a community can affect abundance. The abundance of a single species may not be at equilibrium with the environment for spreading invasive species and species that are range shifting because of climate change. Innovation: We develop methods for incorporating temporal processes into a spatial joint species distribution model for presence/absence and ordinal abundance data. We model non-equilibrium conditions via a temporal random effect and temporal dynamics with a vector-autoregressive process allowing for intra- and interspecific dependence between co-occurring species. The autoregressive term captures how the abundance of each species can enhance or inhibit its own subsequent abundance or the subsequent abundance of other species in the community and is well suited for a ‘community modules’ approach of strongly interacting species within a food web. R code is provided for fitting multispecies models within a Bayesian framework for ordinal data with any number of locations, time points, covariates and ordinal categories. Main conclusions: We model ordinal abundance data of two invasive insects (hemlock woolly adelgid and elongate hemlock scale) that share a host tree and were undergoing northwards range expansion in the eastern U.S.A. during the period 1997–2011. Accounting for range expansion and high inter-annual variability in abundance led to improved estimation of the species–environment relationships. We would have erroneously concluded that winter temperatures did not affect scale abundance had we not accounted for the range expansion of scale. The autoregressive component revealed weak evidence for commensalism, in which adelgid may have predisposed hemlock stands for subsequent infestation by scale. Residual spatial dependence indicated that an unmeasured variable additionally affected scale abundance. Our robust modelling approach could provide similar insights for other community modules of co-occurring species. © 2017 John Wiley & Sons Ltd"
,10.1111/cgf.13585,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054602663&doi=10.1111%2fcgf.13585&partnerID=40&md5=1ab781dd0d2387f19f97aa536456d818,"We present a system to generate a procedural environment that produces a desired crowd behaviour. Instead of altering the behavioural parameters of the crowd itself, we automatically alter the environment to yield such desired crowd behaviour. This novel inverse approach is useful both to crowd simulation in virtual environments and to urban crowd planning applications. Our approach tightly integrates and extends a space discretization crowd simulator with inverse procedural modelling. We extend crowd simulation by goal exploration (i.e. agents are initially unaware of the goal locations), variable-appealing sign usage and several acceleration schemes. We use Markov chain Monte Carlo to quickly explore the solution space and yield interactive design. We have applied our method to a variety of virtual and real-world locations, yielding one order of magnitude faster crowd simulation performance over related methods and several fold improvement of crowd indicators. © 2018 The Authors Computer Graphics Forum © 2018 The Eurographics Association and John Wiley & Sons Ltd."
,10.1016/j.procs.2018.08.180,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053155539&doi=10.1016%2fj.procs.2018.08.180&partnerID=40&md5=d5a00a331d11b466a93733608e8e209f,"Spatio-temporal analysis widely used to describe geo-referenced data that contain information about space and time, with many important response variables and predictors. The models are usually presented as maps to represent the spatial dependence and temporal correlation from time to time. Spatio-temporal models presented in this paper are designed with hierarchical fashion and estimated with INLA (Integrated Nested Laplace Approximation) as the current estimation method for Bayesian analysis. INLA based on latent Gaussian posterior distribution which provides great computational benefit and solve the convergence issue in MCMC (Markov Chain Monte Carlo) algorithm. We model the poverty data set using classical, dynamic and space-time interaction of spatio-temporal models, and investigate the poverty relationship with socio-economics predictors. Using R-INLA package and deviance information criteria for models best fit selection, we conclude dynamical non-parametric is the most proper model on its ecological regressions. © 2018 The Authors. Published by Elsevier Ltd."
,10.1137/16M1107401,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049453677&doi=10.1137%2f16M1107401&partnerID=40&md5=59020629cbcb66b4e3c21a87e750c318,"We propose an algorithm for the efficient and robust sampling of the posterior probability distribution in Bayesian inference problems. The algorithm combines the local search capabilities of the manifold Metropolis adjusted Langevin transition kernels with the advantages of global exploration by a population based sampling algorithm, the transitional Markov chain Monte Carlo (TMCMC). The Langevin diffusion process is determined by either the Hessian or the Fisher information of the target distribution with appropriate modifications for non–positive definiteness. The present method is shown to be superior to other population based algorithms, in sampling probability distributions for which gradients are available, and is shown to handle otherwise unidentifiable models. We demonstrate the capabilities and advantages of the method in computing the posterior distribution of the parameters in a Pharmacodynamics model, for glioma growth and its drug induced inhibition, using clinical data. © 2018 Society for Industrial and Applied Mathematics."
,10.1007/978-3-319-77249-3_28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045965513&doi=10.1007%2f978-3-319-77249-3_28&partnerID=40&md5=cdcb87efc608fc1e99d0d8933ec568ef,"In this study, a simulation-based method for computing joint maximum likelihood estimates of cognitive diagnosis model parameters is proposed. The central theme of the approach is to reduce the complexity of models to focus on their most critical elements. In particular, an approach analogous to joint maximum likelihood estimation is taken, and the latent attribute vectors are regarded as structural parameters, not parameters to be removed by integration with this approach, the joint distribution of the latent attributes does not have to be specified, which reduces the number of parameters in the model. The Markov Chain Monte Carlo algorithm is used to simultaneously evaluate and optimize the likelihood function. This streamlined approach performed as well as more traditional methods for models such as the DINA, and affords the opportunity to fit more complicated models in which other methods may not be feasible. © Springer International Publishing AG, part of Springer Nature 2018."
,10.1137/16M1108716,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045045191&doi=10.1137%2f16M1108716&partnerID=40&md5=78d38a28c34f29d823622eff8d312634,"We propose two algorithms for simulating continuous time Markov chains in the presence of metastability. We show that the algorithms correctly estimate, under the ergodicity assumption, stationary averages of the process. Both algorithms, based on the idea of the parallel replica method, use parallel computing in order to explore metastable sets more efficiently. The algorithms require no assumptions on the Markov chains beyond ergodicity and the presence of identifiable metastability. In particular, there is no assumption on reversibility. For simpler illustration of the algorithms, we assume that a synchronous architecture is used throughout the paper. We present error analyses, as well as numerical simulations on multiscale stochastic reaction network models in order to demonstrate consistency of the method and its efficiency. © 2018 Society for Industrial and Applied Mathematics."
,10.1007/978-3-319-91938-6_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049182096&doi=10.1007%2f978-3-319-91938-6_11&partnerID=40&md5=0ed4a72947a6f3ee00f361b6214dc4a7,"There is an extensive literature using probabilistic models, such as hidden Markov models, for the analysis of biological sequences. These models have a clear theoretical basis, and many heuristics have been developed to reduce the time and memory requirements of the dynamic programming algorithms used for their inference. Nevertheless, mirroring the shift in natural language processing, bioinformatics is increasingly seeing higher accuracy predictions made by recurrent neural networks (RNN). This shift is exemplified by basecalling on the Oxford Nanopore Technologies’ sequencing platform, in which a continuous time series of current measurements is mapped to a string of nucleotides. Current basecallers have applied connectionist temporal classification (CTC), a method originally developed for speech recognition, and focused on the task of decoding RNN output from a single read. We wish to extend this method for the more general task of consensus basecalling from multiple reads, and in doing so, exploit the gains in both accelerated algorithms for sequence analysis and recurrent neural networks, areas that have advanced in parallel over the past decade. To this end, we develop a dynamic programming algorithm for consensus decoding from a pair of RNNs, and show that it can be readily optimized with the use of an alignment envelope. We express this decoding in the notation of finite state automata, and show that pair RNN decoding can be compactly represented using automata operations. We additionally introduce a set of Markov chain Monte Carlo moves for consensus basecalling multiple reads. © 2018, Springer International Publishing AG, part of Springer Nature."
,10.6052/j.issn.1000-4750.2017.01.0075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050101512&doi=10.6052%2fj.issn.1000-4750.2017.01.0075&partnerID=40&md5=53d50463dfd95f38f3c69779c0e233a6,"Traditional models for shear strength of reinforced concrete (RC) beams are generally deterministic models and exhibit low computational accuracy and large numerical fluctuation, due to the fact that they do not take into account the aleatory (physical) uncertainties of various parameters such as geometry, material properties and boundary conditions as well as epistemic (model) uncertainties of the modelling. Based on the modified compression field theory (MCFT) and the critical crack angle model considering the influence of shear span ratio, a deterministic model for shear strength of RC beams was established first. Subsequently, a probabilistic model for shear strength of RC beam was developed by using the Bayesian theory and the Markov Chain Monte Carlo (MCMC) to take into account the influences of both epistemic and aleatory uncertainties. Finally, the applicability and efficiency of the proposed probabilistic model were validated by comparing with experimental data and traditional deterministic models. Analysis results show that the proposed probabilistic model is of good accuracy and adaptability. The model not only can describe the probabilistic distribution characteristics of shear strength of RC beams, but also provide a benchmark to calibrate the confidence level of traditional deterministic models and provide an efficient way to determine the characteristic values of shear strength of RC beams with different confidence levels. © 2018, Engineering Mechanics Press. All right reserved."
,10.1016/j.compenvurbsys.2018.09.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053935382&doi=10.1016%2fj.compenvurbsys.2018.09.003&partnerID=40&md5=d22f537f2a1ac93ed3717680024edbeb,"Social and physical processes often exhibit both macro-level geographic smoothness – implying positive spatial dependence – and micro-level discontinuities – suggesting implicit step changes or boundaries in the data. However, a simultaneous treatment of the two features in a unified statistical model poses great challenges. This study extends an innovative locally adaptive spatial auto-regressive modelling approach to a multi-level modelling framework in order to explore multiple-scale geographical data. It develops a Bayesian locally adaptive spatial multi-level model that takes into account horizontal global spatial dependence and local step changes, as well as a vertical group dependency effect imposed by the multiple-scale data structure. At its heart, the correlation structures of spatial units implied by a spatial weights matrix are learned along with other model parameters using an iterative estimation algorithm, rather than being assumed to be invariant and exogenous. A Bayesian Markov chain Monte Carlo (MCMC) sampler for implementing this new spatial multi-level model is derived. The developed methodology is applied to infer neighbourhood quality using property transaction data, and to examine potential correlates of neighbourhood quality in Liverpool. The results reveal a complex and fragmented geography of neighbourhood quality; besides an overall smoothness trend, boundaries delimiting neighbourhood quality are scattered across Liverpool. Socio-economics, built environment, and locational characteristics are statistically significantly associated with neighbourhood quality. © 2018 The Authors"
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051522665&partnerID=40&md5=622b17c2c196328da09a0d51328c16c0,"The availability of spatial and spatio-temporal data has widely increased and allow researcher to describe potential geographical pattern, including information about space and time (and its interraction) in many scientific fields. Bayesian method to deal with spatial and spatio-temporal data extensively approach with Markov Chain Monte Carlo (MCMC), however, when models are complex and designed with hierarchical fashion, MCMC algorithms may be extremely slow and even become computationally unfeasible. The Integrated Nested Laplace Approximation (INLA) algorithm is current development in R-INLA package in R, designed to deal with fundamental limitation of MCMC computation. This paperpurpose to investigate how the socioeconomic information (i.e. population density, expectation years ofschooling and construction overpriced index) effect the number of poor people in East Java province, Indonesia, using Bayes spatial model. Investigation result that expectation years of schooling has greatesteffect in reducing number of poor people. Not only on its spatial pattern, we also investigate timedependency of poor people data from years 2012 to 2016 using classical, dynamic and space-timeinteraction of Bayes spatio-temporal models. In this paper, the computational aspect is efficiently solvedwith R-INLA, resulting dynamic Bayes Spatio-temporal is the best model based on the smallest DevianceInformation Criteria. © IEOM Society International."
,10.1002/stc.2258,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053530647&doi=10.1002%2fstc.2258&partnerID=40&md5=39503428f97337325213fbaafb6d7d71,"This paper presents a Bayesian model updating methodology for dynamical systems with geometric nonlinearities based on their nonlinear normal modes (NNMs) extracted from broadband vibration data. Model parameters are calibrated by minimizing selected metrics between identified and model-predicted NNMs. In the first approach, a deterministic formulation is adopted, and parameters are updated by minimizing a nonlinear least-squares objective function. A probabilistic approach based on Bayesian inference is next investigated, where a Transitional Markov Chain Monte Carlo is implemented to sample the joint posterior probability distribution of the nonlinear model parameters. Bayesian model calibration has the advantage to quantify parameter uncertainty and to provide an estimation of model evidence for model class selection. The two formulations are evaluated when applied to a numerical cantilever beam with geometrical nonlinearity. The NNMs of the beam are derived from simulated broadband data through nonlinear subspace identification and numerical continuation. Accuracy of model updating results is studied with respect to the level of measurement noise, the number of available datasets, and modeling errors. © 2018 John Wiley & Sons, Ltd."
,10.1137/16M1088417,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046726232&doi=10.1137%2f16M1088417&partnerID=40&md5=f43fa7466e2412f91839314941136cf2,"In parameter estimation problems one computes a posterior distribution over uncertain parameters defined jointly by a prior distribution, a model, and noisy data. Markov chain Monte Carlo (MCMC) is often used for the numerical solution of such problems. An alternative to MCMC is importance sampling, which can exhibit near perfect scaling with the number of cores on high performance computing systems because samples are drawn independently. However, finding a suitable proposal distribution is a challenging task. Several sampling algorithms have been proposed over the past years that take an iterative approach to constructing a proposal distribution. We investigate the applicability of such algorithms by applying them to two realistic and challenging test problems, one in subsurface flow, and one in combustion modeling. More specifically, we implement importance sampling algorithms that iterate over the mean and covariance matrix of Gaussian or multivariate t-proposal distributions. Our implementation leverages massively parallel computers, and we present strategies to initialize the iterations using “coarse” MCMC runs or Gaussian mixture models. © 2018 Society for Industrial and Applied Mathematics."
2,10.1190/GEO2017-0075.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037054481&doi=10.1190%2fGEO2017-0075.1&partnerID=40&md5=b066c0fb98115fd8794987e70cc1395d,"We consider the problem of fluid identification and fracture detection in unconventional reservoir (tight gas sand and shale gas) characterization. We begin with a simplification of the stiffness parameters and the derivation of a linearized reflection coefficient and azimuthal elastic impedance (EI). The accuracy of the simplification is confirmed in application to gas-bearing fractured rocks with low porosity and small fracture density.We have developed a modified fluid factor that is more sensitive to fluid type and less influenced by porosity. A two-step inversion workflow is evaluated based on the derived linearized reflection coefficient and azimuthal EI, including (1) a damped least-squares inversion for azimuthal EI, constrained by an initial model, and (2) a Bayesian Markov chain Monte Carlo inversion for the modified fluid factor and dry fracture weaknesses. Stability and accuracy are examined with synthetic data, from which we conclude that the modified fluid factor and dry fracture weaknesses can be stably determined in the presence of moderate data error/noise. The stability of our approach is further confirmed on a fractured tight gas sand field data set, within which we observe that geologically reasonable parameters (Lamé constants, the modified fluid factor, and dry fracture weaknesses) are determined. We conclude that our inversion workflow and its underlying assumptions form realistic predictions/discriminations of reservoir fracture and fluid parameters. © 2018 Society of Exploration Geophysicists."
,10.1016/j.imavis.2017.11.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034856672&doi=10.1016%2fj.imavis.2017.11.004&partnerID=40&md5=12c5e15442f09f0b3ea3ca42dc0f17bd,"In this paper, we present a novel tracking system based on edge-based object proposal and data association called object proposal association. Our object proposal method accurately detects and localizes objects in an image by searching for object-like regions, with the assumption that an object is represented by a closed boundary. To search for closed boundaries in an image, we present a new Edge Fields (EFs) technique. Using this technique, our method can extract high-quality edges and can obtain accurate boundaries from the image. The EFs technique consists of blurring and thresholding steps, where the former helps extract high-quality edges and the latter prevents the method from losing image details while blurring. After the method extracts object-like regions, we associate the regions in the previous frame with those in the current frame. For this purpose, using the Markov chain Monte Carlo data association (MCMCDA) algorithm, we can find pairs of similar regions across two frames. Experimental results demonstrate that our object proposal method is competitive with state-of-the-art object proposal methods on the PASCAL VOC 2007 dataset. Our tracking method is also competitive with state-of-the-art tracking methods on Object Tracking Benchmark dataset. © 2017 Elsevier B.V."
,10.1007/978-3-319-91436-7_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049896614&doi=10.1007%2f978-3-319-91436-7_2&partnerID=40&md5=4bbd962b0a0634b5ee7d5ece01063ce2,"We survey basic ideas and results on randomized quasi-Monte Carlo (RQMC) methods, discuss their practical aspects, and give numerical illustrations. RQMC can improve accuracy compared with standard Monte Carlo (MC) when estimating an integral interpreted as a mathematical expectation. RQMC estimators are unbiased and their variance converges at a faster rate (under certain conditions) than MC estimators, as a function of the sample size. Variants of RQMC also work for the simulation of Markov chains, for function approximation and optimization, for solving partial differential equations, etc. In this introductory survey, we look at how RQMC point sets and sequences are constructed, how we measure their uniformity, why they can work for high-dimensional integrals, and how can they work when simulating Markov chains over a large number of steps. © 2018, Springer International Publishing AG, part of Springer Nature."
,10.1080/13504851.2018.1512740,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053262490&doi=10.1080%2f13504851.2018.1512740&partnerID=40&md5=15ae135412072bb22c8058e79583dc7d,"The distribution of asset returns has often been proved to be heavy-tailed. In this paper, based on the Fama-French five-factor model with multivariate t-distribution, we develop a convenient and explicit Bayesian approach to test asset pricing. The developed test statistic is only by-product of the Markov Chain Monte Carlo (MCMC) outputs, and hence it is very convenient in practice. Simulation studies demonstrate the effectiveness of the finite sample performance of the proposed approach. Finally, the Fama-French data are used for testing the efficiency of financial markets, and the result shows that the market efficiency is rejected. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.1137/16M1078471,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053600147&doi=10.1137%2f16M1078471&partnerID=40&md5=59fa967d1bc6e4a49a7fd918bd48bd80,"The problem of identifying a planted assignment given a random k-satisfiability (k-SAT) formula consistent with the assignment exhibits a large algorithmic gap: while the planted solution becomes unique and can be identified given a formula with O(n log n) clauses, there are distributions over clauses for which the best-known efficient algorithms require nk/2 clauses. We propose and study a unified model for planted k-SAT, which captures well-known special cases. An instance is described by a planted assignment σ and a distribution on clauses with k literals. We define its distribution complexity as the largest r for which the distribution is not r-wise independent (1 ≤ r ≤ k for any distribution with a planted assignment). Our main result is an unconditional lower bound, tight up to logarithmic factors, for statistical (query) algorithms [M. Kearns, J. ACM, 45 (1998), pp. 983–1006; V. Feldman, E. Grigorescu, L. Reyzin, S. S. Vempala, and Y. Xiao, J. ACM, 64 (2017), pp. 8:1–8:37], matching known upper bounds, which, as we show, can be implemented using a statistical algorithm. Since known approaches for problems over distributions have statistical analogues (spectral, Markov Chain Monte Carlo, gradient-based, convex optimization, etc.), this lower bound provides a rigorous explanation of the observed algorithmic gap. The proof introduces a new general technique for the analysis of statistical query algorithms. It also points to a geometric paring phenomenon in the space of all planted assignments. We describe consequences of our lower bounds to Feige’s refutation hypothesis [U. Feige, Proceedings of the ACM Symposium on Theory of Computing, 2002, pp. 534–543] and to lower bounds on general convex programs that solve planted k-SAT. Our bounds also extend to other planted k-CSP models and, in particular, provide concrete evidence for the security of Goldreich’s one-way function and the associated pseudorandom generator when used with a sufficiently hard predicate [O. Goldreich, preprint, ia.cr/2000/063, 2000]. © 2018 Society for Industrial and Applied Mathematics."
,10.1029/2018TC005207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055029064&doi=10.1029%2f2018TC005207&partnerID=40&md5=524e918764f161701606673bb1550b4e,"Low-temperature thermochronometry is widely used to measure the timing and rate of slip on normal faults. Rates are often derived from suites of footwall thermochronometer samples, but regression of age versus structural depth fails to account for the trajectories of samples during fault slip. We demonstrate that in rotating fault blocks, regression of age-depth data is susceptible to significant errors (>10%) in the identification of the initiation and rate of faulting. Advection of heat and topographic growth influence the thermal histories of exhumed particles, but for a range of geologically reasonable fault geometries and rates these effects produce Apatite (U-Th)/He ages comparable to those derived from rotation through fixed isotherms. We apply the fixed-isotherm model to published data from the Pine Forest Range and the East Range, Nevada, by incorporating field and thermochronologic constraints into a Markov chain Monte Carlo model. Modeled parameters for the Pine Forest Range are described by narrow ranges of geologically reasonable values. Compared to slip rates of 0.3–0.8 km/Myr and an initiation of faulting ca. 11–12 Ma derived from visual inspection, the model predicts an average slip rate of ~1.1 km/Myr and an onset of faulting ca. 9–10 Ma. For the East Range fault block the model suggests that faulting began ~17 Ma with an extension rate of ~3 km/Myr and slowed to an extension rate of ~0.5 km/Myr at ~14 Ma. The absence of a preserved partial retention zone in the East Range sample set limits how well the model can predict fault block geometry. Published 2018. This article is a U.S. Government work and is in the public domain in the USA."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051633534&partnerID=40&md5=5ebf4cdd8d132d9b2eb9f4b1244cc0d9,"We show that statistical methods enable the use of portable industrial scanners (with sparse measurements), suitable for fast on-site whole-core X-ray computerized tomography (CT), as opposed to conventional (medical) devices (with dense measurements). This approach accelerates an informed first-stage general assessment of core samples. To that end, we show that this novel industrial tomographic measurement principle is feasible for rock sample imaging, in conjunction with suitable forms of priors in Bayesian inversion algorithms. We assess the performance of the inversion with Gaussian, Cauchy, and Total Variation (TV) priors. In so doing, we consider, in discrete form, conditional mean (CM) estimators, via Markov Chain Monte Carlo (MCMC) algorithms with noise-contaminated measurements. To benchmark the reliability of whole-core imaging with sparse radiograms via Bayesian inversion, in our study we include X-ray CT from numerical simulations of synthetic and measurement-based whole-core samples. To that end, we consider tomographic measurements of fine- to medium-grained sandstone core samples, with igneous-rich pebbles from the Miocene, off the Shimokita Peninsula in Japan, and fractured welded tuff from Big Bend National Park, Texas. Bayesian inversion results show that with only 16 radiograms, natural fractures with aperture of less than 2mm wide are detectable. Additionally, images show approximately spherical concretions of 6mm diameter. We show that to achieve similar results, filtered back projection (FBP) techniques require hundreds of radiograms, only possible with conventional (medical) laboratory scanners. This paper shows that Bayesian inversion on whole-core X-ray CT is capable of imaging coarse sedimentary features that, with faster, simplified measurement principles, would aid in more efficient operational petrophysical decisions for planning further core analysis. Copyright 2018, held jointly by the Society of Petrophysicists and Well Log Analysts (SPWLA) and the submitting authors."
,10.1111/rssb.12237,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019544964&doi=10.1111%2frssb.12237&partnerID=40&md5=fa2d76c234c1a57c07e0c5375ab83939,"We present a novel inference methodology to perform Bayesian inference for spatiotemporal Cox processes where the intensity function depends on a multivariate Gaussian process. Dynamic Gaussian processes are introduced to enable evolution of the intensity function over discrete time. The novelty of the method lies on the fact that no discretization error is involved despite the non-tractability of the likelihood function and infinite dimensionality of the problem. The method is based on a Markov chain Monte Carlo algorithm that samples from the joint posterior distribution of the parameters and latent variables of the model. A particular choice of the dominating measure to obtain the likelihood function is shown to be crucial to devise a valid Markov chain Monte Carlo algorithm. The models are defined in a general and flexible way but they are amenable to direct sampling from the relevant distributions because of careful characterization of its components. The models also enable the inclusion of regression covariates and/or temporal components to explain the variability of the intensity function. These components may be subject to relevant interaction with space and/or time. Real and simulated examples illustrate the methodology, followed by concluding remarks. © 2017 Royal Statistical Society"
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046161669&partnerID=40&md5=cec362e5ac7eaec34620d059e5ab913f,"Understanding the evolution of cancer is important for the development of appropriate cancer therapies. The task is challenging because tumors evolve as heterogeneous cell populations with an unknown number of genetically distinct subclones of varying frequencies. Conventional approaches based on bulk sequencing are limited in addressing this challenge as clones cannot be observed directly. Single-cell sequencing holds the promise of resolving the heterogeneity of tumors. However, this advantage comes at the cost of elevated noise due to the limited amount of DNA material present in a cell and the extensive DNA amplification required prior to sequencing. Here, we present SCIΦ, the first single-cell-specific variant caller that combines single-cell genotyping with reconstruction of the cell lineage tree. SCIΦ leverages the fact that the somatic cells of an organism are related via a phylogenetic tree where mutations are propagated along tree branches. Our inference scheme starts with an initial identification of possible mutation loci and then performs joint phylogenetic inference and variant calling via posterior sampling. In a first step, likely mutated loci are identified using the posterior probability of observing at least one mutated cell at a specific locus. In order to do so, SCIΦ models the nucleotide counts using a beta-binomial distribution. This is especially useful in the single-cell setting, since the beta-binomial distribution can be described as a Pólya urn model, which in turn is a very close approximation of the multiple displacement amplification commonly used to amplify the genomic material of a single-cell. In a second step, the identified loci are used to infer the tumor phylogeny. Here, we account for dropout events by modeling the likelihood of observing a mutation in a cell as a weighted mixture of the likelihoods of homozygous reference genotype, heterozygous genotype, and homozygous alternative genotype. Our model to infer tumor phylogeny consists of three parts: the genealogical tree, the mutation attachments to edges, and the parameters of the model. Because the tree search space grows superexponentially in the number of cells, we employ a Markov Chain Monte Carlo scheme to traverse through the tree space with mutation assignment and learn the parameters of the model. Using the relationship between cells, we are able to reliably call mutations in each single-cell even in experiments with high dropout rates and missing data. We show that SCIΦ outperforms existing methods on simulated data and apply it to different real-world datasets. Availability: https://github.com/cbg-ethz/SCIPhI © Springer International Publishing AG, part of Springer Nature 2018."
,10.1515/snde-2017-0114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054848873&doi=10.1515%2fsnde-2017-0114&partnerID=40&md5=553115ee64b9514fc2a7960f0ecd2191,"The literature of time series models with threshold effects makes the assumption of a constant threshold value over different periods. However, this time-homogeneity assumption tends to be too restrictive owing to the fact that the threshold value that triggers regime switching could possibly be time-varying. This study herein proposes a threshold model in which the threshold value is assumed to be a latent variable following an autoregressive (AR) process. The newly proposed model was estimated using a Markov Chain Monte Carlo (MCMC) algorithm under a Bayesian framework. The Monte Carlo simulations are presented to assess the effectiveness of the Bayesian approaches. An illustration of the model was made through an application to a regime-sensitive Taylor rule employing U.S. data. ©2018 Walter de Gruyter GmbH, Berlin/Boston."
,10.1007/978-3-319-61566-0_39,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026294120&doi=10.1007%2f978-3-319-61566-0_39&partnerID=40&md5=dec9b5070ee00578c22d4dafcb86b920,"We propose a hardware architecture for solving combinatorial optimization problems and implemented it on an FPGA. The hardware minimizes the energy of Ising model with 1,024 state variables fully connectable through 16-bit weights, which ease restrictions on mapping problems onto the Ising model. The system uses a hardware bit-sieve engine that performs a Markov-chain Monte-Carlo search with a parallel-evaluation of the energy increment prior to the bit selection, achieving a speedup while guaranteeing convergence. The engine is implemented on an Arria 10 GX FPGA and solves 32-city traveling salesman problems 104 times faster than simulated annealing running on a 3.5-GHz Intel Xeon E5-1620v3 processor. © Springer International Publishing AG 2018."
,10.15672/HJMS.2017.441,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046349601&doi=10.15672%2fHJMS.2017.441&partnerID=40&md5=b7137ae37edfefd9d77f59bec00f8b09,"In this paper, we developed the slice sampler algorithm for the generalized Pareto distribution (GPD) model. Two simulation studies have shown the performance of the peaks over given threshold (POT) and GPD density function on various simulated data sets. The results were compared with another commonly used Markov chain Monte Carlo (MCMC) technique called Metropolis-Hastings algorithm. Based on the results, the slice sampler algorithm provides closer posterior mean values and shorter 95% quantile based credible intervals compared to the Metropolis-Hastings algorithm. Moreover, the slice sampler algorithm presents a higher level of stationarity in terms of the scale and shape parameters compared with the Metropolis-Hastings algorithm. Finally, the slice sampler algorithm was employed to estimate the re- turn and risk values of investment in Malaysian gold market. © 2018, Hacettepe University. All rights reserved."
1,10.18637/jss.v083.i11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042686753&doi=10.18637%2fjss.v083.i11&partnerID=40&md5=8dc889deb3bdb55cf7ac4eae4bd7c996,"We present the R package epinet, which provides tools for analyzing the spread of epidemics through populations. We assume that the relationships among individuals in a population are modeled by a contact network described by an exponential-family random graph model and that the disease being studied spreads across the edges of this network from infectious to susceptible individuals. We use a susceptible-exposed-infectious-removed compartmental model to describe the progress of the disease within each host. We describe the functionality of the package, which consists of routines that perform simulation, plotting, and inference. The main inference routine utilizes a Bayesian approach and a Markov chain Monte Carlo algorithm. We demonstrate the use of the package through two examples, one involving simulated data and one using data from an actual measles outbreak. © 2018, American Statistical Association. All rights reserved."
,10.1016/j.ecosta.2018.03.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046161118&doi=10.1016%2fj.ecosta.2018.03.003&partnerID=40&md5=1b9a339feb79be08d956da44a5b11bb0,"The single equicorrelation structure among several daily asset returns is promising and attractive to reduce the number of parameters in multivariate stochastic volatility models. However, such an assumption may not be realistic as the number of assets may increase, for example, in the portfolio optimizations. As a solution to this oversimplification, the multiple-block equicorrelation structure is proposed for high dimensional financial time series, where common correlations within a group of asset returns are assumed, but different correlations for different groups are allowed. The realized volatilities and realized correlations are also jointly modelled to obtain stable and accurate estimates of parameters, latent variables and leverage effects. Using a state space representation, an efficient estimation method of Markov chain Monte Carlo simulation is described. Empirical studies using U.S. daily stock returns data show that the proposed model outperforms other competing models in portfolio performances. © 2018 EcoSta Econometrics and Statistics"
,10.1016/j.patrec.2018.07.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050076279&doi=10.1016%2fj.patrec.2018.07.005&partnerID=40&md5=8787bdb2980091ce83450b8f9d787700,"Graph matching is a powerful tool for computer vision, distance measure and machine learning. However, many factors influences the accuracy of matching. The outliers is a key problem in the process of matching. In this paper, a novel approach is proposed to handle graph matching problem based on Markov Chain Monte Carlo framework. By constructing a target distribution, the proposed can perform a process of sampling to maximize the graph matching objective. In this process, our method can effectively save matching pairwise under one-to-one matching constraints and also avoid the effect of outliers and deformation. The corresponding experiments on synthetic graphs, real images and view-based 3D model retrieval demonstrate the superiority of the proposed method. © 2018 Elsevier B.V."
,10.18517/ijaseit.8.2.3506,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046535847&doi=10.18517%2fijaseit.8.2.3506&partnerID=40&md5=185d475c0085d459ec8e3879fb9c69c2,"In some situations, only observations that are more extreme than the current extreme value are recorded. This kind of data is called record values which have many applications in a lot of fields. In this paper, the Bayesian estimators using squared error and LINEX loss functions for the generalized inverted exponential distribution parameters are considered depending on upper record values and upper record ranked set sampling. The Bayes estimates and credible intervals are derived by considering the independent gamma priors for the parameters. The Markov Chain Monte Carlo (MCMC) method is developed due to the lack of explicit forms for the Bayes estimates. A Simulation study is implemented to compute and compare the performance of estimators in both sampling schemes with respect to relative absolute biases, estimated risks and the width of credible intervals. © IJASEIT."
,10.1007/s11009-018-9676-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054853188&doi=10.1007%2fs11009-018-9676-6&partnerID=40&md5=4a1e1c77b324e5959e001466d186823d,"This article presents different estimation procedure for inverse Lindley distribution for Type-I hybrid censored data. We have obtained the parameter estimate under both the classical and Bayesian paradigm. In the classical set up, method of Maximum Likelihood(ML) and Maximum Product of spacings (MPS) estimates are obtained along with 95% asymptotic confidence interval. Bayesian estimation is implemented under the assumption of squared error loss function. An alternative Bayesian procedure is also proposed by incorporating the sample information through the spacings function instead of likelihood function. The Bayes estimates are computed using Markov Chain Monte Carlo (MCMC) technique due to their implicit nature. Highest posterior density (HPD) intervals based on these MCMC samples are evaluated and compared in terms of simulated risks. Further, a real data of 72 guinea pigs, infected with tuberculosis is analysed to justify the suitability of the afore-said estimation techniques under the specified censoring scheme. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
,10.1111/stan.12143,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047838423&doi=10.1111%2fstan.12143&partnerID=40&md5=f34fe1bd5ab05e33852597f0ee43c627,"In this paper, we introduce a threshold stochastic volatility model with explanatory variables. The Bayesian method is considered in estimating the parameters of the proposed model via the Markov chain Monte Carlo (MCMC) algorithm. Gibbs sampling and Metropolis-Hastings sampling methods are used for drawing the posterior samples of the parameters and the latent variables. In the simulation study, the accuracy of the MCMC algorithm, the sensitivity of the algorithm for model assumptions, and the robustness of the posterior distribution under different priors are considered. Simulation results indicate that our MCMC algorithm converges fast and that the posterior distribution is robust under different priors and model assumptions. A real data example was analyzed to explain the asymmetric behavior of stock markets. © 2018 The Authors."
1,10.1111/rssc.12213,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012938924&doi=10.1111%2frssc.12213&partnerID=40&md5=a1858ba96b6947d170a2b4ba89cfd5d3,"Recent advances in molecular biology allow the quantification of the transcriptome and scoring transcripts as differentially or equally expressed between two biological conditions. Although these two tasks are closely linked, the available inference methods treat them separately: a primary model is used to estimate expression and its output is post processed by using a differential expression model. In the paper, both issues are simultaneously addressed by proposing the joint estimation of expression levels and differential expression: the unknown relative abundance of each transcript can either be equal or not between two conditions. A hierarchical Bayesian model builds on the BitSeq framework and the posterior distribution of transcript expression and differential expression is inferred by using Markov chain Monte Carlo sampling. It is shown that the model proposed enjoys conjugacy for fixed dimension variables; thus the full conditional distributions are analytically derived. Two samplers are constructed, a reversible jump Markov chain Monte Carlo sampler and a collapsed Gibbs sampler, and the latter is found to perform better. A cluster representation of the aligned reads to the transcriptome is introduced, allowing parallel estimation of the marginal posterior distribution of subsets of transcripts under reasonable computing time. Under a fixed prior probability of differential expression the clusterwise sampler has the same marginal posterior distributions as the raw sampler, but a more general prior structure is also employed. The algorithm proposed is benchmarked against alternative methods by using synthetic data sets and applied to real RNA sequencing data. Source code is available on line from https://github.com/mqbssppe/cjBitSeq. © 2017 The Authors Journal of the Royal Statistical Society: Series C (Applied Statistics) Published by John Wiley & Sons Ltd on behalf of the Royal Statistical Society."
1,10.1137/16M1084080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046042349&doi=10.1137%2f16M1084080&partnerID=40&md5=bb1d138201f383597ba922021389accb,"Performing Bayesian inference via Markov chain Monte Carlo (MCMC) can be exceedingly expensive when posterior evaluations invoke the evaluation of a computationally expensive model, such as a system of PDEs. In recent work [J. Amer. Statist. Assoc., 111 (2016), pp. 1591-1607] we described a framework for constructing and refining local approximations of such models during an MCMC simulation. These posterior-adapted approximations harness regularity of the model to reduce the computational cost of inference while preserving asymptotic exactness of the Markov chain. Here we describe two extensions of that work. First, we prove that samplers running in parallel can collaboratively construct a shared posterior approximation while ensuring ergodicity of each associated chain, providing a novel opportunity for exploiting parallel computation in MCMC. Second, focusing on the Metropolis-adjusted Langevin algorithm, we describe how a proposal distribution can successfully employ gradients and other relevant information extracted from the approximation. We investigate the practical performance of our approach using two challenging inference problems, the first in subsurface hydrology and the second in glaciology. Using local approximations constructed via parallel chains, we successfully reduce the run time needed to characterize the posterior distributions in these problems from days to hours and from months to days, respectively, dramatically improving the tractability of Bayesian inference. © 2018 Society for Industrial and Applied Mathematics and American Statistical Association."
,10.1177/0962280217754231,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043466873&doi=10.1177%2f0962280217754231&partnerID=40&md5=ed9c50ac7d9da8d0b7c5ee6c9bb19a07,"We present a computational framework to select the most accurate and precise method of measurement of a certain quantity, when there is no access to the true value of the measurand. A typical use case is when several image analysis methods are applied to measure the value of a particular quantitative imaging biomarker from the same images. The accuracy of each measurement method is characterized by systematic error (bias), which is modeled as a polynomial in true values of measurand, and the precision as random error modeled with a Gaussian random variable. In contrast to previous works, the random errors are modeled jointly across all methods, thereby enabling the framework to analyze measurement methods based on similar principles, which may have correlated random errors. Furthermore, the posterior distribution of the error model parameters is estimated from samples obtained by Markov chain Monte-Carlo and analyzed to estimate the parameter values and the unknown true values of the measurand. The framework was validated on six synthetic and one clinical dataset containing measurements of total lesion load, a biomarker of neurodegenerative diseases, which was obtained with four automatic methods by analyzing brain magnetic resonance images. The estimates of bias and random error were in a good agreement with the corresponding least squares regression estimates against a reference. © 2018, The Author(s) 2018."
1,10.1007/s11222-016-9719-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001837165&doi=10.1007%2fs11222-016-9719-1&partnerID=40&md5=ac527b0c21900651f9836b9f05db59bf,"Integro-difference equations (IDEs) provide a flexible framework for dynamic modeling of spatio-temporal data. The choice of kernel in an IDE model relates directly to the underlying physical process modeled, and it can affect model fit and predictive accuracy. We introduce Bayesian non-parametric methods to the IDE literature as a means to allow flexibility in modeling the kernel. We propose a mixture of normal distributions for the IDE kernel, built from a spatial Dirichlet process for the mixing distribution, which can model kernels with shapes that change with location. This allows the IDE model to capture non-stationarity with respect to location and to reflect a changing physical process across the domain. We address computational concerns for inference that leverage the use of Hermite polynomials as a basis for the representation of the process and the IDE kernel, and incorporate Hamiltonian Markov chain Monte Carlo steps in the posterior simulation method. An example with synthetic data demonstrates that the model can successfully capture location-dependent dynamics. Moreover, using a data set of ozone pressure, we show that the spatial Dirichlet process mixture model outperforms several alternative models for the IDE kernel, including the state of the art in the IDE literature, that is, a Gaussian kernel with location-dependent parameters. © 2016, Springer Science+Business Media New York."
,10.1016/j.matcom.2018.06.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050163861&doi=10.1016%2fj.matcom.2018.06.007&partnerID=40&md5=be37acc033c8da1c6031e5b44704065e,"The reversible jump algorithm is a useful Markov chain Monte Carlo method introduced by Green (1995) that allows switches between subspaces of differing dimensionality, and therefore, model selection. Although this method is now increasingly used in key areas (e.g. biology and finance), it remains a challenge to implement it. In this paper, we focus on a simple sampling context in order to obtain theoretical results that lead to an optimal tuning procedure for the considered reversible jump algorithm, and consequently, to easy implementation. The key result is the weak convergence of the sequence of stochastic processes engendered by the algorithm. It represents the main contribution of this paper as it is, to our knowledge, the first weak convergence result for the reversible jump algorithm. The sampler updating the parameters according to a random walk, this result allows to retrieve the well-known 0.234 rule for finding the optimal scaling. It also leads to an answer to the question: “with what probability should a parameter update be proposed comparatively to a model switch at each iteration?” © 2018"
3,10.1214/17-BA1069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050962301&doi=10.1214%2f17-BA1069&partnerID=40&md5=16d7adab6718d9543d05bd8aa17fc94c,"We introduce a computationally efficient Bayesian model for predicting high-dimensional dependent count-valued data. In this setting, the Poisson data model with a latent Gaussian process model has become the de facto model. However, this model can be difficult to use in high dimensional settings, where the data may be tabulated over different variables, geographic regions, and times. These computational difficulties are further exacerbated by acknowledging that count-valued data are naturally non-Gaussian. Thus, many of the current approaches, in Bayesian inference, require one to carefully calibrate a Markov chain Monte Carlo (MCMC) technique. We avoid MCMC methods that require tuning by developing a new conjugate multivariate distribution. Specifically, we introduce a multivariate log-gamma distribution and provide substantial methodological development of independent interest including: results regarding conditional distributions, marginal distributions, an asymptotic relationship with the multivariate normal distribution, and full-conditional distributions for a Gibbs sampler. To incorporate dependence between variables, regions, and time points, a multivariate spatio-temporal mixed effects model (MSTM) is used. To demonstrate our methodology we use data obtained from the US Census Bureau's Longitudinal Employer-Household Dynamics (LEHD) program. In particular, our approach is motivated by the LEHD's Quarterly Workforce Indicators (QWIs), which constitute current estimates of important US economic variables. © 2018 International Society for Bayesian Analysis."
,10.2427/12777,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045329234&doi=10.2427%2f12777&partnerID=40&md5=80cf21275264ffa6827faf7f92373439,"Background: Previous models based on a limited number of clinical parameters that have been used so far failed to exhibit high accuracy of prediction of asthma persistence in children. The number and significance of factors that are used in a proposed model play a cardinal role in prediction accuracy. Different models may lead to different significant variables. In addition, the accuracy of a model in medicine is really important since an accurate prediction of illness persistence may improve prevention and treatment intervention for the children at risk. The aim of this study is to evaluate a model that could effectively and accurately predict asthma persistence in children. Methods: Data from 147 asthmatic children were analyzed by a new method for predicting asthma outcome using Principal Component Analysis (PCA) in combination with a Bayesian logistic regression approach implemented by the Markov Chain Monte Carlo (MCMC). The use of PCA is required due to multicollinearity among the explanatory variables. Results: This method using the most appropriate models seems to predict asthma with an accuracy of 84.076%, 84.924%, 86.3673% and 86.1951%, a Sensitivity of 84.96%, 85.49%, 87.25% and 86.38% and a Specificity of 83.22%, 84.37%, 85.52% and 86.02% respectively. Conclusion: Our approach predicts asthma with high accuracy, gives steadier results in terms of positive and negative patients and provides better information about the influence of each factor (demographic, symptoms etc.) in asthma prediction. © 2018, Prex S.p.A. All rights reserved."
,10.1504/IJOR.2018.093506,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050805680&doi=10.1504%2fIJOR.2018.093506&partnerID=40&md5=26cad99203b0d3de3a2a69d68f3c7197,"In this paper, we consider the planning of terminal locations for intermodal transportation systems. With a given number of potential locations, we aim to find the most appropriate number of those as terminals to provide the economically most efficient operation when multiple service pairs are needed simultaneously. The problem also has an inherent task to determine the optimal route paths for each service pair. For this NP-hard problem, we present a Markov chain Monte Carlo (MCMC)-based two-layer method to find a suboptimal solution. In the lower layer, the routing for all service pairs given a particular location planning is solved through a table-based heuristic method that considers both efficiency and fairness. In the upper layer, by mapping the cost function into a stationary distribution, the optimal planning is solved based on a MCMC method that integrates advantages of both simulated annealing and slice sampling. Finally, the effectiveness of this heuristic MCMC-based method is demonstrated through computer experiments. Copyright © 2018 Inderscience Enterprises Ltd."
,10.1007/s13171-018-0136-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053228839&doi=10.1007%2fs13171-018-0136-8&partnerID=40&md5=94c4babe421212a9f7f83b85e1105968,"Regression models for size-and-shape analysis are developed, where the model is specified in the Euclidean space of the landmark coordinates. Statistical models in this space (which is known as the top space or ambient space) are often easier for practitioners to understand than alternative models in the quotient space of size-and-shapes. We consider a Bayesian linear size-and-shape regression model in which the response variable is given by labelled configuration matrix, and the covariates represent quantities such as gender and age. It is important to parameterize the model so that it is identifiable, and we use the LQ decomposition in the intercept term in the model for this purpose. Gamma priors for the inverse variance of the error term, matrix Fisher priors for the random rotation matrix, and flat priors for the regression coefficients are used. Markov chain Monte Carlo algorithms are used for sampling from the posterior distribution, in particular by using combinations of Metropolis-Hastings updates and a Gibbs sampler. The proposed Bayesian methodology is illustrated with an application to forensic facial data in three dimensions, where we investigate the main changes in growth by describing relative movements of landmarks for each gender over time. © 2018, The Author(s)."
,10.1007/978-3-319-70942-0_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037863623&doi=10.1007%2f978-3-319-70942-0_6&partnerID=40&md5=48a2972c3ec8b833c28b61b80bdaa483,"This paper evaluates the performances of Value-at-Risk (VaR) and expected shortfall, as well as volatility forecasts in a class of risk models, specifically focusing on GARCH, integrated GARCH, and asymmetric GARCH models (GJR-GARCH, exponential GARCH, and smooth transition GARCH models). Most of the models incorporate four error probability distributions: Gaussian, Student’s t, skew Student’s t, and generalized error distribution (GED). We employ Bayesian Markov chain Monte Carlo sampling methods for estimation and forecasting. We further present backtesting measures for both VaR and expected shortfall forecasts and implement two loss functions to evaluate volatility forecasts. The empirical results are based on the S&P500 in the U.S. and Japan’s Nikkei 225. A VaR forecasting study reveals that at the 1% level the smooth transition model with a second-order logistic function and skew Student’s t error compares most favorably in terms of violation rates for both markets. For the volatility predictive abilities, the EGARCH model with GED error is the best model in both markets. © Springer International Publishing AG 2018."
,10.1002/sim.8010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055176992&doi=10.1002%2fsim.8010&partnerID=40&md5=fd6a160000b9394fbc323c50007974fb,"Models of excess mortality with random effects were used to estimate regional variation in relative or net survival of cancer patients. Statistical inference for these models based on the Markov chain Monte Carlo (MCMC) methods is computationally intensive and, therefore, not feasible for routine analyses of cancer register data. This study assessed the performance of the integrated nested Laplace approximation (INLA) in monitoring regional variation in cancer survival. Poisson regression model of excess mortality including both spatially correlated and unstructured random effects was fitted to the data of patients diagnosed with ovarian and breast cancer in Finland during 1955-2014 with follow up from 1960 through 2014 by using the period approach with five-year calendar time windows. We estimated standard deviations associated with variation (i) between hospital districts and (ii) between municipalities within hospital districts. Posterior estimates based on the INLA approach were compared to those based on the MCMC simulation. The estimates of the variation parameters were similar between the two approaches. Variation within hospital districts dominated in the total variation between municipalities. In 2000-2014, the proportion of the average variation within hospital districts was 68% (95% posterior interval: 35%-93%) and 82% (60%-98%) out of the total variation in ovarian and breast cancer, respectively. In the estimation of regional variation, the INLA approach was accurate, fast, and easy to implement by using the R-INLA package. © 2018 John Wiley & Sons, Ltd."
,10.1177/0962280218754928,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043706966&doi=10.1177%2f0962280218754928&partnerID=40&md5=d1a9e882e8eae06c5f53cd500cf967d6,"Meta-analysis of interventions usually relies on randomized controlled trials. However, when the dominant source of information comes from single-arm studies, or when the results from randomized controlled trials lack generalization due to strict inclusion and exclusion criteria, it is vital to synthesize both sources of evidence. One challenge of synthesizing both sources is that single-arm studies are usually less reliable than randomized controlled trials due to selection bias and confounding factors. In this paper, we propose a Bayesian hierarchical framework for the purpose of bias reduction and efficiency gain. Under this framework, three methods are proposed: bivariate generalized linear mixed effects models, hierarchical power prior model and hierarchical commensurate prior model. Design difference and potential biases are considered in all models, within which the hierarchical power prior and hierarchical commensurate prior models further offer to downweight single-arm studies flexibly. The hierarchical commensurate prior model is recommended as the primary method for evidence synthesis because of its accuracy and robustness. We illustrate our methods by applying all models to two motivating datasets and evaluate their performance through simulation studies. We finish with a discussion of the advantages and limitations of our methods, as well as directions for future research in this area. © 2018, The Author(s) 2018."
,10.1016/B978-0-444-63964-6.00005-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049934939&doi=10.1016%2fB978-0-444-63964-6.00005-2&partnerID=40&md5=a4f2207b30e824a8ea00bda19c7fafc3,"In the previous work, a reduced-order population pharmacokinetic (PK) model was used within a Bayesian inference framework to predict individualized patient dosing regimens. It was shown that a reduced-order model was adequate for individualized dosing of gabapentin given a minimum number of plasma samples from the given patient. However, this purely empirical model could not explain why patients have such different dosing needs. Accordingly, in this work, we couple an advanced compartment and transit oral absorption model with a full physiologically based PK model parameterized using a two-level hierarchical Bayesian approach. The coupled model provides the capability to not only understand the variable oral absorption but also the disposition of the drug. The proposed model-based strategy to individualized dosing is applied to the dosing of gabapentin using the retrospective data from the literature. The computations show that a standard starting regimen of 300 mg every 8 h is not likely to result in efficacious dosing for a substantial proportion of patients. Additionally, the proposed approach was able to incorporate urine data in a seamless way to inform the extent of absorption without assuming that the data were perfect. The mechanistic absorption model elucidated that the uncertainty in absorption plays a role in the variability in exposure seen across the patient population. It has also been suggested that the apparent absorption site is likely not the full upper GI, but a very localized segment due to the transit times required to fit the data. In this study, model implementation and Markov chain Monte Carlo computations were performed using the CmdStan package. © 2018 Elsevier B.V."
,10.1007/978-3-319-96661-8_30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051738688&doi=10.1007%2f978-3-319-96661-8_30&partnerID=40&md5=8c1c6ad13b00227ba48f4efcf02e4a37,"Some new tasks are trivial to learn while others are almost impossible; what determines how easy it is to learn an arbitrary task? Similar to how our prior beliefs about new visual scenes colors our perception of new stimuli, our priors about the structure of new tasks shapes our learning and generalization abilities [2]. While quantifying visual priors has led to major insights on how our visual system works [5, 10, 11], quantifying priors over tasks remains a formidable goal, as it is not even clear how to define a task [4]. Here, we focus on tasks that have a natural mapping to graphs. We develop a method to quantify humans’ priors over these “task graphs”, combining new modeling approaches with Markov chain Monte Carlo with people, MCMCP (a process whereby an agent learns from data generated by another agent, recursively [9]). We show that our method recovers priors more accurately than a standard MCMC sampling approach. Additionally, we propose a novel low-dimensional “smooth” (In the sense that graphs that differ by fewer edges are given similar probabilities.) parametrization of probability distributions over graphs that allows for more accurate recovery of the prior and better generalization. We have also created an online experiment platform that gamifies our MCMCP algorithm and allows subjects to interactively draw the task graphs. We use this platform to collect human data on several navigation and social interactions tasks. We show that priors over these tasks have non-trivial structure, deviating significantly from null models that are insensitive to the graphical information. The priors also notably differ between the navigation and social domains, showing fewer differences between cover stories within the same domain. Finally, we extend our framework to the more general case of quantifying priors over exchangeable random structures. © 2018, Springer Nature Switzerland AG."
,10.1137/16M1066865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043458602&doi=10.1137%2f16M1066865&partnerID=40&md5=fddb24616f3da38375e5060caa1b9ec9,"Our goal is to solve certain dynamic programming equations associated to a given Markov chain X, using a regression-based Monte Carlo algorithm. More specifically, we assume that the model for X is not known in full detail and only a root sample X1, . . ., XM of such process is available. By a stratification of the space and a suitable choice of a probability measure ν, we design a new resampling scheme that allows us to compute local regressions (on basis functions) in each stratum. The combination of the stratification and the resampling allows us to compute the solution to the dynamic programming equation (possibly in large dimensions) using only a relatively small set of root paths. To assess the accuracy of the algorithm, we establish nonasymptotic error estimates in L2(ν). Our numerical experiments illustrate the good performance, even with M = 20 − 40 root paths. © 2018 Society for Industrial and Applied Mathematics."
,10.1016/bs.host.2018.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051678126&doi=10.1016%2fbs.host.2018.07.001&partnerID=40&md5=43a8849f673cbe56113da9c4be643feb,"In this chapter, we focus on exploring some basic ideas on Bayesian and Markov networks and associated inferential procedures under both the classical and Bayesian paradigm. This chapter draws heavily on the article by Friedman et al. (2007). At the end of this chapter some conceptual as well as hands-on-exercises are provided for a better understanding of the subject matter. The author welcomes any constructive criticism, suggestions from the readers of this book. © 2018 Elsevier B.V."
2,10.1190/GEO2017-0239.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038098118&doi=10.1190%2fGEO2017-0239.1&partnerID=40&md5=43bfd6d6089d1c3166da51d11a15a4a8,"Seismic reservoir characterization focuses on the prediction of reservoir properties based on the available geophysical and petrophysical data. The inverse problem generally includes continuous properties, such as petrophysical and elastic attributes, and discrete properties, such as lithology/fluid classes.We have developed a joint probabilistic inversion methodology for the prediction of petrophysical and elastic properties and lithology/fluid classes that combined statistical rock physics and Bayesian seismic inversion. The elastic attributes depend on continuous petrophysical variables, such as porosity and clay content, and discrete lithology/fluid classes, through a nonlinear rock-physics relationship together. The seismic model relates the elastic attributes, such as velocities and density, to their seismic response (reflectivity, traveltime, and amplitudes). The advantage of our integrated approach is that the inversion method accounts for the uncertainty associated to each step of the modeling workflow. The lithology/ fluid classes are assigned by a Markov random field prior model to capture vertical continuity and vertical sorting of the lithology/ fluid classes. Because rock and fluid properties are in general not Gaussian, a spatially coupled Gaussian mixture prior model based on the lithology/fluid classes is constructed. The forward geophysical operator includes a lithology-/fluid-dependent rock physics model and a linearized seismic model based on the convolution of the seismic wavelet with the reflectivity coefficient series. The solution of the inverse problem consists of the posterior distributions of petrophysical and elastic properties and lithology/ fluid classes.We proposed an efficient Markov chainMonte Carlo algorithm to sample from the posterior models and assess the uncertainty. Our methodology is demonstrated on a seismic cross section from a survey in the Norwegian Sea, and it shows promising results consistent with well-log data measured at the well location as well as reliable prediction uncertainties. © 2018 Society of Exploration Geophysicists. All rights reserved."
,10.1007/s13253-018-0335-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054196470&doi=10.1007%2fs13253-018-0335-8&partnerID=40&md5=87b47c1357d8e740eefdb56f28fbfb7f,"In capture-mark-reencounter studies, Pollock’s robust design combines methods for open populations with methods for closed populations. Open population features of the robust design allow for estimation of rates of death or permanent emigration, and closed population features enhance estimation of population sizes. We describe a similar design, but for use with removal data. Data collection occurs on secondary sampling occasions clustered within primary sampling periods. Primary sampling periods are intervals of brief enough duration that it can be safely assumed that the population is unchanged by births, deaths, immigration or emigration during them; all population change and movement occurs between primary sampling periods. Our model provides a basis for inference about population size, changes in population size, and movement rates among sample locations between primary sampling periods. Movement rates are modeled as functions of distance and time. Capture probabilities are modeled as a function of effort. We apply the model to data obtained in attempting to eradicate an introduced population of veiled chameleons (Chamaeleo calyptratus) on the island of Maui in Hawaii. Supplementary materials accompanying this paper appear online. © 2018, International Biometric Society."
,10.1016/j.eneco.2018.03.032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049300505&doi=10.1016%2fj.eneco.2018.03.032&partnerID=40&md5=ee1f6fd972479b0600bdcb2316cc8ddb,"Crude oil markets have been quite volatile and risky in the past few decades due to the large fluctuations of oil prices. We contribute to the current debate by testing for the existence of the leverage effect when considering daily spot returns in the WTI and Brent crude oil markets and by studying the direct impact of the leverage effect on measures of risk such as VaR and CVaR. More specifically, we model spot crude oil returns using Stochastic Volatility (SV) models with various distributions of the errors. We find that the introduction of the leverage effect in the traditional SV model with Normally distributed errors is capable of adequately estimating risk for conservative oil suppliers in both the WTI and Brent markets while it tends to overestimate risk for more speculative oil suppliers. Our results also show that financial regulators’ model choice, both on the supply and on the demand side, would not be affected by the introduction of leverage. Focusing instead on firm's internal risk management, our results show that the introduction of leverage would be useful for firms who are on the demand side for oil, who use VaR for risk management and who are particularly worried about the magnitude of the losses exceeding VaR while wanting to minimize the opportunity cost of capital. Using the same logic, firms who are on the supply side would be better off not considering the leverage effect. © 2018 Elsevier B.V."
,10.3844/jcssp.2018.1115.1125,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052861202&doi=10.3844%2fjcssp.2018.1115.1125&partnerID=40&md5=3099b28c659f6b44195dce287344a173,"Bayesian network structure learning is considered a complex task as the number of possible structures grows exponentially with the number of variables. Two main methods are used for Bayesian network structure learning: Conditional independence, a method in which a structure is created consistently with independence tests performed on data; and the heuristic search method that explores the structure space. Hybrid algorithms combine both of the aforementioned methods. In this study, we propose the combination of common metrics, used to evaluate Bayesian structures, into a fuzzy system. The idea being that different metrics evaluate different properties of the structure. The proposed fuzzy system is then used as a metric to evaluate Bayesian networks structures in a heuristic search algorithm based on Monte Carlo Markov Chains. The algorithm was evaluated within the context of synthetic databases through comparison with other algorithms and processing time. Results have shown that, despite an increase in processing time, the proposed method improved the structure learning process. © 2018 Ademar Crotti Junior, Beatriz Wilges and Silvia Modesto Nassar."
,10.3233/JIFS-171491,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051373996&doi=10.3233%2fJIFS-171491&partnerID=40&md5=d50f27b9239fc13b12e3053207dbc035,"The aim of this paper is introducing a method based on Fuzzy Time-To Failure (FTTF) to improve reliability analysis of complex engineering systems based on fault tree analysis. This method focuses on the quantitative part of fault trees (either static or dynamic) analysis and will compute failure probabilities. FTTF model is developed to estimate the reliability of system and solve aforetime methods problems. The presented FTTF model is able to figure out any construction consist of static and dynamic gates with FTTF distributions integrated on Fuzzy Monte Carlo Simulation (FMCS) techniques to analyzing Possibilistic functions associated with the fuzzy probability distributions for each basic event. Using fuzzy algorithm, gates FTTF are generated, and Top-event TTF evaluated. Some case studies are used to demonstrate the priority of this method in exact evaluation in compared with other solving methods (like: BN, Analytical solution, Markov chain and traditional fuzzy fault tree modeling), but has much less effort while having higher accuracy. Finally, this model is implemented in an Emergency Detection System (EDS) which is a useful system in aerospace and space applications. © 2018 - IOS Press and the authors."
,10.1007/s10955-017-1912-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033437799&doi=10.1007%2fs10955-017-1912-x&partnerID=40&md5=80646aacedd25cec0a97f5b30de31517,"We consider the coupling from the past implementation of the random–cluster heat-bath process, and study its random running time, or coupling time. We focus on hypercubic lattices embedded on tori, in dimensions one to three, with cluster fugacity at least one. We make a number of conjectures regarding the asymptotic behaviour of the coupling time, motivated by rigorous results in one dimension and Monte Carlo simulations in dimensions two and three. Amongst our findings, we observe that, for generic parameter values, the distribution of the appropriately standardized coupling time converges to a Gumbel distribution, and that the standard deviation of the coupling time is asymptotic to an explicit universal constant multiple of the relaxation time. Perhaps surprisingly, we observe these results to hold both off criticality, where the coupling time closely mimics the coupon collector’s problem, and also at the critical point, provided the cluster fugacity is below the value at which the transition becomes discontinuous. Finally, we consider analogous questions for the single-spin Ising heat-bath process. © 2017, Springer Science+Business Media, LLC, part of Springer Nature."
1,10.1016/j.strusafe.2017.10.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032802354&doi=10.1016%2fj.strusafe.2017.10.011&partnerID=40&md5=493afe803a268e70056f6d7787e704cc,"This paper presents a new probabilistic site characterization approach for both soil classification and property estimation using sounding data from multiple cone penetration tests (CPTs) at a project site. A hidden Markov random field (HMRF) model based Bayesian clustering approach is developed, which can describe not only the heterogeneity of properties in statistically homogeneous soil layers, but also the correlation between spatial distributions of different soil layers. The latter has not been well considered in the existing CPT interpretation methods. A Monte Carlo Markov chain based expectation maximization (MCMC-EM) algorithm is adopted to calibrate the established HMRF model, so that both the subsurface soil/rock stratification and the pertinent soil properties can be estimated in a probabilistic manner. The proposed CPT interpretation approach is validated and demonstrated using a series of numerical examples, including using real CPT data. It is shown that the proposed method is able to accurately identify soil layers, pinpoint their boundaries, and provide reasonable estimates of the associated soil properties. In addition, comparative studies show that combining analysis of CPT data from multiple soundings, rather than interpreting them separately, can significantly enhance the accuracy of interpretation and simplify the subsequent task of interpreting stratigraphic profiles. © 2017 Elsevier Ltd"
2,10.1509/jm.15.0523,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045886719&doi=10.1509%2fjm.15.0523&partnerID=40&md5=a7accc46a8ec06d06028e82b1a6c714a,"The authors study specialized personal incentives (SPIs), which are cash rewards granted to salespeople for meeting interim performance goals within the regular sales quota period (monthly, quarterly, etc.). Because firms often institute multiple SPIs, the authors are able to investigate whether different sales achievement trajectories have differential impacts on salespeople's period-end sales performance. The authors find that a steadily growing sales trajectory in a sales period is more strongly associated with period-end success than a sales trajectory that is relatively flat early but has a sharp spike later in the period. Furthermore, although salespeople who had high performance in the prior month (i.e., high-performance state) may be able to draw on superior selling strategies (compared with other salespeople), they too experience a boost in sales performance in the current month by earning SPIs. Notably, the authors also find that although earning SPIs benefits all salespeople, there is a U-shaped relationship between a salesperson's performance state and his or her month-end sales performance. For any specific number of SPIs earned, the probability of meeting and exceeding month-end quotas is boosted more for salespeople with low- and high-performance states than for salespeople with a medium-performance state. © 2018, American Marketing Association"
,10.1007/s11538-018-0518-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055290535&doi=10.1007%2fs11538-018-0518-z&partnerID=40&md5=de84667fc268cea1595caff87821bbf1,"In this study, we apply the Bayesian paradigm for parameter identification to a well-studied semi-linear reaction–diffusion system with activator-depleted reaction kinetics, posed on stationary as well as evolving domains. We provide a mathematically rigorous framework to study the inverse problem of finding the parameters of a reaction–diffusion system given a final spatial pattern. On the stationary domain the parameters are finite-dimensional, but on the evolving domain we consider the problem of identifying the evolution of the domain, i.e. a time-dependent function. Whilst others have considered these inverse problems using optimisation techniques, the Bayesian approach provides a rigorous mathematical framework for incorporating the prior knowledge on uncertainty in the observation and in the parameters themselves, resulting in an approximation of the full probability distribution for the parameters, given the data. Furthermore, using previously established results, we can prove well-posedness results for the inverse problem, using the well-posedness of the forward problem. Although the numerical approximation of the full probability is computationally expensive, parallelised algorithms make the problem solvable using high-performance computing. © 2018, The Author(s)."
1,10.1002/ecs2.2060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041241370&doi=10.1002%2fecs2.2060&partnerID=40&md5=7a153b916821111f83a14e11e7efcebb,"Plant functional traits research has revealed many interesting and important patterns among morphological, physiological, and life-history traits and the environment. These are exemplified in trade-offs between groups of traits such as those embodied in the leaf and wood economics spectra. Inferences from empirical studies are often constrained by the correlative nature of the analyses, availability of trait data, and a focus on easily measured traits. However, empirical studies have been fundamental to modeling endeavors aiming to enhance our understanding of how functional traits scale up to affect, for example, community dynamics and ecosystem productivity. Here, we take a complementary approach utilizing an individual-based model of tree growth and mortality (the allometrically constrained growth and carbon allocation [ACGCA] model) to investigate the theoretical trait space (TTS) of North American trees. The model includes 32 parameters representing allometric, physiological, and anatomical traits, some overlapping leaf and wood economics spectra traits. Using a Bayesian approach, we fit the ACGCA model to individual tree heights and diameters from the USFS Forest Inventory and Analysis (FIA) dataset, with further constraints by literature-based priors. Fitting the model to 1.3 million FIA records—aggregated across individuals, species, and sites—produced a posterior distribution of traits leading to realistic growth. We explored this multidimensional posterior distribution (the TTS) to evaluate trait–trait relationships emerging from the ACGCA model, and compare these against empirical patterns reported in the literature. Only three notable bivariate correlations, among 496 possible trait pairs, were contained in the TTS. However, stepwise regressions uncovered a complicated structure; only a subset of traits—related to photosynthesis (e.g., radiation-use efficiency and maintenance respiration)—exhibited strong multivariate trade-offs with each other, while half of the traits—mostly related to allometries and construction costs—varied independently of other traits. Interestingly, specific leaf area was related to several rarely measured root traits. The trade-offs contained in the TTS generally reflect mass-balance (related to carbon allocation) and engineering (mostly related to allometries) trade-offs represented in the ACGCA model and point to potentially important traits that are under-explored in field studies (e.g., root traits and branch senescence rates). © 2018 Fell et al."
,10.1214/18-EJS1435,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047801825&doi=10.1214%2f18-EJS1435&partnerID=40&md5=10e8f8d8968b34a7b93d306ee4eb6d3c,"An informative sampling design leads to the selection of units whose inclusion probabilities are correlated with the response variable of interest. Inference under the population model performed on the resulting observed sample, without adjustment, will be biased for the population generative model. One approach that produces asymptotically unbiased inference employs marginal inclusion probabilities to form sampling weights used to exponentiate each likelihood contribution of a pseudo likelihood used to form a pseudo posterior distribution. Conditions for posterior consistency restrict applicable sampling designs to those under which pairwise inclusion dependencies asymptotically limit to 0. There are many sampling designs excluded by this restriction; for example, a multi-stage design that samples individuals within households. Viewing each household as a population, the dependence among individuals does not attenuate. We propose a more targeted approach in this paper for inference focused on pairs of individuals or sampled units; for example, the substance use of one spouse in a shared household, conditioned on the substance use of the other spouse. We formulate the pseudo likelihood with weights based on pairwise or second order probabilities and demonstrate consistency, removing the requirement for asymptotic independence and replacing it with restrictions on higher order selection probabilities. Our approach provides a nearly automated estimation procedure applicable to any model specified by the data analyst. We demonstrate our method on the National Survey on Drug Use and Health. © 2018, Institute of Mathematical Statistics. All rights reserved."
,10.1111/twec.12665,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046405702&doi=10.1111%2ftwec.12665&partnerID=40&md5=60a6850912329c25af3638b4c6f85004,"This paper structurally estimates a dynamic discrete choice model of exporting and importing. The model provides a framework to analyse the determinants of a firm's decision to export and import while allowing for its current decision to affect its future productivity. Considering a panel of Danish manufacturing firms over the period 2000-07, a simple description of the data reveals considerable firm heterogeneity; significant export and import activity premia; frequent incidence of simultaneous exporting and importing; and high persistence in the scope of firm trading. Structural estimation of the model shows a marked difference in the demand elasticities in which export markets are characterised by more elastic demand, tougher competition and lower markup than the domestic market. The estimates also indicate that firms with larger capital holding and paying higher wages are cost-efficient even after controlling for their productivity. Additionally, the estimates imply substantial sunk and fixed costs of exporting and importing, and these are consistent with the hypothesis of self-selection of productive firms into trading. There also exists a positive correlation between the size of these costs and the scale of firm operation. Moreover, both exporting and importing improve firm productivity, and therefore, these learning effects further drive the self-selection process. © 2018 John Wiley & Sons Ltd."
,10.1214/18-EJS1479,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053883800&doi=10.1214%2f18-EJS1479&partnerID=40&md5=0868f65602880223cfbfee3e334681a1,"When faced with high frequency streams of data, clustering raises theoretical and algorithmic pitfalls. We introduce a new and adaptive online clustering algorithm relying on a quasi-Bayesian approach, with a dynamic (i.e., time-dependent) estimation of the (unknown and changing) number of clusters. We prove that our approach is supported by minimax regret bounds. We also provide an RJMCMC-flavored implementation (called PACBO, see https://cran.r-project.org/web/packages/PACBO/index.html) for which we give a convergence guarantee. Finally, numerical experiments illustrate the potential of our procedure. © 2018, Institute of Mathematical Statistics. All rights reserved."
,10.1016/j.spasta.2018.08.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053718334&doi=10.1016%2fj.spasta.2018.08.007&partnerID=40&md5=24692881fc589215b9749d78f7f40e80,"We propose a Bayesian spatial model for time-to-event data in which we allow the censoring mechanism to depend on covariates and have a spatial structure. The survival model incorporates a cure rate fraction and assumes that the time-to-event follows a Weibull distribution, with covariates such as race, stage, grade, marital status and age at diagnosis being linked to its scale parameter. With right censoring being a primary concern, we consider a joint logistic regression model for the death versus censoring indicator, allowing dependence on covariates and including a spatial structure via the use of random effects. We apply the models to examine prostate cancer data from the Surveillance, Epidemiology, and End Results (SEER) registry, which displays marked spatial variation. © 2018 Elsevier B.V."
,10.1177/0962280218784757,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049887579&doi=10.1177%2f0962280218784757&partnerID=40&md5=5438087c885324c7f5199dca079c287b,"In the traditional joint models of a longitudinal and time-to-event outcome, a linear mixed model assuming normal random errors is used to model the longitudinal process. However, in many circumstances, the normality assumption is violated and the linear mixed model is not an appropriate sub-model in the joint models. In addition, as the linear mixed model models the conditional mean of the longitudinal outcome, it is not appropriate if clinical interest lies in making inference or prediction on median, lower, or upper ends of the longitudinal process. To this end, quantile regression provides a flexible, distribution-free way to study covariate effects at different quantiles of the longitudinal outcome and it is robust not only to deviation from normality, but also to outlying observations. In this article, we present and advocate the linear quantile mixed model for the longitudinal process in the joint models framework. Our development is motivated by a large prospective study of Huntington’s disease where primary clinical interest is in utilizing longitudinal motor scores and other early covariates to predict the risk of developing Huntington’s disease. We develop a Bayesian method based on the location–scale representation of the asymmetric Laplace distribution, assess its performance through an extensive simulation study, and demonstrate how this linear quantile mixed model-based joint models approach can be used for making subject-specific dynamic predictions of survival probability. © 2018, The Author(s) 2018."
,10.1016/bs.host.2018.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053149918&doi=10.1016%2fbs.host.2018.08.001&partnerID=40&md5=0f4afefd6c4427f48d5efcf306a98834,"Acquisition of time-series data supported by the recent popularization of genomic sequencing has revealed dynamic nature of a microbial community. Recent advances in analyzing community dynamics of microbial species cover not only applying existing traditional methods but also developing new methods. This chapter introduces traditional and novel mathematical and statistical approaches to analyze community dynamics of microbial species. In the former part, we introduce data fitting methods to link time-series data of community dynamics to mechanistic modeling. While data-driven approaches in the later part specifically introduce traditional and new methods to analyze time-series data of community dynamics based on reconstruction of attractors. © 2018 Elsevier B.V."
,10.1111/1556-4029.13926,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054565760&doi=10.1111%2f1556-4029.13926&partnerID=40&md5=c0b1ea6b9e2336d765d4b1759c77e1f1,"The completion of the third molar roots has played an important role in ascertaining whether individuals may be at or over a legal threshold of age, often taken as 18 years. This study demonstrates that root apex completion in the third molar is relatively uninformative regarding the threshold of age 18 years in a sample of 1184 males, where mean age-of-attainment of root apex completion for third mandibular molars is about 19.4 years. This paper also considers the legal age threshold problem for cases where the third mandibular molar is not completely formed, and outlines the use of parametric models and Bayes’ factors to evaluate dental evidence in statistically appropriate ways. It attempts to resolve confusion over age-within-stage versus age-of-attainment, likelihood ratios versus other diagnostic tests, and prior odds for a case versus the prior density for an age distribution. © 2018 American Academy of Forensic Sciences"
,10.18637/jss.v086.i07,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053285199&doi=10.18637%2fjss.v086.i07&partnerID=40&md5=de3e5c5bcdaea6952d4f31aeb5c81e2a,"In probabilistic (Bayesian) inferences, we typically want to compute properties of the posterior distribution, describing knowledge of unknown quantities in the context of a particular dataset and the assumed prior information. The marginal likelihood, also known as the “evidence”, is a key quantity in Bayesian model selection. The diffusive nested sampling algorithm, a variant of nested sampling, is a powerful tool for generating posterior samples and estimating marginal likelihoods. It is effective at solving complex problems including many where the posterior distribution is multimodal or has strong dependencies between variables. DNest4 is an open source (MIT licensed), multi-threaded implementation of this algorithm in C++11, along with associated utilities including: (i) ‘RJObject’, a class template for finite mixture models; and (ii) a Python package allowing basic use without C++ coding. In this paper we demonstrate DNest4 usage through examples including simple Bayesian data analysis, finite mixture models, and approximate Bayesian computation. © 2018, American Statistical Association. All rights reserved."
,10.1080/15598608.2018.1489919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052097024&doi=10.1080%2f15598608.2018.1489919&partnerID=40&md5=5fc357c2ad08099c2da342c9f71db8d7,"Poisson and negative binomial distributions are frequently used to fit count data. A limitation of the Poisson distribution is that the mean and the variance are assumed to be equal, but this assumption is far from being realistic in many practical applications. The negative binomial distribution is more used in cases of overdispersion, given that their variance is higher than the mean. The two-parameter double Poisson distribution introduced by Efron may be considered as a useful alternative to the Poisson and negative binomial distributions, given that it can account for both overdispersion and underdispersion. In this article, we obtain maximum likelihood and Bayesian estimates for the double Poisson distribution. We also extend the proposed methodology for the situation in which there is an excess of zeros in a sample. Applications of the double Poisson distribution are considered assuming simulated and real data sets. © 2018, © 2018 Grace Scientific Publishing, LLC."
2,10.1111/rssa.12266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017102370&doi=10.1111%2frssa.12266&partnerID=40&md5=3e333af4befc4c2dc4817f56ede97e6b,"The objective of this analysis was to explore temporal and spatial variation in teen birth rates TBRs across counties in the USA, from 2003 to 2012, by using hierarchical Bayesian models. Prior examination of spatiotemporal variation in TBRs has been limited by the reliance on large-scale geographies such as states, because of the potential instability in TBRs at smaller geographical scales such as counties. We implemented hierarchical Bayesian models with space–time interaction terms and spatially structured and unstructured random effects to produce smoothed county level TBR estimates, allowing for examination of spatiotemporal patterns and trends in TBRs at a smaller geographic scale across the USA. The results may help to highlight US counties where TBRs are higher or lower and to inform efforts to reduce birth rates to adolescents in the USA further. Published 2017. This article is a U.S. Government work and is in the public domain in the USA"
3,10.1016/j.spinee.2017.06.036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026286858&doi=10.1016%2fj.spinee.2017.06.036&partnerID=40&md5=ec91aaa8e310e3695ddd9c7164005f6a,"Background Context Anterior cervical discectomy and fusion (ACDF) and cervical disc replacement (CDR) are both acceptable surgical options for the treatment of cervical myelopathy and radiculopathy. To date, there are limited economic analyses assessing the relative cost-effectiveness of two-level ACDF versus CDR. Purpose The purpose of this study was to determine the 5-year cost-effectiveness of two-level ACDF versus CDR. Study Design The study design is a secondary analysis of prospectively collected data. Patient Sample Patients in the Prestige cervical disc investigational device exemption (IDE) study who underwent either a two-level CDR or a two-level ACDF were included in the study. Outcome Measures The outcome measures were cost and quality-adjusted life years (QALYs). Materials and Methods A Markov state-transition model was used to evaluate data from the two-level Prestige cervical disc IDE study. Data from the 36-item Short Form Health Survey were converted into utilities using the short form (SF)-6D algorithm. Costs were calculated from the payer perspective. QALYs were used to represent effectiveness. A probabilistic sensitivity analysis (PSA) was performed using a Monte Carlo simulation. Results The base-case analysis, assuming a 40-year-old person who failed appropriate conservative care, generated a 5-year cost of $130,417 for CDR and $116,717 for ACDF. Cervical disc replacement and ACDF generated 3.45 and 3.23 QALYs, respectively. The incremental cost-effectiveness ratio (ICER) was calculated to be $62,337/QALY for CDR. The Monte Carlo simulation validated the base-case scenario. Cervical disc replacement had an average cost of $130,445 (confidence interval [CI]: $108,395–$152,761) with an average effectiveness of 3.46 (CI: 3.05–3.83). Anterior cervical discectomy and fusion had an average cost of $116,595 (CI: $95,439–$137,937) and an average effectiveness of 3.23 (CI: 2.84–3.59). The ICER was calculated at $62,133/QALY with respect to CDR. Using a $100,000/QALY willingness to pay (WTP), CDR is the more cost-effective strategy and would be selected 61.5% of the time by the simulation. Conclusions Two-level CDR and ACDF are both cost-effective strategies at 5 years. Neither strategy was found to be more cost-effective with an ICER greater than the $50,000/QALY WTP threshold. The assumptions used in the analysis were strongly validated with the results of the PSA. © 2017 Elsevier Inc."
,10.1111/rssb.12289,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052365098&doi=10.1111%2frssb.12289&partnerID=40&md5=149b92ad1793de2298d37555cb011acc,"We introduce a class of generative network models that insert edges by connecting the starting and terminal vertices of a random walk on the network graph. Within the taxonomy of statistical network models, this class is distinguished by permitting the location of a new edge to depend explicitly on the structure of the graph, but being nonetheless statistically and computationally tractable. In the limit of infinite walk length, the model converges to an extension of the preferential attachment model—in this sense, it can be motivated alternatively by asking what preferential attachment is an approximation to. Theoretical properties, including the limiting degree sequence, are studied analytically. If the entire history of the graph is observed, parameters can be estimated by maximum likelihood. If only the final graph is available, its history can be imputed by using Markov chain Monte Carlo methods. We develop a class of sequential Monte Carlo algorithms that are more generally applicable to sequential network models and may be of interest in their own right. The model parameters can be recovered from a single graph generated by the model. Applications to data clarify the role of the random-walk length as a length scale of interactions within the graph. © 2018 Royal Statistical Society"
,10.26633/RPSP.2018.10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052016154&doi=10.26633%2fRPSP.2018.10&partnerID=40&md5=72f7b99220c7dda6e18c64af65f23c46,"Objective. To evaluate the cost-effectiveness of an integral model of ambulatory treatment in patients who presented an acute coronary syndrome. Methods. An economic evaluation was made from a quasi-experimental intervention study, which included 442 patients aged 30 to 70 years who presented an acute coronary syndrome. The intervention group (n = 165) received an integral model of ambulatory treatment based on managed care (disease management), while the control group (n = 277) received conventional cardiovascular rehabilitation. During one year of follow-up, the presentation of cardiovascular events and hospitalizations was evaluated. A probabilistic Markov model was developed. The study perspective was applied within the General System of Health Social Security in Colombia, including the direct health costs; the time horizon was 50 years with discounts of 3.42% for costs and effectiveness; and the measure of effectiveness was quality-adjusted life years (QALYs). A probabilistic and multivariate sensitivity analysis was performed using the Montecarlo simulation. Results. During the year of follow-up, the direct costs related to the value paid were, on average, USD 2 577 for the control group and USD 2 245 for the intervention group. In the probabilistic sensitivity analysis, 91.3% of the simulations were located in the quadrant corresponding to incremental negative costs and positive incremental effectiveness (evaluated intervention at a lower cost, more effective). In the simulations, an average annual savings per patient of USD 1 215 per QALY was observed. Conclusions. The integral model of ambulatory treatment implemented in patients who suffered an acute coronary syndrome was found to be less expensive and more effective compared to conventional care. Considering it is a dominant alternative, it is recommended as a model of care in this population. © 2018 Pan American Health Organization. All Rights Reserved."
,10.1016/j.ifacol.2018.08.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052646695&doi=10.1016%2fj.ifacol.2018.08.015&partnerID=40&md5=72663c20a953b2f0503738c533d23a6d,"In this work, we propose a stratified sampling method to statistically check Probabilistic Computation Tree Logic (PCTL) formulas on discrete-time Markov chains with sequential probability ratio test. Distinct from previous statistical verification methods using independent Monte Carlo sampling, our algorithm uses stratified samples that are negatively correlated, thus give lower variance. The experiments demonstrate that the new algorithm uses a smaller number of samples for a given confidence level on several benchmark examples. © 2018"
,10.1016/j.csda.2018.08.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052750838&doi=10.1016%2fj.csda.2018.08.004&partnerID=40&md5=f8202863c6581ed77426db53d8611048,"Latent variable hidden Markov models (LVHMMs) are important statistical methods in exploring the possible heterogeneity of data and explaining the pattern of subjects moving from one group to another over time. Classic subject- and/or time-homogeneous assumptions on transition matrices in transition model as well as the emission distribution in the observed process may be inappropriate to interpret heterogeneity at the subject level. For this end, a general extension of LVHMM is proposed to address the heterogeneity of multivariate longitudinal data both at the subject level and the occasion level. The main modeling strategy is that the observed time sequences are first grouped into different clusters, and then within each cluster the observed sequences are formulated via latent variable hidden Markov model. The local heterogeneity at the occasion level is characterized by the distribution related to the latent states, while the global heterogeneity at the subject level is identified with the finite mixture model. Compared to the existing methods, an appeal underlying the proposal is its capacity of accommodating non-homogeneous patterns of state sequences and emission distributions across the subjects simultaneously. As a result, the proposal provides a comprehensive framework for exploring various kinds of relevance among the multivariate longitudinal data. Within the Bayesian paradigm, Markov Chains Monte Carlo (MCMC) method is used to implement posterior analysis. Gibbs sampler is used to draw observations from the related full conditionals and posterior inferences are carried out based on these simulated observations. Empirical results including simulation studies and a real example are used to illustrate the proposed methodology. © 2018 Elsevier B.V."
,10.1177/0962280218766964,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045035017&doi=10.1177%2f0962280218766964&partnerID=40&md5=32b187c5689934e428b1dd8f16222779,"Hidden Markov models are stochastic models in which the observations are assumed to follow a mixture distribution, but the parameters of the components are governed by a Markov chain which is unobservable. The issues related to the estimation of Poisson-hidden Markov models in which the observations are coming from mixture of Poisson distributions and the parameters of the component Poisson distributions are governed by an m-state Markov chain with an unknown transition probability matrix are explained here. These methods were applied to the data on Vibrio cholerae counts reported every month for 11-year span at Christian Medical College, Vellore, India. Using Viterbi algorithm, the best estimate of the state sequence was obtained and hence the transition probability matrix. The mean passage time between the states were estimated. The 95% confidence interval for the mean passage time was estimated via Monte Carlo simulation. The three hidden states of the estimated Markov chain are labelled as ‘Low’, ‘Moderate’ and ‘High’ with the mean counts of 1.4, 6.6 and 20.2 and the estimated average duration of stay of 3, 3 and 4 months, respectively. Environmental risk factors were studied using Markov ordinal logistic regression analysis. No significant association was found between disease severity levels and climate components. © 2018, The Author(s) 2018."
,10.13031/aea.12420,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046097768&doi=10.13031%2faea.12420&partnerID=40&md5=b368d2b2bcc60c2b2f2c82d864a73056,"The prediction of agricultural machinery's fatigue life is of increasing importance for machine developers who must produce durable and reliable machines for a globalized market with different local operating conditions. Mathematical tools that can model and simulate the variable loading of agricultural machines are necessary for fatigue life prediction. Modeling should be based on measured loads from real-world operations. In this article, the loads of a four-rotor rake were recorded during grass swathing. Markov chains were used to model the transitions between the machine's operating conditions (in-field swathing and headland turning) and the sequences of turning points present in the load signals. The Markov transition probabilities were trained using the recoded data and then fatigue life was predicted via executing 10,000 Monte Carlo simulations based on the trained Markov models. The differences between the accumulated fatigue damage predicted from the simulations and from the measured data had mean value and standard deviation equal to -22% and 12.8%, respectively. Evaluation of the trained model on new data (not present in the training dataset) that were recorded during swathing on a different grass field resulted in fatigue damage difference with mean and standard deviation equal to 34% and 7.5%, respectively. The fatigue damage difference was in a reasonable region considering how fatigue life is affected by high-amplitude individual cycles. © 2018 American Society of Agricultural and Biological Engineers."
,10.1093/aje/kwx201,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040629195&doi=10.1093%2faje%2fkwx201&partnerID=40&md5=9064cda444dc9d32cbfd0112695c7b38,"Seasonal influenza epidemics occur year-round in the tropics, complicating the planning of vaccination programs. We built an individual-level longitudinal model of baseline antibody levels, time of infection, and the subsequent rise and decay of antibodies postinfection using influenza A(H1N1)pdm09 data from 2 sources in Singapore: 1) a noncommunity cohort with real-time polymerase chain reaction-confirmed infections and at least 1 serological sample collected from each participant between May and October 2009 (n = 118) and 2) a community cohort with up to 6 serological samples collected between May 2009 and October 2010 (n = 760). The model was hierarchical, to account for interval censoring and interindividual variation. Model parameters were estimated via a reversible jump Markov chain Monte Carlo algorithm using custom-designed R (https://www.r-project.org/) and C++ (https://isocpp.org/) code. After infection, antibody levels peaked at 4-7 weeks, with a half-life of 26.5 weeks, followed by a slower decrease up to 1 year to approximately preinfection levels. After the third wave, the seropositivity rate and the population-level antibody titer dropped to the same level as they were at the end of the first pandemic wave. The results of this analysis are consistent with the hypothesis that the population-level effect of individuals' waxing and waning antibodies influences influenza seasonality in the tropics. © 2017 The Author(s)."
,10.1016/j.procs.2018.04.074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051263570&doi=10.1016%2fj.procs.2018.04.074&partnerID=40&md5=6695dbe505d0fd67fba622c5b7ff98d3,"Transportation modeling and simulation play an important role in the planning and management of emergency evacuation. It is often indispensable for the preparedness and timely response to extreme events occurring in highly populated areas. Reliable and robust agent-based evacuation models are of great importance to support evacuation decision making. Nevertheless, these models rely on numerous hypothetical causal relationships between the evacuation behavior and a variety of factors including socio-economic characteristics and storm intensity. Understanding the impacts of these factors on evacuation behaviors (e.g., destination and route choices) is crucial in preparing optimal evacuation plans. This paper aims to contribute to the literature by integrating well-calibrated behavior models with an agent-based evacuation simulation model in the context of hurricane evacuation. Specifically, discrete choice models were developed to estimate the evacuation behaviors based on large-scale survey data in Northern New Jersey. Monte-Carlo Markov Chain (MCMC) sampling method was used to estimate evacuation propensity and destination choices for the whole population. Finally, evacuation of over a million residents in the study area was simulated using agent-based simulation built in MATSim. The agent-based modeling framework proposed in this paper provides an integrated methodology for evacuation simulation with specific consideration of agents' behaviors. The simulation results need to be further validated and verified using real-world evacuation data. © 2018 The Authors. Published by Elsevier B.V."
,10.1007/s00165-018-0470-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054536367&doi=10.1007%2fs00165-018-0470-6&partnerID=40&md5=7cf87516fe0d5e5883d7a1ba74b4d5d3,"Computation of steady-state probabilities is an important aspect of analysing biological systems modelled as probabilistic Boolean networks (PBNs). For small PBNs, efficient numerical methods to compute steady-state probabilities of PBNs exist, based on the Markov chain state-transition matrix. However, for large PBNs, numerical methods suffer from the state-space explosion problem since the state-space size is exponential in the number of nodes in a PBN. In fact, the use of statistical methods and Monte Carlo methods remain the only feasible approach to address the problem for large PBNs. Such methods usually rely on long simulations of a PBN. Since slow simulation can impede the analysis, the efficiency of the simulation procedure becomes critical. Intuitively, parallelising the simulation process is the ideal way to accelerate the computation. Recent developments of general purpose graphics processing units (GPUs) provide possibilities to massively parallelise the simulation process. In this work, we propose a trajectory-level parallelisation framework to accelerate the computation of steady-state probabilities in large PBNs with the use of GPUs. To maximise the computation efficiency on a GPU, we develop a dynamical data arrangement mechanism for handling different size PBNs with a GPU. Specially, we propose a reorder-and-split method to handle both large and dense PBNs. Besides, we develop a specific way of storing predictor functions of a PBN and the state of the PBN in the GPU memory. Moreover, we introduce a strongly connected component (SCC)-based network reduction technique to further accelerate the computation speed. Experimental results show that our GPU-based parallelisation gains approximately a 600-fold speedup for a real-life PBN compared to the state-of-the-art sequential method. © 2018, British Computer Society."
3,10.1097/EDE.0000000000000761,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044314160&doi=10.1097%2fEDE.0000000000000761&partnerID=40&md5=baad9228f7cc2596d4f05cb7e5155723,"Background: There is considerable scientific interest in associations between protracted low-dose exposure to ionizing radiation and the occurrence of specific types of cancer. Methods: Associations between ionizing radiation and site-specific solid cancer mortality were examined among 308,297 nuclear workers employed in France, the United Kingdom, and the United States. Workers were monitored for external radiation exposure and follow-up encompassed 8.2 million person-years. Radiation-mortality associations were estimated using a maximum-likelihood method and using a Markov chain Monte Carlo method, the latter used to fit a hierarchical regression model to stabilize estimates of association. Results: The analysis included 17,957 deaths attributable to solid cancer, the most common being lung, prostate, and colon cancer. Using a maximum-likelihood method to quantify associations between radiation dose-and site-specific cancer, we obtained positive point estimates for oral, esophagus, stomach, colon, rectum, pancreas, peritoneum, larynx, lung, pleura, bone and connective tissue, skin, ovary, testis, and thyroid cancer; in addition, we obtained negative point estimates for cancer of the liver and gallbladder, prostate, bladder, kidney, and brain. Most of these estimated coefficients exhibited substantial imprecision. Employing a hierarchical model for stabilization had little impact on the estimated associations for the most commonly observed outcomes, but for less frequent cancer types, the stabilized estimates tended to take less extreme values and have greater precision than estimates obtained without such stabilization. Conclusions: The results provide further evidence regarding associations between low-dose radiation exposure and cancer. © 2017 Wolters Kluwer Health, Inc."
,10.1371/journal.pone.0191822,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041032228&doi=10.1371%2fjournal.pone.0191822&partnerID=40&md5=9ac2cc218b88cae8cf6ecf254eb6a28d,"Objective To evaluate intravoxel incoherent motion (IVIM) diffusion-weighted imaging (DWI) and dynamic contrast-enhanced (DCE) magnetic resonance imaging (MRI) sequences for quantitative characterization of anal fistula activity. Methods This retrospective study was approved by the institutional review board. One hundred and two patients underwent MRI for clinical suspicion of anal fistula. Forty-three patients with demonstrable anal fistulas met the inclusion criteria. Quantitative analysis included measurement of DCE and IVIM parameters. The reference standard was clinical activity based on medical records. Statistical analyses included Bayesian analysis with Markov Chain Monte Carlo, multivariable logistic regression, and receiver operating characteristic analyses. Results Brevity of enhancement, defined as the time difference between the wash-in and wash-out, was longer in active than inactive fistulas (p = 0.02). Regression coefficients of multivariable logistic regression analysis revealed that brevity of enhancement increased and normalized perfusion area under curve decreased with presence of active fistulas (p = 0.03 and p = 0.04, respectively). By cross-validation, a logistic regression model that included quantitative perfusion parameters (DCE and IVIM) performed significantly better than IVIM only (p < 0.001). Area under the curves for distinguishing patients with active from those with inactive fistulas were 0.669 (95% confidence interval [CI]: 0.500, 0.838) for a model with IVIM only, 0.860 (95% CI: 0.742, 0.977) for a model with IVIM and brevity of enhancement, and 0.921 (95% CI: 0.846, 0.997) for a model with IVIM and all DCE parameters. Conclusion The inclusion of brevity of enhancement measured by DCE-MRI improved assessment of anal fistula activity over IVIM-DWI only. © 2018 Lefrancois et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.1177/0021998317704708,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040068349&doi=10.1177%2f0021998317704708&partnerID=40&md5=8b13041c3ea90d91c2e0c525ffea73b0,"This paper presents a probabilistic first ply failure analysis of composite laminates using a high-fidelity multi-scale approach called M-SaF (Micromechanics-based approach for Static Failure). To this end, square and hexagonal representative unit cells of composites are developed to calculate constituent stresses with the help of a bridging matrix between macro and micro stresses referred to as the stress amplification factor matrix. Separate failure criteria are applied to each of the constituents (fiber, matrix, and interface) in order to calculate the damage state. The successful implementation of M-SaF requires strength properties of the constituents which are the most difficult and expensive to characterize experimentally, limiting the use of M-SaF in the early design stages of a structure. This obstacle is overcome by integrating a Bayesian inference approach with M-SaF. An academic sample problem of a cantilever beam is used to first demonstrate the calibration procedure. Bayesian inference calibrates the M-SaF first ply failure model parameters as posterior distributions from the prior probability density functions drawn from lamina test data. The posterior statistics were then used to calculate probabilistic first ply failure for a range of different laminates. © 2017, © The Author(s) 2017."
,10.1007/978-3-030-00928-1_76,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054096938&doi=10.1007%2f978-3-030-00928-1_76&partnerID=40&md5=9ae5d4c8c34ac5ae9d6d4105c36ec4f6,"Typical segmentation methods produce a single optimal solution and fail to inform about (i) the confidence/uncertainty in the object boundaries or (ii) alternate close-to-optimal solutions. To estimate uncertainty, some methods intend to sample segmentations from an associated posterior model using Markov chain Monte Carlo (MCMC) sampling or perturbation models. However, they cannot guarantee sampling from the true posterior, deviating significantly in practice. We propose a novel method that guarantees exact MCMC sampling, in finite time, of multi-label segmentations from generic Bayesian Markov random field (MRF) models. For exact sampling, we propose Fill’s strategy and extend it to generic MRF models via a novel bounding chain algorithm. Results on simulated data and clinical brain images from 4 classic problems show that our uncertainty estimates gain accuracy over the state of the art. © Springer Nature Switzerland AG 2018."
3,10.1109/LCOMM.2017.2756833,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030777097&doi=10.1109%2fLCOMM.2017.2756833&partnerID=40&md5=a7c3af6055a337ef2fef39489c13d4e4,"In this letter, we discuss multiple links with equal weights, in buffer size based relay selection schemes in cooperative wireless networks. A general relay selection factor is defined, which includes the weight of the link as the first metric and the link quality, or priority, as the second metric for different cases of the same weight. The Markov chain based theoretical framework is employed to evaluate the outage probability, delay and throughput of the system. The proposed scheme is evaluated for symmetric and asymmetric channel conditions. The link quality based second selection metric achieves lower outage probability, while the link priority based selection shows significant improvements in terms of delay and throughput. Theoretical results are validated through extensive Monte carlo simulations. © 1997-2012 IEEE."
,10.1667/RR14852.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042348439&doi=10.1667%2fRR14852.1&partnerID=40&md5=55be5e1dba29e3224a4bd77cc08cdee1,"In 2008, Serandour et al. reported on their in vitro experiment involving rat plasma samples obtained after an intravenous intake of plutonium citrate. Different amounts of DTPA were added to the plasma samples and the percentage of low-molecular-weight plutonium measured. Only when the DTPA dosage was three orders of magnitude greater than the recommended 30 μmol/kg was 100% of the plutonium apparently in the form of chelate. These data were modeled assuming three competing chemical reactions with other molecules that bind with plutonium. Here, time-dependent second-order kinetics of these reactions are calculated, intended eventually to become part of a complete biokinetic model of DTPA action on actinides in laboratory animals or humans. The probability distribution of the ratio of stability constants for the reactants was calculated using Markov Chain Monte Carlo. These calculations substantiate that the inclusion of more reactions is needed in order to be in agreement with known stability constants. © 2018 by Radiation Research Society."
1,10.1214/17-BA1088,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054607918&doi=10.1214%2f17-BA1088&partnerID=40&md5=36205cb28a20c223bdefcbede1931b9b,"We propose two new sequential Monte Carlo (SMC) smoothing methods for general state-space models with unknown parameters. The first is a modification of the particle learning and smoothing (PLS) algorithm of Carvalho, Johannes, Lopes, and Polson (2010), with an adjustment in the backward resampling weights. The second, called Refiltering, is a two-stage method that combines sequential parameter learning and particle smoothing algorithms. We illustrate the methods on three benchmark models using simulated data, and apply them to a stochastic volatility model for daily S&P 500 index returns during the financial crisis. We show that both new methods outperform existing SMC approaches, and that Refiltering is competitive with smoothing approaches based on Markov chain Monte Carlo (MCMC) and Particle MCMC. © 2018 International Society for Bayesian Analysis."
,10.1371/journal.pone.0189531,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040003858&doi=10.1371%2fjournal.pone.0189531&partnerID=40&md5=abf4d31bda0fab39aff14f6a89f1a9ec,"Background Tuberculosis (TB) in the elderly remains a challenge in intermediate disease burden areas like Hong Kong. Given a higher TB burden in the elderly and limited impact of current case-finding strategy by patient-initiated pathway, proactive screening approaches for the high-risk group could be optimal and increasingly need targeted economic evaluations. In this study, we examined whether and under what circumstance the screening strategies are cost-effective compared with no screening strategy for the elderly at admission to residential care homes. Methods A decision analytic process based on Markov model was adopted to evaluate the cost-effectiveness of four strategies: (i) no screening, (ii) TB screening (CXR) and (iii) TB screening (Xpert) represent screening for TB in symptomatic elderly by chest X-ray and Xpert® MTB/ RIF respectively, and (iv) LTBI/TB screening represents screening for latent and active TB infection by QuantiFERON®-TB Gold In-Tube and chest X-ray. The target population was a hypothetical cohort of 65-year-old people, using a health service provider perspective and a time horizon of 20 years. The outcomes were direct medical costs, life-years and quality-adjusted life-years (QALYs) measured by incremental cost-effectiveness ratio (ICER). Results In the base-case analysis, no screening was the most cost-saving; TB screening (CXR) was dominated by TB screening (Xpert); LTBI/TB screening resulted in more life-years and QALYs accrued. The ICERs of LTBI/TB screening were US$19,712 and US$29,951 per QALY gained compared with no screening and TB screening (Xpert), respectively. At the willingness-to-pay threshold of US$50,000 per QALY gained, LTBI/TB screening was the most cost-effective when the probability of annual LTBI reactivation was greater than 0.155% and acceptability of LTBI/TB screening was greater than 38%. In 1,000 iterations of Monte Carlo simulation, the probabilities of no screening, TB screening (CXR), TB screening (Xpert), and LTBI/TB screening to be cost-effective were 0, 1.3%, 20.1%, and 78.6% respectively. Conclusions Screening for latent and active TB infection in Hong Kong elderly people at admission to residential care homes appears to be highly effective and cost-effective. The key findings may be the next key factor to bring down TB endemic in the elderly population among intermediate TB burden areas. © 2018 Li et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
1,10.1016/j.matcom.2016.07.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997208447&doi=10.1016%2fj.matcom.2016.07.010&partnerID=40&md5=63dd0bd6458cafd241ecd83bbecc10f5,"We review the Array-RQMC method, its variants, sorting strategies, and convergence results. We are interested in the convergence rate of measures of discrepancy of the states at a given step of the chain, as a function of the sample size n, and also the convergence rate of the variance of the sample average of a (cost) function of the state at a given step, viewed as an estimator of the expected cost. We summarize known convergence rate results and show empirical results that suggest much better convergence rates than those that are proved. We also compare different types of multivariate sorts to match the chains with the RQMC points, including a sort based on a Hilbert curve. © 2016 International Association for Mathematics and Computers in Simulation (IMACS)"
,10.1515/sagmb-2017-0046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048416379&doi=10.1515%2fsagmb-2017-0046&partnerID=40&md5=88b8dd795a52b7bee46b86f9e30fb1c3,"The increasing availability of population-level allele frequency data across one or more related populations necessitates the development of methods that can efficiently estimate population genetics parameters, such as the strength of selection acting on the population(s), from such data. Existing methods for this problem in the setting of the Wright-Fisher diffusion model are primarily likelihood-based, and rely on numerical approximation for likelihood computation and on bootstrapping for assessment of variability in the resulting estimates, requiring extensive computation. Recent work has provided a method for obtaining exact samples from general Wright-Fisher diffusion processes, enabling the development of methods for Bayesian estimation in this setting. We develop and implement a Bayesian method for estimating the strength of selection based on the Wright-Fisher diffusion for data sampled at a single time point. The method utilizes the latest algorithms for exact sampling to devise a Markov chain Monte Carlo procedure to draw samples from the joint posterior distribution of the selection coefficient and the allele frequencies. We demonstrate that when assumptions about the initial allele frequencies are accurate the method performs well for both simulated data and for an empirical data set on hypoxia in flies, where we find evidence for strong positive selection in a region of chromosome 2L previously identified. We discuss possible extensions of our method to the more general settings commonly encountered in practice, highlighting the advantages of Bayesian approaches to inference in this setting. © 2018 Walter de Gruyter GmbH, Berlin/Boston 2018."
3,10.1002/2017JB014847,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040242470&doi=10.1002%2f2017JB014847&partnerID=40&md5=104de12aaf6973e4f78b5c0dcdcb160c,"Using the Markov chain Monte Carlo (MCMC) inversion technique of Mullet et al. (2015), we reassess the validity of the conventionally accepted values of the grain size exponent of diffusion creep in olivine aggregates. A systematic and comprehensive analysis of individual experimental runs taken from three widely cited studies reveals that these data do not tightly constrain the grain size exponent or any other flow law parameter for diffusion and dislocation creep. Our analysis indicates that large data uncertainties can cause inversion results to deviate significantly from true values because of the covariance between the grain size and stress exponents, and that even resolving a grain size exponent of 2 from 3 is difficult. The versatility of our MCMC inversion technique can, however, be exploited to improve this situation by identifying optimal conditions for future experimental studies. Because the uncertainties of the grain size and stress exponents are highly correlated, for example, increasing the range of grain size variation can help better constrain both exponents simultaneously. ©2017. American Geophysical Union. All Rights Reserved."
,10.1016/j.vaccine.2018.01.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040335726&doi=10.1016%2fj.vaccine.2018.01.007&partnerID=40&md5=717a0a207cb067a5fe7134289a068d86,"Background: Current recommendations about dengue vaccination by the World Health Organization depend on seroprevalence levels and serological status in populations and individuals. However, seroprevalence estimation may be difficult due to a diversity of factors. Thus, estimation through models using data from epidemiological surveillance systems could be an alternative procedure to achieve this goal. Objective: To estimate the expected dengue seroprevalence in children of selected areas in Argentina, using a simple model based on data from passive epidemiological surveillance systems. Methods: A Markov model using a simulated cohort of individuals from age 0 to 9 years was developed. Parameters regarding the reported annual incidence of dengue, proportion of inapparent cases, and expansion factors for outpatient and hospitalized cases were considered as transition probabilities. The proportion of immune population at 9 years of age was taken as a proxy of the expected seroprevalence, considering this age as targeted for vaccination. The model was used to evaluate the expected seroprevalence in Misiones and Salta provinces and in Buenos Aires city, three settings showing different climatic favorability for dengue. Results: The estimates of the seroprevalence for the group of 9-year-old children for Misiones was 79% (95%CI:46-100%), and for Salta 22% (95%CI:14-30%), both located in northeastern and northwestern Argentina, respectively. Buenos Aires city, from central Argentina, showed a likely seroprevalence of 7% (95%CI: 3-11%). According to the deterministic sensitivity analyses, the parameter showing the highest influence on these results was the probability of inapparent cases. Conclusions: This model allowed the estimation of dengue seroprevalence in settings where this information is not available. Particularly for Misiones, the expected seroprevalence was higher than 70% in a wide range of scenarios, thus in this province a vaccination strategy directed to seropositive children of >9 years should be analyzed, including further considerations as safety, cost-effectiveness, and budget impact. © 2018 Elsevier Ltd."
,10.7507/1672-2531.201707008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050722310&doi=10.7507%2f1672-2531.201707008&partnerID=40&md5=f554c54779be1e05bef0c6836cf80a09,"Health economics analysis has become increasingly important in recent years. It is essential to master the use of relevant software to conduct research in health economics. TreeAge Pro software is widely used in the healthcare decision analysis. It can carry out decision analysis, cost-effectiveness analysis, and Monte Carlo simulation. With powerful functionlity and outstanding visualization, it can build Markov disease transition models to analyze Markov processes according to disease models and accomplish decision analysis with decision trees and influence diagrams. This paper introduces cost-effectiveness analysis based on Markov model with examples and explains the main graphs. © 2018, West China University of Medical Science. All rights reserved."
1,10.1016/j.acvd.2017.03.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029698719&doi=10.1016%2fj.acvd.2017.03.010&partnerID=40&md5=8779ca76a5e82d240d1ee7418fa71e61,"Background Patient education programmes (PEP) are recommended for patients with heart failure but have not been specifically assessed in heart failure with preserved ejection fraction (HFpEF). Aim To assess the effectiveness of a structured PEP in reducing all-cause mortality in patients with HFpEF. Methods Patients with HFpEF were selected from the ODIN cohort, designed to assess PEP effectiveness in patients with HF whatever their ejection fraction, included from 2007 to 2010, and followed up until 2013. Baseline sociodemographic, clinical, biological and therapeutic characteristics were collected. At inclusion, patients were invited to participate in the PEP, which consisted of educational diagnosis, education sessions and final evaluation. Education focused on HF pathophysiology and medication, symptoms of worsening HF, dietary recommendations and management of exercise. Propensity score matching and Cox models were performed. Results Of 849 patients with HFpEF, 572 (67.4%) participated in the PEP and 277 (32.6%) did not. Patients who participated in the PEP were younger (67.0 ± 13.1 vs 76.1 ± 13.2 years; standardized difference [StDiff] = −54.6%), less often women (39.7% vs 48.4%; StDiff = −17.6%) and presented more often with hypercholesterolaemia (55.2% vs 35.2%; StDiff 41.2%), smoking (35.1% vs 28.7%; StDiff 13.8%), alcohol abuse (14.1% vs 8.9%; StDiff 16.5%) and ischaemic HF (38.7% vs 29.2%; StDiff 20.0%) than those who did not; they also presented with better clinical cardiovascular variables. After propensity score matching, baseline characteristics were balanced, except hypertension (postmatch StDiff 19.1%). The PEP was associated with lower all-cause mortality (pooled hazard ratio 0.70, 95% confidence interval 0.49–0.99; P = 0.042). This association remained significant after adjustment for hypertension (adjusted pooled hazard ratio 0.68, 95% confidence interval 0.48–0.97; P = 0.036). Conclusions In this investigation, a structured PEP was associated with lower all-cause mortality. Patient education might be considered an effective treatment in patients with HFpEF. © 2017 Elsevier Masson SAS"
,10.1111/jfb.13520,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037671980&doi=10.1111%2fjfb.13520&partnerID=40&md5=e14d6237fb71f74f12a291286728795a,"In the current study activity and latency to explore, as well as the correlation of these traits, were examined in individually marked juvenile Gadus morhua at 7, 10 and 13° C. It was concluded that individual rank order of both traits was maintained across temperature but that the level of change differed between individuals. © 2017 The Fisheries Society of the British Isles."
1,10.1016/j.artmed.2017.12.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038819556&doi=10.1016%2fj.artmed.2017.12.003&partnerID=40&md5=e5a33dcfd88fab6bd6c44eb1825830af,"Health care practitioners analyse possible risks of misleading decisions and need to estimate and quantify uncertainty in predictions. We have examined the “gold” standard of screening a patient's conditions for predicting survival probability, based on logistic regression modelling, which is used in trauma care for clinical purposes and quality audit. This methodology is based on theoretical assumptions about data and uncertainties. Models induced within such an approach have exposed a number of problems, providing unexplained fluctuation of predicted survival and low accuracy of estimating uncertainty intervals within which predictions are made. Bayesian method, which in theory is capable of providing accurate predictions and uncertainty estimates, has been adopted in our study using Decision Tree models. Our approach has been tested on a large set of patients registered in the US National Trauma Data Bank and has outperformed the standard method in terms of prediction accuracy, thereby providing practitioners with accurate estimates of the predictive posterior densities of interest that are required for making risk-aware decisions. © 2017 Elsevier B.V."
,10.7334/psicothema2017.92,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040839700&doi=10.7334%2fpsicothema2017.92&partnerID=40&md5=be4e0c1eab26c5345bd1f2dc2ef7f056,"Background: Negative symptoms represent the main cause of disability in schizophrenia, having recently been grouped into two general dimensions: avolition and diminished emotional expression, which includes affective flattening and alogia. The aim of this study was to explore the response of these two symptoms to a set of behavioral interventions based on contingency management, performed in an interdisciplinary context. Method: Behaviors of interest were monitored and evaluations before and after the treatment were performed on 9 schizophrenic inpatients with persistent negative symptoms. The program included 12 group double sessions aimed at developing facial expression and verbal communication, and a nursing care plan to generalize and strengthen these behaviors synergistically. Results: There were appreciable differences in facial expression, which were less clear for alogia. The clinical evaluation using PANSS-N did not find notable differences at group level, but the nursing assessment using NOC indicators did. Conclusions: Although difficult to modify, negative symptoms are not insensitive to the influence of behavioral interventions. Specific psychological interventions that address negative symptoms as a priority focus of attention and care need to be promoted and developed, particularly when considering the crucial role of context in their progression. © 2018 Psicothema."
,10.3923/ijp.2018.151.163,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041293521&doi=10.3923%2fijp.2018.151.163&partnerID=40&md5=e64c1b3b915d5b3b4921d4c3a3f08d56,"Statins inhibit cholesterol synthesis by blocking 3-hydroxy-3-methylglutaryl coenzyme A reductase in the liver, thereby ameliorating hypercholesterolemia. Thus, to determine statins with the best efficacy, a meta-analysis was performed to compare the effects of statins against hypercholesterolemia. Comprehensive literature searches were established, from Cochrane library, Pubmed, Embase. The studies were performed to randomize controlled trials (RCTs), cohort studies or case-control studies about efficacy of different statin drugs and dose against hypercholesterolemia published between 1997 and 20 February, 2017. Study qualities were assessed according to Cochrane collaboration recommendations. The non-programming software Aggregate Data Drug Information System (ADDIS) (version 1.16.5) was used to perform Bayesian network meta-analysis and compare treatments using the Markov Chain Monte Carlo (MCMC) method. Overall, 28 RCTs studies, including 12855 patients, met the inclusion criteria. Total cholesterol (TC) levels significantly reduced (p<0.05) using 2 mg Pitavastatin (Pit) than those using 20 mg Pravastatin (Pra), 10 mg Simvastatin (Sim) or 10 mg Atorvastatin (Ato). Similarly, triglyceride (TG) levels reduced using 2 mg Pit than those using 20 mg Pra (p<0.05), 10 mg Sim (p<0.05) or 20 mg Sim (p<0.05) and reduced apolipoprotein B (Apo B) levels were observed than those using 10 mg Ato or 20 mg Pra (p<0.05). Rosuvastatin (Ros) significantly reduced TC and TG levels (p<0.05) when administered at 20 and 10 mg Ros treatments ameliorated percentage changes in low-density lipoprotein cholesterol more than the other drugs (p<0.05) and increased high-density lipoprotein cholesterol levels more effectively than 10 mg Ato (p<0.05), 20 mg Pra (p<0.05) or 10 mg Sim (p<0.05). Increases in Apo A1 levels did not differ between treatments (p>0.05). Among the present statin drug regimens, 2 mg Pit and 10 or 20 mg Ros had the highest efficacy against hypercholesterolemia. © 2018 Qingzan Kong et al."
,10.1016/j.molmet.2017.10.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033388114&doi=10.1016%2fj.molmet.2017.10.007&partnerID=40&md5=1e58882248d39ac7429ad22d57923d32,"Objectives Body fatness is widely assumed to be regulated by a lipostatic set-point system, which has evolved in response to trade-offs in the risks of mortality. Increasing fatness makes the risk of starvation lower but increases the risk of predation. Yet other models are available. The aim of this work is to evaluate using mathematical modeling whether set-point systems are more likely to evolve than the alternatives. Methods I modeled the trade-off in mortality risks using a simple mathematical model, which generates an optimum level of fatness that is presumed to be the driver for the evolution of a set-point. I then mimicked the likely errors in this optimum level, that derive from the variation in the component parameters of the mortality curves using Markov Chain Monte Carlo (MCMC) simulation by Bayesian inference Using Gibbs Sampling (BUGS). Results The error propagation generated by the simulations showed that even very small errors in the model parameters were magnified enormously in the location of the optimum fatness level. If the model parameters had coefficients of variation of just 1% then the coefficient of variation in the optimum level of fatness was between 20 and 90%. In that situation, a set-point centered at the mathematical optimum from the component curves would be at the correct level of fatness that minimizes mortality, and hence maximizes fitness, on less than 8% of occasions. Conclusions Set-point regulation of body fatness is hence highly unlikely to evolve where there is any realistic level of variation in the parameters that define mortality risks. Using further MCMC modeling, I show that a dual-intervention point system is more likely to evolve. This mathematical simulation work has important implications for how we interpret molecular work concerning regulation of adiposity. © 2017"
1,10.7150/jca.24690,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048330856&doi=10.7150%2fjca.24690&partnerID=40&md5=beadba5088bb323f8d0031386028110b,"Objective: With the increasing recognition of the over-diagnosis and over-treatment of prostate cancer (PCa), the choice of a better prostate biopsy strategy had confused both the patients and clinical surgeons. Hence, this network meta-analysis was conducted to clarify this question. Methods: In the current network meta-analysis, twenty eligible randomized controlled trials (RCTs) with 4,571 participants were comprehensively identified through Pubmed, Embase and Web of Science databases up to July 2017. The pooled odds ratio (OR) with 95% credible interval (CrI) was calculated by Markov chain Monte Carlo methods. A Bayesian network meta-analysis was conducted by using R-3.4.0 software with the help of package ""gemtc"" version 0.8.2. Results: Six different PCa biopsy strategies and four clinical outcomes were ultimately analyzed in this study. Although, the efficacy of different PCa biopsy strategies by ORs with corresponding 95% CrIs had not yet reached statistical differences, the cumulative rank probability indicated that overall PCa detection rate from best to worst was FUS-GB plus TRUS-GB, FUS-GB, CEUS, MRI-GB, TRUS-GB and TPUS-GB. In terms of clinically significant PCa detection, CEUS, FUS-GB or FUS-GB plus TRUS-GB had a higher, whereas TRUS-GB or TPUS-GB had a relatively lower significant detection rate. Meanwhile, TPUS-GB or TRUS-GB had a higher insignificant PCa detection rate. As for TRUS-guided biopsy, the general trend was that the more biopsy cores, the higher overall PCa detection rate. As for targeted biopsy, it could yield a comparable or even a better effect with fewer cores, compared with traditional random biopsy. Conclusion: Taken together, in a comprehensive consideration of four clinical outcomes, our outcomes shed light on that FUS-GB or FUS-GB plus TRUS-GB showed their superiority, compared with other puncture methods in the detection of PCa. Moreover, TPUS or TRUS-GB was more easily associated with the over-diagnosis and over-treatment of PCa. In addition, targeted biopsy was obviously more effective than traditional random biopsy. The subsequent RCTs with larger sample sizes were required to validate our findings. © Ivyspring International Publisher."
,10.1007/s11276-018-1821-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053178807&doi=10.1007%2fs11276-018-1821-1&partnerID=40&md5=52ca82f7082a9ccf57e6ddc8eabfd4a1,"In this paper, we present an analytical framework for the performance evaluation of Radio Resource Allocation (RRA) in Orthogonal Frequency Division Multiple Access (OFDMA) networks. We focus on Quality of Service (QoS) guaranteed traffic whose capacity, in terms of the number of active user connections, depends on the users’ QoS requirements and channel conditions, and the RRA algorithm. The required QoS is guaranteed by restricting the number of admitted calls, which in turn requires an accurate estimate of the QoS metrics and capacity supported by the RRA when a new call arrives. These estimates for OFDMA networks are variable and usually obtained through time-consuming offline computer simulations. Mathematical frameworks on the other hand yield timely and accurate results. However, earlier known works on analytical modelling of RRA either consider a single channel with random traffic arrivals or multiple channels with full buffer data traffic. In contrast, we develop a queueing theoretic framework considering randomly arriving QoS-guaranteed traffic in a variable-rate multi-channel multi-class OFDMA network. The framework can be used online leading to better dynamic Call Admission Control. We characterize the RRA algorithm using a scheduler control parameter which can regulate the call blocking probability while providing predefined QoS constraints. We model the RRA as a variable service rate, multi-server, multi-class, finite buffer queueing system and verify the derived QoS metrics using extensive Monte-Carlo discrete event simulations. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
,10.1115/MSEC2018-6638,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054964869&doi=10.1115%2fMSEC2018-6638&partnerID=40&md5=09306478b03fef4a30e82f60e31050f1,"As a critical element in rotating machines, remaining useful life (RUL) prediction of rolling bearings plays an essential role in realizing predictive and preventative machine maintenance in modern manufacturing. The physics of defect (e.g. spall) initiation and propagation describes bearing's service life as generally divided into three stages: normal operation, defect initiation, and accelerated performance degradation. The transition among the stages are embedded in the variations of monitored data, e.g., vibration. This paper presents a multi-mode particle filter (MMPF) that is aimed to: 1) automatically detect the transition among the three life stages; and 2) accurately characterize bearing performance degradation by integrating physical models with stochastic modeling method. In MMPF, a set of linear and non-linear modes (also called degradation functions) are first defined according to the physical/empirical knowledge as well as statistical analysis of the measured data (e.g. vibration). These modes are subsequently refined during the particle filtering (PF)-based bearing performance tracking process. Each mode corresponds to an individual performance scenario. A finitestate Markov chain switches among these modes, reflecting the transition between the service life stages. Case studies performed on two run-To-failure experiments indicate that the developed technique is effective in tracking the evolution of bearing performance degradation and predicting the remaining useful life of rolling bearings. Copyright © 2018 ASME."
,10.14419/ijet.v7i3.15.17522,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051646789&doi=10.14419%2fijet.v7i3.15.17522&partnerID=40&md5=beb6688ac25a20bd8e6916abd9850db9,"This paper discussed on the Monte-Carlo simulation technique to determine the optimal placement of Phasor Measurement Unit (PMU) in power system whilst ensuring the observability of the system. In addition, the information on Force Outage Rate (FOR) of the system can be calculated using Markov Chain technique. The FOR represents the level of risk security for the transmission line that happened because of unscheduled and unexpected failure or repair in the system. Subsequently, the reliability model of the transmission line can be developed. Using IEEE 57-bus system, the results obtained from Monte-Carlo simulation technique demonstrate the optimal PMU placement, the desired reliability of the Wide Area Monitoring System (WAMS) as well as the number and location of covered contingencies of the system. © 2018 Authors."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053891521&partnerID=40&md5=29ae46155969972170803aa374311c7e,"In this article likelihood and Bayesian estimations for the partially accelerated constant-stress life test model are compared using Type-II censored data from the Pareto distribution of the second kind. The posterior means and posterior variances are obtained under the squared error loss function using Lindley's approximation procedure. Furthermore, the highest posterior density credible intervals of the model parameters based on Gibbs sampling technique are presented. For illustration, simulation studies are provided. It is shown with the Bayesian approach via Gibbs sampling procedure that the statistical precision of parameter estimation is improved. Consequently, the required number of failures could be reduced. That is, more savings in time and cost can be achieved through the Markov chain Monte Carlo (MCMC) technique. Reducing the total testing time and the total number of failures without sacrificing much of the statistical power in inference is often desired in industrial applications. © INTERNATIONAL JOURNAL OF INDUSTRIAL ENGINEERING"
,10.1016/j.csda.2018.08.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054553886&doi=10.1016%2fj.csda.2018.08.025&partnerID=40&md5=167c4bff5f3342b024a9ba9861de3e72,"With high-dimensional data emerging in various domains, sparse logistic regression models have gained much interest of researchers. Variable selection plays a key role in both improving the prediction accuracy and enhancing the interpretability of built models. Bayesian variable selection approaches enjoy many advantages such as high selection accuracy, easily incorporating many kinds of prior knowledge and so on. Because Bayesian methods generally make inference from the posterior distribution with Markov Chain Monte Carlo (MCMC) techniques, however, they become intractable in high-dimensional situations due to the large searching space. To address this issue, a novel variational Bayesian method for variable selection in high-dimensional logistic regression models is presented. The proposed method is based on the indicator model in which each covariate is equipped with a binary latent variable indicating whether it is important. The Bernoulli-type prior is adopted for the latent indicator variable. As for the specification of the hyperparameter in the Bernoulli prior, we provide two schemes to determine its optimal value so that the novel model can achieve sparsity adaptively. To identify important variables and make predictions, one efficient variational Bayesian approach is employed to make inference from the posterior distribution. The experiments conducted with both synthetic and some publicly available data show that the new method outperforms or is very competitive with some other popular counterparts. © 2018 Elsevier B.V."
,10.1080/17415977.2018.1505883,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052084541&doi=10.1080%2f17415977.2018.1505883&partnerID=40&md5=c936aec3b15b140b7e9dd70b4d5b8aea,"In this work, reconstruction and location in time domain of multiple forces acting on a linear elastic structure are achieved through a Bayesian approach to solve an inverse problem. The Bayesian solution of the inverse problem is provided in the form of a posterior probability density function. The unknown forces are determined through Markov chain Monte Carlo method, the Gibbs algorithm. This posterior density integrating both the likelihood and prior information was considered for the particular case of a linear elastic structure. The measurements are affected by an additive random noise. Two particular cases were analysed: unperturbed and uncertain model representing the structure. The unperturbed model was used to identify a single force. When the model is uncertain, compressed sensing technique was used to provide an adequate sparse representation of the inverse problem through a wavelet basis. With this sparse representation, the possibility of achieving automatic location of the forces was investigated. This requires to identify all the degrees of freedom along with the identified actions are not vanishing. Also, the possibility of force identification with less sensors than forces was studied. The proposed approach is illustrated and validated on numerical examples. This proposed approach is compared with classical approach of force identification based on Tikhonov regularization associated with the GCV criterion. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group."
,10.4149/gpb_2018005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052133720&doi=10.4149%2fgpb_2018005&partnerID=40&md5=3f99c4610b29b4411a4e23a4a48e7920,"One of commonly used approaches of biophysical modeling of muscle contractile apparatus is spatially explicit discrete lattice models in Monte Carlo simulation. Such models allow to reproduce structural features and actin-myosin interaction in the muscle contractile system more accurately. Limitation of such models is their low computational efficiency and stochasticity under certain circumstances. This work introduces deterministic approximation of stochastic model that considers a pair of rigid contractile filaments interaction. Approximation background is discreetness of spacing between cross-bridges and binding sites. Due to this property cross-bridges can be divided into discrete groups with the same strain, and considered statistically using the set of ordinary differential equations. Deterministic model is more computationally efficient, operates with average values. Within the given approach isotonic contraction was simulated. A comparison with Monte Carlo simulation demonstrates that approximation reproduces results for stochastic model with large number of cross-bridges. Also within the deterministic model a mechanism and essential conditions for oscillations appearance in isotonic transient response, relations of their parameters with geometrical ones of filaments lattice were examined, theoretical and experimental results were compared. The proposed approach can also be applied to approximation of continuous Huxley-based models solutions. Advantage over existing numerical methods is their greater numerical stability. © 2018 Slovak Academy of Sciences. All rights reserved."
,10.1002/minf.201700130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041219226&doi=10.1002%2fminf.201700130&partnerID=40&md5=ebee7117dbc3e1c20dc423de04ace2c3,"We consider lead discovery as active search in a space of labelled graphs. In particular, we extend our recent data-driven adaptive Markov chain approach, and evaluate it on a focused drug design problem, where we search for an antagonist of an αv integrin, the target protein that belongs to a group of Arg−Gly−Asp integrin receptors. This group of integrin receptors is thought to play a key role in idiopathic pulmonary fibrosis, a chronic lung disease of significant pharmaceutical interest. As an in silico proxy of the binding affinity, we use a molecular docking score to an experimentally determined αvβ6 protein structure. The search is driven by a probabilistic surrogate of the activity of all molecules from that space. As the process evolves and the algorithm observes the activity scores of the previously designed molecules, the hypothesis of the activity is refined. The algorithm is guaranteed to converge in probability to the best hypothesis from an a priori specified hypothesis space. In our empirical evaluations, the approach achieves a large structural variety of designed molecular structures for which the docking score is better than the desired threshold. Some novel molecules, suggested to be active by the surrogate model, provoke a significant interest from the perspective of medicinal chemistry and warrant prioritization for synthesis. Moreover, the approach discovered 19 out of the 24 active compounds which are known to be active from previous biological assays. © 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim"
,10.1029/2018WR023366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054557187&doi=10.1029%2f2018WR023366&partnerID=40&md5=80a328c87c1b8ae8ec7430c9e7330675,"Intensity-duration-frequency (IDF) curves are one of the most common rainfall statistical models used in hydrologic design and analysis projects. The uncertainties related to the elaboration of these IDF curves have nevertheless seldom been evaluated in the past. The article will recall the existing link between the IDF formulation and some properties of the rainfall series such as simple scaling and multifractal structure. Assuming that these properties are valid, the IDF curves formulation is then the product of a dimensionless (i.e., reduced) distribution function for the annual maximum rainfall intensities/depths and a duration-dependent scaling factor. Its parameters can be evaluated in an integrated way (i.e., based on a unique pooled sample of peak intensities over a range of durations: from 15 min to 24 hr). The use of likelihood-based Bayesian Markov chain Monte Carlo statistical inference methods for this evaluation provides consistent uncertainties for all the parameters of the IDF relation and for the corresponding rainfall quantiles. This methodology has been tested, via a local analysis, on a large data set of 48 rain gauge records, spread over the north central part of Algeria (25,000 km2), under various climatic regimes. The integrated approach is undoubtedly consistent with estimates from annual maximum rainfall fitted to single durations. Furthermore, credibility intervals are significantly reduced. Also, this integrated approach appears to be robust: Unlike the traditional method based single durations, it generally provides rational quantile estimates, even when short observed series are available. This is a significant advantage for engineering applications. ©2018. American Geophysical Union. All Rights Reserved."
,10.1137/16M1090466,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044504789&doi=10.1137%2f16M1090466&partnerID=40&md5=fd14023ca86684f83d93c276d8302fb1,"Approximate Bayesian Computation (ABC) methods have gained in popularity over the last decade because they expand the horizon of Bayesian parameter inference methods to the range of models for which an analytical formula for the likelihood function might be difficult, or even impossible, to establish. The majority of the ABC methods rely on the choice of a set of summary statistics to reduce the dimension of the data. However, as has been noted in the ABC literature, the lack of convergence guarantees induced by the absence of a vector of sufficient summary statistics that assures intermodel sufficiency over the set of competing models hinders the use of the usual ABC methods when applied to Bayesian model selection or assessment. In this paper, we present a novel ABC model selection procedure for dynamical systems based on a recently introduced multilevel Markov chain Monte Carlo method, self-regulating ABC-SubSim, and a hierarchical state-space formulation of dynamic models. We show that this formulation makes it possible to independently approximate the model evidence required for assessing the posterior probability of each of the competing models. We also show that ABC-SubSim not only provides an estimate of the model evidence as a simple by-product but also gives the posterior probability of each model as a function of the tolerance level, which allows the ABC model choices made in previous studies to be understood. We illustrate the performance of the proposed framework for ABC model updating and model class selection by applying it to two problems in Bayesian system identification: a single-degree-of-freedom bilinear hysteretic oscillator and a three-story shear building with Masing hysteresis, both of which are subject to a seismic excitation. © 2018 Society for Industrial and Applied Mathematics."
1,10.1214/17-AOS1629,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054717605&doi=10.1214%2f17-AOS1629&partnerID=40&md5=f318cbd702fc6960024d3371b8c6c852,"This paper introduces a new way to compact a continuous probability distribution F into a set of representative points called support points. These points are obtained by minimizing the energy distance, a statistical potential measure initially proposed by Székely and Rizzo [InterStat 5 (2004) 1–6] for testing goodness-of-fit. The energy distance has two appealing features. First, its distance-based structure allows us to exploit the duality between powers of the Euclidean distance and its Fourier transform for theoretical analysis. Using this duality, we show that support points converge in distribution to F, and enjoy an improved error rate to Monte Carlo for integrating a large class of functions. Second, the minimization of the energy distance can be formulated as a difference-of-convex program, which we manipulate using two algorithms to efficiently generate representative point sets. In simulation studies, support points provide improved integration performance to both Monte Carlo and a specific quasi-Monte Carlo method. Two important applications of support points are then highlighted: (a) as a way to quantify the propagation of uncertainty in expensive simulations and (b) as a method to optimally compact Markov chain Monte Carlo (MCMC) samples in Bayesian computation. © Institute of Mathematical Statistics, 2018."
,10.15866/iree.v13i3.14413,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052236962&doi=10.15866%2firee.v13i3.14413&partnerID=40&md5=d3a798f757c7ac065ba98fad2d0b5e75,"The agriculture and food-processing industries demand a lot of energy. The same can be applied to small agriculture farms isolated from the electric power grid. The use of electrical and heat energy makes a significant business cost in these economies. As a result, the decision on how the electrical and the heat energy can be provided for the agriculture farms isolated from the electric power grid in economical and reliable way is of a crucial importance for their further business and life quality. The electrical and the heat energy need has been investigated for the farms scattered through the Bosnia mountains sites that are away from electric power grid. Besides the energy needs, other aspects of energy solutions for the farms have been researched, such as those of the environmental, technical, and economical nature, and the reliability of the offered solutions. The analysis was carried out by using a hybrid algorithm based on the analytical hierarchy process and techno-economic analysis. © 2018 Praise Worthy Prize S.r.l. - All rights reserved."
,10.7717/peerj.5140,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049448400&doi=10.7717%2fpeerj.5140&partnerID=40&md5=77632243945056b98f46057ad7ed84a3,"Time-resolved phylogenetic methods use information about the time of sample collection to estimate the rate of evolution. Originally, the models used to estimate evolutionary rates were quite simple, assuming that all lineages evolve at the same rate, an assumption commonly known as the molecular clock. Richer and more complex models have since been introduced to capture the phenomenon of substitution rate variation among lineages. Two well known model extensions are the local clock, wherein all lineages in a clade share a common substitution rate, and the uncorrelated relaxed clock, wherein the substitution rate on each lineage is independent from other lineages while being constrained to fit some parametric distribution. We introduce a further model extension, called the flexible local clock (FLC), which provides a flexible framework to combine relaxed clock models with local clock models. We evaluate the flexible local clock on simulated and real datasets and show that it provides substantially improved fit to an influenza dataset. An implementation of the model is available for download from https://www.github.com/4ment/flc. © 2018 Fourment and Darling."
,10.1155/2018/1543034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045724326&doi=10.1155%2f2018%2f1543034&partnerID=40&md5=ca28fb7b31d6ec8df8c8cfd1c71d55b4,"Background. Treatment of schizophrenia with first- and second-generation antipsychotics has been associated with elevated prolactin levels, which may increase the risk for prolactin-related adverse events. Methods. Randomized controlled trials (RCTs) included in a recent systematic review were considered for this analysis. A Bayesian network meta-analysis was used to compare changes in prolactin levels in pediatric patients diagnosed with schizophrenia or schizophrenia spectrum disorders treated with second-generation antipsychotics (SGAs). Results. Five RCTs, including 989 patients combined, have evaluated the changes in prolactin for pediatric patients after 6 weeks of treatment with risperidone, quetiapine, aripiprazole, olanzapine, and paliperidone. In the overall study population, treatment with risperidone was associated with the highest increase in mean prolactin levels compared to other SGAs. Patients treated with risperidone 4-6 mg/day were found to experience the greatest increases (55.06 ng/ml [95% CrI: 40.53-69.58]) in prolactin levels, followed by risperidone 1-3 mg/day, paliperidone 3-6 mg/day, and paliperidone 6-12 mg/day. Conclusions. This study shows that there are differences in SGAs ability to cause hyperprolactinemia. Further, there is clear evidence of safety concerns with risperidone and paliperidone treatment in adolescent schizophrenia patients. Copyright © 2018 Chakrapani Balijepalli et al."
,10.7717/peerj.4723,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047517702&doi=10.7717%2fpeerj.4723&partnerID=40&md5=9d47593a8670c12f6808549887e93f25,"Colobanthus apetalus is a member of the genus Colobanthus, one of the 86 genera of the large family Caryophyllaceae which groups annual and perennial herbs (rarely shrubs) that are widely distributed around the globe, mainly in the Holarctic. The genus Colobanthus consists of 25 species, including Colobanthus quitensis, an extremophile plant native to the maritime Antarctic. Complete chloroplast (cp) genomes are useful for phylogenetic studies and species identification. In this study, next-generation sequencing (NGS) was used to identify the cp genome of C. apetalus. The complete cp genome of C. apetalus has the length of 151,228 bp, 36.65% GC content, and a quadripartite structure with a large single copy (LSC) of 83,380 bp and a small single copy (SSC) of 17,206 bp separated by inverted repeats (IRs) of 25,321 bp. The cp genome contains 131 genes, including 112 unique genes and 19 genes which are duplicated in the IRs. The group of 112 unique genes features 73 protein-coding genes, 30 tRNA genes, four rRNA genes and five conserved chloroplast open reading frames (ORFs). A total of 12 forward repeats, 10 palindromic repeats, five reverse repeats and three complementary repeats were detected. In addition, a simple sequence repeat (SSR) analysis revealed 41 (mono-, di-, tri-, tetra-, penta- and hexanucleotide) SSRs, most of which were AT-rich. A detailed comparison of C. apetalus and C. quitensis cp genomes revealed identical gene content and order. A phylogenetic tree was built based on the sequences of 76 protein-coding genes that are shared by the eleven sequenced representatives of Caryophyllaceae and C. apetalus, and it revealed that C. apetalus and C. quitensis form a clade that is closely related to Silene species and Agrostemma githago. Moreover, the genus Silene appeared as a polymorphic taxon. The results of this study expand our knowledge about the evolution and molecular biology of Caryophyllaceae. © 2018 Androsiuk et al."
1,10.1080/17513758.2017.1401677,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041770077&doi=10.1080%2f17513758.2017.1401677&partnerID=40&md5=7fc006c0e1ed7368a1143176044fa01f,"Erlang differential equation models of epidemic processes provide more realistic disease-class transition dynamics from susceptible (S) to exposed (E) to infectious (I) and removed (R) categories than the ubiquitous SEIR model. The latter is itself is at one end of the spectrum of Erlang SEmInR models with m ≥ 1 concatenated E compartments and n ≥ 1 concatenated I compartments. Discrete-time models, however, are computationally much simpler to simulate and fit to epidemic outbreak data than continuous-time differential equations, and are also much more readily extended to include demographic and other types of stochasticity. Here we formulate discrete-time deterministic analogs of the Erlang models, and their stochastic extension, based on a time-to-go distributional principle. Depending on which distributions are used (e.g. discretized Erlang, Gamma, Beta, or Uniform distributions), we demonstrate that our formulation represents both a discretization of Erlang epidemic models and generalizations thereof. We consider the challenges of fitting SEmInR models and our discrete-time analog to data (the recent outbreak of Ebola in Liberia). We demonstrate that the latter performs much better than the former; although confining fits to strict SEIR formulations reduces the numerical challenges, but sacrifices best-fit likelihood scores by at least 7%. © 2017 The Author(s)."
,10.11909/j.issn.1671-5411.2018.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049683076&doi=10.11909%2fj.issn.1671-5411.2018.05.001&partnerID=40&md5=a966f1cdcb27759c8c8303126d753e2e,[No abstract available]
,10.1002/asmb.2371,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050940648&doi=10.1002%2fasmb.2371&partnerID=40&md5=146baca24b384a39d46c42a6aaaae43b,"This paper studies how the lasting effects of common credit events influence default probability distribution and the prices of multiname credit derivatives. Based on a joint defaults model where common credit events are used to generate simultaneous defaults, we extend the model to allow for their impacts to last for a longer while. The default intensity of each entity is heightened significantly while the impact still has an influence, until some time later when this effect fades away. Incorporating these lasting effects helps to generate higher default correlation, which is more consistent with today's highly correlated financial markets. The proposed model can be either formulated as a Markov chain or implemented by Monte Carlo simulation in order to calculate the default probability distributions and multiname derivatives prices. Our numerical results demonstrate the strong influences from the lasting effects and provide a justification of their incorporation. © 2018 John Wiley & Sons, Ltd."
,10.1248/yakushi.17-00159,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040451128&doi=10.1248%2fyakushi.17-00159&partnerID=40&md5=dc378f5dac5c771f6d0adc0b70270dd6,"We evaluated four representative chemotherapy regimens for unresectable advanced or recurrent KRAS-wild type colorectal cancer: mFOLFOX6, mFOLFOX6+bevacizumab (Bmab), cetuximab (Cmab), or panitumumab (Pmab). We employed a decision analysis method in combination with clinical and economic evidence. The health outcomes of the regimens were analyzed on the basis of overall and progression-free survival. The data were drawn from the literature on randomized controlled clinical trials of the above-mentioned drugs. The total costs of the regimens were calculated on the basis of direct costs obtained from the medical records of patients diagnosed with unresectable advanced or recurrent colorectal cancer at Yamagata University Hospital and Yamagata Prefecture Central Hospital. Cost effectiveness was analyzed using a Markov chain Monte Carlo (MCMC) method. The study was designed from the viewpoint of public medical care. The MCMC analysis revealed that expected life months and expected cost were 20 months/3,527,119 yen for mFOLFOX6, 27 months/8,270,625 yen for mFOLFOX6+Bmab, 29 months/13,174,6297 yen for mFOLFOX6+Cmab, and 6 months/12,613,445 yen for mFOLFOX6+Pmab. Incremental costs per effectiveness ratios per life month against mFOLFOX6 were 637,592 yen for mFOLFOX6+Bmab, 1,075,162 yen for mFOLFOX6+Cmab, and 587,455 yen for mFOLFOX6+Pmab. Compared to the conventional mFOLFOX6 regimen, molecular-targeted drug regimens provide better health outcomes, but the cost increases accordingly. mFOLFOX 6+Pmab is the most cost-effective regimen among those surveyed in this study. © 2018 The Pharmaceutical Society of Japan."
2,10.1209/0295-5075/121/10008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044192494&doi=10.1209%2f0295-5075%2f121%2f10008&partnerID=40&md5=f0706526efb39ddd670e394b352317be,"We analyze the convergence of the irreversible event-chain Monte Carlo algorithm for continuous spin models in the presence of topological excitations. In the two-dimensional XY model, we show that the local nature of the Markov-chain dynamics leads to slow decay of vortexantivortex correlations while spin waves decorrelate very quickly. Using a Fréchet description of the maximum vortex-antivortex distance, we quantify the contributions of topological excitations to the equilibrium correlations, and show that they vary from a dynamical critical exponent z ∼ 2 at the critical temperature to z ∼ 0 in the limit of zero temperature. We confirm the event-chain algorithm's fast relaxation (corresponding to z = 0) of spin waves in the harmonic approximation to the XY model. Mixing times (describing the approach towards equilibrium from the least favorable initial state) however remain much larger than equilibrium correlation times at low temperatures. We also describe the respective influence of topological monopole-antimonopole excitations and of spin waves on the event-chain dynamics in the three-dimensional Heisenberg model. © EPLA, 2018."
,10.1051/0004-6361/201731450,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040469598&doi=10.1051%2f0004-6361%2f201731450&partnerID=40&md5=8e339e7104f905dd5d09a10590ab2def,"Aims. We present results from deep and very spatially extended CTIO/DECam g and r photometry (reaching out to ∼2 mag below the oldest main-sequence turn-off and covering ∼20 deg 2) around the Sextans dwarf spheroidal galaxy. We aim to use this dataset to study the structural properties of Sextans overall stellar population and its member stars in different evolutionary phases, as well as to search for possible signs of tidal disturbance from the Milky Way, which would indicate departure from dynamical equilibrium. Methods. We performed the most accurate and quantitative structural analysis to-date of Sextans' stellar components by applying Bayesian Monte Carlo Markov chain methods to the individual stars' positions. Surface density maps are built by statistically decontaminating the sample through a matched filter analysis of the colour-magnitude diagram, and then analysed for departures from axisymmetry. Results. Sextans is found to be significantly less spatially extended and more centrally concentrated than early studies suggested. No statistically significant distortions or signs of tidal disturbances were found down to a surface brightness limit of ∼31.8 mag/arcsec 2 in V-band. We identify an overdensity in the central regions that may correspond to previously reported kinematic substructure(s). In agreement with previous findings, old and metal-poor stars such as Blue Horizontal Branch stars cover a much larger area than stars in other evolutionary phases, and bright Blue Stragglers (BSs) are less spatially extended than faint ones. However, the different spatial distribution of bright and faint BSs appears consistent with the general age and metallicity gradients found in Sextans' stellar component. This is compatible with Sextans BSs having formed by evolution of binaries and not necessarily due to the presence of a central disrupted globular cluster, as suggested in the literature. We provide structural parameters for the various populations analysed and make publicly available the photometric catalogue of point-sources as well as a catalogue of literature spectroscopic measurements with updated membership probabilities. © ESO, 2018."
,10.1007/s00542-018-4070-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050931118&doi=10.1007%2fs00542-018-4070-4&partnerID=40&md5=16d8183b6f7fe2566a8d1709608ca752,"The flexible interconnection controller (FIC) is a type of power electronic device based on Insulated Gate Bipolar Transistors (IGBT) modules, which is applied to electrical distribution systems to realize the flexible control of power flow. The reliability of this device plays an important role in the reliable and continuous operation of the active distribution system. In this paper, a reliability model of FIC which consists of three modular multilevel converters (MMCs) is proposed, considering the uncertainty of current loading. First, the structure and operating modes of FIC are analyzed. Based on the fault tree analysis, the reliability model of single-terminal MMC is established. Next, considering the impact of random current loading on the IGBT modules’ reliability, Monte Carlo simulation is used to obtain the loading expectation correction factor, thus the equivalent reliability model of IGBT module is built. Furthermore, the state space model of FIC is established, and the analytical method based on Markov chain is used to solve it. Taking the improved three IEEE33 node feeder groups as the testing system, the equivalent reliability indices of IGBT module as well as the converter reliability of the three terminals of MMC are obtained via simulation, and based on which, the probabilities and average durations of FIC’s eight states are calculated to verify the proposed model. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
,10.4310/SII.2018.v11.n4.a9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054681514&doi=10.4310%2fSII.2018.v11.n4.a9&partnerID=40&md5=247af221c9c444f00158e20e5e2c1736,"We propose two new methods for sampling undirected, loopless multigraphs with fixed degree. The first is a sequential importance sampling method, with the proposal based on an asymptotic approximation to the total number of multigraphs with fixed degree. The multigraphs and their associated importance weights can be used to approximate the null distribution of test statistics and additionally estimate the total number of multigraphs. The second is a Markov chain Monte Carlo method that samples multigraphs based on similar moves used to sample contingency tables with fixed margins.We apply both methods to a number of examples and demonstrate excellent performance. © 2018, International Press of Boston, Inc."
,10.1214/18-BA1101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054632269&doi=10.1214%2f18-BA1101&partnerID=40&md5=996a89cd9a749efb16e76106c6231b8c,"A Markov equivalence class contains all the Directed Acyclic Graphs (DAGs) encoding the same conditional independencies, and is represented by a Completed Partially Directed Acyclic Graph (CPDAG), also named Essential Graph (EG).We approach the problem of model selection among noncausal sparse Gaussian DAGs by directly scoring EGs, using an objective Bayes method. Specifically, we construct objective priors for model selection based on the Fractional Bayes Factor, leading to a closed form expression for the marginal likelihood of an EG. Next we propose a Markov Chain Monte Carlo (MCMC) strategy to explore the space of EGs using sparsity constraints, and illustrate the performance of our method on simulation studies, as well as on a real dataset. Our method provides a coherent quantification of inferential uncertainty, requires minimal prior specification, and shows to be competitive in learning the structure of the data-generating EG when compared to alternative state-of-the-art algorithms. © 2018 International Society for Bayesian Analysis."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049773915&partnerID=40&md5=65f93a9100b101a9a89a14158790273e,"In this study, we examined the variation of eight autosomal microsatellite markers, and explored the genetic diversity and the measure of population structuring of Eurasian Woodcock (Scolopax rusticola) in Hungary in spring. We tested whether any subpopulations can be identified in our sample, and we also examined whether the individuals that occurred closer to each other in space and in timewere alsomore similar in genetic terms. We analysed samples from 240 free-ranging birds collected during legal hunting from mid-February to the end of April in 2015 from different parts of the country. The Bayesian clustering method and Markov Chain Monte Carlo simulation were used to infer the most probable number of genetic clusters without a priori definition of populations. The second approach was a discriminant analysis of principal components in order to identify clusters of individuals without population geneticmodels. Additionally, we fitted a general linear model with the genetic distance between samples as the dependent variable, the temporal (days) and the geographical (meters) distances and the interactions of these factors as independent variables. We found high genetic diversity and lowlevel of population structuring in our samples. Moreover, our results did not support the assumptions that Woodcocks occurring in different places or at different times in Hungary would also belong to different breeding populations. © 2018 University of Helsinki. All rights reserved."
1,10.7717/peerj.4333,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041327577&doi=10.7717%2fpeerj.4333&partnerID=40&md5=bcaff929f373280f696393dc639a046d,"Background. During the winter of 2014-2015, a rarely reported norovirus (NoV) genotype GII.17 was found to have increased its frequency in norovirus outbreaks in East Asia, surpassing the GII.4 NoV infections. GII.17 genotype has been detected for over three decades in the world. The aim of this study is to examine the evolutionary dynamics of GII.17 over the last four decades. Methods. NoV GII.17 sequences with complete or nearly complete VP1 were down- loaded from GenBank and the phylogenetic analyses were then conducted. Results. The maximum likelihood analysis showed that GII.17 genotype could be divided into four different clades (Clades A-D). The strains detected after 2012, which could be the cause of the outbreaks, were separated into Clades C-D with their mean amino acid distance being 4.5%. Bayesian Markov chain Monte Carlo analyses indicated that the rate of nucleotide substitution per sites was 1.68×10-3 nucleotide substitutions/site/year and the time of themost recent common ancestor was 1840. The P2 subdomain of GII.17 was highly variable with 44% (56/128) amino acids variations including two insertions at positions 295-296 and one deletion at position 385 (Clades C and D) and one insertion at position 375 (Clade D). Variations existed in Epitopes A, B and D corresponding to GII.4 and human histo-blood group antigens binding site I in P2 subdomain. Conclusion. The novelGII.17 strains that caused outbreaks in 2013-2015may have two new variants. The evolvement of HBGAs binding site and epitopes in P2 subdomain might contribute to the novel GII.17 strains predominance in some regions. © 2018 Sang and Yang."
,10.1016/j.bpj.2018.09.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054088301&doi=10.1016%2fj.bpj.2018.09.005&partnerID=40&md5=51ce7c7840fcb8152d366f696b2af3b5,"State-of-the-art single-particle tracking (SPT) techniques can generate long trajectories with high temporal and spatial resolution. This offers the possibility of mechanistically interpreting particle movements and behavior in membranes. To this end, a number of statistical techniques have been developed that partition SPT trajectories into states with distinct diffusion signatures, allowing a statistical analysis of diffusion state dynamics and switching behavior. Here, we develop a confinement model, within a hidden Markov framework, that switches between phases of free diffusion and confinement in a harmonic potential well. By using a Markov chain Monte Carlo algorithm to fit this model, automated partitioning of individual SPT trajectories into these two phases is achieved, which allows us to analyze confinement events. We demonstrate the utility of this algorithm on a previously published interferometric scattering microscopy data set, in which gold-nanoparticle-tagged ganglioside GM1 lipids were tracked in model membranes. We performed a comprehensive analysis of confinement events, demonstrating that there is heterogeneity in the lifetime, shape, and size of events, with confinement size and shape being highly conserved within trajectories. Our observations suggest that heterogeneity in confinement events is caused by both individual nanoparticle characteristics and the binding-site environment. The individual nanoparticle heterogeneity ultimately limits the ability of interferometric scattering microscopy to resolve molecule dynamics to the order of the tag size; homogeneous tags could potentially allow the resolution to be taken below this limit by deconvolution methods. In a wider context, the presented harmonic potential well confinement model has the potential to detect and characterize a wide variety of biological phenomena, such as hop diffusion, receptor clustering, and lipid rafts. © 2018 Biophysical Society"
,10.1002/jmv.25296,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053736791&doi=10.1002%2fjmv.25296&partnerID=40&md5=f5aee34c8ad0176550358b01d6573c63,"Hepatitis E virus (HEV) infection in Bulgaria is endemic, as demonstrated by the seroprevalence of antibody against the virus in the general population and by the high prevalence of clinical cases registered. In this study, a deep Bayesian phylogenetic analysis has been performed to provide information on the genetic diversity and the spread of HEV genotypes in Bulgaria. Three different data sets of HEV virus was built for genotyping by the maximum likelihood method, for evolutionary rate estimated by Bayesian Markov Chain Monte Carlo approach, for demographic history investigation and for selective pressure analysis. The evolutionary rate for genotype 3e, was 351 × 10−3 substitution/site/year (95% highest posterior density [95% HPD]: 145 × 10 −3-575 × 10 −3). The root of the time to the most recent common ancestor of the Bayesian maximum clade credibility tree of HEV 3e genotype corresponded to 1965 (HPD 95% 1949-1994). The Bulgarian sequences mainly clustered in the main clade (clade A). The monophyletic clade included all Bulgarian genotype 3e sequences. The demographic history showed a slight growth from 1995 to 2000, followed by a sort of bottleneck in 2010s, a peak in 2011 and a new growth to 2015. Selection pressure analysis did not show sites under positive pressure but 64 statistically significant sites under negative selection. Molecular epidemiological surveillance by Bayesian phylogeny of HEV virus can contribute to trace the way of human infection after contact with swine source directly or heating meat improving public health control. © 2018 Wiley Periodicals, Inc."
1,10.1371/journal.pone.0202545,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053669712&doi=10.1371%2fjournal.pone.0202545&partnerID=40&md5=a985720f0a171ed40de47c04e29d4b0c,"An integrated model assessing the status and productivity of Antarctic krill (Euphausia superba, hereafter krill) was configured to estimate different subsets of 118 potentially estimable parameters in alternative configurations. We fixed the parameters that were not estimated in any given configuration at pre-specified values. The model was fitted to over forty years of fisheries and survey data for krill in Subarea 48.1, a statistical reporting area around the Antarctic Peninsula used by the Commission for the Conservation of Antarctic Marine Living Resources (CCAMLR). The number of estimated parameters was gradually increased across model configurations. Configurations that estimated more parameters fitted the data better, but the order in which the parameters were estimated became more important in finding the best fit. Twenty-two configurations estimating from 48 to 107 parameters were able to obtain an invertible Hessian matrix that was subsequently used to estimate parameter uncertainty. Parameter uncertainties calculated using asymptotic approximation around the maximum likelihood estimates were often larger than uncertainties based on Markov chain Monte Carlo sampling for the same parameters. Diagnostics applied to MCMC samples in the best model of each configuration that obtained an invertible Hessian indicated that the most highly parameterized configurations did not reach stationary distributions. A 96-parameter configuration was the best fitting model of those that passed the MCMC diagnostics. The ?AIC and ?BIC scores indicated essentially no support relative to the best model for the alternative models that also passed MCMC diagnostics. Simulated data using the configurations as operating models showed that while all configurations passed ""self-tests"" for spawning biomass and recruitment, there was a small negative bias due to model penalties in the fishing mortality estimates for years with the highest fishing mortalities. ""Cross-tests"" of configurations that estimated different parameters often differed from the operating model values. © 2018 Public Library of Science. All rights reserved."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050612203&partnerID=40&md5=875360e137ab58f10d12eac7dc867ef8,"This paper proposes a novel methodology for evaluating the travel time reliability of an urban road network using latest advances in the evolutionary game theory and MCMC (Markov chain Monte Carlo). Our proposed method has the following notable features: (i) it obtains a stationary distribution of traffic flow patterns which stems from a stochastic day-to-day dynamics of a population with rational users; (ii) this distribution is consistent with the traditional SUE (stochastic user equilibrium) in terms of traffic flow; and (iii) it can estimate the 95 percentile of travel time of each OD pair for a practical size network. In this paper, we provide a numerical example using Sioux-Falls network. © 2018 Hong Kong Society for Transportation Studies Limited. All rights reserved."
,10.1007/s11235-018-0491-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050196954&doi=10.1007%2fs11235-018-0491-8&partnerID=40&md5=94ec1ce4e4a4d0c656b2326bd7b5878d,"Energy harvesting (EH) body sensor nodes (BSNs) operate independently in the system and are the emerging solution to multiple replacements of battery operated BSNs. After deployment, the stored energy of the BSN falls to a minimum level due to uncertain energy harvesting process. Therefore, the node is unable to transmit the occurred events to the base station and stores them in storage buffer (SB) in a queue. Due to the queue overflow in SB, the BSN is unable to store the occurred events, therefore it is lost. In health monitoring system, loss of emergency or critical information has a bad impact on quality of service in the network. It is essential to have an estimate of the duration to occur an event loss in order to take precautions and prior control on nodes in critical situations for medical applications. We calculate the duration after which event loss occurs in SB by absorbing discrete-time Markov chain (DTMC) model to evaluate performance of the EH BSN with temporal death. We also derive a closed form expression of event loss duration which reduces the computational complexity of the conventional DTMC model. The analytical results are validated by Monte Carlo simulation using MATLAB. © 2018, Springer Science+Business Media, LLC, part of Springer Nature."
,10.29220/CSAM.2018.25.1.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044024184&doi=10.29220%2fCSAM.2018.25.1.029&partnerID=40&md5=c2ba93b2d8640f7daff535924043893f,"We combine the integer-valued GARCH(1, 1) model with a generalized regime-switching model to propose a dynamic count time series model. Our model adopts Markov-chains with time-varying dependent transition probabilities to model dynamic count time series called the generalized regime-switching integer-valued GARCH(1, 1) (GRS-INGARCH(1, 1)) models. We derive a recursive formula of the conditional probability of the regime in the Markov-chain given the past information, in terms of transition probabilities of the Markovchain and the Poisson parameters of the INGARCH(1, 1) process. In addition, we also study the forecasting of the Poisson parameter as well as the cumulative impulse response function of the model, which is a measure for the persistence of volatility. A Monte-Carlo simulation is conducted to see the performances of volatility forecasting and behaviors of cumulative impulse response coefficients as well as conditional maximum likelihood estimation; consequently, a real data application is given. © 2018 The Korean Statistical Society, and Korean International Statistical Society."
,10.1016/j.csda.2017.08.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029393305&doi=10.1016%2fj.csda.2017.08.005&partnerID=40&md5=c223e7223a6e6cad6dc26c75e0fbbdd8,"An efficient and flexible Bayesian approach is proposed for a dual-semiparametric regression model that models mean function semiparametrically and estimates the distribution of the error term nonparametrically. Using a weighted Dirichlet process mixture (WDPM), a Bayesian approach has been developed on the assumption that the distributions of the response variables are unknown. The WDPM approach is especially useful for real applications that have heterogeneous error distributions or come from a mixture of distributions. In the mean function, the unknown functions are estimated using natural cubic smoothing splines. For the error terms, several different WDPMs are proposed using different weights that depend on the distances between the covariates. Their marginal likelihoods are derived, and the computation of marginal likelihood for WDPM is provided. Efficient Markov chain Monte Carlo (MCMC) algorithms are also provided. The Bayesian approaches based on different WDPMs are compared with the parametric error model and the Dirichlet process mixture (DPM) error model in terms of the Bayes factor using a simulation study, suggesting better performance of the Bayesian approach based on WDPM. The advantage of the proposed Bayesian approach is also demonstrated using the credit rating data. © 2017 Elsevier B.V."
1,10.1137/17M1110535,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045069326&doi=10.1137%2f17M1110535&partnerID=40&md5=8ac9152414d41851e887c2b77147988c,"In the paper, we present a strategy for accelerating posterior inference for unknown inputs in time fractional diffusion models. In many inference problems, the posterior may be concentrated in a small portion of the entire prior support. It will be much more efficient if we build and simulate a surrogate only over the significant region of the posterior. To this end, we construct a coarse model using the generalized multiscale finite element method (GMsFEM) and solve a least-squares problem for the coarse model with a regularizing Levenberg-Marquart algorithm. An intermediate distribution is built based on the approximate sampling distribution. For Bayesian inference, we use GMsFEM and the least-squares stochastic collocation method to obtain a reduced coarse model based on the intermediate distribution. To increase the sampling speed of Markov chain Monte Carlo, the DREAMZS algorithm is used to explore the surrogate posterior density, which is based on the surrogate likelihood and the intermediate distribution. The proposed method with lower generalized polynomial chaos order gives the approximate posterior as accurately as the surrogate model directly based on the original prior. A few numerical examples for time fractional diffusion equations are carried out to demonstrate the performance of the proposed method with applications of the Bayesian inversion. © 2018 Society for Industrial and Applied Mathematics."
,10.1061/(ASCE)HE.1943-5584.0001583,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032219421&doi=10.1061%2f%28ASCE%29HE.1943-5584.0001583&partnerID=40&md5=a4fd0de53670456967e1d7ddabbe3c28,"Variations in precipitation extremes over the relatively small spatial scales of urban areas could be significantly different from those over larger regions. An understanding of such variations is critical for urban infrastructure design and operation. Urban climatology and sparse spatial data lead to uncertainties in the estimates of spatial precipitation. In this study, a Bayesian hierarchical model is used to obtain the spatial distribution of return levels of precipitation extremes in urban areas and quantify the associated uncertainty. The generalized extreme value (GEV) distribution is used for modeling precipitation extremes. A spatial component is introduced in the parameters of the GEV through a latent spatial process by considering geographic and climatologic covariates. A Markov-chain Monte Carlo algorithm is used for sampling the parameters of the GEV distribution and latent-process model. Applicability of the methodology is demonstrated with data from 19 telemetric rain-gauge stations in Bangalore city, India. For this case study, it is inferred that the elevation and mean monsoon precipitation are the predominant covariates for annual maximum precipitation. Variation of seasonal extremes are also examined in the study. For the monsoon maximum precipitation, it is observed that the geographic covariates dominate, whereas for the summer maximum precipitation, elevation and mean summer precipitation are the predominant covariates. A significant variation in spatial return levels of extreme precipitation is observed over the city. © 2017 American Society of Civil Engineers."
,10.1007/s00477-018-1617-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055260777&doi=10.1007%2fs00477-018-1617-y&partnerID=40&md5=884a9cd140dfa7038bcacecc13ef37bd,"With the rapid growth of nanotechnology industry, nanomaterials as an emerging pollutant are gradually released into subsurface environments and become great concerns. Simulating the transport of nanomaterials in groundwater is an important approach to investigate and predict the impact of nanomaterials on subsurface environments. Currently, a number of transport models are used to simulate this process, and the outputs of these models could be inconsistent with each other due to conceptual model uncertainty. However, the performances of different models on simulating nanoparticles transport in groundwater are rarely assessed in Bayesian framework in previous researches, and these will be the primary objective of this study. A porous media column experiment is conducted to observe the transport of Titanium Dioxide Nanoparticles (nano-TiO2). Ten typical transport models which consider different chemical reaction processes are used to simulate the transport of nano-TiO2, and the observed nano-TiO2 breakthrough curves data are used to calibrate these models. For each transport model, the parameter uncertainty is evaluated using Markov Chain Monte Carlo, and the DREAM(ZS) algorithm is used to sample parameter probability space. Moreover, the Bayesian model averaging (BMA) method is used to incorporate the conceptual model uncertainty arising from different chemical reaction based transport models. The results indicate that both two-sites and nonequilibrium sorption models can well reproduce the retention of nano-TiO2 transport in porous media. The linear equilibrium sorption isotherm, first-order degradation, and mobile-immobile models fail to describe the nano-TiO2 retention and transport. The BMA method could instead provide more reliable estimations of the predictive uncertainty compared to that using a single model. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature."
,10.1155/2018/3187807,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049321653&doi=10.1155%2f2018%2f3187807&partnerID=40&md5=16be4eb47666a809516a27e2938f3d24,"Media coverage reduces the transmission rate from infective to susceptible individuals and is reflected by suitable nonlinear functions in mathematical modeling of the disease. We here focus on estimating the parameters in the transmission rate based on a stochastic SIR epidemic model with media coverage. In order to reduce the computational load, the Newton-Raphson algorithm and Markov Chain Monte Carlo (MCMC) technique are incorporated with maximum likelihood estimation. Simulations validate our estimation results and the necessity of a model with media coverage when modeling the contagious diseases. © 2018 Changguo Li et al."
,10.1016/B978-0-444-64241-7.50252-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050611520&doi=10.1016%2fB978-0-444-64241-7.50252-4&partnerID=40&md5=5c1b3b45c3717059d25bee8463fe6c57,"This contribution employs a simulation-based approach to assessing the accuracy and computational efficiency of different strategies for estimation of probability distributions, applied to ODE/DAE systems. Specifically, two Bayesian Markov-chain Monte Carlo approaches are compared to a fast, new PDF estimation strategy, based on projection techniques. The two case studies analyzed confirm that this new PDF estimation strategy offers a very good trade-off between accuracy and computational efficiency, thus being excellent for time-critical PDF estimation tasks. © 2018 Elsevier B.V."
,10.1155/2018/8134132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053049325&doi=10.1155%2f2018%2f8134132&partnerID=40&md5=f86f60515f66244f99eae9aaeed18d6d,"We consider a Bayesian approach for assessing hypotheses of equivalence in two-arm trials with binary Data. We discuss the development of likelihood, the prior, and the posterior distributions of parameters of interest. We then examine the suitability of a normal approximation to the posterior distribution obtained via a Taylor series expansion. The Bayesian inference is carried out using Markov Chain Monte Carlo (MCMC) methods. We illustrate the methods using actual data arising from two-arm clinical trials on preventing mortality after myocardial infarction. © 2018 Cynthia Kpekpena and Saman Muthukumarana."
,10.1285/i20705948v11n2p655,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055165370&doi=10.1285%2fi20705948v11n2p655&partnerID=40&md5=cb5d2c559d3f5a83b6409bfca1a83043,"Under a context of survival lifetime analysis, we introduce in this paper Bayesian and maximum likelihood approaches for the bivariate Basu-Dhar geometric model in the presence of covariates and a cure fraction. This distribution is useful to model bivariate discrete lifetime data. In the Bayesian estimation, posterior summaries of interest were obtained using standard Markov Chain Monte Carlo methods in the OpenBUGS software. Maximum likelihood estimates for the parameters of interest were computed using the ""maxLik"" package of the R software. Illustrations of the proposed approaches are given for two real data sets. © 2018, Università del Salento."
1,10.4310/SII.2018.v11.n2.a4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043349294&doi=10.4310%2fSII.2018.v11.n2.a4&partnerID=40&md5=b3611d138bee2d09dd44f272dbf41e17,"The multivariate-t nonlinear mixed-effects model (MtNLMM) has been shown to be a promising robust tool for analyzing multiple longitudinal trajectories following arbitrary growth patterns in the presence of outliers and possible missing responses. Owing to intractable likelihood function of the model, we devise a fully Bayesian estimating procedure to account for the uncertainties of model parameters, random effects, and missing responses via the Markov chain Monte Carlo method. Posterior predictive inferences for the future values and missing responses are also investigated. We conduct a simulation study to demonstrate the feasibility of our Bayesian sampling schemes. The proposed techniques are illustrated through applications to two case studies. © International Press of Boston, Inc."
,10.1007/978-3-319-99465-9_4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053832988&doi=10.1007%2f978-3-319-99465-9_4&partnerID=40&md5=8ecdf12f4c1f6df698d4afb4eeccff4e,"This chapter introduces classical frequentist and Bayesian inference applied to analyzing diffraction profiles, and the methods are compared and contrasted. The methods are applied to both the modelling of single diffraction profiles and the full profile refinement of crystallographic structures. In the Bayesian method, Markov chain Monte Carlo algorithms are used to sample the distribution of model parameters, allowing for the construction of posterior probability distributions, which provide both parameter estimates and quantifiable uncertainties. We present the application of this method to single peak fitting in lead zirconate titanate, and the crystal structure refinement of a National Institute of Standards and Technology silicon standard reference material. © 2018, Springer Nature Switzerland AG."
,10.17713/ajs.v47i1.578,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041334692&doi=10.17713%2fajs.v47i1.578&partnerID=40&md5=bcbca2656d726f1e5ffab03bb2b2d56f,"This paper deals with the estimation procedure for inverse Weibull distribution under progressive type-II censored samples when removals follow Beta-binomial probability law. To estimate the unknown parameters, the maximum likelihood and Bayes estimators are obtained under progressive censoring scheme mentioned above. Bayes estimates are obtained using Markov chain Monte Carlo (MCMC) technique considering square error loss function and compared with the corresponding MLE’s. Further, the expected total time on test is obtained under considered censoring scheme. Finally, a real data set has been analysed to check the validity of the study. © 2018, Austrian Statistical Society. All rights reserved."
,10.1515/snde-2017-0062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046625671&doi=10.1515%2fsnde-2017-0062&partnerID=40&md5=67ce91b23ccd917890b3abd0448ea7c8,"In this paper, we propose and study an effective Bayesian subset selection method for two-threshold variable autoregressive (TTV-AR) models. The usual complexity of model selection is increased by capturing the uncertainty of the two unknown threshold levels and the two unknown delay lags. By using Markov chain Monte Carlo (MCMC) techniques with driven by a stochastic search, we can identify the best subset model from a large number of possible choices. Simulation experiments show that the proposed method works very well. As applied to the application to the Hang Seng index, we successfully distinguish the best subset TTV-AR model. © 2018 Walter de Gruyter GmbH, Berlin/Boston."
1,10.1214/16-BA1043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039871540&doi=10.1214%2f16-BA1043&partnerID=40&md5=a30f8afd9883d2e693ba0bb01e94a52b,"Bayesian item response models have been used in modeling educational testing and Internet ratings data. Typically, the statistical analysis is carried out using Markov Chain Monte Carlo methods. However, these may not be computationally feasible when real-time data continuously arrive and online parameter estimation is needed. We develop an efficient algorithm based on a deterministic moment-matching method to adjust the parameters in real-time. The proposed online algorithm works well for two real datasets, achieving good accuracy but with considerably less computational time. © 2018 International Society for Bayesian Analysis."
1,10.1214/17-BA1083,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054697234&doi=10.1214%2f17-BA1083&partnerID=40&md5=89b0f3d63808f121f1cc1dff7bb4f3c4,"We introduce the normal-inverse-gamma summation operator, which combines Bayesian regression results from different data sources and leads to a simple split-and-merge algorithm for big data regressions. The summation operator is also useful for computing the marginal likelihood and facilitates Bayesian model selection methods, including Bayesian LASSO, stochastic search variable selection, Markov chain Monte Carlo model composition, etc. Observations are scanned in one pass and then the sampler iteratively combines normal-inversegamma distributions without reloading the data. Simulation studies demonstrate that our algorithms can efficiently handle highly correlated big data. A real-world data set on employment and wage is also analyzed. © 2018 International Society for Bayesian Analysis."
,10.1029/2018JB016079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054831528&doi=10.1029%2f2018JB016079&partnerID=40&md5=208188419077a9bad1a7e28ee25e5cd5,"When different geophysical observables are sensitive to the same volume, it is possible to invert them simultaneously to jointly constrain different physical properties. The question addressed in this study is to determine which structures (e.g., interfaces) are common to different properties and which ones are separated. We present an algorithm for resolving the level of spatial coupling between physical properties and to enable both common and separate structures in the same model. The new approach, called structure decoupling (SD) algorithm, is based on a Bayesian trans-dimensional adaptive parameterization, where models can display the full spectra of spatial coupling between physical properties, from fully coupled models, that is, where identical model geometries are imposed across all inverted properties, to completely decoupled models, where an independent parameterization is used for each property. We apply the algorithm to three 1-D geophysical inverse problems, using both synthetic and field data. For the synthetic cases, we compare the SD algorithm to standard Markov chain Monte Carlo and reversible-jump Markov chain Monte Carlo approaches that use either fully coupled or fully decoupled parameterizations. In case of coupled structures, the SD algorithm does not behave differently from methods that assume common interfaces. In case of decoupled structures, the SD approach is demonstrated to correctly retrieve the portion of profiles where the physical properties do not share the same structure. The application of the new algorithm to field data demonstrates its ability to decouple structures where a common stratification is not supported by the data. ©2018. The Authors."
,10.1137/17M1153157,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054970164&doi=10.1137%2f17M1153157&partnerID=40&md5=926a27c08850fa8a70138de4db7ca27b,"Introducing inequality constraints in Gaussian processes can lead to more realistic uncertainties in learning a great variety of real-world problems. We consider the finite-dimensional Gaussian model from Maatouk and Bay [Math. Geosci., 49 (2017), pp. 557-582] which can satisfy inequality conditions everywhere (either boundedness, monotonicity, or convexity). Our contributions are threefold. First, we extend their approach in order to deal with sets of linear inequalities. Second, we explore different Markov chain Monte Carlo (MCMC) methods to approximate the posterior distribution. Third, we investigate theoretical and numerical properties of a constrained likelihood for covariance parameter estimation. According to experiments on both artificial and real data, our framework together with a Hamiltonian Monte Carlo sampler provides efficient results on both data fitting and uncertainty quantification. Copyright © by SIAM and ASA."
,10.1137/17M1137218,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054967728&doi=10.1137%2f17M1137218&partnerID=40&md5=051a96953d3488c102618c22df9351bc,"In Bayesian inverse problems, the posterior distribution is used to quantify uncertainty about the reconstructed solution. In fully Bayesian approaches in which prior parameters are assigned hyperpriors, Markov chain Monte Carlo algorithms often are used to draw samples from the posterior distribution. However, implementations of such algorithms can be computationally expensive. We present a computationally efficient scheme for sampling high-dimensional Gaussian distributions in ill-posed Bayesian linear inverse problems. Our approach uses Metropolis-Hastings independence sampling with a proposal distribution based on a low-rank approximation of the prior-preconditioned Hessian. We show the dependence of the acceptance rate on the number of eigenvalues retained and discuss conditions under which the acceptance rate is high. We demonstrate our proposed sampler by using it with Metropolis-Hastings-within-Gibbs sampling in numerical experiments in image deblurring, computerized tomography, and NMR relaxometry. Copyright © by SIAM and ASA."
,10.1007/978-3-030-00129-2_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053934038&doi=10.1007%2f978-3-030-00129-2_15&partnerID=40&md5=62954dba3da849b287a3db8af7d2c787,"Accurate noise modelling is important for training of deep learning reconstruction algorithms. While noise models are well known for traditional imaging techniques, the noise distribution of a novel sensor may be difficult to determine a priori. Therefore, we propose learning arbitrary noise distributions. To do so, this paper proposes a fully connected neural network model to map samples from a uniform distribution to samples of any explicitly known probability density function. During the training, the Jensen-Shannon divergence between the distribution of the model’s output and the target distribution is minimized. We experimentally demonstrate that our model converges towards the desired state. It provides an alternative to existing sampling methods such as inversion sampling, rejection sampling, Gaussian mixture models and Markov-Chain-Monte-Carlo. Our model has high sampling efficiency and is easily applied to any probability distribution, without the need of further analytical or numerical calculations. © 2018, Springer Nature Switzerland AG."
,10.1504/IJKBD.2018.094896,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054136953&doi=10.1504%2fIJKBD.2018.094896&partnerID=40&md5=f23d46241d852e337797f4cfc1328265,"A computer-generated 3D model illustrates the advantages of virtual in silico techniques. Derived from data for SMEs in service industries it enables a business owner (or consultant) to identify where any organisation is on a three-dimensional landscape and draw quantitative conclusions about fruitful future directions of travel plus how high the resulting benefits will be and what costs are due along the journey. This ‘ready-to-go’ landscape map is of immense value for academics and practitioners alike, and is easily-applicable. Anyone can create the three-dimensional fold and discuss the implications of growth and development with specific clients. Markov Chain Monte Carlo modelling is presented which, put simply, is throwing virtual balls down the basic fold to show how to predict outcomes of Knowledge Engineering projects. Results are shown for; adding multiskilled innovators, adding network input from external environments, costing management control effectively and explaining how IPR adds extraneous value. Copyright © 2018 Inderscience Enterprises Ltd."
,10.1002/sim.7916,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052374881&doi=10.1002%2fsim.7916&partnerID=40&md5=6ed32f815e552cc87715e45d79ee5dff,"In this paper, we propose a semiparametric failure time model to analyze multivariate censored data with latent variables. The proposed model generalizes the conventional accelerated failure time model to accommodate latent risk factors that could be measured by multiple observed variables through a factor analysis and to incorporate additive nonparametric functions of observed and latent risk factors to examine their functional effects on multivariate failure times of interest. A Bayesian approach, along with Bayesian P-splines and Markov chain Monte Carlo techniques, is developed to estimate the unknown parameters and functions. The empirical performance of the proposed methodology is evaluated by a simulation study. An application to a study on the risk factors of two diabetes complications is presented. © 2018 John Wiley & Sons, Ltd."
,10.1027/1614-2241/a000147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049162218&doi=10.1027%2f1614-2241%2fa000147&partnerID=40&md5=0536840c0211196efd73fb29a1a2f933,"Circular data is different from linear data and its analysis also requires methods different from conventional methods. In this study a Bayesian embedding approach to estimating circular regression models is investigated, by means of simulation studies, in terms of performance, efficiency, and flexibility. A new Markov chain Monte Carlo (MCMC) sampling method is proposed and contrasted to an existing method. An empirical example of a regression model predicting teachers' scores on the interpersonal circumplex will be used throughout. Performance and efficiency are better for the newly proposed sampler and reasonable to good in most situations. Furthermore, the method in general is deemed very flexible. Additional research should be done that provides an overview of what circular data looks like in practice, investigates the interpretation of the circular effects and examines how we might conduct a way of hypothesis testing or model checking for the embedding approach. © 2018 Hogrefe Publishing."
,10.1007/s10763-018-9917-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050752738&doi=10.1007%2fs10763-018-9917-8&partnerID=40&md5=7c99a076da26bb6ef76e580644770605,"This study critically examines if digital divides, comprising access to and use of information technology (IT) in two spheres (schools and at home), affect student achievement in Confucian heritage cultures (CHCs). The sample comprised 38,158 students from 1030 schools in seven CHCs who participated in Program for International Student Assessment (PISA) 2012. Markov chain Monte Carlo multiple imputation, hierarchical linear modeling (HLM), and latent class analysis (LCA) were employed in the analysis. Results showed that home (but not school) IT use benefited student mathematics achievement, and students with the overall least IT resources were most academically successful. These results indicate the importance of understanding the nuanced effects of digital divides in different contexts. © 2018, Ministry of Science and Technology, Taiwan."
,10.1016/j.sste.2018.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048020824&doi=10.1016%2fj.sste.2018.05.001&partnerID=40&md5=70c7a07b3900fb5c75fb186afcb8934f,"Hospital length of stay (LOS) is often used as an indicator for hospital efficiency and resource utilization. LOS is nonnegative with presence of zeros and typically positively skewed with a long right tail, which may not be adequately modelled by traditional distributions, such as lognormal. We developed a zero-augmented accelerated frailty model for modeling the extreme skewness with the presence of zeros. Levels of utilization of health services may vary geographically, so conditional autoregressive priors were used to provide spatial smoothing across neighboring hospital health districts. The random effect terms are further linked to investigate if the capacity for longer LOS are consistently higher or lower at the health district level. Modeling and inference used the Bayesian approach via Markov Chain Monte Carlo simulation techniques. We demonstrated the proposed model for modeling the LOS of patients admitted due to chronic lower respiratory disease in Saskatchewan, Canada. © 2018 Elsevier Ltd"
,10.4310/SII.2018.v11.n3.a12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054672429&doi=10.4310%2fSII.2018.v11.n3.a12&partnerID=40&md5=76c93079b1b896df6e0088f07dc0b697,"In analysis of scientific data, it is often of interest to learn which features and feature interactions are relevant to the prediction task. We present here Selective Bayesian Forest Classifier, which strikes a balance between predictive power and interpretability by simultaneously performing classification, feature selection, feature interaction detection and visualization. It builds parsimonious yet flexible models using tree-structured Bayesian networks, and samples an ensemble of such models using Markov chain Monte Carlo. We build in its feature selection capability by dividing the trees into two groups according to their relevance to the outcome of interest. Our method performed competitively compared to top classification algorithms on both simulated data sets and real data sets in terms of classification accuracy, and often outperformed these methods in terms of feature selections and interaction visualizations. © 2018 International Press of Boston, Inc."
,10.1016/j.sste.2018.08.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053104917&doi=10.1016%2fj.sste.2018.08.002&partnerID=40&md5=012043669ee8fd3ea0945dde25ccf74f,"In an emerging epidemic, public health officials must move quickly to contain the spread. Information obtained from statistical disease transmission models often informs the development of containment strategies. Inference procedures such as Bayesian Markov chain Monte Carlo allow researchers to estimate parameters of such models, but are computationally expensive. In this work, we explore supervised statistical and machine learning methods for fast inference via supervised classification, with a focus on deep learning. We apply our methods to simulated epidemics through two populations of swine farms in Iowa, and find that the random forest performs well on the denser population, but is outperformed by a deep learning model on the sparser population. © 2018 Elsevier Ltd"
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051487450&partnerID=40&md5=f136fcb27bc5b8a1eeb919de423e1b14,The proceedings contain 18 papers. The topics discussed include: centrifugal compressors transient surge analysis: when do you need a hot gas bypass line?; a comprehensive model for predicting efficiency of DRA in liquid pipelines; effect of placing a relief system outside the flow path in a crude oil pipeline system; evaluation of a probabilistic leak detection testing approach for CPM systems; mitigation of slugging phenomena in offshore oil export lines; modeling and mitigation of acoustic induced vibration (AIV) in piping systems; modeling of heavy crude; modeling of rapid transients in natural gas pipelines; operational experience with introduction of a light oil into a heavy oil pipeline system; optimization of gas networks under transient conditions; and performance testing data interpretation using Markov chain Monte Carlo methods for a natural gas transmission pipeline with compressor station.
,10.26802/jaots.2018.12216,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051342130&doi=10.26802%2fjaots.2018.12216&partnerID=40&md5=77025ce3a3b4f657cf691fe34a2797d1,"MMA (Multiple Model Analysis) was used to determine diffusion modeling uncertainties of groundwater solutes arising from different grid scales. A series of seepage field models were constructed using the AM-MCMC (Adaptive Metropolis algorithm of Markov Chain Monte Carlo) method, which was based on a large hydro-geological test dataset. Next, the optimal seepage model was selected to simulate solute diffusion and determine the uncertainties arising from different grid scales. While the effect of the solute diffusion calculations due to ""numerical dispersion"" was reduced by our numerical simulations, it was still influenced by the chosen time steps and spatial scales. In our simulated example, we show that increased scale resolution does not lead to more accurate results. Finally, we show that the simulated solute accuracy is controlled by the seepage velocity, the time step size of the simulation and the model grid scale. © 2018 Walter de Gruyter GmbH. All rights reserved."
,10.1017/asb.2017.21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030717499&doi=10.1017%2fasb.2017.21&partnerID=40&md5=5a8a95d6813e682537f08b43366402c2,"Age-period-cohort models used in life and general insurance can be over-parameterized, and actuaries have used several methods to avoid this, such as cubic splines. Regularization is a statistical approach for avoiding over-parameterization, and it can reduce estimation and predictive variances compared to MLE. In Markov Chain Monte Carlo (MCMC) estimation, regularization is accomplished by the use of mean-zero priors, and the degree of parsimony can be optimized by numerically efficient out-of-sample cross-validation. This provides a consistent framework for comparing a variety of regularized MCMC models, such as those built with cubic splines, linear splines (as ours is), and the limiting case of non-regularized estimation. We apply this to the multiple-Trend model of Hunt and Blake (2014). © Astin Bulletin 2017."
,10.1285/i20705948v11n2p463,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055152861&doi=10.1285%2fi20705948v11n2p463&partnerID=40&md5=6ba15f17dcfa2d1cde55c5768a324cc2,"In recent years, the use of copulas has grown rapidly, especially in survival analysis. In this paper, we introduce a bivariate modified Weibull distribution derived from the Farlie{Gumbel{Morgenstern (FGM), a copula function commonly used to model very weak linear dependences. Considering the presence of non censored data and censored data, an extensive simulation study was developed to check the performance of the maximum likelihood method in estimating the parameters of the proposed model. Maximum like- lihood and Bayesian approaches for the estimation of the model parameters are presented. In the Bayesian analysis, the posterior distributions of the parameters are estimated using Markov chain Monte Carlo (MCMC) methodology. An example, considering a real data set, is introduced to illustrate the proposed methodology. © 2018, Università del Salento."
,10.1504/IJTGM.2018.092487,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049063241&doi=10.1504%2fIJTGM.2018.092487&partnerID=40&md5=3c5ce46be807f9bf9531e16dbc8a7976,"This paper is based on the Bayesian inference to explore two purposes. First, the specific objective is proposed to consider an empirical analysis employing the panel Bayesian vector autoregressive (PBVAR) model. Secondly, the empirical study is aimed to analyse the driven factor between tourism demands and service sectors by adapting Granger-causality Bayesian test. Technically, the simulated computation called Markov chain Monte Carlo (MCMC) approach is employed to investigate the research’s findings. Yearly time-series data of tourism growth (tourist arrivals) and economic growth (service sectors) in ASEAN-3 countries such as Singapore, Thailand, and Malaysia were observed during 1996–2015. The empirical results of this study will deeply clarify the linkage between tourism demands and economy in ASEAN-3 countries and give recommendations to more assure that a suitable tourism policy is appropriately activated. Copyright © 2018 Inderscience Enterprises Ltd."
,10.1162/COLI_a_00326,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054024956&doi=10.1162%2fCOLI_a_00326&partnerID=40&md5=79645774ff52b73de8795f560202179a,"Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction (decipherment) for closely related language pairs. The existing decipherment models, however, are not well suited for exploiting these orthographic similarities. We propose a log-linear model with latent variables that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed log-linear model. To address this challenge, we perform approximate inference via Markov chain Monte Carlo sampling and contrastive divergence. Our results show that the proposed log-linear model with contrastive divergence outperforms the existing generative decipherment models by exploiting the orthographic features. The model both scales to large vocabularies and preserves accuracy in low- and no-resource contexts. © 2018 Association for Computational Linguistics."
,10.1140/epjc/s10052-017-5479-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042845258&doi=10.1140%2fepjc%2fs10052-017-5479-0&partnerID=40&md5=4957238f6286e82349e2ff001ea1c478,"The cosmological redshift drift could lead to the next step in high-precision cosmic geometric observations, becoming a direct and irrefutable test for cosmic acceleration. In order to test the viability and possible properties of this effect, also called Sandage–Loeb (SL) test, we generate a model-independent mock data set in order to compare its constraining power with that of the future mock data sets of Type Ia Supernovae (SNe) and Baryon Acoustic Oscillations (BAO). The performance of those data sets is analyzed by testing several cosmological models with the Markov chain Monte Carlo (MCMC) method, both independently as well as combining all data sets. Final results show that, in general, SL data sets allow for remarkable constraints on the matter density parameter today Ωm on every tested model, showing also a great complementarity with SNe and BAO data regarding dark energy parameters. © 2018, The Author(s)."
1,10.1111/tops.12315,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038122348&doi=10.1111%2ftops.12315&partnerID=40&md5=a27292f2a67f543ae3bdd4fcf069b089,"Cognitive architectures have often been applied to data from individual experiments. In this paper, I develop an ACT-R reader that can model a much larger set of data, eye-tracking corpus data. It is shown that the resulting model has a good fit to the data for the considered low-level processes. Unlike previous related works (most prominently, Engelmann, Vasishth, Engbert & Kliegl,), the model achieves the fit by estimating free parameters of ACT-R using Bayesian estimation and Markov-Chain Monte Carlo (MCMC) techniques, rather than by relying on the mix of manual selection + default values. The method used in the paper is generalizable beyond this particular model and data set and could be used on other ACT-R models. Copyright © 2017 Cognitive Science Society, Inc."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044994797&partnerID=40&md5=c43f6cde8d2ef838bc9f7b442687c10d,"In this paper, we follow the idea of using an invariant loss function in a decision theoretic approach for point estimation in Bayesian mixture models presented in [1]. Although using this approach the so-called label switching is no longer a problem, it is difficult to assess the uncertainty. We propose a simple and accessible way for assessing uncertainty using the leaving-out idea from the jackknife method to compute the Bayes estimates called jackknife-Bayes estimates, then use them to visualize the uncertainty of Bayesian point estimates. This paper is primarily related to simulation-based point estimation using Markov Chain Monte Carlo (MCMC) samples; hence the MCMC methods, in particular Gibbs sampling and Metropolis Hastings method are used to approximate the posterior mixture models. We also present the use of importance sampling in reduced posterior mixture distribution corresponding to the leaving-out observation. © 2018 by the Mathematical Association of Thailand. All rights reserved."
,10.1016/j.gfj.2018.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052972964&doi=10.1016%2fj.gfj.2018.07.002&partnerID=40&md5=a97e8d02a5f779281dc614fb35f21174,"Using firm-level panel data, this paper examines whether the cost of capital (COC) differs significantly between U.S.-based multinational corporations (MNCs) and U.S. domestic corporations (DCs). The results suggest that U.S.-based MNCs have higher COC than U.S. DCs and that industry importantly influences COC. The study also finds that there is a significant time effect on COC, and the time effect follows the trend of the U.S. economic growth rate. Using a Bayesian Markov chain Monte Carlo approach, we estimate jointly cost of equity, cost of debt, and capital structure, and find that the higher cost of capital for MNCs is due mainly to their higher cost of equity and greater use of equity financing; the cost of debt financing does not differ significantly for MNCs versus DCs. © 2018 Elsevier Inc."
1,10.1214/17-AOS1649,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054759831&doi=10.1214%2f17-AOS1649&partnerID=40&md5=c70da9099d8e4d5a63f9c780abd24bbc,"The Contrastive Divergence (CD) algorithm has achieved notable success in training energy-based models including Restricted Boltzmann Machines and played a key role in the emergence of deep learning. The idea of this algorithm is to approximate the intractable term in the exact gradient of the log-likelihood function by using short Markov chain Monte Carlo (MCMC) runs. The approximate gradient is computationally-cheap but biased. Whether and why the CD algorithm provides an asymptotically consistent estimate are still open questions. This paper studies the asymptotic properties of the CD algorithm in canonical exponential families, which are special cases of the energy-based model. Suppose the CD algorithm runs m MCMC transition steps at each iteration t and iteratively generates a sequence of parameter estimates {θt}t≥0 given an i.i.d. data sample {Xi}n i=1 ∼ pθ. Under conditions which are commonly obeyed by the CD algorithm in practice, we prove the existence of some bounded m such that any limit point of the time average t s − = 1 0 θs/t as t → ∞ is a consistent estimate for the true parameter θ. Our proof is based on the fact that {θt}t≥0 is a homogenous Markov chain conditional on the data sample {Xi}n i=1. This chain meets the Foster–Lyapunov drift criterion and converges to a random walk around the maximum likelihood estimate. The range of the random walk shrinks to zero at rate O(1/3 n) as the sample size n → ∞. © Institute of Mathematical Statistics, 2018."
,10.1080/10618600.2018.1482765,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053268730&doi=10.1080%2f10618600.2018.1482765&partnerID=40&md5=9c74e8d1fb2f23480a04a55d66b24bb9,"We develop efficient Bayesian inference for the one-factor copula model with two significant contributions over existing methodologies. First, our approach leads to straightforward inference on dependence parameters and the latent factor; only inference on the former is available under frequentist alternatives. Second, we develop a reversible jump Markov chain Monte Carlo algorithm that averages over models constructed from different bivariate copula building blocks. Our approach accommodates any combination of discrete and continuous margins. Through extensive simulations, we compare the computational and Monte Carlo efficiency of alternative proposed sampling schemes. The preferred algorithm provides reliable inference on parameters, the latent factor, and model space. The potential of the methodology is highlighted in an empirical study of 10 binary measures of socio-economic deprivation collected for 11,463 East Timorese households. The importance of conducting inference on the latent factor is motivated by constructing a poverty index using estimates of the factor. Compared to a linear Gaussian factor model, our model average improves out-of-sample fit. The relationships between the poverty index and observed variables uncovered by our approach are diverse and allow for a richer and more precise understanding of the dependence between overall deprivation and individual measures of well-being. © 2018, © 2018 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
1,10.1002/bimj.201600206,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031421931&doi=10.1002%2fbimj.201600206&partnerID=40&md5=a760a844f3667d4b9b433388eb709fe6,"In this paper, the development of a probabilistic network for the diagnosis of acute cardiopulmonary diseases is presented in detail. A panel of expert physicians collaborated to specify the qualitative part, which is a directed acyclic graph defining a factorization of the joint probability distribution of domain variables into univariate conditional distributions. The quantitative part, which is a set of parametric models defining these univariate conditional distributions, was estimated following the Bayesian paradigm. In particular, we exploited an original reparameterization of Beta and categorical logistic regression models to elicit the joint prior distribution of parameters from medical experts, and updated it by conditioning on a dataset of hospital records via Markov chain Monte Carlo simulation. Refinement was iteratively performed until the probabilistic network provided satisfactory concordance index values for several acute diseases and reasonable diagnosis for six fictitious patient cases. The probabilistic network can be employed to perform medical diagnosis on a total of 63 diseases (38 acute and 25 chronic) on the basis of up to 167 patient findings. © 2017 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim"
4,10.1093/mnras/stx2304,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045895593&doi=10.1093%2fmnras%2fstx2304&partnerID=40&md5=1d453535f465613bc6f60abcd983ec0e,"We present a novel algorithm based on a Bayesian method for 2D tilted-ring analysis of disc galaxy velocity fields. Compared to the conventional algorithms based on a chi-squared minimization procedure, this new Bayesian-based algorithm suffers less from local minima of the model parameters even with highly multimodal posterior distributions. Moreover, the Bayesian analysis, implemented via Markov Chain Monte Carlo sampling, only requires broad ranges of posterior distributions of the parameters, which makes the fitting procedure fully automated. This feature will be essential when performing kinematic analysis on the large number of resolved galaxies expected to be detected in neutral hydrogen (H I) surveys with the Square Kilometre Array and its pathfinders. The so-called 2D Bayesian Automated Tiltedring fitter (2DBAT) implements Bayesian fits of 2D tilted-ring models in order to derive rotation curves of galaxies. We explore 2DBAT performance on (a) artificial HI data cubes built based on representative rotation curves of intermediate-mass and massive spiral galaxies, and (b) Australia Telescope Compact Array HI data from the Local Volume HI Survey. We find that 2DBAT works best for well-resolved galaxies with intermediate inclinations (20° < i < 70°), complementing 3D techniques better suited to modelling inclined galaxies. © 2016 The Authors."
,10.1080/10618600.2018.1482761,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053562527&doi=10.1080%2f10618600.2018.1482761&partnerID=40&md5=e3e6ab97c0c6591908fd1f7ddb4e4873,"Fitting hierarchical Bayesian models to spatially correlated datasets using Markov chain Monte Carlo (MCMC) techniques is computationally expensive. Complicated covariance structures of the underlying spatial processes, together with high-dimensional parameter space, mean that the number of calculations required grows cubically with the number of spatial locations at each MCMC iteration. This necessitates the need for efficient model parameterizations that hasten the convergence and improve the mixing of the associated algorithms. We consider partially centred parameterizations (PCPs) which lie on a continuum between what are known as the centered (CP) and noncentered parameterizations (NCP). By introducing a weight matrix we remove the conditional posterior correlation between the fixed and the random effects, and hence construct a PCP which achieves immediate convergence for a three-stage model, based on multiple Gaussian processes with known covariance parameters. When the covariance parameters are unknown we dynamically update the parameterization within the sampler. The PCP outperforms both the CP and the NCP and leads to a fully automated algorithm which has been demonstrated in two simulation examples. The effectiveness of the spatially varying PCP is illustrated with a practical dataset of nitrogen dioxide concentration levels. Supplemental materials consisting of appendices, datasets, and computer code to reproduce the results are available online. © 2018, © 2018 The Author(s). Published with license by Taylor & Francis."
3,10.1093/mnras/stx2433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042700422&doi=10.1093%2fmnras%2fstx2433&partnerID=40&md5=834605c06b7ef1dc8b61aa670adc419f,"We apply an analytic Markov Chain Monte Carlo model to a sample of 18 active galactic nucleus (AGN)-driven biconical outflows that we identified from a sample of active galaxies with double-peaked narrow emission lines at z < 0.1 in the Sloan Digital Sky Survey. We find that 8 of 18 are best described as asymmetric bicones, 8 of 18 are nested bicones, and 2 of 18 are symmetric bicones. From the geometry and kinematics of the models, we find that these moderate-luminosity AGN outflows are large and energetic. The biconical outflows axes are randomly oriented with respect to the photometric major axis of the galaxy, implying a randomly oriented and clumpier torus to collimate the outflow, but the torus also allows some radiation to escape equatorially. We find that 16 of 18 (89 per cent) outflows are energetic enough to drive a two-staged feedback process in their host galaxies. All of these outflows geometrically intersect the photometric major axis of the galaxy, and 23 per cent of outflow host galaxies are significantly redder or have significantly lower specific star formation rates when compared to a matched sample of active galaxies. © 2017 The Author(s)."
3,10.1093/MNRAS/STX2464,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040258364&doi=10.1093%2fMNRAS%2fSTX2464&partnerID=40&md5=85195d5d9ff06fc749d398376bc46f41,"We have developed a novel Markov Chain Monte Carlo chemical 'painting' technique to explore possible radial and vertical metallicity gradients for the thick disc progenitor. In our analysis, we match an N-body simulation to the data from the Apache Point Observatory Galactic Evolution Experiment survey.We assume that the thick disc has a constant scaleheight and has completed its formation at an early epoch, after which time radial mixing of its stars has taken place. Under these assumptions, we find that the initial radial metallicity gradient of the thick disc progenitor should not be negative, but either flat or even positive, to explain the current negative vertical metallicity gradient of the thick disc. Our study suggests that the thick disc was built-up in an inside-out and upside-down fashion, and older, smaller and thicker populations are more metal poor. In this case, star-forming discs at different epochs of the thick disc formation are allowed to have different radial metallicity gradients, including a negative one, which helps to explain a variety of slopes observed in high-redshift disc galaxies. This scenario helps to explain the positive slope of the metallicity-rotation velocity relation observed for the Galactic thick disc. On the other hand, radial mixing flattens the slope of an existing gradient. © 2017 The Authors."
,10.1016/j.epidem.2018.08.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052987683&doi=10.1016%2fj.epidem.2018.08.004&partnerID=40&md5=ff7ea578efc133c91ccbf944677d94f9,"Dengue dynamics are shaped by the complex interplay between several factors, including vector seasonality, interaction between four virus serotypes, and inapparent infections. However, paucity or quality of data do not allow for all of these to be taken into account in mathematical models. In order to explore separately the importance of these factors in models, we combined surveillance data with a local-scale cluster study in the rural province of Kampong Cham (Cambodia), in which serotypes and asymptomatic infections were documented. We formulate several mechanistic models, each one relying on a different set of hypotheses, such as explicit vector dynamics, transmission via asymptomatic infections and coexistence of several virus serotypes. Models are confronted with the observed time series using Bayesian inference, through Markov chain Monte Carlo. Model selection is then performed using statistical information criteria, and the coherence of epidemiological characteristics (reproduction numbers, incidence proportion, dynamics of the susceptible classes) is assessed in each model. Our analyses on transmission dynamics in a rural endemic setting highlight that two-strain models with interacting effects better reproduce the long term data, but they are difficult to parameterize when relying on incidence cases only. On the other hand, considering the available data, incorporating vector and asymptomatic components seems of limited added-value when seasonality and underreporting are already accounted for. © 2018 The Authors"
,10.1093/MNRAS/STY2062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055162888&doi=10.1093%2fMNRAS%2fSTY2062&partnerID=40&md5=0e06152ef1a2c96b5a855ba23125ca93,"The cosmic distance duality relation (DDR), which connects the angular diameter distance and luminosity distance through a simple formula DA(z)(1 + z)2/DL(z) = 1, is an important relation in cosmology. Therefore, testing the validity of DDR is of great importance. In this paper, we test the possible violation of DDR using the available local data including type Ia supernovae (SNe Ia), galaxy clusters, and baryon acoustic oscillations (BAO). We write the modified DDR as DA(z)(1 + z)2/DL(z) = η(z), and consider two different parameterizations of η(z), namely η1(z) = 1 + η0z and η2(z) = 1 + η0z/(1 + z). The luminosity distance from SNe Ia are compared with the angular diameter distance from galaxy clusters and BAO at the same redshift. Two different cluster data are used here, i.e. elliptical clusters and spherical clusters. The parameter η0 is obtained using the Markov chain Monte Carlo methods. It is found that η0 can be strictly constrained by the elliptical clusters + BAO data, with the best-fitting values η0 = -0.04 ± 0.12 and η0 = -0.05 ± 0.22 for the first and second parametrizations, respectively. However, the spherical clusters + BAO data couldn't strictly constrain η0 due to the large intrinsic scatter. In any case studied here, no evidence for the violation of DDR is found. © 2018 The Author(s)."
1,10.1214/17-BA1089,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054617184&doi=10.1214%2f17-BA1089&partnerID=40&md5=848ea89ae157e3b70894493780799453,"Regular vine copulas are a flexible class of dependence models, but Bayesian methodology for model selection and inference is not yet fully developed. We propose sparsity-inducing but otherwise non-informative priors, and present novel proposals to enable reversible jump Markov chain Monte Carlo posterior simulation for Bayesian model selection and inference. Our method is the first to jointly estimate the posterior distribution of all trees of a regular vine copula. This represents a substantial improvement over existing frequentist and Bayesian strategies, which can only select one tree at a time and are known to induce bias. A simulation study demonstrates the feasibility of our strategy and shows that it combines superior selection and reduced computation time compared to Bayesian tree-by-tree selection. In a real data example, we forecast the daily expected tail loss of a portfolio of nine exchange-traded funds using a fully Bayesian multivariate dynamic model built around Bayesian regular vine copulas to illustrate our model's viability for financial analysis and risk estimation. © 2018 International Society for Bayesian Analysis."
,10.1504/IJEBR.2018.089689,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041750373&doi=10.1504%2fIJEBR.2018.089689&partnerID=40&md5=0d82573e0e8c7642a4f524244b80858b,"This paper presents a variant of the mixed logit model, in the form of a panel-like error components mixed logit that relies on a multinomial logit formulation of the weighted logit formula, as opposed to the usual conditional logit representation; then uses the model to evaluate the consistency of consumers’ preferences for health insurance by jointly modelling stated health insurance preferences with revealed health insurance choices of respondents from the 2007 Medical Expenditure Panel Survey (MEPS). Estimation is implemented within the Bayesian paradigm using Markov Chain Monte Carlo (MCMC) methods, and the results suggest that 2007 MEPS respondents do present stable preferences for health insurance. In fact, respondents who initially express health insurance as not worth its cost are found to be 23.28% less likely to be privately insured and 81.53% less likely to be publicly insured. On the other hand, those initially expressing health insurance as worth its cost are found to be 21.72% more likely to be privately insured and 81.68% more likely to be publicly insured. Copyright © 2018 Inderscience Enterprises Ltd."
,10.1093/mnras/sty2140,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054734358&doi=10.1093%2fmnras%2fsty2140&partnerID=40&md5=512a781b5adc00b16afddf27ff1c02a8,"We present the umbrella sampling (US) technique and show that it can be used to sample extremely low-probability areas of the posterior distribution that may be required in statistical analyses of data. In this approach sampling of the target likelihood is split into sampling of multiple biased likelihoods confined within individual umbrella windows. We show that the US algorithm is efficient and highly parallel and that it can be easily used with other existing Markov Chain Monte Carlo (MCMC) samplers. The method allows the user to capitalize on their intuition and define umbrella windows and increase sampling accuracy along specific directions in the parameter space. Alternatively, one can define umbrella windows using an approach similar to parallel tempering. We provide a public code that implements US as a standalone PYTHON package. We present a number of tests illustrating the power of the US method in sampling low-probability areas of the posterior and show that this ability allows a considerablymore robust sampling ofmultimodal distributions compared to standard sampling methods. We also present an application of the method in a real world example of deriving cosmological constraints using the supernova type Ia data. We show that US can sample the posterior accurately down to the ≈15 σ credible region in the Ωm - ΩΛ plane, while for the same computational effort the affine-invariant MCMC sampling implemented in the emcee code samples the posterior reliably only to ≈3 σ. © 2018 The Author(s)."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046085294&partnerID=40&md5=7e53cfd67c633ebe43663a5da1dd6ac8,"In the literature, several macroeconomic economic factors such as GDP, inflation rate, unemployment, and exchange rate have been identified to influence the level of non-performing loan ratio (NPL) in the banking sector. Other macroeconomic variables such as industry production index, stock exchange index, and oil price are also well documented to have strong explanatory power on NPLs. In this study, we examine the effects of some macroeconomic variables (exchange rates (TL=$ and TL/€), industrial production index (IPI), stock exchange index (BIST100), and oil price) on NPL ratio. Further more, we focus on estimating the parameters related to the above variables in a non-performing loan ratio model via Frequentist approach and Bayesian analysis. In the Bayesian method, we provide uninformative and informative priors and a likelihood function that determines the posterior distributions of the parameters. Using Markov Chain Monte Carlo (MCMC) algorithm, we sample the estimates of the parameters from their posterior distributions. The results of the analysis show that the above mentioned macroeconomic variables examined in this study have significant effects on non-performing loan ratio. © 2018, World Scientific and Engineering Academy and Society. All rights reserved."
5,10.1093/mnras/stx2109,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042586837&doi=10.1093%2fmnras%2fstx2109&partnerID=40&md5=788aa3e68e915a7aa1fa13a775cb3679,"Variability in the light curves of spotted, rotating stars is often non-sinusoidal and quasiperiodic - spots move on the stellar surface and have finite lifetimes, causing stellar flux variations to slowly shift in phase. A strictly periodic sinusoid therefore cannot accurately model a rotationally modulated stellar light curve. Physical models of stellar surfaces have many drawbacks preventing effective inference, such as highly degenerate or high-dimensional parameter spaces. In this work, we test an appropriate effective model: a Gaussian Process with a quasi-periodic covariance kernel function. This highly flexible model allows sampling of the posterior probability density function of the periodic parameter, marginalizing over the other kernel hyperparameters using a Markov Chain Monte Carlo approach. To test the effectiveness of this method, we infer rotation periods from 333 simulated stellar light curves, demonstrating that the Gaussian process method produces periods that are more accurate than both a sinefitting periodogram and an autocorrelation function method. We also demonstrate that it works well on real data, by inferring rotation periods for 275 Kepler stars with previously measured periods. We provide a table of rotation periods for these and many more, altogether 1102 Kepler objects of interest, and their posterior probability density function samples. Because this method delivers posterior probability density functions, it will enable hierarchical studies involving stellar rotation, particularly those involving population modelling, such as inferring stellar ages, obliquities in exoplanet systems, or characterizing star-planet interactions. The code used to implement this method is available online. © 2017 The Author(s)."
,10.18637/jss.v083.i01,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042469222&doi=10.18637%2fjss.v083.i01&partnerID=40&md5=2cd9deddcfa58d2f55063d6d570a4405,"This paper introduces the R package meta4diag for implementing Bayesian bivariate meta-analyses of diagnostic test studies. Our package meta4diag is a purpose-built front end of the R package INLA. While INLA offers full Bayesian inference for the large set of latent Gaussian models using integrated nested Laplace approximations, meta4diag extracts the features needed for bivariate meta-analysis and presents them in an intuitive way. It allows the user a straightforward model specification and offers user-specific prior distributions. Further, the newly proposed penalized complexity prior framework is supported, which builds on prior intuitions about the behaviors of the variance and correlation parameters. Accurate posterior marginal distributions for sensitivity and specificity as well as all hyperparameters, and covariates are directly obtained without Markov chain Monte Carlo sampling. Further, univariate estimates of interest, such as odds ratios, as well as the summary receiver operating characteristic (SROC) curve and other common graphics are directly available for interpretation. An interactive graphical user interface provides the user with the full functionality of the package without requiring any R programming. The package is available from the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=meta4diag/ and its usage will be illustrated using three real data examples. © 2018, American Statistical Association. All rights reserved."
,10.1002/pst.1888,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051062880&doi=10.1002%2fpst.1888&partnerID=40&md5=8ee60574275ce29bb889005c1d720b1b,"This article focuses on 2 objectives in the analysis of efficacy in long-term extension studies of chronic diseases: (1) defining and discussing estimands of interest in such studies and (2) evaluating the performance of several multiple imputation methods that may be useful in estimating some of these estimands. Specifically, 4 estimands are defined and their clinical utility and inferential ramifications discussed. The performance of several multiple imputation methods and approaches were evaluated using simulated data. Results suggested that when interest is in a binary outcome derived from an underlying continuous measurement, it is preferable to impute the underlying continuous value that is subsequently dichotomized rather than to directly impute the binary outcome. Results also demonstrated that multivariate Gaussian models with Markov chain Monte Carlo imputation and sequential regression have minimal bias and the anticipated confidence interval coverage, even in settings with ordinal data where departures from normality are a concern. These approaches are further illustrated using a long-term extension study in psoriasis. © 2018 John Wiley & Sons, Ltd."
,10.1007/s10596-018-9769-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053181646&doi=10.1007%2fs10596-018-9769-x&partnerID=40&md5=919914974269ad6dde28f2c2836a3042,"Methane gas hydrates have increasingly become a topic of interest because of their potential as a future energy resource. There are significant economical and environmental risks associated with extraction from hydrate reservoirs, so a variety of multiphysics models have been developed to analyze prospective risks and benefits. These models generally have a large number of empirical parameters which are not known a priori. Traditional optimization-based parameter estimation frameworks may be ill-posed or computationally prohibitive. Bayesian inference methods have increasingly been found effective for estimating parameters in complex geophysical systems. These methods often are not viable in cases of computationally expensive models and high-dimensional parameter spaces. Recently, methods have been developed to effectively reduce the dimension of Bayesian inverse problems by identifying low-dimensional structures that are most informed by data. Active subspaces is one of the most generally applicable methods of performing this dimension reduction. In this paper, Bayesian inference of the parameters of a state-of-the-art mathematical model for methane hydrates based on experimental data from a triaxial compression test with gas hydrate-bearing sand is performed in an efficient way by utilizing active subspaces. Active subspaces are used to identify low-dimensional structure in the parameter space which is exploited by generating a cheap regression-based surrogate model and implementing a modified Markov chain Monte Carlo algorithm. Posterior densities having means that match the experimental data are approximated in a computationally efficient way. © 2018, Springer Nature Switzerland AG."
,10.1137/15M1033162,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046039781&doi=10.1137%2f15M1033162&partnerID=40&md5=1fe3de1abe08f53866e61338a9bdfdfa,"Making good predictions of a physical system using a computer code requires the inputs to be carefully specified. Some of these inputs, called control variables, reproduce physical conditions, whereas other inputs, called parameters, are specific to the computer code and most often uncertain. The goal of statistical calibration consists in reducing their uncertainty with the help of a statistical model which links the code outputs with the field measurements. In a Bayesian setting, the posterior distribution of these parameters is typically sampled using Markov Chain Monte Carlo methods. However, they are impractical when the code runs are highly time-consuming. A way to circumvent this issue consists of replacing the computer code with a Gaussian process emulator, then sampling a surrogate posterior distribution based on it. Doing so, calibration is subject to an error which strongly depends on the numerical design of experiments used to fit the emulator. Under the assumption that there is no code discrepancy, we aim to reduce this error by constructing a sequential design by means of the expected improvement criterion. Numerical illustrations in several dimensions assess the efficiency of such sequential strategies. © 2018 Society for Industrial and Applied Mathematics and American Statistical Association."
,10.1080/01621459.2018.1473776,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052073228&doi=10.1080%2f01621459.2018.1473776&partnerID=40&md5=09812cdd5d40c801b68080a57084a510,"A key challenge for modern Bayesian statistics is how to perform scalable inference of posterior distributions. To address this challenge, variational Bayes (VB) methods have emerged as a popular alternative to the classical Markov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while achieving comparable predictive performance. However, there are few theoretical results around VB. In this article, we establish frequentist consistency and asymptotic normality of VB methods. Specifically, we connect VB methods to point estimates based on variational approximations, called frequentist variational approximations, and we use the connection to prove a variational Bernstein–von Mises theorem. The theorem leverages the theoretical characterizations of frequentist variational approximations to understand asymptotic properties of VB. In summary, we prove that (1) the VB posterior converges to the Kullback–Leibler (KL) minimizer of a normal distribution, centered at the truth and (2) the corresponding variational expectation of the parameter is consistent and asymptotically normal. As applications of the theorem, we derive asymptotic properties of VB posteriors in Bayesian mixture models, Bayesian generalized linear mixed models, and Bayesian stochastic block models. We conduct a simulation study to illustrate these theoretical results. Supplementary materials for this article are available online. © 2018, © 2018 American Statistical Association."
,10.7566/JPSJ.87.054004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046033756&doi=10.7566%2fJPSJ.87.054004&partnerID=40&md5=1a9097c379be2bece9013f402c59f6c3,"We study the unlearning of mixed states in the Hopfield model for the extensive loading case. Firstly, we focus on case I, where several embedded patterns are correlated with each other, whereas the rest are uncorrelated. Secondly, we study case II, where patterns are divided into clusters in such a way that patterns in any cluster are correlated but those in two different clusters are not correlated. By using the replica method, we derive the saddle point equations for order parameters under the ansatz of replica symmetry. The same equations are also derived by self-consistent signal-to-noise analysis in case I. In both cases I and II, we find that when the correlation between patterns is large, the network loses its ability to retrieve the embedded patterns and, depending on the parameters, a confused memory, which is a mixed state and=or spin glass state, emerges. By unlearning the mixed state, the network acquires the ability to retrieve the embedded patterns again in some parameter regions. We find that to delete the mixed state and to retrieve the embedded patterns, the coefficient of unlearning should be chosen appropriately. We perform Markov chain Monte Carlo simulations and find that the simulation and theoretical results agree reasonably well, except for the spin glass solution in a parameter region due to the replica symmetry breaking. Furthermore, we find that the existence of many correlated clusters reduces the stabilities of both embedded patterns and mixed states. ©2018 The Physical Society of Japan."
,10.3159/TORREY-D-17-00012.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041058605&doi=10.3159%2fTORREY-D-17-00012.1&partnerID=40&md5=194d8ab429e6838e6c7d5969ec23f2c0,"The population structure of invasive species is determined both by species-specific breeding strategies and by region-specific conditions and events during the history of invasion. In this context, we investigate the role of human land-use policy on the gene flow within population of the widespread invasive tree, Ailanthus altissima. We predicted that genetic diversity would be elevated in urban environments and reduced in exurban, and that human mediated dispersal would allow propagules to break geographic isolation. Six populations of A. altissima, divided evenly among urban, exurban and suburban sites, were surveyed using a set of eight microsatellite loci. A total of 276 individuals were sampled. Populations were assessed for partitioning of genetic variation, gene flow between sites, and genetic cluster estimation. Effective population size based on genetic variation was also modeled using Bayesian Markov chain Monte Carlo simulations. Despite a strong propensity for clonal growth, the microsatellite data revealed no evidence of clonal reproduction at the population level. Gene flow between sites was found to be independent of geographic distance; instead, gene flow was correlated with the level of human traffic at a site. Genetic diversity was found to generally increase in correlation to human development; however, reduced admixture at managed sites suggests that land management practices were effective at inhibiting gene flow into managed sites. © Copyright 2018 by The Torrey Botanical Society."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051558141&partnerID=40&md5=55463a6845268644df657172d863fa2d,"Probit models with spatial dependencies were first studied by McMillen (1992), where an EM Algorithm was developed to produce consistent (maximum likelihood) estimates for these models. In spatial autoregressive probit model, the spatial dependent structure adds complexity in the estimation of parameters. LeSage and Smith (2001) use Bayesian estimation via Markov Chain Monte Carlo methods that sample sequentially from the complete set of conditional distributions for all parameters. Klier and McMillen (2008) have proposed a linearized version of the GMM estimator that avoids the infeasible problem of inverting n-by-n matrices when employing large samples. They show that standard GMM reduces to a nonlinear two-stage least squares problem. Martinetti and Geniaux (2017) proposed approximate likelihood estimation which based on the full maximization of likelihood of an approximate multivariate normal distribution function. We use some extensive simulation for these methods and show the best estimation method which can handle sample sizes with many observations and various value of coefficient spatial lag, provided the spatial weight matrix is inconvenient sparse form, as is for large data sets, where each observation neighbours only a few other observations. © IEOM Society International."
,10.1080/10618600.2018.1459303,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052067282&doi=10.1080%2f10618600.2018.1459303&partnerID=40&md5=ba6b289d4b839c55e5b98e0d3a4f3d4c,"The performance of Markov chain Monte Carlo (MCMC) algorithms like the Metropolis Hastings Random Walk (MHRW) is highly dependent on the choice of scaling matrix for the proposal distributions. A popular choice of scaling matrix in adaptive MCMC methods is to use the empirical covariance matrix (ECM) of previous samples. However, this choice is problematic if the dimension of the target distribution is large, since the ECM then converges slowly and is computationally expensive to use. We propose two algorithms to improve convergence and decrease computational cost of adaptive MCMC methods in cases when the precision (inverse covariance) matrix of the target density can be well-approximated by a sparse matrix. The first is an algorithm for online estimation of the Cholesky factor of a sparse precision matrix. The second estimates the sparsity structure of the precision matrix. Combining the two algorithms allows us to construct precision-based adaptive MCMC algorithms that can be used as black-box methods for densities with unknown dependency structures. We construct precision-based versions of the adaptive MHRW and the adaptive Metropolis adjusted Langevin algorithm and demonstrate the performance of the methods in two examples. Supplementary materials for this article are available online. © 2018, © 2018 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047349519&partnerID=40&md5=983bf2bcdf8844c3d93691465808f129,The proceedings contain 114 papers. The topics discussed include: sulcal depth-based cortical shape analysis in normal healthy control and schizophrenia groups; skull segmentation from MR scans using a higher-order shape model based on convolutional restricted Boltzmann machines; constructing statistically unbiased cortical surface templates using feature-space covariance; segmentation of anatomical structures in cardiac CTA using multi-label V-Net; iterative convolutional neural networks for automatic vertebra identification and segmentation in CT images; visualization of coronary artery calcium in dual energy chest radiography using automatic rib suppression; radiation dose reduction in digital breast tomosynthesis (DBT) by means of deep-learning-based supervised image processing; left ventricle segmentation in 3D ultrasound by combining structured random forests with active shape models; automatic and fast CT liver segmentation using sparse ensemble with machine learned contexts; a multilevel Markov chain Monte Carlo approach for uncertainty quantification in deformable registration; self-reference-based and during-registration detection of motion artifacts in spatiotemporal image data; a novel framework for the local extraction of extra-axial cerebrospinal fluid from MR brain images; and regional autonomy changes in resting-state functional MRI in patients with HIV associated neurocognitive disorder.
,10.1007/s11009-018-9670-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053930994&doi=10.1007%2fs11009-018-9670-z&partnerID=40&md5=2f0cb1b8ef3f9791f0ddea3cf3901de1,"K-Nearest Neighbours (k-NN) is a popular classification and regression algorithm, yet one of its main limitations is the difficulty in choosing the number of neighbours. We present a Bayesian algorithm to compute the posterior probability distribution for k given a target point within a data-set, efficiently and without the use of Markov Chain Monte Carlo (MCMC) methods or simulation—alongside an exact solution for distributions within the exponential family. The central idea is that data points around our target are generated by the same probability distribution, extending outwards over the appropriate, though unknown, number of neighbours. Once the data is projected onto a distance metric of choice, we can transform the choice of k into a change-point detection problem, for which there is an efficient solution: we recursively compute the probability of the last change-point as we move towards our target, and thus de facto compute the posterior probability distribution over k. Applying this approach to both a classification and a regression UCI data-sets, we compare favourably and, most importantly, by removing the need for simulation, we are able to compute the posterior probability of k exactly and rapidly. As an example, the computational time for the Ripley data-set is a few milliseconds compared to a few hours when using a MCMC approach. © 2018, The Author(s)."
,10.1080/01621459.2018.1476244,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052155485&doi=10.1080%2f01621459.2018.1476244&partnerID=40&md5=31764d5f836137e9111f4b3e1878af96,"Voxel functional magnetic resonance imaging (fMRI) time courses are complex-valued signals giving rise to magnitude and phase data. Nevertheless, most studies use only the magnitude signals and thus discard half of the data that could potentially contain important information. Methods that make use of complex-valued fMRI (CV-fMRI) data have been shown to lead to superior power in detecting active voxels when compared to magnitude-only methods, particularly for small signal-to-noise ratios (SNRs). We present a new Bayesian variable selection approach for detecting brain activation at the voxel level from CV-fMRI data. We develop models with complex-valued spike-and-slab priors on the activation parameters that are able to combine the magnitude and phase information. We present a complex-valued EM variable selection algorithm that leads to fast detection at the voxel level in CV-fMRI slices and also consider full posterior inference via Markov chain Monte Carlo (MCMC). Model performance is illustrated through extensive simulation studies, including the analysis of physically based simulated CV-fMRI slices. Finally, we use the complex-valued Bayesian approach to detect active voxels in human CV-fMRI from a healthy individual who performed unilateral finger tapping in a designed experiment. The proposed approach leads to improved detection of activation in the expected motor-related brain regions and produces fewer false positive results than other methods for CV-fMRI. Supplementary materials for this article are available online. © 2018, © 2018 American Statistical Association."
2,10.3847/1538-4357/aa9b7e,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040326811&doi=10.3847%2f1538-4357%2faa9b7e&partnerID=40&md5=6031fd22f0c10abdf9a1733075b20cf7,"We model simultaneous or quasi-simultaneous multi-band spectral energy distributions (SEDs) for a sample of 25 blazars that have radio core-shift measurements, where a one-zone leptonic model and Markov chain Monte Carlo technique are adopted. In the SED fitting for 23 low-synchrotron-peaked (LSP) blazars, the seed photons from the broad-line (BLR) and molecular torus are considered respectively in the external Compton process. We find that the SED fitting with the seed photons from the torus are better than those utilizing BLR photons, which suggest that the γ-ray emitting region may be located outside the BLR. Assuming the magnetic field strength in the γ-ray emitting region as constrained from the SED fitting follows the magnetic field distribution as derived from the radio core-shift measurements (i.e.,B(R) ≃ B1 pc (R/1pc)-1, where R is the distance from the central engine and is the magnetic field strength at 1 pc), we further calculate the location of the γ-ray emitting region, Rγ, for these blazars. We find that Rγ ∼ 2 × 104RS≃ 10 RBLR(Rs is the Schwarzschild radius and is the BLR size), where is estimated from the broad-line luminosities using the empirical correlations obtained using the reverberation mapping methods. © 2018. The American Astronomical Society."
,10.1007/978-3-319-75408-6_41,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047780529&doi=10.1007%2f978-3-319-75408-6_41&partnerID=40&md5=f0f8b41d22958732d779af0049bd4735,"Determining how to trade off individual criteria is often not obvious, especially when attributes of very different nature are juxtaposed, e.g. health and money. The difficulty stems both from the lack of adequate market experience and strong ethical component when valuing some goods, resulting in inherently imprecise preferences. Fuzzy sets can be used to model willingness-to-pay/accept (WTP/WTA), so as to quantify this imprecision and support the decision making process. The preferences need then to be estimated based on available data. In the paper, I show how to estimate the membership function of fuzzy WTP/WTA, when decision makers’ preferences are collected via survey with Likert-based questions. I apply the proposed methodology to a data set on WTP/WTA for health. The mathematical model contains two elements: the parametric representation of the membership function and the mathematical model how it is translated into Likert options. The model parameters are estimated in a Bayesian approach using Markov-chain Monte Carlo. The results suggest a slight WTP-WTA disparity and WTA being more fuzzy as WTP. The model is fragile to single respondents with lexicographic preferences, i.e. not willing to accept any trade-offs between health and money. © 2018, Springer International Publishing AG, part of Springer Nature."
,10.1093/imammb/dqx010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052222190&doi=10.1093%2fimammb%2fdqx010&partnerID=40&md5=8ead552019f3ae8f0738010091c16dcb,"We consider extensions to previous models for patient level nosocomial infection in several ways, provide a specification of the likelihoods for these new models, specify new update steps required for stochastic integration, and provide programs that implement these methods to obtain parameter estimates and model choice statistics. Previous susceptible-infected models are extended to allow for a latent period between initial exposure to the pathogen and the patient becoming themselves infectious, and the possibility of decolonization. We allow for multiple facilities, such as acute care hospitals or long-term care facilities and nursing homes, and for multiple units or wards within a facility. Patient transfers between units and facilities are tracked and accounted for in the models so that direct importation of a colonized individual from one facility or unit to another might be inferred. We allow for constant transmission rates, rates that depend on the number of colonized individuals in a unit or facility, or rates that depend on the proportion of colonized individuals. Statistical analysis is done in a Bayesian framework using Markov chain Monte Carlo methods to obtain a sample of parameter values from their joint posterior distribution. Cross validation, deviance information criterion and widely applicable information criterion approaches to model choice fit very naturally into this framework and we have implemented all three. We illustrate our methods by considering model selection issues and parameter estimation for data on methicilin-resistant Staphylococcus aureus surveillance tests over 1 year at a Veterans Administration hospital comprising seven wards. © The authors 2017."
,10.1093/MNRAS/STY2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055158105&doi=10.1093%2fMNRAS%2fSTY2015&partnerID=40&md5=d24cc6b11316cd9d1fbcdcf31bb889db,"Uncertainty quantification is a critical missing component in radio interferometric imaging that will only become increasingly important as the big-data era of radio interferometry emerges. Statistical sampling approaches to perform Bayesian inference, like Markov Chain Monte Carlo (MCMC) sampling, can in principle recover the full posterior distribution of the image, from which uncertainties can then be quantified. However, for massive data sizes, like those anticipated from the Square Kilometre Array, it will be difficult if not impossible to apply any MCMC technique due to its inherent computational cost. We formulate Bayesian inference problems with sparsity-promoting priors (motivated by compressive sensing), for which we recover maximum a posteriori (MAP) point estimators of radio interferometric images by convex optimization. Exploiting recent developments in the theory of probability concentration, we quantify uncertainties by post-processing the recovered MAP estimate. Three strategies to quantify uncertainties are developed: (i) highest posterior density credible regions, (ii) local credible intervals (cf. error bars) for individual pixels and superpixels, and (iii) hypothesis testing of image structure. These forms of uncertainty quantification provide rich information for analysing radio interferometric observations in a statistically robust manner. OurMAP-based methods are approximately 105 times faster computationally than state-of-theart MCMC methods and, in addition, support highly distributed and parallelized algorithmic structures. For the first time, our MAP-based techniques provide a means of quantifying uncertainties for radio interferometric imaging for realistic data volumes and practical use, and scale to the emerging big data era of radio astronomy. © 2018 The Author(s)."
,10.29220/CSAM.2018.25.1.043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044041996&doi=10.29220%2fCSAM.2018.25.1.043&partnerID=40&md5=969457fe80fa5bb4c5fe21325eb88695,"This paper considers the use of classical and Bayesian inference methods to analyze data generated by variables whose natural behavior can be modeled using asymmetric distributions in the presence of left censoring. Our approach used a Lévy distribution in the presence of left censored data and covariates. This distribution could be a good alternative to model data with asymmetric behavior in many applications as lifetime data for instance, especially in engineering applications and health research, when some observations are large in comparison to other ones and standard distributions commonly used to model asymmetry data like the exponential, Weibull or log-logistic are not appropriate to be fitted by the data. Inferences for the parameters of the proposed model under a classical inference approach are obtained using a maximum likelihood estimators (MLEs) approach and usual asymptotical normality for MLEs based on the Fisher information measure. Under a Bayesian approach, the posterior summaries of interest are obtained using standard Markov chain Monte Carlo simulation methods and available software like SAS. A numerical illustration is presented considering data of thyroglobulin levels present in a group of individuals with differentiated cancer of thyroid. © 2018 The Korean Statistical Society, and Korean International Statistical Society."
,10.1515/phys-2018-0062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052003425&doi=10.1515%2fphys-2018-0062&partnerID=40&md5=beefa6353434fe878b077c056c50475f,"A method to judge shale gas flow regimes based on digital core analysis is proposed in this work. Firstly, three-dimensional shale digital cores in an anonymous shale formation in the Sichuan Basin are reconstructed by a Markov Chain Monte Carlo (MCMC) algorithm based on two-dimensional Scanning Electron Microscope (SEM) images. Then a voxel-based method is proposed to calculate the characteristic length of the three-dimensional shale digital core. The Knudsen number for three-dimensional shale digital cores is calculated by the ratio of the molecular mean free path to the characteristic length and is used to judge the flow regimes under different reservoir conditions. The results indicate that shale gas flow regimes are mainly located at the slip flow and transition flow region. Furthermore, adsorption has no obvious influence on the free gas flow regimes. Because adsorption only exists in organic pores, three-dimensional inorganic pores and organic pores in the Haynesville shale formation are reconstructed by a MCMC algorithm based on two-dimensional SEM images. The characteristic lengths of the three-dimensional inorganic pores and three-dimensional organic pores are both calculated and gas flow regimes in organic pores and inorganic pores are judged. © 2018 W. Song et al., published by De Gruyter 2018."
,10.1002/sta4.184,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046788371&doi=10.1002%2fsta4.184&partnerID=40&md5=b461500da7914260cec25f0d211295a3,"In geostatistics, inference on spatial covariance parameters of the Gaussian process is often critical to scientists for understanding structural dependence in data. Finite-sample inference customarily proceeds either using posterior distributions from fully a Bayesian approach or via resampling/subsampling techniques in a frequentist setting. Resampling methods, in particular, the bootstrap, have become more attractive in the modern age of big data as, unlike Bayesian models that require sequential sampling from Markov chain Monte Carlo, they naturally lend themselves to parallel computing resources. However, a spatial bootstrap involves an expensive Cholesky decomposition to decorrelate the data. In this manuscript, we develop a highly scalable parametric spatial bootstrap that uses sparse Cholesky factors for parameter estimation and decorrelation. The proposed bootstrap for rapid inference on spatial covariances (BRISC) algorithm requires linear memory and computations and is embarrassingly parallel, thereby delivering substantial scalability. Simulation studies highlight the accuracy and computational efficiency of our approach. Analysing large satellite temperature data, BRISC produces inference that closely matches that delivered from a state-of-the-art Bayesian approach, while being several times faster. The R package BRISC is now available for download from GitHub (https://github.com/ArkajyotiSaha/BRISC) and will be available on CRAN soon. © 2018 John Wiley & Sons, Ltd."
1,10.1093/mnras/sty2160,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054755960&doi=10.1093%2fmnras%2fsty2160&partnerID=40&md5=3a03eec0e3c897b84fe3c3cd39e54860,"The standardmodel of cosmology, lambda cold darkmatter (ΛCDM), is the simplest model that matches the current observations, but it relies on two hypothetical components, to wit, dark matter and dark energy. Future galaxy surveys and cosmic microwave background (CMB) experiments will independently shed light on these components, but a joint analysis that includes cross-correlations will be necessary to extract as much information as possible from the observations. In this paper, we carry out a multiprobe analysis based on pseudo-spectra and test it on publicly available data sets. We use CMB temperature anisotropies and CMB lensing observations from Planck as well as the spectroscopic galaxy and quasar samples of SDSS-III/BOSS, taking advantage of the large areas covered by these surveys. We build a likelihood to simultaneously analyse the auto and cross spectra of CMB lensing and tracer overdensity maps before running Markov chain Monte Carlo to assess the constraining power of the combined analysis.We then add the CMBtemperature anisotropies likelihood and obtain constraints on cosmological parameters (H0, ωb, ωc, ln1010As, ns, and zre) and galaxy biases. We demonstrate that the joint analysis can additionally constrain the total mass of neutrinos ∑mv as well as the dark energy equation of state w at once (for a total of eight cosmological parameters), which is impossible with either of the data sets considered separately. Finally, we discuss limitations of the analysis related to, e.g. the theoretical precision of the models, particularly in the non-linear regime. © 2018 The Author(s)."
,10.1016/j.cegh.2018.02.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044162659&doi=10.1016%2fj.cegh.2018.02.009&partnerID=40&md5=c983203d75ff603c75991df2b230f224,"Background: We aimed to compare the treatments of esophageal cancer in association with the risk of disease recurrence, and to compare the frequentist and Bayesian methods in data analysis. Methods: Web of Science, Medline, Scopus, the Cochrane Library and EMBASE were searched. We assessed statistical heterogeneity, and the assumption of consistency was assessed using loop-specific and design-by-treatment interaction methods. The Markov chain Monte Carlo method was used to obtain the pooled estimates of effect size in the Bayesian approach. The random effect model was used to report the pooled Risk Ratios (RR). The results of this study were reported with 95% Confidence Interval (CI), and Credible interval (CrI). Results: We included 17 randomized controlled trials (RCTs) that reported the local recurrence of esophageal cancer. The RR of local recurrence for surgery plus paclitaxel, cisplatin, and radiotherapy (SPCRT) compared to surgery alone was 0.42 (95% CI: 0.21, 0.88). The RR for SPCRT compared with surgery plus cisplatin and vindesine (SCV) was 0.38 (95% CI: 0.12, 1.18). Compared to cisplatin, fluorouracil, and radiotherapy plus surgery (CFRTS), SPCRT was a better treatment (RR = 0.46, 95% CI: 0.17, 1.24). Conclusions: This network meta-analysis indicated that SPCRT compared to surgery alone and other treatments was a better treatment. In terms of ranking, SPCRT and radiotherapy plus surgery (RTS) were the first and second treatments in the network. It seems the precision of frequentist is better than Bayesian approach, however, the results of the ranking of treatments were same in both frequentist and Bayesian approaches. © 2018"
2,10.1111/rssc.12220,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016420357&doi=10.1111%2frssc.12220&partnerID=40&md5=68af52e08fb4dc4dbd933f45620cf190,"Many randomized controlled trials report more than one primary outcome. As a result, multivariate meta-analytic methods for the assimilation of treatment effects in systematic reviews of randomized controlled trials have received increasing attention in the literature. These methods show promise with respect to bias reduction and efficiency gain compared with univariate meta-analysis. However, most methods for multivariate meta-analysis have focused on pairwise treatment comparisons (i.e. when the number of treatments is 2). Current methods for mixed treatment comparisons meta-analysis (i.e. when the number of treatments is more than 2) have focused on univariate or, very recently, bivariate outcomes. To broaden their application, we propose a framework for mixed treatment comparisons meta-analysis of multivariate (two or more) outcomes where the correlations between multivariate outcomes within and between studies are accounted for through copulas, and the joint modelling of multivariate random effects respectively. We consider a Bayesian hierarchical model using Markov chain Monte Carlo methods for estimation. An important feature of the framework proposed is that it allows for borrowing of information across correlated outcomes. We show via simulation that our approach reduces the effect of outcome reporting bias in a variety of missing outcome scenarios. We apply the method to a systematic review of randomized controlled trials of pharmacological treatments for alcohol dependence, which tends to report multiple outcomes potentially subject to outcome reporting bias. © 2017 Royal Statistical Society"
,10.1111/rssc.12298,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050888680&doi=10.1111%2frssc.12298&partnerID=40&md5=f4948a9109ae975b1dbd3e16c5403d84,"Ranking sportsmen whose careers took place in different eras is often a contentious issue and the topic of much debate. We focus on cricket and examine what conclusions may be drawn about the ranking of test batsmen by using data on batting scores from the first test in 1877 onwards. The overlapping nature of playing careers is exploited to form a bridge from past to present so that all players can be compared simultaneously, rather than just relative to their contemporaries. The natural variation in runs scored by a batsman is modelled by an additive log-linear model with year, age and cricket-specific components used to extract the innate ability of an individual cricketer. Incomplete innings are handled via censoring and a zero-inflated component is incorporated in the model to allow for an excess of frailty at the start of an innings. The innings-by-innings variation of runs scored by each batsman leads to uncertainty in their ranking position. A Bayesian approach is used to fit the model and realizations from the posterior distribution are obtained by deploying a Markov chain Monte Carlo algorithm. Posterior summaries of innate player ability are then used to assess uncertainty in ranking position and this is contrasted with rankings determined via the posterior mean runs scored. Posterior predictive checks show that the model provides a reasonably accurate description of runs scored. © 2018 The Royal Statistical Society and Blackwell Publishing Ltd."
,10.1007/s11222-018-9826-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051553582&doi=10.1007%2fs11222-018-9826-2&partnerID=40&md5=540cc3c110eaf81f30e4e49b02b1e67b,"It is well known that Markov chain Monte Carlo (MCMC) methods scale poorly with dataset size. A popular class of methods for solving this issue is stochastic gradient MCMC (SGMCMC). These methods use a noisy estimate of the gradient of the log-posterior, which reduces the per iteration computational cost of the algorithm. Despite this, there are a number of results suggesting that stochastic gradient Langevin dynamics (SGLD), probably the most popular of these methods, still has computational cost proportional to the dataset size. We suggest an alternative log-posterior gradient estimate for stochastic gradient MCMC which uses control variates to reduce the variance. We analyse SGLD using this gradient estimate, and show that, under log-concavity assumptions on the target distribution, the computational cost required for a given level of accuracy is independent of the dataset size. Next, we show that a different control-variate technique, known as zero variance control variates, can be applied to SGMCMC algorithms for free. This postprocessing step improves the inference of the algorithm by reducing the variance of the MCMC output. Zero variance control variates rely on the gradient of the log-posterior; we explore how the variance reduction is affected by replacing this with the noisy gradient estimate calculated by SGMCMC. © 2018, The Author(s)."
,10.1111/jomf.12527,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052496054&doi=10.1111%2fjomf.12527&partnerID=40&md5=8bd8fe8a987a34dcc82a43191070014e,"Objective: This study assesses whether parenthood influences repartnering for women and men and explores how repartnering is associated with parental status of the prospective partners. Background: Previous research has not demonstrated whether gender differences in repartnering are conditional on the presence of children. This study aims to better disentangle the specific gender differentials in repartnering probabilities conditional on parenthood and child custody status. Method: The analytical sample consists of 5,372 women and 3,375 men who reported at least one partnership dissolution in the British Understanding Society survey. Multilevel event history models with Markov Chain Monte Carlo simulations are used to estimate the probabilities of (a) finding a new partner and (b) finding a new childless partner or a new partner who has children. Results: The results suggest that mothers, and to a lesser extent fathers, are less likely to repartner than their childless counterparts. Among parents who have child custody, there emerges a distinct gender gap because mothers exhibit a significantly lower rate of repartnering than fathers. Finally, coresident single parents are relatively less likely to repartner with childless individuals, and single fathers more frequently form two-parent stepfamilies than do mothers. Conclusion: This suggests the presence of a gender divide in repartnering that is especially apparent when child custody is taken into account. The presence of children also reduces the possibility of forming unions with childless individuals. © 2018 National Council on Family Relations"
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032707507&partnerID=40&md5=9a82a9919f79312cdcb884d843a43f8f,"The proceedings contain 34 papers. The special focus in this conference is on Intelligent Systems Technologies and Applications. The topics include: Markov Chain monte carlo methods and evolutionary algorithms for automatic feature selection from legal documents; an adaptive soft set based diagnostic risk prediction system; weighted bipartite graph model for recommender system using entropy based similarity measure; automated quiz generator; temporal modelling of bug numbers of open source software applications using LSTM; direct demodulator for amplitude modulated signals using artificial neural network; real-time detection of atrial fibrillation from short time single lead ECG traces using recurrent neural networks; styloLIT: Stylometry and location indicative terms based geographic location estimation using convolutional neural networks; an energy-efficient fuzzy based data fusion and tree based clustering algorithm for wireless sensor networks; eMG pattern classification using neural networks; crime against women: A state level analysis using a hierarchical and k-means clustering techniques; zero pronouns and their resolution in Sanskrit texts; semantic analysis using pairwise sentence comparison with word embeddings; illuminant color inconsistency as a powerful clue for detecting digital image forgery: A survey; a fast, block based, copy-move forgery detection approach using image gradient and modified K-means; oR operation based deterministic extended visual cryptography using complementary cover images; empirical comparison of different key frame extraction approaches with differential evolution based algorithms; breast cancer diagnosis and prognosis using machine learning techniques; performance assessment framework for computational models of visual attention; biologically-inspired foraging decision making in distributed cognitive radio networks; pAM4-based RADAR counter-measures in hostile environments."
,10.3923/ajsr.2018.376.382,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048624986&doi=10.3923%2fajsr.2018.376.382&partnerID=40&md5=8334a7c636184ca75b2b8125f18ac44b,"Background and Objective: The Weibull distribution is widely used to model and to analyze data on the survival time. Bayesian estimation approach has received much attention as it has been in contention with other estimation methods. In this study, it was examined the performance of the Bayesian estimator using conjugate prior information for estimating the parameters of Weibull distribution with censored survival data for dengue fever (DF). Materials and Methods: Through the simulated Weibull distributed survival dataset, the performance of conjugate estimator for estimating the Weibull distribution parameters can be checked before applying it to the DF survival dataset in Makassar, Indonesia. Statistical analysis of the simulation data and collected DF data were analyzed through summary tables and Markov Chain Monte Carlo method via Gibbs sampling algorithm. It was performed using R version 3.3.3 and Win BUGS. Results: Based on the simulation study, the mean of posterior means of all Weibull distribution parameter estimates are still reasonably accurate. After fitting the Weibull models to the DF survival time's dataset using the conjugate prior distribution, the age factor substantially described DF patients' survival times and had a positive effect on the estimated survival time. Conclusion: To choose sample size and censoring level, the estimates generated by the conjugate prior do not only depend on the data but also on the parameters of the prior distribution. The amount of uncensored data must be more than one in order to obtain an estimate of greater than zero. The results of estimated parameters of Weibull model using conjugate prior either with simulated survival data or the DF data is good. © 2018 Sri Astuti Thamrin et al."
,10.1016/j.prevetmed.2017.10.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033441699&doi=10.1016%2fj.prevetmed.2017.10.009&partnerID=40&md5=0fadfebddd4bdf5ece7ab8229f97bfde,"Paratuberculosis or Johne's disease (JD), is a chronic infectious disease causing intractable diarrhea in cattle, which leads to less productivity, such as decreased milk yield, and lower daily weight gain. As a control measure against JD in cattle, national serological surveillance has been conducted in Japan since 1998. To conduct modeling studies that are useful to evaluate the effectiveness of control measures against JD, reliable parameter values, such as length of time from infection to the start of fecal shedding or antibody expression, are especially important. These parameters in the Japanese cattle population are assumed to be different from those in other countries with a higher prevalence of JD or in experimental infection settings; therefore, they must be estimated for the cattle population in Japan. Data from national surveillance conducted in Tokachi District, Hokkaido Prefecture, were used for this study. Using data from JD diagnostic tests for all cattle in Tokachi District between 1998 and 2014, all testing histories for infected animals were estimated as the number of tested cattle and positive cattle at each age of month for both fecal and antibody tests. A deterministic mathematical model for JD development, from infection to fecal shedding and antibody expression in infected cattle, was constructed to obtain the probability of testing positive when applied to both fecal and antibody tests at a given age. Likelihood was obtained from these estimated test results and best values for parameters were obtained using the Markov Chain Monte-Carlo method. Fifty-five percent of infected cattle were projected to have a transient shedding period, which was estimated to start 12 months after infection and last for 4 months. Persistent shedding was projected to occur in all infected cattle, and estimated to begin 7–84 months from infection. Following persistent shedding, antibody expression was estimated to start 7 months later. These values are useful for developing models to evaluate the status of JD infection and the effectiveness of control measures in the Japanese cattle population. © 2017 Elsevier B.V."
14,10.1093/mnras/stx2443,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040256870&doi=10.1093%2fmnras%2fstx2443&partnerID=40&md5=8f8ac49cef4d522ac2a420f65873abdb,"In the light of the recent Planck downward revision of the electron scattering optical depth, and of the discovery of a faint active galactic nuclei (AGN) population at z &gt; 4, we reassess the actual contribution of quasars to cosmic reionization. To this aim, we extend our previous Markov Chain Monte Carlo based data-constrained semi-analytic reionization model and study the role of quasars on global reionization history. We find that the quasars can alone reionize the Universe only for models with very high AGN emissivities at high redshift. These models are still allowed by the recent cosmic microwave background data and most of the observations related to HI reionization. However, they predict an extended and early He II reionization ending at z ≳ 4 and a much slower evolution in the mean He II Ly-α forest opacity than what the actual observation suggests. Thus, when we further constrain our model against the He II Ly-α forest data, this AGN-dominated scenario is found to be clearly ruled out at 2σ limits. The data seems to favour a standard two-component picture where quasar contributions become negligible at z ≳ 6 and a non-zero escape fraction of ~10 per cent is needed from early-epoch galaxies. For such models, mean neutral hydrogen fraction decreases to ~10-4 at z = 6.2 from ~0.8 at z = 10.0 and helium becomes doubly ionized at much later time, z ~ 3. We find that these models are as well in good agreement with the observed thermal evolution of IGM as opposed to models with very high AGN emissivities. © 2018 The Author(s)."
1,10.3847/1538-4357/aa9c81,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040250094&doi=10.3847%2f1538-4357%2faa9c81&partnerID=40&md5=2fe383317e76faf703fbd6a886f0a326,"We present a new measurement of the Lyα forest power spectrum at 1.8 &lt; z &lt; 3.4 using 74 Keck/HIRES and VLT/UVES high-resolution, high-signal-to-noise-ratio quasar spectra. We developed a custom pipeline to measure the power spectrum and its uncertainty, which fully accounts for finite resolution and noise and corrects for the bias induced by masking missing data, damped Lyα absorption systems, and metal absorption lines. Our measurement results in unprecedented precision on the small-scale modes k &gt; 0.02 s km-1, inaccessible to previous SDSS/BOSS analyses. It is well known that these high-k modes are highly sensitive to the thermal state of the intergalactic medium, but contamination by narrow metal lines is a significant concern. We quantify the effect of metals on the small-scale power and find a modest effect on modes with k &lt; 0.1 s km-1. As a result, by masking metals and restricting to k &lt; 0.1 s km-1, their impact is completely mitigated. We present an end-to-end Bayesian forward-modeling framework whereby mock spectra with the same noise, resolution, and masking as our data are generated from Lyα forest simulations. These mock spectra are used to build a custom emulator, enabling us to interpolate between a sparse grid of models and perform Markov chain Monte Carlo fits. Our results agree well with BOSS on scales &lt; 0.02 s km-1, where the measurements overlap. The combination of the percent-level low-k precision of BOSS with our 5%-15% high-k measurements results in a powerful new data set for precisely constraining the thermal history of the intergalactic medium, cosmological parameters, and the nature of dark matter. The power spectra and their covariance matrices are provided as electronic tables. © 2017. The American Astronomical Society. All rights reserved."
,10.1111/ane.13025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053729753&doi=10.1111%2fane.13025&partnerID=40&md5=ae1706a8a1bca60e0c7f0b378aba112d,"Second and third generation AEDs have been directly compared to controlled-release carbamazepine (CBZ-CR) as initial monotherapy for new-onset focal epilepsy. Conversely, no head-to-head trials have been performed. The aim of this study was to estimate the comparative efficacy and tolerability of the antiepileptic monotherapies in adults with newly diagnosed focal epilepsy through a network meta-analysis (NMA). Randomized, double-blinded, parallel group, monotherapy studies comparing any AED to CBZ-CR in adults with newly diagnosed untreated epilepsy with focal-onset seizures was identified. The outcome measures were the seizure freedom for 6 and 12 months, the occurrence of treatment-emergent adverse events (TEAEs), and the treatment withdrawal due to TEAEs. Mixed treatment comparisons were conducted by a Bayesian NMA using the Markov chain Monte Carlo methods. Effect sizes were calculated as odds ratios (ORs) with 95% credible intervals (CrIs). Four trials were included involving 2856 participants, 1445 for CBZ-CR and 1411 for the comparative AEDs. Monotherapy AEDs compared to CBR-CR were levetiracetam (LEV), zonisamide (ZNS), lacosamide (LCM), and eslicarbazepine acetate (ESL). There were no statistical differences in the 6- and 12-month seizure freedom and TEAEs occurrence between LEV, ZNS, LCM, ESL, and CBZ-CR In the analysis of drug withdrawal due to TEAEs, LCM treatment was associated with a significantly lower discontinuation rate than CBZ-CR (OR 0.659, 95% CrI 0.428-0.950). LEV, ZNS, LCM, and ESL are effective initial monotherapy treatments in adult patients with newly diagnosed focal epilepsy and represent suitable alternatives to CBZ-CR. © 2018 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd"
2,10.1289/EHP1289,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041418759&doi=10.1289%2fEHP1289&partnerID=40&md5=e155d5d018fbf6be2d79c2c3dd492fcb,"BACKGROUND: Benchmark dose (BMD) modeling is an important step in human health risk assessment and is used as the default approach to identify the point of departure for risk assessment. A probabilistic framework for dose–response assessment has been proposed and advocated by various institutions and organizations; therefore, a reliable tool is needed to provide distributional estimates for BMD and other important quantities in dose– response assessment. OBJECTIVES: We developed an online system for Bayesian BMD (BBMD) estimation and compared results from this software with U.S. Environmental Protection Agency’s (EPA’s) Benchmark Dose Software (BMDS). METHODS: The system is built on a Bayesian framework featuring the application of Markov chain Monte Carlo (MCMC) sampling for model parameter estimation and BMD calculation, which makes the BBMD system fundamentally different from the currently prevailing BMD software packages. In addition to estimating the traditional BMDs for dichotomous and continuous data, the developed system is also capable of computing model-averaged BMD estimates. RESULTS: A total of 518 dichotomous and 108 continuous data sets extracted from the U.S. EPA’s Integrated Risk Information System (IRIS) database (and similar databases) were used as testing data to compare the estimates from the BBMD and BMDS programs. The results suggest that the BBMD system may outperform the BMDS program in a number of aspects, including fewer failed BMD and BMDL calculations and estimates. CONCLUSIONS: The BBMD system is a useful alternative tool for estimating BMD with additional functionalities for BMD analysis based on most recent research. Most importantly, the BBMD has the potential to incorporate prior information to make dose–response modeling more reliable and can provide distributional estimates for important quantities in dose–response assessment, which greatly facilitates the current trend for probabilistic risk assessment. © 2018, Public Health Services, US Dept of Health and Human Services. All rights reserved."
,10.1017/ice.2017.241,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038252824&doi=10.1017%2fice.2017.241&partnerID=40&md5=ac03dcb37d71eadffe232d5d101f8184,"BACKGROUND Extended-spectrum β-lactamase-producing Enterobacteriaceae (ESBL-E) are emerging worldwide. Contact precautions are recommended for known ESBL-E carriers to control the spread of ESBL-E within hospitals. OBJECTIVE This study quantified the acquisition of ESBL-E rectal carriage among patients in Dutch hospitals, given the application of contact precautions. METHODS Data were used from 2 cluster-randomized studies on isolation strategies for ESBL-E: (1) the SoM study, performed in 14 Dutch hospitals from 2011 through 2014 and (2) the R-GNOSIS study, for which data were limited to those collected in a Dutch hospital in 2014. Perianal cultures were obtained, either during ward-based prevalence surveys (SoM), or at admission and twice weekly thereafter (R-GNOSIS). In both studies, contact precautions were applied to all known ESBL-E carriers. Estimates for acquisition of ESBL-E were based on the results of admission and discharge cultures from patients hospitalized for more than 2 days (both studies) and a Markov chain Monte Carlo (MCMC) model, applied to all patients hospitalized (R-GNOSIS). RESULTS The absolute risk of acquisition of ESBL-E rectal carriage ranged from 2.4% to 2.9% with an ESBL-E acquisition rate of 2.8 to 3.8 acquisitions per 1,000 patient days. In addition, 28% of acquisitions were attributable to patient-dependent transmission, and the per-admission reproduction number was 0.06. CONCLUSIONS The low ESBL-E acquisition rate in this study demonstrates that it is possible to control the nosocomial transmission of ESBL in a low-endemic, non-ICU setting where Escherichia coli is the most prevalent ESBL-E and standard and contact precautions are applied for known ESBL-E carriers. © 2017 by The Society for Healthcare Epidemiology of America."
,10.1002/ecs2.2046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040547401&doi=10.1002%2fecs2.2046&partnerID=40&md5=4730218cd6f7eef800bfde86d83b86b6,"Leaf area index (LAI) is often used to quantify plant production and evapotranspiration with terrestrial ecosystem models (TEMs). This study evaluated the LAI simulation in North America using a data assimilation technique and a process-based TEM as well as in situ and satellite data. We first optimized the parameters related to LAI in the TEM using a Markov Chain Monte Carlo method, and Ameri- Flux site-level and regional LAI data from advanced very high-resolution radiometer. The parameterized model was then verified with the observed monthly LAI of major ecosystem types at site level. Simulated LAI was compared well with the observed data at sites of Harvard Forest (R2 = 0.96), University of Michigan Biological Station (R2 = 0.87), Howland Forest (R2 = 0.96), Morgan Monroe State Forest (R2 = 0.85), Shidler Tallgrass Prairie (R2 = 0.82), and Donaldson (R2 = 0.75). The root-mean-square error (RMSE) between modeled and satellite-based monthly LAI in North America is 1.4 m2/m2 for the period of 1985- 2010. The simulated average monthly LAI in recent three decades increased by (3 0.5)% in the region, with 1.24, 1.46, and 2.21 m2/m2 on average, in Alaska, Canada, and the conterminous United States, respectively, which is consistent with satellite data. The model performed well for wet tundra, boreal forest, temperate coniferous forests, temperate deciduous forests, grasslands, and xeric shrublands (RMSE &lt; 1.5 m2/m2), but not for alpine tundra and xeric woodlands (RMSE &gt; 1.5 m2/m2). Both the spring and fall LAI in the 2000s are higher than that in the 1980s in the region, suggesting that the leaf phenology has an earlier onset and later senescence in the 2000s. The average LAI increased in April and September by 0.03 and 0.24 m2/m2, respectively. This study provides a way to quantify LAI with ecosystem models, which will improve future carbon and water cycling studies. © 2018 Qu and Zhuang."
,10.1186/s13071-017-2588-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042625734&doi=10.1186%2fs13071-017-2588-4&partnerID=40&md5=74cd011a350825b4b8d287e741f57811,"Background Dengue remains an important public health problem in Timor-Leste, with several major epidemics occurring over the last 10 years. The aim of this study was to identify dengue clusters at high geographical resolution and to determine the association between local environmental characteristics and the distribution and transmission of the disease. Methods Notifications of dengue cases that occurred from January 2005 to December 2013 were obtained from the Ministry of Health, Timor-Leste. The population of each suco (the third-level administrative subdivision) was obtained from the Population and Housing Census 2010. Spatial autocorrelation in dengue incidence was explored using Moran's I statistic, Local Indicators of Spatial Association (LISA), and the Getis-Ord statistics. A multivariate, Zero-Inflated, Poisson (ZIP) regression model was developed with a conditional autoregressive (CAR) prior structure, and with posterior parameters estimated using Bayesian Markov chain Monte Carlo (MCMC) simulation with Gibbs sampling. Results The analysis used data from 3206 cases. Dengue incidence was highly seasonal with a large peak in January. Patients ≥ 14 years were found to be 74% [95% credible interval (CrI): 72-76%] less likely to be infected than those < 14 years, and females were 12% (95% CrI: 4-21%) more likely to suffer from dengue as compared to males. Dengue incidence increased by 0.7% (95% CrI: 0.6-0.8%) for a 1 °C increase in mean temperature; and 47% (95% CrI: 29-59%) for a 1 mm increase in precipitation. There was no significant residual spatial clustering after accounting for climate and demographic variables. Conclusions Dengue incidence was highly seasonal and spatially clustered, with positive associations with temperature, precipitation and demographic factors. These factors explained the observed spatial heterogeneity of infection. © 2018, The Author(s)."
,10.1016/j.jaip.2018.08.036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054453175&doi=10.1016%2fj.jaip.2018.08.036&partnerID=40&md5=b7330d48cb7e4ebb67efa6607139a368,"Background: The interaction of IL-5 with its receptor on eosinophils increases the activation and maintenance of eosinophils; blocking this interaction reduces asthma symptoms in patients with the eosinophilic phenotype. Reslizumab, which binds to IL-5, and benralizumab, which targets the IL-5 receptor α subunit, have not been compared in head-to-head trials. Objective: To indirectly compare reslizumab with benralizumab in similar patient populations using a network meta-analysis. Methods: A systematic literature review was conducted and a network meta-analysis was performed on eligible studies using the Markov Chain Monte-Carlo simulation method and a Bayesian statistical framework. Results: Eleven studies were identified, 4 of which evaluated clinically relevant doses and had outcomes at similar time points. To control for population differences, subgroups were selected for the base-case efficacy analysis: a benralizumab subgroup with blood eosinophil levels of greater than or equal to 300 cells/μL (n = 1537) and a reslizumab subgroup in Global Initiative for Asthma step 4/5 with 2 or more previous exacerbations and greater than or equal to 400 eosinophils/μL (n = 318). Safety was analyzed in the full population (N = 3462). Reslizumab significantly improved Asthma Control Questionnaire (ACQ) and Asthma Quality of Life Questionnaire (AQLQ) scores compared with benralizumab once every 4 weeks and there were reasonably high posterior probabilities that reslizumab is superior to benralizumab once every 4 weeks and once every 8 weeks for ACQ score, AQLQ score, FEV1, and clinical asthma exacerbations. Conclusions: This indirect comparison suggests that reslizumab may be more efficacious than benralizumab in patients with eosinophilic asthma in Global Initiative for Asthma step 4/5 with elevated blood eosinophil levels (benralizumab, ≥300/μL; reslizumab, ≥400/μL) and 2 or more exacerbations in the previous year. © 2018 American Academy of Allergy, Asthma & Immunology"
2,10.1007/s00168-015-0705-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944570061&doi=10.1007%2fs00168-015-0705-x&partnerID=40&md5=9237c84ec9b15389d36139fcab7a7837,"Applications of spatial probit regression models that have appeared in the literature have incorrectly interpreted estimates from these models. Spatially dependent choices frequently arise in various modeling scenarios, including situations involving analysis of regional voting behavior, decisions by states or cities to change tax rates relative to neighboring jurisdictions, decisions by households to move or stay in a particular location. We use county-level voting results from the 2004 presidential election as an illustrative example of some issues that arise when drawing inferences from spatial probit model estimates. Although the voting example holds particular intuitive appeal that allows us to focus on interpretive issues, there are numerous other situations where these same considerations come into play. Past work regarding Bayesian Markov Chain Monte Carlo estimation of spatial probit models from LeSage and Pace (Introduction to spatial econometrics. Taylor and Francis, New York, 2009) is used, as well as derivations from LeSage et al. (J R Stat Soc Ser A Stat Soc 174(4):1007–1027, 2011) regarding proper interpretation of the partial derivative impacts from changes in the explanatory variables on the probability of voting for a candidate. As in the case of conventional probit models, the effects arising from changes in the explanatory variables depend in a nonlinear way on the levels of these variables. In non-spatial probit regressions, a common way to explore the nonlinearity in this relationship is to calculate “marginal effects” estimates using particular values of the explanatory variables (e.g., mean values or quintile intervals). The motivation for this practice is consideration of how the impact of changing explanatory variable values varies across the range of values encompassed by the sample data. Given the nonlinear nature of the normal cumulative density function transform on which the (non-spatial) probit model relies, we know that changes in explanatory variable values near the mean may have a very different impact on decision probabilities than changes in very low or high values. For spatial probit regression models, the effects or impacts from changes in the explanatory variables are more highly nonlinear. In addition, since spatial models rely on observations that each represent a location or region located on a map, the levels of the explanatory variables can be viewed as varying over space. We discuss important implications of this for proper interpretation of spatial probit regression models in the context of our election application. © 2015, Springer-Verlag Berlin Heidelberg."
,10.1159/000493951,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054994407&doi=10.1159%2f000493951&partnerID=40&md5=d10cffba4381285fa6778b2922803ee6,"Background/Aims: Urothelial cancer (UC) as a chemotherapy-sensitive tumor, has achieved remarkable progresses in therapeutic paradigm, particularly in the advanced/metastatic stages. However, both clinicians and patients are confused when it comes to choosing the optimal chemotherapy. Hence, this article was aimed to conduct a comprehensive comparison of different chemotherapy regimens for advanced or metastatic UC in terms of survival benefits or adverse events. Methods: The online databases PubMed, EMBASE and Web of Science were searched systematically and comprehensively for randomized controlled trials (RCTs) up to September 15, 2017. The pooled hazard ratios (HRs) or odds ratios (ORs) with 95% credible interval (CrI) were calculated by Markov chain Monte Carlo methods. The effectiveness and safety of included regimens were conducted to provide a hierarchy by means of rank probabilities with the help of ""R-3.4.0"" software and the ""gemtc-0.8.2"" package. The surface under the cumulative ranking curve (SUCRA) was also incorporated in our analysis for ranking the corresponding chemotherapy regimens. Results: Ten different chemotherapy regimens involved in this article were predominantly of trials in a first-line setting, and eight clinical outcomes were ultimately analyzed in this study. In terms of Overall response rate (ORR), Overall survival (OS) or Progression-free survival (PFS)/Time to progression (TTP), the rank probabilities and SUCRA indicated that Paclitaxel/cisplatin/gemcitabine (PCG) was superior to gemcitabine/cisplatin (GC) or methotrexate/vinblastine/doxorubicin/cisplatin (MVAC), the traditional first-line treatment for advanced/metastatic UC. In the case of ORR or PFS/TTP, GC+sorafenib also displayed its superiority in comparison with GC or MVAC. Despite their survival benefits, PCG or GC+sorafenib presented a relatively higher incidence of adverse events. Conclusion: Our results revealed that by adding a paclitaxel or sorafenib into the first-line GC, it could yield a better survival benefit, but also worsen adverse events for advanced/ metastatic UC. Clinically, physicians should weigh the merits of these approaches to maximize the survival benefits of eligible patients. © 2018 The Author(s). Published by S. Karger AG, Basel."
,10.1093/milmed/usx015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044634522&doi=10.1093%2fmilmed%2fusx015&partnerID=40&md5=8c29bde341ea1982fa2131e0284cef2f,"Background: Tobacco use is a major concern to the Military Health System of the Department of Defense (DoD). The 2011 DoD Health Related Behavior Survey reported that 24.5% of active duty personnel are current smokers, which is higher than the national estimate of 20.6% for the civilian population. Overall, it is estimated that tobacco use costs the DoD $1.6 billion a year through related medical care, increased hospitalization, and lost days of work, among others. Methods: This study evaluated future health outcomes of Tricare Prime beneficiaries aged 18-64 yr (N = 3.2 million, including active duty and retired military members and their dependents) and the potential economic impact of initiatives that DoD may take to further its effort to transform the military into a tobacco-free environment. Our analysis simulated the future smoking status, risk of developing 25 smoking-related diseases, and associated medical costs for each individual using a Markov Chain Monte Carlo microsimulation model. Data sources included Tricare administrative data, national data such as Centers for Disease Control and Prevention mortality data and National Cancer Institute's cancer registry data, as well as relative risks of diseases obtained from a literature review. Findings: We found that the prevalence of active smoking among the Tricare Prime population will decrease from about 24% in 2015 to 18% in 2020 under a status quo scenario. However, if a comprehensive tobacco control initiative that includes a 5% price increase, a tighter clean air policy, and an intensified media campaign were to be implemented between 2016 and 2020, the prevalence of smoking could further decrease to 16%. The near 2 percentage points reduction in smoking prevalence represents an additional 81,240 quitters and translates to a total lifetime medical cost savings (in 2016 present value) of $968 million, with 39% ($382 million) attributable to Tricare savings. Discussion: A comprehensive tobacco control policy within the DoD could significantly decrease the prevalence and lifetime medical cost of tobacco use. If the smoking prevalence among Prime beneficiaries could reach the Healthy People 2020 goal of 12%, through additional measures, the lifetime savings could mount to $2.08 billion. To achieve future savings, DoD needs to pay close attention to program design and implementation issues of any additional tobacco control initiatives. © Association of Military Surgeons of the United States 2017."
,10.1007/978-3-319-70942-0_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037824086&doi=10.1007%2f978-3-319-70942-0_10&partnerID=40&md5=c13fe16d7fa0f30002982cb6c6db4209,"Finding effective methods to compute or estimate posterior distributions of model parameters is of paramount importance in Bayesian statistics. In fact, Bayesian inference has only been extraordinarily popular in applications after the births of efficient algorithms like the Monte Carlo Markov Chain. Practicality of posterior distributions depends heavily on the combination of likelihood functions and prior distributions. In certain cases, closed-form formulas for posterior distributions can be attained; in this paper, based on the theory of distortion functions, a calibration-like method to calculate explicitly the posterior distributions for three crucial models, namely the normal, Poisson and Bernoulli is introduced. The paper ends with some applications in stock market. © Springer International Publishing AG 2018."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053018955&partnerID=40&md5=7aae97762cfea7d045a3c55ce474ac5c,"Background. Most growth analyses of Yellowtail Snapper neglect consideration of model and parameter uncertainty. Goals. In this paper, we explore model uncertainty using three models (von Bertalanffy, logistic, and Gompertz) as well as the Akaike criterion for model selection. We also estimate growth parameters and its uncertainty using the maximum likelihood estimation approach (under different assumptions of error variance) and Bayesian methods. Methods. Models were fitted to length-at-age data from organisms caught in Antón Lizardo, Veracruz. Regarding the Bayesian methods, a prior distribution for the asymptotic length was built based on data gathered from literature. We used Monte Carlo Markov Chains (MCMC) methods to fit the logistic model. Results. The Akaike criterion results suggest that the logistic model provided the best fit for the observed data (lowest AIC = 31.4). Parameter estimates included asymptotic length (L∞ = 64.9 ± 5.43), growth rate (K = 0.49 ± 0.07), and age at the curve inflection point (I = 3.28 ± 0.42). Regarding the Bayesian analysis, MCMC simulations suggest that the most probable value for the asymptotic length was 64.3 cm with an interval of 95% probability (58.7, 70.1). The most probable value for the growth rate was 0.48 with an interval of 95% probability (0.42, 0.55). Last, the most probable value for the age at the curve inflection point was 1.7 years with a range of 95% probability (1.31, 2.16). Conclusions. The maximum likelihood estimation (MLE) and the Bayesian framework should be considered basic statistical techniques in the evaluation of individual growth of the species of interest, as they provide a robust analysis of available information of the species and the opportunity to incorporate such analysis to sustainable management practices. © 2018 Universidad Autonoma Metropolitana. All rights reserved."
,10.1080/21645515.2018.1515455,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053877092&doi=10.1080%2f21645515.2018.1515455&partnerID=40&md5=52d69c2223d2b8897bec5a271c517b9b,"The live-attenuated Japanese encephalitis chimeric virus vaccine JE-CV (IMOJEV®, Sanofi Pasteur) elicits a robust antibody response in children, which wanes over time. Clinical efficacy is based on a correlate of protection against JE infection defined as neutralizing antibody levels equal to or greater than the threshold of 10 (1/dil). Information on the duration of persistence of the JE antibody response above this threshold is needed. We constructed statistical models using 5-year persistence data from a randomised clinical trial (NCT00621764) in children (2–5 years old) primed with inactivated JE vaccine who received a booster dose of JE-CV, and in JE-naïve toddlers (12–24 months) who received a JE-CV single dose primary vaccination. Models were constructed using a Bayesian Monte-Carlo Markov Chain approach and implemented with OpenBugs V3.2.1. Antibody persistence was predicted for up to 10 years following JE-CV vaccination. Findings from a piecewise model with 2 phases (children) and a classic linear model (toddlers) are presented. For children, predicted median antibody titers (77 [2.5th–97.5th percentile range 41–144] 1/dil) remained above the threshold for seroprotection over the 10 years following booster JE-CV vaccination; the predicted median duration of protection was 19.5 years. For toddlers, 10 years after JE-CV primary vaccination median antibody titers were predicted to wane to around the level required for seroprotection (10.8 [5.8–20.1] 1/dil). A booster dose of JE-CV in children is predicted to provide long-term protection against JE. Such data are useful to facilitate decisions on implementation of and recommendations for future vaccination strategies. © 2018, © 2018 Sanofi Pasteur. Published with license by Taylor & Francis Group, LLC."
,10.2495/ST180241,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054623014&doi=10.2495%2fST180241&partnerID=40&md5=ac6faea82103c12b7763c999665fc637,"This paper is intended for clarifying and forecasting the dynamic structural components inside the relationship between tourism income and economic cycling parameters. Quarterly time-series variables such as Thailand tourism revenues and gross domestic products are collected during the period of 1997 to 2016. The simulation methods using the Marko Chain Monte-Carlo approach (MCMC) and Metropolis-Hasting algorithm (MH) which are Bayesian seasonal unit-root testing (B-HEGY), Markov Switching Bayesian VAR (MSBVAR), and Bayesian Dynamic Stochastic General Equilibrium modelling (B-DSGE). Empirically, the results estimated by the MSBVAR model strongly confirm that the business cycles of Thailand tourism are divided into two stages based on business cycle facts, which are in high-season and low-season periods. From the results of the B-DSGE model, the outcomes represent both capital and labour factors of tourism sectors in high-seasonal moments that are positive for Thailand economic expansions. This can be explained that there is no anxiety and any increments of tourism revenues that could lead to systematically boot up employments in economy system. Tourism policies should be implemented for extending high-season moments for as long as possible. Conversely, the results of the dynamic simulated model show that low-seasonal periods negatively cause fluctuations in the economic system, especially the sudden shut down of labour sectors. Therefore, training programs for skilled labour improving in service sectors and technology adaptations should be intensively activated. Accordingly, changing low-season periods to high-seasonal moments is the main purpose that policy makers should focus on. © 2018 WIT Press."
,10.1049/iet-com.2018.0039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055089518&doi=10.1049%2fiet-com.2018.0039&partnerID=40&md5=cd1b9e22a1ddbcb04903a482eae138ef,"The authors propose an efficient spectrum sharing scheme in cooperative cognitive radio networks, where an energyconstrained secondary transmitter (ST) first scavenges radio frequency (RF) energy from the received primary signals, and then the ST assists the primary transmission to obtain the opportunity of spectrum access. Specifically, the ST can forward the primary signal with its own signal by adopting both the Alamouti coding technique and superposition scheme only if the harvested energy is sufficient while the primary data is decoded correctly by the ST. Otherwise, the ST will continue to harvest RF energy. The authors use the discrete Markov chain to model the processes of charging and discharging of the battery. Moreover, two different joint decoding and interference cancellation schemes are employed at the receivers to restore the desired data. Closed-form expressions of outage probabilities for both the primary and secondary systems are derived. Aiming to minimise the outage probability of the secondary system with guaranteeing the primary transmission, an optimal power allocation factor for the ST is determined by Monte-Carlo simulation. Numerical results demonstrate that the proposed scheme can effectively improve the transfer performance of the secondary system while realising the transfer requirement of the primary system. © The Institution of Engineering and Technology 2018."
6,10.1016/j.scitotenv.2017.07.201,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026255020&doi=10.1016%2fj.scitotenv.2017.07.201&partnerID=40&md5=2ecf0c0960e406164ade32a25a7dc8ad,"Understanding the uncertainty in spatial modelling of environmental variables is important because it provides the end-users with the reliability of the maps. Over the past decades, Bayesian statistics has been successfully used. However, the conventional simulation-based Markov Chain Monte Carlo (MCMC) approaches are often computationally intensive. In this study, the performance of a novel Bayesian inference approach called Integrated Nested Laplace Approximation with Stochastic Partial Differential Equation (INLA-SPDE) was evaluated using independent calibration and validation datasets of various skewed and non-skewed soil properties and was compared with a linear mixed model estimated by residual maximum likelihood (REML-LMM). It was found that INLA-SPDE was equivalent to REML-LMM in terms of the model performance and was similarly robust with sparse datasets (i.e. 40–60 samples). In comparison, INLA-SPDE was able to estimate the posterior marginal distributions of the model parameters without extensive simulations. It was concluded that INLA-SPDE had the potential to map the spatial distribution of environmental variables along with their posterior marginal distributions for environmental management. Some drawbacks were identified with INLA-SPDE, including artefacts of model response due to the use of triangle meshes and a longer computational time when dealing with non-Gaussian likelihood families. © 2017 Elsevier B.V."
,10.1002/sim.7505,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037544487&doi=10.1002%2fsim.7505&partnerID=40&md5=18c31b8649c6f39266218d5aaffd6232,"Traditional Chinese medicine (TCM) is a very complex mixture containing many different ingredients. Thus, statistical analysis of traditional Chinese medicine data becomes challenging, as one needs to handle the association among the observed data across different time points and across different ingredients of the multivariate response. This paper builds a 3-stage Bayesian hierarchical model for analyzing multivariate response pharmacokinetic data. Usually, the dimensionality of the parameter space is very huge, which leads to the parameter-estimation difficulty. So we take the hybrid Markov chain Monte Carlo algorithms to obtain the posterior Bayesian estimation of corresponding parameters in our model. Both simulation study and real-data analysis show that our theoretical model and Markov chain Monte Carlo algorithms perform well, and especially the correlation among different ingredients can be calculated very accurately. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.1002/sim.7456,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037547018&doi=10.1002%2fsim.7456&partnerID=40&md5=a7ae6f6966261235b8d1197b78b79fda,"A major challenge when monitoring risks in socially deprived areas of under developed countries is that economic, epidemiological, and social data are typically underreported. Thus, statistical models that do not take the data quality into account will produce biased estimates. To deal with this problem, counts in suspected regions are usually approached as censored information. The censored Poisson model can be considered, but all censored regions must be precisely known a priori, which is not a reasonable assumption in most practical situations. We introduce the random-censoring Poisson model (RCPM) which accounts for the uncertainty about both the count and the data reporting processes. Consequently, for each region, we will be able to estimate the relative risk for the event of interest as well as the censoring probability. To facilitate the posterior sampling process, we propose a Markov chain Monte Carlo scheme based on the data augmentation technique. We run a simulation study comparing the proposed RCPM with 2 competitive models. Different scenarios are considered. RCPM and censored Poisson model are applied to account for potential underreporting of early neonatal mortality counts in regions of Minas Gerais State, Brazil, where data quality is known to be poor. Copyright © 2017 John Wiley & Sons, Ltd."
3,10.1002/2017GL076101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039757485&doi=10.1002%2f2017GL076101&partnerID=40&md5=4e56b653854b8b320621ee6b843ca5a1,"Climate projections continue to be marred by large uncertainties, which originate in processes that need to be parameterized, such as clouds, convection, and ecosystems. But rapid progress is now within reach. New computational tools and methods from data assimilation and machine learning make it possible to integrate global observations and local high-resolution simulations in an Earth system model (ESM) that systematically learns from both and quantifies uncertainties. Here we propose a blueprint for such an ESM. We outline how parameterization schemes can learn from global observations and targeted high-resolution simulations, for example, of clouds and convection, through matching low-order statistics between ESMs, observations, and high-resolution simulations. We illustrate learning algorithms for ESMs with a simple dynamical system that shares characteristics of the climate system; and we discuss the opportunities the proposed framework presents and the challenges that remain to realize it. ©2017. American Geophysical Union. All Rights Reserved."
1,10.1142/9789813232105_0006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045310094&doi=10.1142%2f9789813232105_0006&partnerID=40&md5=94cdab49d6c73235830cee12c969cffe,"Applications that require substantial computational resources today cannot avoid the use of heavily parallel machines. Embracing the opportunities of parallel computing and especially the possibilities provided by a new generation of massively parallel accelerator devices such as GPUs, Intel's Xeon Phi or even FPGAs enables applications and studies that are inaccessible to serial programs. Here we outline the opportunities and challenges of massively parallel computing for Monte Carlo simulations in statistical physics, with a focus on the simulation of systems exhibiting phase transitions and critical phenomena. This covers a range of canonical ensemble Markov chain techniques as well as generalized ensembles such as multicanonical simulations and population annealing. While the examples discussed are for simulations of spin systems, many of the methods are more general and moderate modifications allow them to be applied to other lattice and off-lattice problems including polymers and particle systems. We discuss important algorithmic requirements for such highly parallel simulations, such as the challenges of random-number generation for such cases, and outline a number of general design principles for parallel Monte Carlo codes to perform well. © 2018 by World Scientific Publishing Co. Pte. Ltd. All rights reserved."
,10.1142/9789813232105_0004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045337492&doi=10.1142%2f9789813232105_0004&partnerID=40&md5=224ec29b94c105ffbfb69715d346c7ed,"In the last two decades computer simulations in generalized ensembles based on Markov chain Monte Carlo sampling such as the multicanonical, Wang-Landau, and parallel tempering (or replica exchange) methods have emerged as a strong numerical tool for investigations of the statistical physics of macromolecular systems. Many studies have focused on coarse-grained models of polymers on the lattice and in the continuum. Phase diagrams of polymer chains in bulk and in interaction with surfaces were extensively studied. Also, the aggregation behavior in solution has been investigated. In this chapter, first the theoretical background for these simulations will be described, the employed algorithms explained and their performance assessed. Implementations of these algorithms on parallel computers are also briefly discussed. As an illustration of these concepts, an overview of polymer systems investigated with multicanonical and parallel tempering simulations will be given, focusing on our own recent studies of coarse-grained models. © 2018 by World Scientific Publishing Co. Pte. Ltd. All rights reserved."
3,10.1186/s12711-017-0369-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040197047&doi=10.1186%2fs12711-017-0369-3&partnerID=40&md5=f46eed694c38d69e767bd36df2ca940a,"Background: Non-linear Bayesian genomic prediction models such as BayesA/B/C/R involve iteration and mostly Markov chain Monte Carlo (MCMC) algorithms, which are computationally expensive, especially when whole-genome sequence (WGS) data are analyzed. Singular value decomposition (SVD) of the genotype matrix can facilitate genomic prediction in large datasets, and can be used to estimate marker effects and their prediction error variances (PEV) in a computationally efficient manner. Here, we developed, implemented, and evaluated a direct, non-iterative method for the estimation of marker effects for the BayesC genomic prediction model. Methods: The BayesC model assumes a priori that markers have normally distributed effects with probability $$ \uppi $$ π and no effect with probability (1 - $$ \uppi $$ π). Marker effects and their PEV are estimated by using SVD and the posterior probability of the marker having a non-zero effect is calculated. These posterior probabilities are used to obtain marker-specific effect variances, which are subsequently used to approximate BayesC estimates of marker effects in a linear model. A computer simulation study was conducted to compare alternative genomic prediction methods, where a single reference generation was used to estimate marker effects, which were subsequently used for 10 generations of forward prediction, for which accuracies were evaluated. Results: SVD-based posterior probabilities of markers having non-zero effects were generally lower than MCMC-based posterior probabilities, but for some regions the opposite occurred, resulting in clear signals for QTL-rich regions. The accuracies of breeding values estimated using SVD- and MCMC-based BayesC analyses were similar across the 10 generations of forward prediction. For an intermediate number of generations (2 to 5) of forward prediction, accuracies obtained with the BayesC model tended to be slightly higher than accuracies obtained using the best linear unbiased prediction of SNP effects (SNP-BLUP model). When reducing marker density from WGS data to 30 K, SNP-BLUP tended to yield the highest accuracies, at least in the short term. Conclusions: Based on SVD of the genotype matrix, we developed a direct method for the calculation of BayesC estimates of marker effects. Although SVD- and MCMC-based marker effects differed slightly, their prediction accuracies were similar. Assuming that the SVD of the marker genotype matrix is already performed for other reasons (e.g. for SNP-BLUP), computation times for the BayesC predictions were comparable to those of SNP-BLUP. © 2017 The Author(s)."
2,10.1002/cpe.4245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027989229&doi=10.1002%2fcpe.4245&partnerID=40&md5=6900ec604a3e38793c29c5de8ee7289f,"In this paper, we study the issue of improving the performance of Markov chain Monte Carlo method to solve local PageRank problem under General Purpose Graphics Processing Unit environment. As a large number of dangling vertices cause large storage space of dangling vertices and thus slow down the Markov chain procession, we propose a reordering strategy to compress the storage space and reduce the computational complexity of Markov chain procession. In our performance study, by parallelizing and optimizing the proposed algorithm based on GPU, the reordering strategy can be up 12× faster compared with basic method, where the graphs have high-proportion dangling vertices. According to our investigation on this issue, the variance of random walks determines the number of random walks in the computation; we thus introduce low-discrepancy sequences to enhance the performance. Moreover, the low-discrepancy sequences are organized to load in the on-chip shared memory to accelerate fetching with a wise warp scheduling for bank conflict schema. A series of experiments have been conducted to evaluate the optimization efficiency. Compared with fetching data from off-chip global memory, the shared-memory-based strategy can have over 10× speedup ratio performance. The experiments indicate that the size of shared memory has a significant impact on the parallelism of the proposed method as well. Copyright © 2017 John Wiley & Sons, Ltd."
3,10.1109/TCBB.2017.2786239,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039791634&doi=10.1109%2fTCBB.2017.2786239&partnerID=40&md5=772ad908850995b5a113ae0a6c7a5378,"Association mapping of genetic diseases has attracted extensive research interest during the recent years. However, most of the methodologies introduced so far suffer from spurious inference of the associated sites due to population inhomogeneities. In this paper, we introduce a statistical framework to compensate for this shortcoming by equipping the current methodologies with a state-of-the-art clustering algorithm being widely used in population genetics applications. The proposed framework jointly infers the disease-associated factors and the hidden population structures. In this regard, a Markov Chain-Monte Carlo (MCMC) procedure has been employed to assess the posterior probability distribution of the model parameters. We have implemented our proposed framework on a software package whose performance is extensively evaluated on a number of synthetic datasets, and compared to some of the well-known existing methods such as STRUCTURE. It has been shown that in extreme scenarios, up to <formula><tex>$10-15 \%$</tex></formula> of improvement in the inference accuracy is achieved with a moderate increase in computational complexity. IEEE"
1,10.1109/SSPD.2017.8233232,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047562066&doi=10.1109%2fSSPD.2017.8233232&partnerID=40&md5=98a5d5ea310e6b6eb0d236638fcd06f6,"In blind source separation (BSS), the number of sources present in the measured speech mixtures is unknown. The focus of this work is therefore to automatically estimate the number of sources from binaural speech mixtures. Collapsed Gibbs sampling (CGS), a Markov chain Monte Carlo (MCMC) technique, is used to obtain samples from the joint distribution of the speech mixtures. Then the Chinese Restaurant Process (CRP) within the framework of the Dirichlet Process (DP) is exploited to cluster samples into different components to finally estimate the number of speakers. The accuracy of the proposed method, under different reverberant environments, is evaluated with real binaural room impulse responses (BRIRs) and speech signals from the TIMIT database. The experimental results confirm the accuracy and robustness of the proposed method. © 2017 IEEE."
,10.1515/rnam-2017-0034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037128821&doi=10.1515%2frnam-2017-0034&partnerID=40&md5=f23333bc2d765747928224fd49c0ef08,The issues of finite computational cost of some vector weighted Monte Carlo algorithms are studied in the paper relative to estimation of linear functionals of solutions to systems of the 2nd kind integral equations. A universal modification of the weight vector collision estimator with branching of the chain trajectory relative to the elements of matrix weight is constructed. It is proved that the computational cost of the constructed algorithm is finite in the case when the basic functionals are bounded. The results of numerical calculations are presented for the case of use of a modified weight estimator for some problems of the radiation transfer theory with allowance for polarization. © 2017 Walter de Gruyter GmbH Berlin/Boston 2017.
,10.1002/sim.7444,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034864373&doi=10.1002%2fsim.7444&partnerID=40&md5=b4ecb0259cc2ceebed3a251ee1512e03,"Modeling of correlated biomarkers jointly has been shown to improve the efficiency of parameter estimates, leading to better clinical decisions. In this paper, we employ a joint modeling approach to a unique diabetes dataset, where blood glucose (continuous) and urine glucose (ordinal) measures of disease severity for diabetes are known to be correlated. The postulated joint model assumes that the outcomes are from distributions that are in the exponential family and hence modeled as multivariate generalized linear mixed effects model associated through correlated and/or shared random effects. The Markov chain Monte Carlo Bayesian approach is used to approximate posterior distribution and draw inference on the parameters. This proposed methodology provides a flexible framework to account for the hierarchical structure of the highly unbalanced data as well as the association between the 2 outcomes. The results indicate improved efficiency of parameter estimates when blood glucose and urine glucose are modeled jointly. Moreover, the simulation studies show that estimates obtained from the joint model are consistently less biased and more efficient than those in the separate models. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1145/3175684.3175700,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046662673&doi=10.1145%2f3175684.3175700&partnerID=40&md5=1e5fa821117621a78f6ae1806df0e802,"Black box variational inference (BBVI) is a recently proposed estimation method for parameters of statistical models. BBVI is an order of magnitude faster than Markov chain Monte Carlo (MCMC). The computation of BBVI is similar to maximum a posteriori estimation, but in addition to the point estimation given by the latter, BBVI also estimate another important statistic, i.e. the range of the parameters. Assuming normal distribution for the parameters, BBVI minimizes the KL divergence between the normal distribution and the statistical models. However if the parameters are not normal distributed, the estimation might not be precise. Similarly, if outliers are allowed then fat tail distributions are more suitable to model parameters. This paper discusses the problems of Cauchy and Student's t-distribution as a variational distribution. Inspired by Lasso, this paper replaces normal with Laplace distribution to implement BBVI. Experiment on linear regression shows, under non-ideal condition (e.g. with outlier data), BBSI based on Laplace provides more stable and correct estimation. � 2017 Association for Computing Machinery."
,10.1109/CHILECON.2017.8229662,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042689297&doi=10.1109%2fCHILECON.2017.8229662&partnerID=40&md5=89555048dcd56edd00d1e9900f757a50,"The interferometry problem addresses the estimation of an unknown quantity exploiting the interference among measurements from different sources. These measurements are obtained from the Fourier domain but are sparse and contaminated with noise. We propose a parametric, sum-of-basis, model for these observations together with a Bayesian approach for reconstructing interferometry images. Our main contributions are the construction of a model with a complex-valued noise source, an implementation of an approximate inference method to train the model using Markov chain Monte Carlo and a quantitative comparison against the so-called dirty algorithm, where the proposed approach outperformed the considered baseline. © 2017 IEEE."
,10.13465/j.cnki.jvs.2017.23.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044126443&doi=10.13465%2fj.cnki.jvs.2017.23.014&partnerID=40&md5=77ea16d9f99a2fa896b840e26b1d692f,"In order to reflect the real vibration characteristics of rod-fastening rotors of high pressure spool(HPS) in an aero-engine, Here, a FE (finite element) model modal characteristics confirmation method based on Bayesian theory was proposed. An elastoplastic slip model with non-linear hysteretic behavior was introduced to determine regions of uncertain parameters. According to this model, the likelihood function for modal data characteristics was built using Bayesian theory, Bayesian updating procedure was implemented using a multi-level Markov chain Monte Carlo (MCMC) algorithm. In addition, the adaptive hierarchical sparse grid collocation (ASGC) method was used to construct the stochastic surrogate model for the posterior probability distribution calculation of uncertain parameters, it reduced the amount of computation of the MCMC for large FE models like HPS. The real example of an aero-engine's high pressure rotor was given, the results using this modal characteristics confirmation method were compared with its test data, it was shown that the proposed method can determine regions and varying law of HPS feature frequencies, its effectiveness is verified. © 2017, Editorial Office of Journal of Vibration and Shock. All right reserved."
1,10.1016/j.physa.2017.07.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026400641&doi=10.1016%2fj.physa.2017.07.012&partnerID=40&md5=754d5fdd1cdca9386012419a5a9b6709,"We propose a dynamic economic model of networks where agents can be friends or enemies with one another. This is a decentralized relationship model in that agents decide whether to change their relationships so as to minimize their imbalanced triads. In this model, there is a single parameter, which we call social temperature, that captures the degree to which agents care about social balance in their relationships. We show that the global structure of relationship configuration converges to a unique stationary distribution. Using this stationary distribution, we characterize the maximum likelihood estimator of the social temperature parameter. Since the estimator is computationally challenging to calculate from real social network datasets, we provide a simple simulation algorithm and verify its performance with real social network datasets. © 2017 Elsevier B.V."
,10.1007/s00181-017-1364-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038112055&doi=10.1007%2fs00181-017-1364-9&partnerID=40&md5=f4286c5cb55795733fcce88f12691626,"In this paper, we generalize the stochastic frontier model to allow for heterogeneous technologies and inefficiencies in a structured way that allows for learning and adapting. We propose a general model and various special cases, organized around the idea that there is switching or transition from one technology to the other(s), and construct threshold stochastic frontier models. We suggest Bayesian inferences for the general model proposed here and its special cases using Gibbs sampling with data augmentation. The new techniques are applied, with very satisfactory results, to a panel of world production functions using, as switching or transition variables, human capital, age of capital stock (representing input quality), as well as a time trend to capture structural switching. © 2017 Springer-Verlag GmbH Germany, part of Springer Nature"
1,10.1016/j.energy.2017.05.193,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020911964&doi=10.1016%2fj.energy.2017.05.193&partnerID=40&md5=8224ba420137fd828ee3ea944c51532a,"The paper presents a solution for the thermoacoustic inverse problem using the Bayesian approach. Using some probabilistic information about the disturbance of data, the method takes into account the uncertainty of measurements in parameter identification. All variables, both measurements and estimated parameters are treated as random variables. The solution of this task is an expected value of the numerically generated Markov chain from appropriate probability distribution. In this work, the thermoacoustic inverse problem in which the heat release rate was identified on the basis of information obtained from the pressure measurements on the walls of the combustion chamber. The problem discussed has been formulated and the method used for its solution has been presented. Apart from the Bayesian approach, the Marcov chain Monte Carlo algorithm (used for obtaining the Markov chain in a numerical way) is also presented. To verify this methodology, two numerical examples formulated as the thermoacoustic inverse problem are solved and obtained results are presented. © 2017 Elsevier Ltd"
2,10.1103/PhysRevD.96.123011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040172755&doi=10.1103%2fPhysRevD.96.123011&partnerID=40&md5=4497ddfe9c518eb2ea759ab90e10e111,"Models of gravitational waveforms play a critical role in detecting and characterizing the gravitational waves (GWs) from compact binary coalescences. Waveforms from numerical relativity (NR), while highly accurate, are too computationally expensive to produce to be directly used with Bayesian parameter estimation tools like Markov-chain-Monte-Carlo and nested sampling. We propose a Gaussian process regression (GPR) method to generate reduced-order-model waveforms based only on existing accurate (e.g. NR) simulations. Using a training set of simulated waveforms, our GPR approach produces interpolated waveforms along with uncertainties across the parameter space. As a proof of concept, we use a training set of IMRPhenomD waveforms to build a GPR model in the 2-d parameter space of mass ratio q and equal-and-aligned spin χ1=χ2. Using a regular, equally-spaced grid of 120 IMRPhenomD training waveforms in q[1,3] and χ1[-0.5,0.5], the GPR mean approximates IMRPhenomD in this space to mismatches below 4.3×10-5. Our approach could in principle use training waveforms directly from numerical relativity. Beyond interpolation of waveforms, we also present a greedy algorithm that utilizes the errors provided by our GPR model to optimize the placement of future simulations. In a fiducial test case we find that using the greedy algorithm to iteratively add simulations achieves GPR errors that are ∼1 order of magnitude lower than the errors from using Latin-hypercube or square training grids. © 2017 American Physical Society."
,10.23919/RPIC.2017.8214311,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046465841&doi=10.23919%2fRPIC.2017.8214311&partnerID=40&md5=891fe12673ef37bb4d98e8088d081a73,"This article analyzes the performance of combining information from Scanning Electron Microscopy (SEM) micrographs with Static Light Scattering (SLS) measurements for retrieving the so-called Particle Size Distribution (PSD). The corresponding data fusion, which is formulated as a Bayesian inverse problem, is implemented using an emblematic Monte Carlo Markov Chain (MCMC) technique, the Metropolis-Hastings (MH) algorithm. Furthermore, the actual PSD is assumed to be exactly represented by a log-normal distribution, in order to reduce additional processing errors. The prior statistics corresponding to the SEM micrographs have been achieved by means of the Jackknife procedure used as a resampling technique. Monte Carlo-based statistical tools are also employed to assess the quality of these priors. The likelihood term for the Bayesian approach is computed considering independent normal measurements generated from a simplified SLS model, the Local Monodisperse Approximation (LMA), also used as the forward linear model. Finally, an experimental example is analyzed using the priors generated with the proposed procedure and parametrization and resulting estimations are compared to the achieved in a previous article and discussed. © 2017 Comisión Permanente RPIC."
,10.1109/IROS.2017.8206336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041959396&doi=10.1109%2fIROS.2017.8206336&partnerID=40&md5=2db4589124cce7cff5fe9643589b4da1,"In human-robot collaborative tasks, incorporating qualitative information provided by humans can greatly enhance the robustness and efficacy of robot state estimation. We introduce an algorithmic framework to model qualitative information as quantitative constraints on and between states. Our approach, named Sequentially Constrained Hamiltonian Monte Carlo, integrates Hamiltonian dynamics into Sequentially Constrained Monte Carlo sampling. We are able to generate samples that satisfy arbitrarily complex, non-smooth and discontinuous constraints, which in turn allows us to support a wide range of qualitative information. We evaluate our approach for constrained sampling qualitatively and quantitatively with several classes of constraints. SCHMC significantly outperforms the Metropolis-Hastings algorithm (a standard Markov Chain Monte Carlo (MCMC) method) and the Hamiltonian Monte Carlo (HMC) method, in terms of both the accuracy of the sampling (for satisfying constraints) and the quality of approximation. Compared to Sequentially Constrained Monte Carlo (SCMC), which supports similar kinds of constraints, our SCHMC approach has faster convergence rates and lower parameter sensitivity. © 2017 IEEE."
,10.23919/ICCAS.2017.8204399,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044462711&doi=10.23919%2fICCAS.2017.8204399&partnerID=40&md5=66ba035bb773e334cad380e3e85c8159,"This article examines the volatility spillover effects among USA and five East Asian stock markets using multivariate stochastic volatility with regime-switching model. The five East Asian stock markets are China, Hong Kong, Japan, Korea and Singapore respectively. We choose the most representative stock index in each area. Our empirical analysis of these indices daily data suggest that multivariate stochastic volatility with regime-switching model performs better in different market condition. This study found that there are no significant volatility spillover between China and USA. But there exist some significant volatility spillover among East Asian markets. © 2017 Institute of Control, Robotics and Systems - ICROS."
2,10.1080/01457632.2016.1262721,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017420646&doi=10.1080%2f01457632.2016.1262721&partnerID=40&md5=026362cc47e10594d8c801c71d044378,"The knowledge of tissues' properties and the noninvase monitoring of internal temperature are required in novel medical diagnostic and therapeutic techniques. For example, in the hyperthermia therapy of cancer, local heating must be accurately controlled in order to promote necrosis of the cancerous cells in thermoablation, or to induce apoptosis as an adjuvant treatment to chemotherapy or radiotherapy, without thermally affecting healthy cells. Photoacoustic imaging, also called optoacoustic imaging, is a new biomedical technique based on laser-generated ultrasound, which combines the high contrast of optical imaging with the high spatial resolution of ultrasound. Since parameters appearing in the mathematical formulation of the photoacoustic problem are temperature dependent, their estimation can be used as indirect temperature measurement. In the present work, sound speed, absorption coefficient, and a parameter that includes thermal expansion coefficient, laser energy density, and specific heat, are estimated through inverse analysis aiming at the identification of the tissue temperature. The forward problem is solved analytically using Laplace's transform, while the inverse problem is solved with a Markov chain Monte Carlo method within the Bayesian framework. Results obtained with simulated measurements reveal the capabilities of the proposed technique of parameter and temperature estimation. © 2017 Taylor & Francis Group, LLC."
1,10.1080/00949655.2017.1370649,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029513935&doi=10.1080%2f00949655.2017.1370649&partnerID=40&md5=08b5af55d4fee196cfc0a3734d58255f,"In this paper, we study the identification of Bayesian regression models, when an ordinal covariate is subject to unidirectional misclassification. Xia and Gustafson [Bayesian regression models adjusting for unidirectional covariate misclassification. Can J Stat. 2016;44(2):198–218] obtained model identifiability for non-binary regression models, when there is a binary covariate subject to unidirectional misclassification. In the current paper, we establish the moment identifiability of regression models for misclassified ordinal covariates with more than two categories, based on forms of observable moments. Computational studies are conducted that confirm the theoretical results. We apply the method to two datasets, one from the Medical Expenditure Panel Survey (MEPS), and the other from Translational Research Investigating Underlying Disparities in Acute Myocardial infarction Patients Health Status (TRIUMPH). © 2017 Informa UK Limited, trading as Taylor & Francis Group."
1,10.3758/s13428-017-0986-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037739851&doi=10.3758%2fs13428-017-0986-3&partnerID=40&md5=9081de397702e0f730330735bbf49f33,"In many behavioral research areas, multivariate generalizability theory (mG theory) has been typically used to investigate the reliability of certain multidimensional assessments. However, traditional mG-theory estimation—namely, using frequentist approaches—has limits, leading researchers to fail to take full advantage of the information that mG theory can offer regarding the reliability of measurements. Alternatively, Bayesian methods provide more information than frequentist approaches can offer. This article presents instructional guidelines on how to implement mG-theory analyses in a Bayesian framework; in particular, BUGS code is presented to fit commonly seen designs from mG theory, including single-facet designs, two-facet crossed designs, and two-facet nested designs. In addition to concrete examples that are closely related to the selected designs and the corresponding BUGS code, a simulated dataset is provided to demonstrate the utility and advantages of the Bayesian approach. This article is intended to serve as a tutorial reference for applied researchers and methodologists conducting mG-theory studies. © 2017 Psychonomic Society, Inc."
,10.1080/00949655.2017.1376063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029487015&doi=10.1080%2f00949655.2017.1376063&partnerID=40&md5=b58089c028c802fd224073060fcc321b,"A vector of k positive coordinates lies in the k-dimensional simplex when the sum of all coordinates in the vector is constrained to equal 1. Sampling distributions efficiently on the simplex can be difficult because of this constraint. This paper introduces a transformed logit-scale proposal for Markov Chain Monte Carlo that naturally adjusts step size based on the position in the simplex. This enables efficient sampling on the simplex even when the simplex is high dimensional and/or includes coordinates of differing orders of magnitude. Implementation of this method is shown with the SALTSamplerR package and comparisons are made to other simpler sampling schemes to illustrate the improvement in performance this method provides. A simulation of a typical calibration problem also demonstrates the utility of this method. ©, This work has been authored by an employee of Los Alamos National Security, LLC"
,10.1080/02664763.2016.1267120,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006870986&doi=10.1080%2f02664763.2016.1267120&partnerID=40&md5=a36c08496ebb44ddfe77bea47ca83413,"Multivariate regression models, i.e. regression models where the left-hand side of the regression equation denotes a matrix of dependent variables, have long been developed. However, the statistical analysis of empirical data is usually restricted to multivariable regression methods with only one dependent variable. Within the framework of hierarchical Bayesian methods, the present study illustrates (i) how multivariate regression models offer new possibilities of information synthesis and theory testing in survey data analysis, and (ii) how sensitive the results are to different specifications of prior distributions. To this end, a large representative survey is utilized to specify two multivariate hierarchical Bayesian regression models (N = 39, 280) which are calculated under two different prior distribution specifications. The estimation procedures and their implementation in R are described, convergence and predictive power analysis of each model are presented, and the advantages and disadvantages of multivariate regression methods are discussed. In general, the results obtained under each prior specification are to some extent similar, although differences were observed regarding model complexity, efficiency and predictive power. It is concluded that these methods facilitate the development and testing of complex research hypotheses, and are promising alternatives to a more efficient data analysis of large survey data sets. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
,10.1002/sim.7371,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021739344&doi=10.1002%2fsim.7371&partnerID=40&md5=24885136c4e0f50a543c72b00aca2939,"The LIPGENE-SU.VI.MAX study, like many others, recorded high-dimensional continuous phenotypic data and categorical genotypic data. LIPGENE-SU.VI.MAX focuses on the need to account for both phenotypic and genetic factors when studying the metabolic syndrome (MetS), a complex disorder that can lead to higher risk of type 2 diabetes and cardiovascular disease. Interest lies in clustering the LIPGENE-SU.VI.MAX participants into homogeneous groups or sub-phenotypes, by jointly considering their phenotypic and genotypic data, and in determining which variables are discriminatory. A novel latent variable model that elegantly accommodates high dimensional, mixed data is developed to cluster LIPGENE-SU.VI.MAX participants using a Bayesian finite mixture model. A computationally efficient variable selection algorithm is incorporated, estimation is via a Gibbs sampling algorithm and an approximate BIC-MCMC criterion is developed to select the optimal model. Two clusters or sub-phenotypes (‘healthy’ and ‘at risk’) are uncovered. A small subset of variables is deemed discriminatory, which notably includes phenotypic and genotypic variables, highlighting the need to jointly consider both factors. Further, 7 years after the LIPGENE-SU.VI.MAX data were collected, participants underwent further analysis to diagnose presence or absence of the MetS. The two uncovered sub-phenotypes strongly correspond to the 7-year follow-up disease classification, highlighting the role of phenotypic and genotypic factors in the MetS and emphasising the potential utility of the clustering approach in early screening. Additionally, the ability of the proposed approach to define the uncertainty in sub-phenotype membership at the participant level is synonymous with the concepts of precision medicine and nutrition. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.3847/1538-4357/aa9990,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038814722&doi=10.3847%2f1538-4357%2faa9990&partnerID=40&md5=47a7f564440bf8c01e81626355f6cc73,"Class I protostars are thought to represent an early stage in the lifetime of protoplanetary disks, when they are still embedded in their natal envelope. Here we measure the disk masses of 10 Class I protostars in the Taurus Molecular Cloud to constrain the initial mass budget for forming planets in disks. We use radiative transfer modeling to produce synthetic protostar observations and fit the models to a multi-wavelength data set using a Markov Chain Monte Carlo fitting procedure. We fit these models simultaneously to our new Combined Array for Research in Millimeter-wave Astronomy 1.3 mm observations that are sensitive to the wide range of spatial scales that are expected from protostellar disks and envelopes so as to be able to distinguish each component, as well as broadband spectral energy distributions compiled from the literature. We find a median disk mass of 0.018 Mo on average, more massive than the Taurus Class II disks, which have median disk mass of ∼0.0025 Mo. This decrease in disk mass can be explained if dust grains have grown by a factor of 75 in grain size, indicating that by the Class II stage, at a few Myr, a significant amount of dust grain processing has occurred. However, there is evidence that significant dust processing has occurred even during the Class I stage, so it is likely that the initial mass budget is higher than the value quoted here. © 2017. The American Astronomical Society. All rights reserved."
,10.1007/s10586-017-1400-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037617285&doi=10.1007%2fs10586-017-1400-8&partnerID=40&md5=57ea07cf01a352e8ca12c8723e3b9c29,"Multi-users interference in direct sequence ultra wideband (DS-UWB) communication is the hotspot research for wireless communication. In this paper, using the wavelet coefficient transform signal in DS-UWB communication, we establish the tree-based hierarchy shrinkage Bayesian compressive sensing (HSBCS) framework for multi-users interference models and noise model generalization. The Markov chain Monte Carlo (MCMC) multi-user interference algorithm by use of HSBCS framework is proposed to suppress noise in the presence of multi-user interference in the DS-UWB communications. The target of this paper is to detect the noise suppression performance of multi-user interference in the DS-UWB communications including normal mean square error (NMSE) performance and peak signal-to-noise-ratio (PSNR) performance using HSBCS framework. Simulation results show that NMSE and PSNR performance of tree-based HSBCS algorithm is better than that of the other algorithms with no tree structure. Otherwise, with more users increasing, the error probability performance of MCMC multiuser interference algorithm in the presence of multiuser interference by use of HSBCS framework is lowest and gradually approach zero. © 2017 Springer Science+Business Media, LLC, part of Springer Nature"
,10.1186/s12859-017-1920-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043287122&doi=10.1186%2fs12859-017-1920-5&partnerID=40&md5=d8795f98a6f74ff5c6366c6144aa23f7,"BACKGROUND: In recent years, protein-protein interaction (PPI) networks have been well recognized as important resources to elucidate various biological processes and cellular mechanisms. In this paper, we address the problem of predicting protein complexes from a PPI network. This problem has two difficulties. One is related to small complexes, which contains two or three components. It is relatively difficult to identify them due to their simpler internal structure, but unfortunately complexes of such sizes are dominant in major protein complex databases, such as CYC2008. Another difficulty is how to model overlaps between predicted complexes, that is, how to evaluate different predicted complexes sharing common proteins because CYC2008 and other databases include such protein complexes. Thus, it is critical how to model overlaps between predicted complexes to identify them simultaneously.RESULTS: In this paper, we propose a sampling-based protein complex prediction method, RocSampler (Regularizing Overlapping Complexes), which exploits, as part of the whole scoring function, a regularization term for the overlaps of predicted complexes and that for the distribution of sizes of predicted complexes. We have implemented RocSampler in MATLAB and its executable file for Windows is available at the site, http://imi.kyushu-u.ac.jp/~om/software/RocSampler/ .CONCLUSIONS: We have applied RocSampler to five yeast PPI networks and shown that it is superior to other existing methods. This implies that the design of scoring functions including regularization terms is an effective approach for protein complex prediction."
,10.1063/1.5016667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037843872&doi=10.1063%2f1.5016667&partnerID=40&md5=0bfcd23e288c0d3744c2f5b365696eb6,"In the Bayesian mixture modeling requires stages the identification number of the most appropriate mixture components thus obtained mixture models fit the data through data driven concept. Reversible Jump Markov Chain Monte Carlo (RJMCMC) is a combination of the reversible jump (RJ) concept and the Markov Chain Monte Carlo (MCMC) concept used by some researchers to solve the problem of identifying the number of mixture components which are not known with certainty number. In its application, RJMCMC using the concept of the birth/death and the split-merge with six types of movement, that are w updating, θ updating, z updating, hyperparameter β updating, split-merge for components and birth/death from blank components. The development of the RJMCMC algorithm needs to be done according to the observed case. The purpose of this study is to know the performance of RJMCMC algorithm development in identifying the number of mixture components which are not known with certainty number in the Bayesian mixture modeling for microarray data in Indonesia. The results of this study represent that the concept RJMCMC algorithm development able to properly identify the number of mixture components in the Bayesian normal mixture model wherein the component mixture in the case of microarray data in Indonesia is not known for certain number. © 2017 Author(s)."
,10.1002/jcc.25065,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032189034&doi=10.1002%2fjcc.25065&partnerID=40&md5=86681c8925bccfd1c207e39d52331d02,"Four pseudorandom number generators were compared with a physical, quantum-based random number generator using the NIST suite of statistical tests, which only the quantum-based random number generator could successfully pass. We then measured the effect of the five random number generators on various calculated properties in different Markov-chain Monte Carlo simulations. Two types of systems were tested: conformational sampling of a small molecule in aqueous solution and liquid methanol under constant temperature and pressure. The results show that poor quality pseudorandom number generators produce results that deviate significantly from those obtained with the quantum-based random number generator, particularly in the case of the small molecule in aqueous solution setup. In contrast, the widely used Mersenne Twister pseudorandom generator and a 64-bit Linear Congruential Generator with a scrambler produce results that are statistically indistinguishable from those obtained with the quantum-based random number generator. © 2017 Wiley Periodicals, Inc. © 2017 Wiley Periodicals, Inc."
,10.1109/MLSP.2017.8168171,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042280281&doi=10.1109%2fMLSP.2017.8168171&partnerID=40&md5=909c8b369d2a68921336c49191ec2af6,"In this paper, we consider parameter estimation in latent, spatiotemporal Gaussian processes using particle Markov chain Monte Carlo methods. In particular, we use spectral decomposition of the covariance function to obtain a high-dimensional state-space representation of the Gaussian processes, which is assumed to be observed through a nonlinear non-Gaussian likelihood. We develop a Rao-Blackwellized particle Gibbs sampler to sample the state trajectory and show how to sample the hyperparameters and possible parameters in the likelihood. The proposed method is evaluated on a spatio-temporal population model and the predictive performance is evaluated using leave-one-out cross-validation. © 2017 IEEE."
,10.1051/matecconf/201713900175,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039444965&doi=10.1051%2fmatecconf%2f201713900175&partnerID=40&md5=c278fed768dfdfacf83d49256b008cb6,"In view of the current social background of power system becoming more and more complicated, and the current research situation of related fields, this paper discusses the common methods of power system risk assessment, including the method of hierarchy analysis, Markov chain model, Monte Carlo method, fuzzy comprehensive evaluation and related research progress. The advantages and disadvantages of the four methods are enumerated respectively. It is considered that the power system should be evaluated synthetically in the practical application and the effective risk assessment method should be adopted rationally. © The Authors, published by EDP Sciences, 2017."
2,10.3389/fnins.2017.00669,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037053927&doi=10.3389%2ffnins.2017.00669&partnerID=40&md5=180c32f2cfe920fd7816ceb8e8745abf,"We develop an integrative Bayesian predictive modeling framework that identifies individual pathological brain states based on the selection of fluoro-deoxyglucose positron emission tomography (PET) imaging biomarkers and evaluates the association of those states with a clinical outcome. We consider data from a study on temporal lobe epilepsy (TLE) patients who subsequently underwent anterior temporal lobe resection. Our modeling framework looks at the observed profiles of regional glucose metabolism in PET as the phenotypic manifestation of a latent individual pathologic state, which is assumed to vary across the population. The modeling strategy we adopt allows the identification of patient subgroups characterized by latent pathologies differentially associated to the clinical outcome of interest. It also identifies imaging biomarkers characterizing the pathological states of the subjects. In the data application, we identify a subgroup of TLE patients at high risk for post-surgical seizure recurrence after anterior temporal lobe resection, together with a set of discriminatory brain regions that can be used to distinguish the latent subgroups. We show that the proposed method achieves high cross-validated accuracy in predicting post-surgical seizure recurrence. © 2017 Chiang, Guindani, Yeh, Dewar, Haneef, Stern and Vannucci."
2,10.3389/fmicb.2017.02399,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037662654&doi=10.3389%2ffmicb.2017.02399&partnerID=40&md5=9d2fc4236d75e857eecb3dc0a1b279df,"Human norovirus (HuNoV) is a leading cause of viral gastroenteritis worldwide, of which GII.4 is the most predominant genotype. Unlike other genotypes, GII.4 has created various variants that escaped from previously acquired immunity of the host and caused repeated epidemics. However, the molecular evolutionary differences among all GII.4 variants, including recently discovered strains, have not been elucidated. Thus, we conducted a series of bioinformatic analyses using numerous, globally collected, full-length GII.4 major capsid (VP1) gene sequences (466 strains) to compare the evolutionary patterns among GII.4 variants. The time-scaled phylogenetic tree constructed using the Bayesian Markov chain Monte Carlo (MCMC) method showed that the common ancestor of the GII.4 VP1 gene diverged from GII.20 in 1840. The GII.4 genotype emerged in 1932, and then formed seven clusters including 14 known variants after 1980. The evolutionary rate of GII.4 strains was estimated to be 7.68 × 10-3 substitutions/site/year. The evolutionary rates probably differed among variants as well as domains [protruding 1 (P1), shell, and P2 domains]. The Osaka 2007 variant strains probably contained more nucleotide substitutions than any other variant. Few conformational epitopes were located in the shell and P1 domains, although most were contained in the P2 domain, which, as previously established, is associated with attachment to host factors and antigenicity. We found that positive selection sites for the whole GII.4 genotype existed in the shell and P1 domains, while Den Haag 2006b, New Orleans 2009, and Sydney 2012 variants were under positive selection in the P2 domain. Amino acid substitutions overlapped with putative epitopes or were located around the epitopes in the P2 domain. The effective population sizes of the present strains increased stepwise for Den Haag 2006b, New Orleans 2009, and Sydney 2012 variants. These results suggest that HuNoV GII.4 rapidly evolved in a few decades, created various variants, and altered its evolutionary rate and antigenicity. © 2017 Motoya, Nagasawa, Matsushima, Nagata, Ryo, Sekizuka, Yamashita, Kuroda, Morita, Suzuki, Sasaki, Katayama and Kimura."
,10.1142/S1793525319500262,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038396101&doi=10.1142%2fS1793525319500262&partnerID=40&md5=cc0cf3a7decfee82a9946a7dfaa45901,"We show that a uniform probability measure supported on a specific set of piecewise linear loops in a nontrivial free homotopy class in a multi-punctured plane is overwhelmingly concentrated around loops of minimal lengths. Our approach is based on extending Mogulskii’s theorem to closed paths, which is a useful result of independent interest. In addition, we show that the above measure can be sampled using standard Markov Chain Monte Carlo techniques, thus providing a simple method for approximating shortest loops. © 2019 World Scientific Publishing Company"
1,10.3390/ijerph14121497,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036652198&doi=10.3390%2fijerph14121497&partnerID=40&md5=ce447930c826dc4455990700058e9667,"Aircraft noise increases the risk of cardiovascular diseases and mental illness. The allowable limit for sound in the vicinity of an airport is 65 decibels (dB) averaged over a 24-hour ‘day and night’ period (DNL) in the United States. We evaluate the trade-off between the cost and the health benefits of changing the regulatory DNL level from 65 dB to 55 dB using a Markov model. The study used LaGuardia Airport (LGA) as a case study. In compliance with 55 dB allowable limit of aircraft noise, sound insulation would be required for residential homes within the 55 dB to 65 dB DNL. A Markov model was built to assess the cost-effectiveness of installing sound insulation. One-way sensitivity analyses and Monte Carlo simulation were conducted to test uncertainty of the model. The incremental cost-effectiveness ratio of installing sound insulation for residents exposed to airplane noise from LGA was $11,163/QALY gained (95% credible interval: cost-saving and life-saving to $93,054/QALY gained). Changing the regulatory standard for noise exposure around airports from 65 dB to 55 dB comes at a very good value. © 2017 by the authors. Licensee MDPI, Basel, Switzerland."
2,10.1016/j.jappgeo.2017.10.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033713903&doi=10.1016%2fj.jappgeo.2017.10.004&partnerID=40&md5=ea65c357b58db382013b83385cdc754f,"In this study we developed an efficient Bayesian inversion framework for interpreting marine seismic Amplitude Versus Angle and Controlled-Source Electromagnetic data for marine reservoir characterization. The framework uses a multi-chain Markov-chain Monte Carlo sampler, which is a hybrid of DiffeRential Evolution Adaptive Metropolis and Adaptive Metropolis samplers. The inversion framework is tested by estimating reservoir-fluid saturations and porosity based on marine seismic and Controlled-Source Electromagnetic data. The multi-chain Markov-chain Monte Carlo is scalable in terms of the number of chains, and is useful for computationally demanding Bayesian model calibration in scientific and engineering problems. As a demonstration, the approach is used to efficiently and accurately estimate the porosity and saturations in a representative layered synthetic reservoir. The results indicate that the seismic Amplitude Versus Angle and Controlled-Source Electromagnetic joint inversion provides better estimation of reservoir saturations than the seismic Amplitude Versus Angle only inversion, especially for the parameters in deep layers. The performance of the inversion approach for various levels of noise in observational data was evaluated — reasonable estimates can be obtained with noise levels up to 25%. Sampling efficiency due to the use of multiple chains was also checked and was found to have almost linear scalability. © 2017"
1,10.1109/TWC.2017.2750667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030650321&doi=10.1109%2fTWC.2017.2750667&partnerID=40&md5=7098e384240a6c7ed8f3d4c74223f462,"We introduce a revised derivation of the bitwise Markov Chain Monte Carlo (MCMC) multiple-input multipleoutput (MIMO) detector. The new approach resolves the previously reported high SNR stalling problem of MCMC without the need for hybridization with another detector method or adding heuristic temperature scaling factors. Another common problem with MCMC algorithms is the unknown convergence time making predictable fixed-length implementations problematic. When an insufficient number of iterations are used on a slowly converging example, the output log likelihood ratios can be unstable and overconfident. Therefore, we develop a method to identify rare slowly converging runs and mitigate their degrading effects on the soft-output information. This improves forward-error-correcting code performance and removes a symptomatic error floor in bit error rate plots. Next, pseudo-convergence is identified with a novel way to visualize the internal behavior of the Gibbs sampler. An effective and efficient pseudo-convergence detection and escape strategy is suggested. Finally, the new excited MCMC (X-MCMC) detector is shown to have near maximuma- posteriori performance even with challenging, realistic, and highly-correlated channels at the maximum MIMO sizes and modulation rates supported by the 802.11ac WiFi specification, 8 × 8 MIMO 256 quadrature amplitude modulation. © 2017 IEEE."
,10.1007/JHEP12(2017)001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037670141&doi=10.1007%2fJHEP12%282017%29001&partnerID=40&md5=482dbaf37aaeec42817e413b12d7889d,"For a given Markov chain Monte Carlo algorithm we introduce a distance between two configurations that quantifies the difficulty of transition from one configuration to the other configuration. We argue that the distance takes a universal form for the class of algorithms which generate local moves in the configuration space. We explicitly calculate the distance for the Langevin algorithm, and show that it certainly has desired and expected properties as distance. We further show that the distance for a multimodal distribution gets dramatically reduced from a large value by the introduction of a tempering method. We also argue that, when the original distribution is highly multimodal with large number of degenerate vacua, an anti-de Sitter-like geometry naturally emerges in the extended configuration space. © 2017, The Author(s)."
,10.1140/epjd/e2017-80181-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039547031&doi=10.1140%2fepjd%2fe2017-80181-y&partnerID=40&md5=65659d7e8fbea161d0269e6c1ef742c3,"Abstract: A new simple Monte Carlo method is introduced for the study of electrostatic screening by surrounding ions. The proposed method is not based on the generally used Markov chain method for sample generation. Each sample is pristine and there is no correlation with other samples. As the main novelty, the pairs of ions are gradually added to a sample provided that the energy of each ion is within the boundaries determined by the temperature and the size of ions. The proposed method provides reliable results, as demonstrated by the screening of ion in plasma and in water. Graphical abstract: [Figure not available: see fulltext.]. © 2017, EDP Sciences, SIF, Springer-Verlag GmbH Germany, part of Springer Nature."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040666086&partnerID=40&md5=5000a1b7616979f44bbc5bf27671a8a5,"We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015. © 2017 Valerio Perrone, Paul A. Jenkins, Dario Spano and Yee Whye Teh."
,10.1109/TFUZZ.2016.2617377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038853498&doi=10.1109%2fTFUZZ.2016.2617377&partnerID=40&md5=7dea03bf63bb5d863414d1fffe8fe93c,"In this paper, the Takagi-Sugeno-Kang (TSK) fuzzy classifier is casted into the Bayesian inference framework and a new fuzzy classifier called Bayesian TSK fuzzy classifier (B-TSK-FC) is proposed accordingly. The proposed classifier can be constructed by learning both the antecedent and consequent parameters of the involved fuzzy rules simultaneously. As a result of the introduction of Bayesian inference, the proposed B-TSK-FC can be distinguished as follows. 1) Unlike most existing TSK fuzzy classifiers where the antecedent and consequent parameters of fuzzy rules are learnt in a decoupled manner and the antecedent parameters are learnt only in the input space, the antecedent parameters of the fuzzy rules in B-TSK-FC are learnt by developing a fuzzy clustering method in the input-output space, and the consequent parameters of fuzzy rules are learnt in accordance with the maximum margin of separation principle, thereby resulting to form an intrinsic link between the input and output spaces to achieve improved classification performance and better interpretability. 2) With a Dirichlet prior assumption about fuzzy memberships in fuzzy clustering, a Markov-Chain Monte-Carlo technique is employed to estimate the parameters of the proposed classifier from a sampling perspective. 3) Rather than being rivals, fuzziness and probability in B-TSK-FC are collaboratively modeled to enhance the performance of TSK fuzzy classifier, in terms of classification and interpretability. Our experimental results in synthetic datasets as well as several real-world datasets confirm such merits of the proposed fuzzy classifier. © 1993-2012 IEEE."
,10.1007/s12665-017-7087-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036470153&doi=10.1007%2fs12665-017-7087-6&partnerID=40&md5=4de9aa49bb2dabada318320ec4113c7c,"Estimation of the magnitude of designed flood is a fundamental task crucial for the determination of scale of engineering construction and for the development of flood disaster risk management projects. Due to a high level of uncertainty in observed data, selection of frequency distribution model, and estimation of model parameters, the process of designed flood has uncertainties consequently. A Bayesian flood frequency analysis method is adopted for designed flood estimation with P-III probability distribution as its flood frequency model. In the Bayesian method, the adaptive metropolis Markov Chain Monte Carlo (AM-MCMC) sampling algorithm is employed to estimate posterior distributions of parameters, upon which estimation of expectations and credible intervals of designed floods is obtained. With analyzing the drawback of likelihood function expressed with the product of probability of occurrence of each sample individual, four likelihood functions expressed on residuals are presented, and then based on Bayesian AM-MCMC method, performance of presented likelihood functions is compared with that of the classical likelihood function, with taking peak flow uncertainty analysis of Panjiakou Reservoir as a case study. The results show that expectations of flood peak quantiles estimation with likelihood functions based on residuals between observed/censored and calculated values of flood peaks are almost the same, but there are obvious differences between likelihood function based on occurrence probability of flood sample and those based on residuals with respect to expectation of quantiles estimation and also show that expectation and credible interval of quantiles estimation with Bayesian AM-MCMC method based on the whole likelihood function are more reasonable than those acquired with maximum likelihood function. Finally, some relevant flood frequency analyses issues based on Bayesian AM-MCMC algorithm which need to be further studied are also presented. © 2017, Springer-Verlag GmbH Germany."
,10.1016/j.spa.2017.03.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018780278&doi=10.1016%2fj.spa.2017.03.021&partnerID=40&md5=c775b299bb0d43fa45ed08d91e78619e,"This paper shows how the theory of Dirichlet forms can be used to deliver proofs of optimal scaling results for Markov chain Monte Carlo algorithms (specifically, Metropolis–Hastings random walk samplers) under regularity conditions which are substantially weaker than those required by the original approach (based on the use of infinitesimal generators). The Dirichlet form methods have the added advantage of providing an explicit construction of the underlying infinite-dimensional context. In particular, this enables us directly to establish weak convergence to the relevant infinite-dimensional distributions. © 2017 The Author(s)"
1,10.1002/qre.2228,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035101546&doi=10.1002%2fqre.2228&partnerID=40&md5=1aaea8fb4142aa2b2f315d149a31f002,"Assurance test plans are chosen to manage consumer's and producer's risks. We develop methods for planning Bayesian assurance tests for degradation data, ie, on the basis of the degradation data collected in the test, a decision is made whether to accept or reject a product. Bayesian assurance tests incorporate prior knowledge in the planning stage and use this information to evaluate posterior consumer's and producer's risks. We consider prior knowledge that takes the form of related degradation data. Assurance test plans are then found that meet the specified requirements for consumer's and producer's risks. We illustrate the planning of such assurance tests with an example involving printhead migration data. We also investigate the impact of measurement error on these assurance test plans. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1007/s00477-016-1337-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994085882&doi=10.1007%2fs00477-016-1337-0&partnerID=40&md5=ed49cdc7bc73ae150992682265556b39,"In many studies, the distribution of soil attributes depends on both spatial location and environmental factors, and prediction and process identification are performed using existing methods such as kriging. However, it is often too restrictive to model soil attributes as dependent on a known, parametric function of environmental factors, which kriging typically assumes. This paper investigates a semiparametric approach for identifying and modeling the nonlinear relationships of spatially dependent soil constituent levels with environmental variables and obtaining point and interval predictions over a spatial region. Frequentist and Bayesian versions of the proposed method are applied to measured soil nitrogen levels throughout Florida, USA and are compared to competing models, including frequentist and Bayesian kriging, based an array of point and interval measures of out-of-sample forecast quality. The semiparametric models outperformed competing models in all cases. Bayesian semiparametric models yielded the best predictive results and provided empirical coverage probability nearly equal to nominal. © 2016, Springer-Verlag Berlin Heidelberg."
,10.1007/s13226-017-0242-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040092413&doi=10.1007%2fs13226-017-0242-7&partnerID=40&md5=e9094b16d6c6b29482aa9c8c93e1917d,"Sampling from an intractable probability distribution is a common and important problem in scientific computing. A popular approach to solve this problem is to construct a Markov chain which converges to the desired probability distribution, and run this Markov chain to obtain an approximate sample. In this paper, we provide two methods to improve the performance of a given discrete reversible Markov chain. These methods require the knowledge of the stationary distribution only up to a normalizing constant. Each of these methods produces a reversible Markov chain which has the same stationary distribution as the original chain, and dominates the original chain in the ordering introduced by Peskun [11]. We illustrate these methods on two Markov chains, one connected to hidden Markov models and one connected to card shuffling. We also prove a result which shows that the Metropolis-Hastings algorithm preserves the Peskun ordering for Markov transition matrices. © 2017, The Indian National Science Academy."
,10.1002/qre.2215,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028913717&doi=10.1002%2fqre.2215&partnerID=40&md5=f702fd0f817d60f333190e18ded0035e,"The number of effects can be studied in a log-location-scale regression model used in analyzing a reliability improvement experiment is restricted to the number of runs, which is usually small. In many real examples, only the main effects and a few 2-factor interactions are considered. In this work, we propose using a Bayesian approach to analyze reliability improvement experiments. By specifying a prior on the effects, the number of effects can be studied is no longer restricted to the number of runs, and aliased effects can all be identified and estimated simultaneously. We analyze 2 real data sets to demonstrate the proposed approach. The results show that when complex interactions are present, the proposed approach can provide a more reliable result. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.3837/tiis.2017.12.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041025602&doi=10.3837%2ftiis.2017.12.012&partnerID=40&md5=a77b88fd6217005cdf867d5323ebe7b7,"Sentiments can profoundly affect individual behavior as well as decision-making. Confronted with the ever-increasing amount of review information available online, it is desirable to provide an effective sentiment model to both detect and organize the available information to improve understanding, and to present the information in a more constructive way for consumers. This study developed a unified phrase-based topic and sentiment detection model, combined with a tracking model using incremental hierarchical dirichlet allocation (PTSM_IHDP). This model was proposed to discover the evolutionary trend of topic-based sentiments from online reviews. PTSM_IHDP model firstly assumed that each review document has been composed by a series of independent phrases, which can be represented as both topic information and sentiment information. PTSM_IHDP model secondly depended on an improved time-dependency non-parametric Bayesian model, integrating incremental hierarchical dirichlet allocation, to estimate the optimal number of topics by incrementally building an up-to-date model. To evaluate the effectiveness of our model, we tested our model on a collected dataset, and compared the result with the predictions of traditional models. The results demonstrate the effectiveness and advantages of our model compared to several state-of-the-art methods. © 2017 KSII."
,10.1016/j.matcom.2017.05.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020387450&doi=10.1016%2fj.matcom.2017.05.005&partnerID=40&md5=9daae3b24055b93c226c685a6ef62e7e,"Ebola is a highly infectious disease generally characterized by sporadic outbreaks. A deterministic Ebola model is formulated and converted into Itô stochastic differential equations by adding noise on each compartment. In order to estimate the model parameter values, we use the extended Kalman filter technique as the filtering method and sum of square of errors to compute an approximation of the likelihood. From the obtained likelihood function, the maximum likelihood and MCMC methods for parameters estimation are then used. These parameter estimates provide useful information on quantities of epidemiological interest. Two cases are analyzed: (1) the model error covariance is set to be zero and (2) the bias is fully incorporated into the model. A comparison between these two cases is carried out to assess whether the bias is having a measure effect on parameters and states estimation. Finally, we investigate whether an estimate obtained from a biased study differs systematically from the true source population of the study. Our results indicate that the more the increase of bias, the more the noise in states simulation and parameters estimation compared to the deterministic model. © 2017 International Association for Mathematics and Computers in Simulation (IMACS)"
4,10.1002/etc.3913,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030149697&doi=10.1002%2fetc.3913&partnerID=40&md5=20fc2bdff35add3afa039be8fed5b2db,"Little is known about the effect of metal mixtures on marine organisms, especially after exposure to environmentally realistic concentrations. This information is, however, required to evaluate the need to include mixtures in future environmental risk assessment procedures. We assessed the effect of copper (Cu)–Nickel (Ni) binary mixtures on Mytilus edulis larval development using a full factorial design that included environmentally relevant metal concentrations and ratios. The reproducibility of the results was assessed by repeating this experiment 5 times. The observed mixture effects were compared with the effects predicted with the concentration addition model. Deviations from the concentration addition model were estimated using a Markov chain Monte-Carlo algorithm. This enabled the accurate estimation of the deviations and their uncertainty. The results demonstrated reproducibly that the type of interaction—synergism or antagonism—mainly depended on the Ni concentration. Antagonism was observed at high Ni concentrations, whereas synergism occurred at Ni concentrations as low as 4.9 μg Ni/L. This low (and realistic) Ni concentration was 1% of the median effective concentration (EC50) of Ni or 57% of the Ni predicted-no-effect concentration (PNEC) in the European Union environmental risk assessment. It is concluded that results from mixture studies should not be extrapolated to concentrations or ratios other than those investigated and that significant mixture interactions can occur at environmentally realistic concentrations. This should be accounted for in (marine) environmental risk assessment of metals. Environ Toxicol Chem 2017;36:3471–3479. © 2017 SETAC. © 2017 SETAC"
,10.1109/TCBB.2017.2779141,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037596238&doi=10.1109%2fTCBB.2017.2779141&partnerID=40&md5=71d07b6d73dfda6a0ecc06eb8be61250,"Drosophila melanogaster is an important model organism for ongoing research in neuro- and behavioral biology. Especially the locomotion analysis has become an integral part of such studies and thus elaborated automated tracking systems have been proposed in the past. However, most of these approaches share the inability to precisely segment the contours of colliding animals leading to the absence of model and motion-related features during collisions. Here, we translate the task of tracking and resolving colliding animals into a filtering problem solvable by Markov Chain Monte Carlo methods and elaborate an adequate larva model. By comparing our method with state-of-the-art approaches we demonstrate that our algorithm produces significantly better results in a fraction of time and facilitates the analysis of animal behavior during interaction in more detail. IEEE"
4,10.1785/0120170098,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037365317&doi=10.1785%2f0120170098&partnerID=40&md5=dd526cac75a93a3df0664653917b0214,"We estimate corner frequencies and stress drops for the 2011 Mw 5.65 Mineral, Virginia, earthquake and aftershocks using a multiwindow coda spectral ratio method. We apply a Bayesian approach using a Markov chain Monte Carlo algorithm to fit the observed spectral ratios of selected event pairs with the Brune omega-square source spectral model to obtain reliable measurements of corner frequency as well as the associated uncertainties. We show that the use of S-wave coda provides more stable source spectral ratios than using direct S waves, and select event pairs by examining the common decay characteristics of narrowband coda envelopes. We determined an unusually high stress drop of ∼67 MPa for the mainshock, assuming a simple Brune-type source model. The results of our modeling of the mainshock spectrum at the nearest recording station suggest similar values of stress drop for the two main subevents responsible for most of the moment release. The estimated stress drops for 46 aftershocks (Mw 0.71-4.13) range from ∼0:02 to 50 MPa with a median value of 3 MPa. The aftershocks with Mw ≥2:5 have high stress drops within a small range of ∼3-50 MPa, with a median value of 16 MPa. We observe an apparent decrease of stress drop with decreasing magnitude below Mw ∼ 2:5. Rather than suggesting a possible breakdown of self-similar source scaling, we attribute this apparent magnitude dependence to limited signal-to-noise ratios for small shocks, limited frequency bandwidth, and particularly, to a group of small magnitude earthquakes that occurred in a loose cluster several kilometers to the northeast of the mainshock. Those events have unusually low stress drops, compared with aftershocks in the main aftershock cluster, and may possibly indicate triggering of seismicity on a set of relatively weak fault or joint planes. We found no temporal pattern or depth dependence in the estimated stress drops. © 2017, Seismological Society of America. All rights reserved."
,10.1002/nav.21778,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043336871&doi=10.1002%2fnav.21778&partnerID=40&md5=acb7f7bb7ca09d7ee3b963a1f6f54e2a,"We propose a novel simulation-based approach for solving two-stage stochastic programs with recourse and endogenous (decision dependent) uncertainty. The proposed augmented nested sampling approach recasts the stochastic optimization problem as a simulation problem by treating the decision variables as random. The optimal decision is obtained via the mode of the augmented probability model. We illustrate our methodology on a newsvendor problem with stock-dependent uncertain demand both in single and multi-item (news-stand) cases. We provide performance comparisons with Markov chain Monte Carlo and traditional Monte Carlo simulation-based optimization schemes. Finally, we conclude with directions for future research. © 2018 Wiley Periodicals, Inc."
,10.1115/1.4035438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047053120&doi=10.1115%2f1.4035438&partnerID=40&md5=8548033c8b7d545bcc3af75c98af2170,"The reliable prediction and diagnosis of abnormal events provide much needed guidance for risk management. The traditional Bayesian network (traditional BN) has been used to dynamically predict and diagnose abnormal events. However, its inherent limitation caused by discrete categorization of random variables degrades the assessment reliability. This paper applied a continuous Bayesian network (CBN)-based model to reduce the above-mentioned limitation. To compute complex posterior distributions of CBN, the Markov chain Monte Carlo method (MCMC) was used. A case study was conducted to demonstrate the application of CBN, based on which a comparative analysis of the traditional BN and CBN was presented. This work highlights that the use of CBN can overcome the drawbacks of traditional BN to make dynamic prediction and diagnosis analysis more reliable. Copyright © 2017 by ASME."
1,10.1016/j.ijepes.2017.05.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021161562&doi=10.1016%2fj.ijepes.2017.05.031&partnerID=40&md5=12126d6f6406bf4fe848bd1daed3163a,"Accurate wind speed simulation is an essential prerequisite to analyze the power systems with wind power. A wind speed model considering meteorological conditions and seasonal variations is proposed in this paper. Firstly, using the path analysis method, the influence weights of meteorological factors are calculated. Secondly, the meteorological data are classified into several states using an improved Fuzzy C-means (FCM) algorithm. Then the Markov chain is used to model the chronological characteristics of meteorological states and wind speed. The proposed model was proved to be more accurate in capturing the characteristics of probability distribution, auto-correlation and seasonal variations of wind speed compared with the traditional Markov chain Monte Carlo (MCMC) and autoregressive moving average (ARMA) model. Furthermore, the proposed model was applied to adequacy assessment of generation systems with wind power. The assessment results of the modified IEEE-RTS79 and IEEE-RTS96 demonstrated the effectiveness and accuracy of the proposed model. © 2017 Elsevier Ltd"
,10.1088/1742-6596/936/1/012073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041235520&doi=10.1088%2f1742-6596%2f936%2f1%2f012073&partnerID=40&md5=d2aa394cdf768bdc1c032c8966353682,"We design oscillator networks to generate a signal with a prescribed power spectrum. We consider networks of identical sin-wave oscillators and the Kuramoto order parameter as an output signal of the system. We use the Kullback-Leibler entropy as a measure of the distance between the power spectrum of the output signal and those of desired one. By optimizing the connection network through the Markov chain Monte Carlo method with replica exchange, we found that even oscillator network with a small number of elements can be generate a variety of time signals. The output signals include periodic and quasi-periodic signals with prescribed periods, and aperiodic signals with prescribed power spectrums. © Published under licence by IOP Publishing Ltd."
1,10.1016/j.sigpro.2017.05.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020312846&doi=10.1016%2fj.sigpro.2017.05.031&partnerID=40&md5=6ff865a58f170d67565962e463476634,"Volterra systems have had significant success in modelling nonlinear systems in various real-world applications. However, it is generally assumed that the nonlinearity degree of the system is known beforehand. In this paper, we contribute to the literature on Volterra system identification (VSI) with a numerical Bayesian approach which identifies model coefficients and the nonlinearity degree concurrently. Although this numerical Bayesian method, namely reversible jump Markov chain Monte Carlo (RJMCMC) algorithm has been used with success in various model selection problems, our use is in a novel context in the sense that both memory size and nonlinearity degree are estimated. The aforementioned study ensures an anomalous approach to RJMCMC and provides a new understanding on its flexible use which enables trans-structural transitions between different classes of models in addition to transdimensional transitions for which it is classically used. We study the performance of the method on synthetically generated data including OFDM communications over a nonlinear channel. © 2017 Elsevier B.V."
2,10.1016/j.jtcvs.2017.08.034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029638991&doi=10.1016%2fj.jtcvs.2017.08.034&partnerID=40&md5=68270ad519fa4483967bbbe03f570804,"Objectives To create and validate a prediction model to assess outcomes associated with the Norwood operation. Methods The public-use dataset from a multicenter, prospective, randomized single-ventricle reconstruction trial was used to create this novel prediction tool. A Bayesian lasso logistic regression model was used for variable selection. We used a hierarchical framework by representing discrete probability models with continuous latent variables that depended on the risk factors for a particular patient. Bayesian conditional probit regression and Markov chain Monte Carlo simulations were then used to estimate the effects of the predictors on the means of these latent variables to create a score function for each of the study outcomes. We also devised a method to calculate the risk of outcomes associated with the Norwood operation before the actual heart operation. The 2 study outcomes evaluated were in-hospital mortality and composite poor outcome. Results The training dataset used 520 patients to generate the prediction model. The model included patient demographics, baseline characteristics, cardiac diagnosis, operation details, site volume, and surgeon experience. An online calculator for the tool can be accessed at https://soipredictiontool.shinyapps.io/NorwoodScoreApp/. Model validation was performed on 520 observations using an internal 10-fold cross-validation approach. The prediction model had an area under the curve of 0.77 for mortality and 0.72 for composite poor outcome on the validation dataset. Conclusions Our new prognostic tool is a promising first step in creating real-time risk stratification in children undergoing a Norwood operation; this tool will be beneficial for the purposes of benchmarking, family counseling, and research. © 2017 The American Association for Thoracic Surgery"
1,10.1007/s11336-017-9576-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029144762&doi=10.1007%2fs11336-017-9576-7&partnerID=40&md5=6d9bcdfcc6ee80f40c134fb0d2a9bac2,"We propose a nonparametric item response theory model for dichotomously-scored items in a Bayesian framework. The model is based on a latent class (LC) formulation, and it is multidimensional, with dimensions corresponding to a partition of the items in homogenous groups that are specified on the basis of inequality constraints among the conditional success probabilities given the latent class. Moreover, an innovative system of prior distributions is proposed following the encompassing approach, in which the largest model is the unconstrained LC model. A reversible-jump type algorithm is described for sampling from the joint posterior distribution of the model parameters of the encompassing model. By suitably post-processing its output, we then make inference on the number of dimensions (i.e., number of groups of items measuring the same latent trait) and we cluster items according to the dimensions when unidimensionality is violated. The approach is illustrated by two examples on simulated data and two applications based on educational and quality-of-life data. © 2017, The Psychometric Society."
,10.1177/0962280215613378,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038817929&doi=10.1177%2f0962280215613378&partnerID=40&md5=ddd5dd2189761aa29084b73f57583b7b,"Multi-type recurrent event data occur frequently in longitudinal studies. Dependent termination may occur when the terminal time is correlated to recurrent event times. In this article, we simultaneously model the multi-type recurrent events and a dependent terminal event, both with nonparametric covariate functions modeled by B-splines. We develop a Bayesian multivariate frailty model to account for the correlation among the dependent termination and various types of recurrent events. Extensive simulation results suggest that misspecifying nonparametric covariate functions may introduce bias in parameter estimation. This method development has been motivated by and applied to the lipid-lowering trial component of the Antihypertensive and Lipid-Lowering Treatment to Prevent Heart Attack Trial. © 2015, © The Author(s) 2015."
3,10.1007/s11336-017-9554-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013422747&doi=10.1007%2fs11336-017-9554-0&partnerID=40&md5=264f64b5422243116260c3e14e4f6c09,"Samejima’s graded response model (GRM) has gained popularity in the analyses of ordinal response data in psychological, educational, and health-related assessment. Obtaining high-quality point and interval estimates for GRM parameters attracts a great deal of attention in the literature. In the current work, we derive generalized fiducial inference (GFI) for a family of multidimensional graded response model, implement a Gibbs sampler to perform fiducial estimation, and compare its finite-sample performance with several commonly used likelihood-based and Bayesian approaches via three simulation studies. It is found that the proposed method is able to yield reliable inference even in the presence of small sample size and extreme generating parameter values, outperforming the other candidate methods under investigation. The use of GFI as a convenient tool to quantify sampling variability in various inferential procedures is illustrated by an empirical data analysis using the patient-reported emotional distress data. © 2017, The Psychometric Society."
5,10.1016/j.jappgeo.2017.10.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033596338&doi=10.1016%2fj.jappgeo.2017.10.002&partnerID=40&md5=cbd15b4ca6422646345d2e448e67f02a,"We apply a target-oriented amplitude versus angle (AVA) inversion to estimate the petrophysical properties of a gas-saturated reservoir in offshore Nile Delta. A linear empirical rock-physics model derived from well log data provides the link between the petrophysical properties (porosity, shaliness and saturation) and the P-wave, S-wave velocities and density. This rock-physics model, properly calibrated for the investigated reservoir, is used to re-parameterize the exact Zoeppritz equations. The so derived equations are the forward model engine of a linearized Bayesian AVA-petrophysical inversion that, for each data gather, inverts the AVA of the target reflections to estimate the petrophysical properties of the reservoir layer, keeping fixed the cap-rock properties. We make use of the iterative Gauss-Newton method to solve the inversion problem. For each petrophysical property of interest, we discuss the benefits introduced by wide-angle reflections in constraining the inversion and we compare the posterior probability distributions (PPDs) analytically obtained via a local linearization of the inversion with the PPDs numerically computed with a Markov Chain Monte Carlo (MCMC) method. It results that the porosity is the best resolved parameter and that wide-angle reflections effectively constrain the shaliness estimates but do not guarantee reliable saturation estimates. It also results that the local linearization returns accurate PPDs in good agreement with the MCMC estimates. © 2017 Elsevier B.V."
,10.1109/SDF.2017.8126349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046647111&doi=10.1109%2fSDF.2017.8126349&partnerID=40&md5=19d268e6c5f13ffbe2bccd3818758b53,"Bayesian recursive estimation using large volumes of data is a challenging research topic. The problem becomes particularly complex for high dimensional non-linear state spaces. Markov chain Monte Carlo (MCMC) based methods have been successfully used to solve such problems. The main issue when employing MCMC is the evaluation of the likelihood function at every iteration, which can become prohibitively expensive to compute. Alternative methods are therefore sought after to overcome this difficulty. One such method is the adaptive sequential MCMC (ASMCMC), where the use of the confidence sampling is proposed as a method to reduce the computational cost. The main idea is to make use of the concentration inequalities to sub-sample the measurements for which the likelihood terms are evaluated. However, ASMCMC methods require appropriate proposal distributions. In this work, we propose a novel ASMCMC framework in which the log-homotopy based particle flow filter form an adaptive proposal. We show the performance can be significantly enhanced by our proposed algorithm, while still maintaining a comparatively low processing overhead. © 2017 IEEE."
2,10.1016/j.cma.2017.08.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028698824&doi=10.1016%2fj.cma.2017.08.009&partnerID=40&md5=1566fed322b89b7b442fc7e4fbe204a5,"The use of mathematical and computational models for reliable predictions of tumor growth and decline in living organisms is one of the foremost challenges in modern predictive science, as it must cope with uncertainties in observational data, model selection, model parameters, and model inadequacy, all for very complex physical and biological systems. In this paper, large classes of parametric models of tumor growth in vascular tissue are discussed including models for radiation therapy. Observational data is obtained from MRI of a murine model of glioma and observed over a period of about three weeks, with X-ray radiation administered 14.5 days into the experimental program. Parametric models of tumor proliferation and decline are presented based on the balance laws of continuum mixture theory, particularly mass balance, and from accepted biological hypotheses on tumor growth. Among these are new model classes that include characterizations of effects of radiation and simple models of mechanical deformation of tumors. The Occam Plausibility Algorithm (OPAL) is implemented to provide a Bayesian statistical calibration of the model classes, 39 models in all, as well as the determination of the most plausible models in these classes relative to the observational data, and to assess model inadequacy through statistical validation processes. Discussions of the numerical analysis of finite element approximations of the system of stochastic, nonlinear partial differential equations characterizing the model classes, as well as the sampling algorithms for Monte Carlo and Markov chain Monte Carlo (MCMC) methods employed in solving the forward stochastic problem, and in computing posterior distributions of parameters and model plausibilities are provided. The results of the analyses described suggest that the general framework developed can provide a useful approach for predicting tumor growth and the effects of radiation. © 2017"
3,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040045843&partnerID=40&md5=d5d78e70c62c269e502d791501a12376,"Response-surface methodology (RSM) has been widely used in the petroleum industry as an assistive tool for numerical-reservoir-simulation studies. Instead of generating simulation cases exhaustively to solve history-matching (HM) problems, proxy models that are created by RSM provide useful benefits in terms of simplicity and computational efficiency. However, the capability of proxy models to fully capture uncertainty ranges of HM results and production forecasts might be deficient in complex problems if the simplified proxy models only deliver partial solutions. Hence, to decide whether the uncertainty-assessment process by proxy models is complete, the models should deliver as many HM solutions as required to build probability distribution of production forecasts. Therefore, we developed the work flow to combine both processes into an integrated proxy-based approach that searches for the accepted HM solution while probabilistic forecasts are evaluated simultaneously. In addition, the combined work flow is an iterative approach. It gradually modifies the proxy models dependent on the increasing number of completed simulation runs, which continually update the original proxy model into higher degree of polynomial. The use of higher-degree-polynomial equations appears to have the benefit to provide an expanded set of HM solutions inside the uncertain parameter space compared with the commonly used quadratic form. More solutions found from the iterations could eventually approximate wider uncertainty ranges of probabilistic forecasts, which is consistent with the direct Markov-chain Monte Carlo (MCMC) method but with a significant reduction of simulated cases. Finally, this paper applies the proxy-based approach to a reservoir-simulation model containing a horizontal hydraulic-fractured well in the Marcellus Shale formation. This proxy-based approach helps assess the uncertainty of reservoir and fracture properties of unconventional reservoirs. Also, it is useful for evaluating the ranges of ultimate gas recovery. © 2017 Society of Petroleum Engineers."
5,10.1051/0004-6361/201731601,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038019657&doi=10.1051%2f0004-6361%2f201731601&partnerID=40&md5=1f67677d6a3f8476098ff1eae097e469,"Context. Important information on the evolution of a jet can be obtained by comparing the physical state of the plasma at its propagation through the broad-line region (where the jet is most likely formed) into the intergalactic medium, where it starts to decelerate significantly. Aims. We compare the constraints on the physical parameters in the innermost (≤ pc) and outer (≥kpc) regions of the 3C 120 jet by means of a detailed multiwavelength analysis and theoretical modeling of their broadband spectra. Methods. The data collected by Fermi LAT ( -ray band), Swift (X-ray and ultraviolet bands), and Chandra (X-ray band) are analyzed together and the spectral energy distributions are modeled using a leptonic synchrotron and inverse Compton model, taking into account the seed photons originating inside and outside the jet. The model parameters are estimated using the Markov chain Monte Carlo method. Results. The -ray flux from the inner jet of 3C 120 was characterized by rapid variation from MJD 56 900 to MJD 57 300. Two strong flares were observed on April 24, 2015, when within 19.0 min and 3.15 h the flux was as high as (7:46 ± 1:56) × 10-6 photon cm-2 s-1 and (4:71 ± 0:92) × 10-6 photon cm-2 s-1, respectively, with ≤10σ. During these flares the apparent isotropic -ray luminosity was L ' (1:20-1:66) × 1046 erg s-1 which is not common for radio galaxies. The broadband emission in the quiet and flaring states can be described as synchrotron self-Compton emission, while inverse Compton scattering of dusty torus photons cannot be excluded for the flaring states. The X-ray emission from the knots can be reproduced by inverse Compton scattering of cosmic microwave background photons only if the jet is highly relativistic (even when Δ = 10; Ue=UB is still ≤80). These extreme requirements can be somewhat softened assuming the X-rays are from the synchrotron emission of a second population of very high energy electrons. Conclusions.We found that the jet power estimated at two scales is consistent, suggesting that the jet does not suffer severe dissipation, it simply becomes radiatively inefficient."
2,10.1037/met0000134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017382897&doi=10.1037%2fmet0000134&partnerID=40&md5=3cfb62a49725ca395709cbee3eab95c4,"Although immediacy is one of the necessary criteria to show strong evidence of a causal relation in single case designs (SCDs), no inferential statistical tool is currently used to demonstrate it. We propose a Bayesian unknown change-point model to investigate and quantify immediacy in SCD analysis. Unlike visual analysis that considers only 3-5 observations in consecutive phases to investigate immediacy, this model considers all data points. Immediacy is indicated when the posterior distribution of the unknown change-point is narrow around the true value of the change-point. This model can accommodate delayed effects. Monte Carlo simulation for a 2-phase design shows that the posterior standard deviations of the change-points decrease with increase in standardized mean difference between phases and decrease in test length. This method is illustrated with real data. © 2017 American Psychological Association."
,10.1007/s13253-017-0300-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027995281&doi=10.1007%2fs13253-017-0300-y&partnerID=40&md5=40c3acad0a99f650483cb179b5ded1d1,"Abundance estimates from animal point-count surveys require accurate estimates of detection probabilities. The standard model for estimating detection from removal-sampled point-count surveys assumes that organisms at a survey site are detected at a constant rate; however, this assumption can often lead to biased estimates. We consider a class of N-mixture models that allows for detection heterogeneity over time through a flexibly defined time-to-detection distribution (TTDD) and allows for fixed and random effects for both abundance and detection. Our model is thus a combination of survival time-to-event analysis with unknown-N, unknown-p abundance estimation. We specifically explore two-parameter families of TTDDs, e.g., gamma, that can additionally include a mixture component to model increased probability of detection in the initial observation period. Based on simulation analyses, we find that modeling a TTDD by using a two-parameter family is necessary when data have a chance of arising from a distribution of this nature. In addition, models with a mixture component can outperform non-mixture models even when the truth is non-mixture. Finally, we analyze an Ovenbird data set from the Chippewa National Forest using mixed effect models for both abundance and detection. We demonstrate that the effects of explanatory variables on abundance and detection are consistent across mixture TTDDs but that flexible TTDDs result in lower estimated probabilities of detection and therefore higher estimates of abundance. Supplementary materials accompanying this paper appear on-line. © 2017, International Biometric Society."
1,10.1093/biomet/asx051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039161035&doi=10.1093%2fbiomet%2fasx051&partnerID=40&md5=1dff8d605fce57c6e83dad9dc7d87e13,"Sampling from the posterior probability distribution of the latent states of a hidden Markov model is nontrivial even in the context of Markov chain Monte Carlo. To address this, Andrieu et al. (2010) proposed a way of using a particle filter to construct a Markov kernel that leaves the posterior distribution invariant. Recent theoretical results have established the uniform ergodicity of this Markov kernel and shown that the mixing rate does not deteriorate provided the number of particles grows at least linearly with the number of latent states. However, this gives rise to a cost per application of the kernel that is quadratic in the number of latent states, which can be prohibitive for long observation sequences. Using blocking strategies, we devise samplers that have a stable mixing rate for a cost per iteration that is linear in the number of latent states and which are easily parallelizable. © 2017 Biometrika Trust."
,10.1002/sam.11349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020475269&doi=10.1002%2fsam.11349&partnerID=40&md5=8ef98b2975693a5340a7a7399965ca09,"In this paper we propose a Bayesian semiparametric regression model to estimate and test the effect of a genetic pathway on prostate-specific antigen (PSA) measurements for patients with prostate cancer. The underlying functional relationship between the genetic pathway and PSA is modeled using reproducing kernel Hilbert space (RKHS) theory. The RKHS formulation makes our model highly flexible, which can capture the complex multidimensional relationship between the genes in a genetic pathway and the response. Moreover, the higher order and nonlinear interactions among the genes in a pathway are also automatically modeled through our kernel-based representation. We illustrate the connection between our semiparametric regression based on RKHS and a linear mixed model by choosing a special prior distribution on the model parameters. To test the significance of a genetic pathway toward the phenotypic response like PSA, we propose a Bayesian hypothesis testing scheme based on the Bayes factor. An efficient Markov chain Monte Carlo algorithm is designed to estimate the model parameters, Bayes factors, and the genetic pathway effect simultaneously. We illustrate the effectiveness of our model by five simulation studies and one real prostate cancer gene expression data analysis. © 2017 Wiley Periodicals, Inc."
2,10.1177/0278364917743319,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039870175&doi=10.1177%2f0278364917743319&partnerID=40&md5=20b31e40098f4b584481881c9409bbad,"Demonstration trajectories collected from a supervisor in teleoperation are widely used for robot learning, and temporally segmenting the trajectories into shorter, less-variable segments can improve the efficiency and reliability of learning algorithms. Trajectory segmentation algorithms can be sensitive to noise, spurious motions, and temporal variation. We present a new unsupervised segmentation algorithm, transition state clustering (TSC), which leverages repeated demonstrations of a task by clustering segment endpoints across demonstrations. TSC complements any motion-based segmentation algorithm by identifying candidate transitions, clustering them by kinematic similarity, and then correlating the kinematic clusters with available sensory and temporal features. TSC uses a hierarchical Dirichlet process Gaussian mixture model to avoid selecting the number of segments a priori. We present simulated results to suggest that TSC significantly reduces the number of false-positive segments in dynamical systems observed with noise as compared with seven probabilistic and non-probabilistic segmentation algorithms. We additionally compare algorithms that use piecewise linear segment models, and find that TSC recovers segments of a generated piecewise linear trajectory with greater accuracy in the presence of process and observation noise. At the maximum noise level, TSC recovers the ground truth 49% more accurately than alternatives. Furthermore, TSC runs 100× faster than the next most accurate alternative autoregressive models, which require expensive Markov chain Monte Carlo (MCMC)-based inference. We also evaluated TSC on 67 recordings of surgical needle passing and suturing. We supplemented the kinematic recordings with manually annotated visual features that denote grasp and penetration conditions. On this dataset, TSC finds 83% of needle passing transitions and 73% of the suturing transitions annotated by human experts. © 2017, © The Author(s) 2017."
,10.1177/1471082X17703855,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033444443&doi=10.1177%2f1471082X17703855&partnerID=40&md5=a97ea7979b67e0d9c439b7f774aa03ff,"This study proposes a new model for integer-valued time series—the hysteretic Poisson integer-valued generalized autoregressive conditionally heteroskedastic (INGARCH) model—which has an integrated hysteresis zone in the switching mechanism of the conditional expectation. Our modelling framework provides a parsimonious representation of the salient features of integer-valued time series, such as discreteness, over-dispersion, asymmetry and structural change. We adopt Bayesian methods with a Markov chain Monte Carlo sampling scheme to estimate model parameters and utilize the Bayesian information criteria for model comparison. We then apply the proposed model to five real time series of criminal incidents recorded by the New South Wales Police Force in Australia. Simulation results and empirical analysis highlight the better performance of hysteresis in modelling the integer-valued time series. © 2017, © 2017 SAGE Publications."
,10.1186/s12874-017-0427-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036625039&doi=10.1186%2fs12874-017-0427-0&partnerID=40&md5=662f8b9f88a65d48c0b6f77bb61cc7ef,"Background: Exploratory preclinical, as well as clinical trials, may involve a small number of patients, making it difficult to calculate and analyze the pharmacokinetic (PK) parameters, especially if the PK parameters show very high inter-individual variability (IIV). In this study, the performance of a classical first-order conditional estimation with interaction (FOCE-I) and expectation maximization (EM)-based Markov chain Monte Carlo Bayesian (BAYES) estimation methods were compared for estimating the population parameters and its distribution from data sets having a low number of subjects. Methods: In this study, 100 data sets were simulated with eight sampling points for each subject and with six different levels of IIV (5%, 10%, 20%, 30%, 50%, and 80%) in their PK parameter distribution. A stochastic simulation and estimation (SSE) study was performed to simultaneously simulate data sets and estimate the parameters using four different methods: FOCE-I only, BAYES(C) (FOCE-I and BAYES composite method), BAYES(F) (BAYES with all true initial parameters and fixed ω 2 ), and BAYES only. Relative root mean squared error (rRMSE) and relative estimation error (REE) were used to analyze the differences between true and estimated values. A case study was performed with a clinical data of theophylline available in NONMEM distribution media. NONMEM software assisted by Pirana, PsN, and Xpose was used to estimate population PK parameters, and R program was used to analyze and plot the results. Results: The rRMSE and REE values of all parameter (fixed effect and random effect) estimates showed that all four methods performed equally at the lower IIV levels, while the FOCE-I method performed better than other EM-based methods at higher IIV levels (greater than 30%). In general, estimates of random-effect parameters showed significant bias and imprecision, irrespective of the estimation method used and the level of IIV. Similar performance of the estimation methods was observed with theophylline dataset. Conclusions: The classical FOCE-I method appeared to estimate the PK parameters more reliably than the BAYES method when using a simple model and data containing only a few subjects. EM-based estimation methods can be considered for adapting to the specific needs of a modeling project at later steps of modeling. © 2017 The Author(s)."
,10.1515/mcma-2017-0119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037596441&doi=10.1515%2fmcma-2017-0119&partnerID=40&md5=70f4553c105fed5b8727ba3cb571bcab,"A new Monte Carlo algorithm for solving the Robin boundary-value problem is described and applied to the calculation of the electron beam induced current in a simplified model of the imaging measurements. © 2017 Walter de Gruyter GmbH, Berlin/Boston."
1,10.1214/17-AOAS1075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042669957&doi=10.1214%2f17-AOAS1075&partnerID=40&md5=ebf766c5938c62af0627249ccb18c643,"Copy number variations in cancer cells and volatility fluctuations in stock prices are commonly manifested as changepoints occurring at the same positions across related data sequences. We introduce a Bayesian modeling framework, BASIC, that employs a changepoint prior to capture the cooccurrence tendency in data of this type. We design efficient algorithms to sample from and maximize over the BASIC changepoint posterior and develop a Monte Carlo expectation-maximization procedure to select prior hyperparameters in an empirical Bayes fashion. We use the resulting BASIC framework to analyze DNA copy number variations in the NCI-60 cancer cell lines and to identify important events that affected the price volatility of S&P 500 stocks from 2000 to 2009. © Institute of Mathematical Statistics, 2017."
1,10.1017/jpr.2017.61,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041309990&doi=10.1017%2fjpr.2017.61&partnerID=40&md5=2dc0ee10017a2b63a89fc77a42771b35,"In this paper we consider the optimal scaling of high-dimensional random walk Metropolis algorithms for densities differentiable in the L p mean but which may be irregular at some points (such as the Laplace density, for example) and/or supported on an interval. Our main result is the weak convergence of the Markov chain (appropriately rescaled in time and space) to a Langevin diffusion process as the dimension d goes to As the log-density might be nondifferentiable, the limiting diffusion could be singular. The scaling limit is established under assumptions which are much weaker than the one used in the original derivation of Roberts et al. (1997). This result has important practical implications for the use of random walk Metropolis algorithms in Bayesian frameworks based on sparsity inducing priors. Copyright © Applied Probability Trust 2017."
,10.1177/1471082X17699299,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033442361&doi=10.1177%2f1471082X17699299&partnerID=40&md5=15fd25aae3124345d844b73fb4927d18,"In this article, the Dirichlet process (DP) is applied to cluster subjects with longitudinal observations. The basis of clustering is the ability of subjects to adapt themselves to new circumstances. Indeed, the basis of clustering depends on the time of changing response variability. This is done by providing a random change-point time in the variance structure of mixed-effects models. The DP is assumed as a prior for the distribution of the random change point. The discrete nature of the DP is utilized to cluster subjects according to the time of adaption. The proposed model is useful to identify groups of subjects with distinctive time-based progressions or declines. Transition mixed-effects models are also used to account for the serial correlation among observations over time. A joint modelling approach is utilized to handle the bias created in these models. The Gibbs sampling technique is adopted to achieve parameter estimates. Performance of the proposed method is evaluated via conducting a simulation study. The usefulness of the proposed model is assessed on a course-evaluation dataset. © 2017, © 2017 SAGE Publications."
,10.1111/anzs.12207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035128247&doi=10.1111%2fanzs.12207&partnerID=40&md5=fc27426761a7ad891537fd17c1f8f6cc,"We present methods to fit monotone polynomials in a Bayesian framework, including implementations in the popular, readily available, modeling languages BUGS and Stan. The sum-of-squared polynomials parameterisation of monotone polynomials previously considered in the frequentist framework by Murray, Müller & Turlach (2016), is again considered here, due to its superior flexibility compared to other parameterisations. The specifics of our implementation are discussed, enabling end users to adapt this work to their applications. Testing was undertaken on real and simulated data sets, the output and diagnostics of which are presented. We demonstrate that Stan is preferable for high degree polynomials, with the component-wise nature of Gibbs sampling being potentially inappropriate for such highly connected models. All code discussed here, and sample scripts that show how to use it from R, is freely available at https://github.com/hhau/BayesianMonPol. © 2017 Australian Statistical Publishing Association Inc. Published by John Wiley & Sons Australia Pty Ltd."
,10.1049/iet-cvi.2016.0429,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038841166&doi=10.1049%2fiet-cvi.2016.0429&partnerID=40&md5=08b5ba832725fede13cc0eb949fb3940,"Markov random fields (MRFs) are prominent in modelling image to handle image processing problems. However, they confront the bottleneck of model selection in further improving the performance. That is difficult to decide how many objects in an image automatically. Motivated by Bayesian non-parametric (BN) models, a layered BN MRF is proposed. The proposed model is hierarchical: the lower level is a random-field like model, while the higher level is a Chinese restaurant process (CRP). The clustering procedure can be formulated briefly as follows. The input data is first clustered with the lower level MRF to form a set of components. Then the higher level CRP is used to merge the components into larger clusters. Furthermore, a split-merge Monte Carlo Markov chain is employed. Quantitative evaluations over BSD500 data set and MSRC data set show the proposed model is comparable to the state-of-the-art BN models and other graphical models in modelling unsupervised distancedependent problems. © The Institution of Engineering and Technology 2017."
,10.13224/j.cnki.jasp.2017.12.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043527027&doi=10.13224%2fj.cnki.jasp.2017.12.007&partnerID=40&md5=b8fc8b05a65b49271e783684e976448a,"According to the acquisition of inspection and maintenance information of civil aero-engine hot-section subassembly system and typical degradation state judgment by experts, the models of sojourn time estimation of each typical macro degradation state based on expert estimation opinion, inspection information and their fusion data were set up under the assumptions of the system state degradation process obeying a discrete semi-Markov chain. The methods of maximum likelihood estimation and MCMC(Monte Carlo Markov Chain) were applied to estimate the model parameters, the sojourn time and the state transition coefficients in each macro degradation state based on these three kinds of data. Meanwhile, the state transition probability models were built in a certain service cycle for optimal inspection and maintenance cost, and then the optimal inspection intervals in three typical macro degradation states, i.e. 1750, 350, 70cycles were obtained by simulation. The results showed the optimal inspection intervals were relatively close to the reality situations in the civil aviation operation and production, providing the technical supports in the aspect of customer-oriented inspection and maintenance decision-making and improving economic benefits for civil aviation transport enterprises. © 2017, Editorial Department of Journal of Aerospace Power. All right reserved."
1,10.1016/j.beproc.2017.09.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030681541&doi=10.1016%2fj.beproc.2017.09.017&partnerID=40&md5=9939843020739b6bba6e91170e6263ac,"This study aims to describe a Bayesian Hierarchical Linear Model (HLM) approach for longitudinal designs in fish's experimental aggressive behavior studies as an alternative to classical methods In particular, we discuss the advantages of Bayesian analysis in dealing with combined variables, non-statistically significant results and required sample size using an experiment of angelfish (Pterophyllum scalare) species as case study. Groups of 3 individuals were subjected to daily observations recorded for 10 min during 5 days. The frequencies of attacks, displays and the total attacks (attacks + displays) of each record were modeled using Monte Carlo Markov chains. In addition, a Bayesian HLM was performed for measuring the rate of increase/decrease of the aggressive behavior during the time and to assess the probability of difference among days. Results highlighted that using the combined variable of total attacks could lead to biased conclusions as displays and attacks showed an opposite pattern in the experiment. Moreover, depending of the study, this difference in pattern can happen more clearly or more subtly. Subtle changes cannot be detected when p-values are implemented. On the contrary, Bayesian methods provide a clear description of the changes even when patterns are subtle. Additionally, results showed that the number of replicates (15 or 11) invariant the study conclusions as well that using a small sample size could be more evident within the overlapping days, that includes the social rank stability. Therefore, Bayesian analysis seems to be a richer and an adequate statistical approach for fish's aggressive behavior longitudinal designs. © 2017 Elsevier B.V."
,10.1115/1.4036064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047053529&doi=10.1115%2f1.4036064&partnerID=40&md5=b762cee5fea581fc59baebaeb8e3e6b7,"Corrosion degradation is a common problem for boiler tubes in power plants, resulting in an unscheduled plant shutdown. In this research, degradation of the corrosion is investigated for boiler tubes by estimating the corrosion lifetime. A special focus is made on the corrosion failures, important failure modes, and mechanisms for the metallic boiler tubes via failure modes and effect analysis (FMEA) method, thereby evaluating the pitting corrosion as the most common failure mode in the tubes. Majority of the available approaches estimates lifetime of the pitting corrosion by deterministic approaches, in which the results are valid only for limited conditions. In order to improve deficiencies of available models, a stochastic method is proposed here to study the corrosion life. The temporal behavior of the metal degradation is analyzed in different conditions through the developed approach, and a proper degradation model is selected. Uncertainty intervals/ distributions are determined for some of the model parameters. The deterministic model is converted to a probabilistic model by taking into account the variability of the uncertain input parameters. The model is simulated using Monte Carlo method via simple sampling. The result of the life estimation is updated by the Bayesian framework using Monte Carlo Markov Chain. Finally, for the element that is subjected to the pitting corrosion degradation, the life distribution is obtained. The modeling results show that the pitting corrosion has stochastic behavior with lognormal distribution as proper fit for the pitting corrosion behavior. In order to validate the results, the estimations were compared with the power plant field failure data. Copyright © 2017 by ASME."
,10.3969/j.issn.0372-2112.2017.12.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046136774&doi=10.3969%2fj.issn.0372-2112.2017.12.013&partnerID=40&md5=b8f651915bce5ceca165b1f9d86aed28,"Previous approaches on integrated circuit parametric yield estimation usually model chip performance by pre-setting variation basis functions. It is easy to result in high complexity. On the other hand, random reduction of the number of the basis functions may result in accuracy loss. In order to avoid the issues, a sparse estimation approach for chip-level parametric yield is proposed. Taking power yield as an instance, the proposed approach models leakage power stochastically. Then according to the importance level, several key basis functions are adaptively selected to constribute a sparse leakage power model based on elastic net. Finally according to Bias theory and Markov chain method, the power yield is estimated efficiently. Experimental results show that the proposed approach not only makes the established power model general and sparse, but estimates the power yield accurately. Comparing to Monte Carlo(MC)simulation, the relative errors of power yield estimation based on proposed method are less than 5%. In addition, this approach can lead to a large cost reduction compared with MC simulation, and thus has higher efficiency. © 2017, Chinese Institute of Electronics. All right reserved."
,10.1002/jmv.24910,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031127710&doi=10.1002%2fjmv.24910&partnerID=40&md5=4b3ef76f7c0946721d0221827bac06fb,"Zika virus (ZIKV) is a member of the family Flaviviridae. ZIKV emerged in Brazil in 2015, causing an unprecedented epidemic and since then the virus has rapidly spread throughout the Americas. These facts highlight the need of detailed phylogenetic studies to understand the emergence, spread, and evolution of ZIKV populations. For these reasons, a Bayesian coalescent Markov Chain Monte Carlo analysis of complete genome sequences of ZIKV strains recently isolated in the American continent was performed. The results of these studies revealed an increasing diversification of ZIKV strains in different genetic lineages and co-circulation of distinct genetic lineages in several countries in the region. The time of the most recent common ancestor (tMRCA) was established to be around February 20, 2014 for ZIKV strains circulating in the American region. A mean rate of evolution of 1.55 × 10−3 substitutions/site/year was obtained for ZIKV strains included in this study. A Bayesian skyline plot indicate a sharp increase in population size from February 2014 to July 2015 and a decline during 2016. These results are discussed in terms of the emergence and evolution of ZIKV populations in the American continent. © 2017 Wiley Periodicals, Inc."
2,10.1534/genetics.117.300403,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037057470&doi=10.1534%2fgenetics.117.300403&partnerID=40&md5=627d7c1de6aac65d4b416bcacc6fc5ab,"A crucial step in plant breeding is the selection and combination of parents to form new crosses. Genome-based prediction guides the selection of high-performing parental lines in many crop breeding programs which ensures a high mean performance of progeny. To warrant maximum selection progress, a new cross should also provide a large progeny variance. The usefulness concept as measure of the gain that can be obtained from a specific cross accounts for variation in progeny variance. Here, it is shown that genetic gain can be considerably increased when crosses are selected based on their genomic usefulness criterion compared to selection based on mean genomic estimated breeding values. An efficient and improved method to predict the genetic variance of a cross based on Markov chain Monte Carlo samples of marker effects from a whole-genome regression model is suggested. In simulations representing selection procedures in crop breeding programs, the performance of this novel approach is compared with existing methods, like selection based on mean genomic estimated breeding values and optimal haploid values. In all cases, higher genetic gain was obtained compared with previously suggested methods. When 1% of progenies per cross were selected, the genetic gain based on the estimated usefulness criterion increased by 0.14 genetic standard deviation compared to a selection based on mean genomic estimated breeding values. Analytical derivations of the progeny genotypic variance-covariance matrix based on parental genotypes and genetic map information make simulations of progeny dispensable, and allow fast implementation in large-scale breeding programs. © 2017 by the Genetics Society of America."
,10.5194/npg-24-701-2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036670404&doi=10.5194%2fnpg-24-701-2017&partnerID=40&md5=9396abfde92d901da0bab9e855ffb36c,"When taking the model error into account in data assimilation, one needs to evaluate the prior distribution represented by the Onsager-Machlup functional. Through numerical experiments, this study clarifies how the prior distribution should be incorporated into cost functions for discrete-time estimation problems. Consistent with previous theoretical studies, the divergence of the drift term is essential in weak-constraint 4D-Var (w4D-Var), but it is not necessary in Markov chain Monte Carlo with the Euler scheme. Although the former property may cause difficulties when implementing w4D-Var in large systems, this paper proposes a new technique for estimating the divergence term and its derivative. © 2017 Author (s)."
1,10.1002/env.2476,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029395268&doi=10.1002%2fenv.2476&partnerID=40&md5=a07a223886ff5aa9235198dd5fb889c3,"We analyze the behavior of extreme winds occurring in Southern California during the Santa Ana wind season using a latent mixture model. This mixture representation is formulated as a hierarchical Bayesian model and fit using Markov chain Monte Carlo. The two-stage model results in generalized Pareto margins for exceedances and generates temporal dependence through a latent Markov process. This construction induces asymptotic independence in the response, while allowing for dependence at extreme, but subasymptotic, levels. We compare this model with a frequentist analogue where inference is performed via maximum pairwise likelihood. We use interval censoring to account for data quantization and estimate the extremal index and probabilities of multiday occurrences of extreme Santa Ana winds over a range of high thresholds. Copyright © 2017 John Wiley & Sons, Ltd."
2,10.1111/jcpe.12775,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037151028&doi=10.1111%2fjcpe.12775&partnerID=40&md5=7b56e9851d067801a024ee51599a40a0,"Aim: Professional oral health care (POHC) prevents nursing home-acquired pneumonia (NHAP) and its related mortality. We assessed the cost-effectiveness of POHC versus no POHC (nPOHC) and the monetary value of eliminating uncertainty by future research. Methods: A German public–private payer perspective was adopted. A Markov model was used, following long-term care residents from admission to death. Cost-effectiveness was estimated as Euro/disability-adjusted life year (DALY) using Monte Carlo microsimulations. Value-of-information analyses were performed. The willingness-to-pay threshold/DALY was assumed to be 66% (range 50%–100%) of per-capita gross domestic product (GDP). Results: nPOHC was less costly (€3,024) but also less effective (0.89 DALYs) than POHC (€10,249, 0.55 DALYs). For most presumed payers, POHC was cost-effective. The cost-effectiveness of POHC was higher in smokers, underweight or pulmonary disease patients. Eliminating uncertainty about the NHAP costs, NHAP incidence/mortality, and POHC effectiveness would result in an expected net value of 47 million €/year (and even higher values at lower GDP thresholds), and is likely to decrease with time. Conclusions: Within the chosen setting and on the basis of current evidence, POHC was cost-effective. Given the detected uncertainty, further research seems warranted. © 2017 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd"
,10.1007/s10651-017-0391-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034604201&doi=10.1007%2fs10651-017-0391-1&partnerID=40&md5=ca1cf0f990a21f3c986b1ca4a177b01c,"Models for multivariate space–time geostatistical data have received a growing interest in spatial and spatiotemporal epidemiology. However, specifying models that can capture associations within and among multivariate measurements is usually a challenge. The main goal of this paper is to introduce and review cross-covariance functions that are rich in structure and are computationally feasible. Integrated nested Laplace approximation combined with stochastic partial differential equations were used for inference and prediction, as a fast and precise alternative to the computationally intensive Markov chain Monte Carlo methods. A large set of models is considered in this paper: models assuming independent, shared or correlated spatial and temporal processes (with nine possible combinations), and models with independent, shared and linear models of coregionalization spatiotemporal processes. Different processes are applied to Culicoides data and compared. Bayesian spatial prediction results show that the central and Northeastern parts of Belgium had the highest prevalence of Culicoides in summer months and the lowest prevalence in winter months. © 2017, Springer Science+Business Media, LLC, part of Springer Nature."
6,10.1200/JCO.2016.70.4767,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036563163&doi=10.1200%2fJCO.2016.70.4767&partnerID=40&md5=1e374eba2728d573730de8539abf0014,"Purpose To estimate the prevalence of sperm banking among adolescent males newly diagnosed with cancer and to identify factors associated with banking outcomes. Patients and Methods A prospective, single-group, observational study design was used to test the contribution of sociodemographic, medical, psychological/health belief, communication, and developmental factors to fertility preservation outcomes. At-risk adolescent males (N = 146; age 13.00 to 21.99 years; Tanner stage $ 3), their parents, and medical providers from eight leading pediatric oncology centers across the United States and Canada completed self-report questionnaires within 1 week of treatment initiation. Multivariable logistic regression was used to calculate odds ratios (ORs) and 95% CIs for specified banking outcomes (collection attempt v no attempt and successful completion of banking v no banking). Results Among adolescents (mean age, 16.49 years; standard deviation, 2.02 years), 53.4% (78 of 146) made a collection attempt, with 43.8% (64 of 146) successfully banking sperm (82.1% of attempters). The overall attempt model revealed adolescent consultation with a fertility specialist (OR, 29.96; 95% CI, 2.48to 361.41;P =.007), parent recommendation tobank (OR, 12.30; 95% CI, 2.01 to75.94; P =.007), and higher Tanner stage (OR, 5.42; 95% CI, 1.75 to 16.78; P =.003) were associated with anincreased likelihood of acollection attempt. Adolescent history of masturbation (OR, 5.99; 95% CI, 1.25 to 28.50; P =.025), banking self-efficacy (OR, 1.23; 95% CI, 1.05 to 1.45; P =.012), and parent (OR, 4.62; 95% CI, 1.46 to 14.73; P =.010) or medical team (OR, 4.26; 95% CI, 1.45 to 12.43; P =.008) recommendation to bank were associated with increased likelihood of sperm banking completion. Conclusion Although findings suggest that banking is underutilized, modifiable adolescent, parent, and provider factors associated with banking outcomes were identified and should be targeted in future intervention efforts. © 2017 by American Society of Clinical Oncology."
1,10.1016/j.fertnstert.2017.08.039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036586747&doi=10.1016%2fj.fertnstert.2017.08.039&partnerID=40&md5=1b8a3aaad12316cdb57c2ca40b76e749,"Objective To investigate the influence of parental sociodemographic, communication, and psychological factors on sperm collection attempts among at-risk adolescent males newly diagnosed with cancer. Design Prospective, single group, observational study design. Setting Pediatric oncology centers. Patient(s) Parents (N = 144) of 122 newly diagnosed adolescent males at increased risk for infertility secondary to cancer therapy. Intervention(s) Survey-based assessment of parent factors associated with adolescent collection attempts. Main Outcome Measure(s) Attempt of manual collection of sperm. Result(s) Parental recommendation to bank sperm (odds ratio [OR] 3.72; 95% confidence interval [CI] 1.18–11.76) and perceived self-efficacy to facilitate banking (OR 1.20; 95% CI 1.02–1.41) were associated with an increased likelihood of making a collection attempt. Conclusion(s) Parental recommendation to bank is a critical influence for sperm banking among adolescent males newly diagnosed with cancer. These findings highlight the importance of effective communication between parents, patients, and health-care teams when discussing preservation options. Parent perceptions of their ability to facilitate sperm banking at the time of diagnosis should also be targeted in future interventions. Clinical Trial Registration Number NCT01152268 © 2017"
4,10.1177/0962280215602040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038816278&doi=10.1177%2f0962280215602040&partnerID=40&md5=31c9a87ce1a7857d1f8065312b9b87d8,"The receiver operating characteristic (ROC) curve is frequently used as a measure of accuracy of continuous markers in diagnostic tests. The area under the ROC curve (AUC) is arguably the most widely used summary index for the ROC curve. Although the small sample size scenario is common in medical tests, a comprehensive study of small sample size properties of various methods for the construction of the confidence/credible interval (CI) for the AUC has been by and large missing in the literature. In this paper, we describe and compare 29 non-parametric and parametric methods for the construction of the CI for the AUC when the number of available observations is small. The methods considered include not only those that have been widely adopted, but also those that have been less frequently mentioned or, to our knowledge, never applied to the AUC context. To compare different methods, we carried out a simulation study with data generated from binormal models with equal and unequal variances and from exponential models with various parameters and with equal and unequal small sample sizes. We found that the larger the true AUC value and the smaller the sample size, the larger the discrepancy among the results of different approaches. When the model is correctly specified, the parametric approaches tend to outperform the non-parametric ones. Moreover, in the non-parametric domain, we found that a method based on the Mann–Whitney statistic is in general superior to the others. We further elucidate potential issues and provide possible solutions to along with general guidance on the CI construction for the AUC when the sample size is small. Finally, we illustrate the utility of different methods through real life examples. © 2015, © The Author(s) 2015."
,10.1109/TPAMI.2016.2646685,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038214555&doi=10.1109%2fTPAMI.2016.2646685&partnerID=40&md5=c1b1374a65cf08f4f7120ab581189e43,"We propose novel finite-dimensional spaces of well-behaved Rn →Rn transformations. The latter are obtained by (fast and highly-accurate) integration of continuous piecewise-affine velocity fields. The proposed method is simple yet highly expressive, effortlessly handles optional constraints (e.g., volume preservation and/or boundary conditions), and supports convenient modeling choices such as smoothing priors and coarse-to-fine analysis. Importantly, the proposed approach, partly due to its rapid likelihood evaluations and partly due to its other properties, facilitates tractable inference over rich transformation spaces, including using Markov-Chain Monte-Carlo methods. Its applications include, but are not limited to: monotonic regression (more generally, optimization over monotonic functions); modeling cumulative distribution functions or histograms; time-warping; image warping; image registration; real-time diffeomorphic image editing; data augmentation for image classifiers. Our GPU-based code is publicly available. © 1979-2012 IEEE."
,10.1111/1475-6773.12786,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033475031&doi=10.1111%2f1475-6773.12786&partnerID=40&md5=4dd2c150f7cd67cf7fc7d18c265e059d,"Objective: To estimate the societal economic and health impacts of Maine's school-based influenza vaccination (SIV) program during the 2009 A(H1N1) influenza pandemic. Data Sources: Primary and secondary data covering the 2008–09 and 2009–10 influenza seasons. Study Design: We estimated weekly monovalent influenza vaccine uptake in Maine and 15 other states, using difference-in-difference-in-differences analysis to assess the program's impact on immunization among six age groups. We also developed a health and economic Markov microsimulation model and conducted Monte Carlo sensitivity analysis. Data Collection: We used national survey data to estimate the impact of the SIV program on vaccine coverage. We used primary data and published studies to develop the microsimulation model. Principal Findings: The program was associated with higher immunization among children and lower immunization among adults aged 18–49 years and 65 and older. The program prevented 4,600 influenza infections and generated $4.9 million in net economic benefits. Cost savings from lower adult vaccination accounted for 54 percent of the economic gain. Economic benefits were positive in 98 percent of Monte Carlo simulations. Conclusions: SIV may be a cost-beneficial approach to increase immunization during pandemics, but programs should be designed to prevent lower immunization among nontargeted groups. © Health Research and Educational Trust"
1,10.1287/isre.2017.0724,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038120427&doi=10.1287%2fisre.2017.0724&partnerID=40&md5=436ba988a7a52bd205e4b742f30e684d,"Although keyword auctions are often studied in the context of a single keyword in the literature, firms generally have to participate in multiple keyword auctions at the same time. Advertisers purchase a variety of keywords that can be categorized as genericrelevant, focal-brand, and competing-brand keywords. At the same time, firms also have to choose how the keywords can be matched to search queries: exact, phrase, or broad. This study empirically examines how keyword categories and match types influence the performance of advertising campaigns.We build a hierarchical Bayesian model to address the endogeneity problem contained in the simultaneous equations of the click-through rate, the conversion rate, cost per click, and rank, and we use the Markov Chain Monte Carlo method to identify the parameters. Our results suggest that it is important to differentiate among the various bidding strategies for various keyword categories and match types.We also report results related to financial performance such as number of sales, profit, and return on investment for different keywords. These findings shed light on the practice of sponsored search advertising by offering insights into how to manage ad campaigns when advertisers have to bid on multiple keywords. © 2017 INFORM."
2,10.1371/journal.pone.0189994,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038838341&doi=10.1371%2fjournal.pone.0189994&partnerID=40&md5=73a4fb1cb05faeb478b7b51e06a98de0,"The Malliavin calculus is an extension of the classical calculus of variations from deterministic functions to stochastic processes. In this paper we aim to show in a practical and didactic way how to calculate the Malliavin derivative, the derivative of the expectation of a quantity of interest of a model with respect to its underlying stochastic parameters, for four problems found in mechanics. The non-intrusive approach uses the Malliavin Weight Sampling (MWS) method in conjunction with a standard Monte Carlo method. The models are expressed as ODEs or PDEs and discretised using the finite difference or finite element methods. Specifically, we consider stochastic extensions of; a 1D Kelvin-Voigt viscoelastic model discretised with finite differences, a 1D linear elastic bar, a hyperelastic bar undergoing buckling, and incompressible Navier-Stokes flow around a cylinder, all discretised with finite elements. A further contribution of this paper is an extension of the MWS method to the more difficult case of non-Gaussian random variables and the calculation of second-order derivatives. We provide open-source code for the numerical examples in this paper. © 2017 Hauseux et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
8,10.1002/eqe.2922,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021317396&doi=10.1002%2feqe.2922&partnerID=40&md5=5c9a2b393efc0586aa2f7753cb32998f,"It is desirable that nonlinear dynamic analyses for structural fragility assessment are performed using unscaled ground motions. The widespread use of a simple dynamic analysis procedure known as Cloud Analysis, which uses unscaled records and linear regression, has been impeded by its alleged inaccuracies. This paper investigates fragility assessment based on Cloud Analysis by adopting, as the performance variable, a scalar demand to capacity ratio that is equal to unity at the onset of limit state. It is shown that the Cloud Analysis, performed based on a careful choice of records, leads to reasonable and efficient fragility estimates. There are 2 main rules to keep in mind for record selection: to make sure that a good portion of the records leads to a demand to capacity ratio greater than unity and that the dispersion in records' seismic intensity is considerable. An inevitable consequence of implementing these rules is that one often needs to deal with the so-called collapse cases. To formally consider the collapse cases, a 5-parameter fragility model is proposed that mixes the simple regression in the logarithmic scale with logistic regression. The joint distribution of fragility parameters can be obtained by adopting a Markov Chain Monte Carlo simulation scheme leading directly to the fragility and its confidence intervals. The resulting fragility curves compare reasonably with those obtained from the Incremental Dynamic Analysis and Multiple Stripe Analysis with (variable) conditional spectrum–compatible suites of records at different intensity levels for 3 older reinforced concrete frames with shear-, shear-flexure-, and flexure-dominant behavior. Copyright © 2017 John Wiley & Sons, Ltd."
3,10.1016/j.epidem.2017.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020911978&doi=10.1016%2fj.epidem.2017.06.002&partnerID=40&md5=aada12a3eeaca4a1f0a6cc9670e65c1c,"Although different structures are used in modern tuberculosis (TB) models to simulate TB latency, it remains unclear whether they are all capable of reproducing the particular activation dynamics empirically observed. We aimed to determine which of these structures replicate the dynamics of progression accurately. We reviewed 88 TB-modelling articles and classified them according to the latency structure employed. We then fitted these different models to the activation dynamics observed from 1352 infected contacts diagnosed in Victoria (Australia) and Amsterdam (Netherlands) to obtain parameter estimates. Six different model structures were identified, of which only those incorporating two latency compartments were capable of reproducing the activation dynamics empirically observed. We found important differences in parameter estimates by age. We also observed marked differences between our estimates and the parameter values used in many previous models. In particular, when two successive latency phases are considered, the first period should have a duration that is much shorter than that used in previous studies. In conclusion, structures incorporating two latency compartments and age-stratification should be employed to accurately replicate the dynamics of TB latency. We provide a catalogue of parameter values and an approach to parameter estimation from empiric data for calibration of future TB-models. © 2017 The Authors"
2,10.1371/journal.pone.0189234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039734061&doi=10.1371%2fjournal.pone.0189234&partnerID=40&md5=4110b92f37fe15f24906efd6eeee7bde,"With globalization the Western honey bee has become a nearly cosmopolitan species, but it was originally restricted to the Old World. This renowned model of biodiversity has diverged into five evolutionary lineages and several geographic “subspecies.” If Apis mellifera unicolor is indubitably an African subspecies endemic to Madagascar, its relationship with honey bees from three archipelagos in the southwest Indian Ocean (SWIO) hotspot of biodiversity is misunderstood. We compared recent mtDNA diversity data to an original characterization of the nuclear diversity from honey bees in the Mascarenes and Comoros archipelagos, using 14 microsatellites, but also additional mtDNA tRNALeu-cox2 analysis. Our sampling offers the most comprehensive dataset for the SWIO populations with a total of 3,270 colonies from 10 islands compared with 855 samples from Madagascar, 113 from Africa, and 138 from Europe. Comprehensive mitochondrial screening confirmed that honey bees from La Réunion, Mauritius, and Comoros archipelagos are mainly of African origin (88.1% out of 2,746 colonies) and that coexistence with European lineages occurs only in the Mascarenes. PCA, Bayesian, and genetic differentiation analysis showed that African colonies are not significantly distinct on each island, but have diversified among islands and archipelagos. FST levels progressively decreased in significance from European and African continental populations, to SWIO insular and continental populations, and finally among islands from the same archipelago. Among African populations, Madagascar shared a nuclear background with and was most closely related to SWIO island populations (except Rodrigues). Only Mauritius Island presented clear cytoplasmic disequilibrium and genetic structure characteristic of an admixed population undergoing hybridization, in this case, between A. m. unicolor and A. m. ligustica, A. m. carnica and A. m. mellifera-like individuals. Finally, global genetic clustering analysis helped to better depict the colonization and introduction pattern of honey bee populations in these archipelagos. © 2017 Techer et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
2,10.1038/s41598-017-12172-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030322043&doi=10.1038%2fs41598-017-12172-2&partnerID=40&md5=ed4d6c0ad34b1bfeb6986addce458471,"In Escherichia coli DNA replication yields interlinked chromosomes. Controlling topological changes associated with replication and returning the newly replicated chromosomes to an unlinked monomeric state is essential to cell survival. In the absence of the topoisomerase topoIV, the site-specific recombination complex XerCD- dif-FtsK can remove replication links by local reconnection. We previously showed mathematically that there is a unique minimal pathway of unlinking replication links by reconnection while stepwise reducing the topological complexity. However, the possibility that reconnection preserves or increases topological complexity is biologically plausible. In this case, are there other unlinking pathways? Which is the most probable? We consider these questions in an analytical and numerical study of minimal unlinking pathways. We use a Markov Chain Monte Carlo algorithm with Multiple Markov Chain sampling to model local reconnection on 491 different substrate topologies, 166 knots and 325 links, and distinguish between pathways connecting a total of 881 different topologies. We conclude that the minimal pathway of unlinking replication links that was found under more stringent assumptions is the most probable. We also present exact results on unlinking a 6-crossing replication link. These results point to a general process of topology simplification by local reconnection, with applications going beyond DNA. © 2017 The Author(s)."
13,10.1002/2016JE005133,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020619860&doi=10.1002%2f2016JE005133&partnerID=40&md5=21ec32c0a342f4041da9622a4fb1a1ef,"During its ascent up Mount Sharp, the Mars Science Laboratory Curiosity rover traversed the Bagnold Dune Field. We model sand modal mineralogy and grain size at four locations near the rover traverse, using orbital shortwave infrared single-scattering albedo spectra and a Markov chain Monte Carlo implementation of Hapke's radiative transfer theory to fully constrain uncertainties and permitted solutions. These predictions, evaluated against in situ measurements at one site from the Curiosity rover, show that X-ray diffraction-measured mineralogy of the basaltic sands is within the 95% confidence interval of model predictions. However, predictions are relatively insensitive to grain size and are nonunique, especially when modeling the composition of minerals with solid solutions. We find an overall basaltic mineralogy and show subtle spatial variations in composition in and around the Bagnold Dunes, consistent with a mafic enrichment of sands with cumulative aeolian-transport distance by sorting of olivine, pyroxene, and plagioclase grains. Furthermore, the large variations in Fe and Mg abundances (~20 wt %) at the Bagnold Dunes suggest that compositional variability may be enhanced by local mixing of well-sorted sand with proximal sand sources. Our estimates demonstrate a method for orbital quantification of composition with rigorous uncertainty determination and provide key constraints for interpreting in situ measurements of compositional variability within Martian aeolian sandstones. ©2017. The Authors."
,10.3892/ol.2017.7158,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032679956&doi=10.3892%2fol.2017.7158&partnerID=40&md5=20375cdd6def8b3e4a130c3ddbd616d7,"The aim of the present study was to identify differentially expressed molecular functions (DEMFs) for breast cancer using the Gibbs sampling approach. Molecular functions (MFs) were obtained on the basis of the Bayesian Approach for Geneset Selection package. Subsequently, MFs were converted into Markov chains (MCs) prior to calculating their probabilities, utilizing the MC Monte Carlo algorithm. DEMFs were identified with probabilities ≥0.8 and the gene compositions were studied. Finally, a co-expression network was constructed via the empirical Bayes method and a pathway enrichment analysis of genes in DEMFs was performed. A total of 396 MFs were identified and all transformed to MCs. With the threshold, 2 DEMFs (structural molecule activity and protein heterodimerization activity) were obtained. The DEMFs were comprised of 297 genes, 259 of which were mapped to the co-expression network. These 297 genes were identified to be enriched in 10 pathways, and ribosome was the most significant pathway. The results of the present study revealed 2 DEMFs (structural molecule activity and protein heterodimerization activity) which may be associated with the pathological molecular mechanisms underlying breast cancer, based on Gibbs sampling. © 2017, Spandidos Publications. All rights reserved."
2,10.1007/JHEP12(2017)010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037652562&doi=10.1007%2fJHEP12%282017%29010&partnerID=40&md5=fdc8bd3ce03bbde193cadcfd301fa712,"We have considered a model of Dark Minimal Flavour Violation (DMFV), in which a triplet of dark matter particles couple to right-handed up-type quarks via a heavy colour-charged scalar mediator. By studying a large spectrum of possible constraints, and assessing the entire parameter space using a Markov Chain Monte Carlo (MCMC), we can place strong restrictions on the allowed parameter space for dark matter models of this type. © 2017, The Author(s)."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040354332&partnerID=40&md5=0c2ca75ff9bb34af7e387e821f653411,"This paper aims to analyze judicial political attitudes of justices of the Turkish Constitutional Court (TCC) between 1962 and 1982 by means of Item Response Theory using Markov Chain Monte Carlo (MCMM) algorithms. We have created an original database in which the Court's decisions on merit concerning abstract and concrete norms reviews and dissolution of political parties are classified into categories of libertarian, statist and unclassified and votes of the justices are coded under one of these categories. As a result of ideal point estimations, we have found that neither a libertarian nor a statist attitude was dominant for a great majority of the Court's justices. We conclude that contrary to the widely-held belief, the justices of the TCC did not adopt a strict statist attitude in the examined period."
4,10.1128/JVI.01372-17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032823152&doi=10.1128%2fJVI.01372-17&partnerID=40&md5=b2bb5ed8744e4123860bb665678ff670,"Hepatitis C virus (HCV) can be transmitted from mother to child during pregnancy and childbirth. However, the timing and precise biological mechanisms that are involved in this process are incompletely understood, as are the determinants that influence transmission of particular HCV variants. Here we report results of a longitudinal assessment of HCV quasispecies diversity and composition in 5 cases of vertical HCV transmission, including 3 women coinfected with human immunodeficiency virus type 1 (HIV-1). The population structure of HCV variant spectra based on E2 envelope gene sequences (nucleotide positions 1491 to 1787), including hypervariable regions 1 and 2, was characterized using next-generation sequencing and median-joining network analysis. Compatible with a loose transmission bottleneck, larger numbers of shared HCV variants were observed in the presence of maternal coinfection. Coalescent Bayesian Markov chain Monte Carlo simulations revealed median times of transmission between 24.9 weeks and 36.1 weeks of gestation, with some confidence intervals ranging into the 1st trimester, considerably earlier than previously thought. Using recombinant autologous HCV pseudoparticles, differences were uncovered in HCV-specific antibody responses between coinfected mothers and mothers infected with HCV alone, in whom generalized absence of neutralization was observed. Finally, shifts in HCV quasispecies composition were seen in children around 1 year of age, compatible with the disappearance of passively transferred maternal immunoglobulins and/or the development of HCVspecific humoral immunity. Taken together, these results provide insights into the timing, dynamics, and biologic mechanisms involved in vertical HCV transmission and inform preventative strategies. © 2017 American Society for Microbiology."
4,10.4430/bgta0199,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040327267&doi=10.4430%2fbgta0199&partnerID=40&md5=3aabb25f58a31a7d492f1cee55eb838f,"We cast the genetic algorithm-full waveform inversion (GA-FWI) in a probabilistic framework that through a multi-step procedure, allows us to estimate the posterior probability distribution (PPD) in model space. Since GA is not a Markov chain Monte Carlo method, it is necessary to refine the PPD estimated by GA (GA PPD) via a resampling of the model space with a Gibbs sampler (GS), thus obtaining the GA+GS PPDs. We apply this procedure to two acoustic 2D models, an inclusion model and the Marmousi model, and we find a good agreement between the derived PPDs and the varying resolution due to changes in the seismic illumination. Finally, we randomly extract several models from the so derived PPDs to start many local full-waveform inversions (LFWIs), which produce final high-resolution models. This set of models is then used to numerically estimate the final uncertainty (GA+GS+LFWI PPD). The multimodal and wide PPDs derived from the GA optimization, become unimodal and narrower after LFWI and, in the well illuminated parts of the subsurface, the final GA+GS+LFWI PPDs contain the true model parameters. This confirms the ability of the GA optimization in finding a velocity model suitable as input to LFWI. © 2017 - OGS."
1,10.3102/0034654317723009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032927817&doi=10.3102%2f0034654317723009&partnerID=40&md5=e48abe3ba8d3b3e4212965c38b25da0a,"Computer-based scaffolding provides temporary support that enables students to participate in and become more proficient at complex skills like problem solving, argumentation, and evaluation. While meta-analyses have addressed between-subject differences on cognitive outcomes resulting from scaffolding, none has addressed within-subject gains. This leaves much quantitative scaffolding literature not covered by existing meta-analyses. To address this gap, this study used Bayesian network meta-analysis to synthesize within-subjects (pre–post) differences resulting from scaffolding in 56 studies. We generated the posterior distribution using 20,000 Markov Chain Monte Carlo samples. Scaffolding has a consistently strong effect across student populations, STEM (science, technology, engineering, and mathematics) disciplines, and assessment levels, and a strong effect when used with most problem-centered instructional models (exception: inquiry-based learning and modeling visualization) and educational levels (exception: secondary education). Results also indicate some promising areas for future scaffolding research, including scaffolding among students with learning disabilities, for whom the effect size was particularly large (ḡ = 3.13). © 2017, © 2017 AERA."
,10.1038/s41598-017-09962-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028505039&doi=10.1038%2fs41598-017-09962-z&partnerID=40&md5=e76432f36027f98a5061eb90dc57e562,"In the immediate aftermath of a strong earthquake and in the presence of an ongoing aftershock sequence, scientific advisories in terms of seismicity forecasts play quite a crucial role in emergency decision-making and risk mitigation. Epidemic Type Aftershock Sequence (ETAS) models are frequently used for forecasting the spatio-temporal evolution of seismicity in the short-term. We propose robust forecasting of seismicity based on ETAS model, by exploiting the link between Bayesian inference and Markov Chain Monte Carlo Simulation. The methodology considers the uncertainty not only in the model parameters, conditioned on the available catalogue of events occurred before the forecasting interval, but also the uncertainty in the sequence of events that are going to happen during the forecasting interval. We demonstrate the methodology by retrospective early forecasting of seismicity associated with the 2016 Amatrice seismic sequence activities in central Italy. We provide robust spatio-temporal short-term seismicity forecasts with various time intervals in the first few days elapsed after each of the three main events within the sequence, which can predict the seismicity within plus/minus two standard deviations from the mean estimate within the few hours elapsed after the main event. © 2017 The Author(s)."
,10.1007/s12561-016-9150-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974823989&doi=10.1007%2fs12561-016-9150-3&partnerID=40&md5=e3a0d137a28626e0a7ef67e8a7cb5479,"Performing studies on the risks of environmental hazards on human health requires accurate estimates of exposures that might be experienced by the populations at risk. Often there will be missing data and in many epidemiological studies, the locations and times of exposure measurements and health data do not match. To a large extent this will be due to the health and exposure data having arisen from completely different data sources and not as the result of a carefully designed study, leading to problems of both ‘change of support’ and ‘misaligned data’. In such cases, a direct comparison of the exposure and health outcome is often not possible without an underlying model to align the two in the spatial and temporal domains. The Bayesian approach provides the natural framework for such models; however, the large amounts of data that can arise from environmental networks means that inference using Markov Chain Monte Carlo might not be computationally feasible in this setting. Here we adapt the integrated nested Laplace approximation to implement spatio–temporal exposure models. We also propose methods for the integration of large-scale exposure models and health analyses. It is important that any model structure allows the correct propagation of uncertainty from the predictions of the exposure model through to the estimates of risk and associated confidence intervals. The methods are demonstrated using a case study of the levels of black smoke in the UK, measured over several decades, and respiratory mortality. © 2016, The Author(s)."
2,10.1111/2041-210X.12826,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026288459&doi=10.1111%2f2041-210X.12826&partnerID=40&md5=cc62c6292ac143888b1796701a8b503d,"Evolutionary integration occurs when two or more phenotypes evolve in a correlated fashion. Correlated evolution among traits can happen due to genetic constraints, ontogeny, and selection and have an important impact on the trajectory of phenotypic evolution. Phylogenetic trees can be used to study such pattern on macroevolutionary time scales by estimating the strength of evolutionary covariance among traits through time and across clades. However, only few applications implement models to conduct comparative analyses of evolutionary integration. We introduce a Bayesian Markov chain Monte Carlo approach to estimate the evolutionary correlation among two or more traits using the evolutionary rate matrix (R). R is a covariance matrix that represents both the rates of evolution of each trait and the structure of evolutionary correlation among traits. Here, we present the R package ratematrix, a resource to test hypotheses of evolutionary integration using multivariate data and phylogenetic trees. ratematrix provides a flexible framework allowing for any number of evolutionary rate matrix regimes fitted to the same phylogenetic tree and it incorporates the uncertainty associated with parameter estimates, ancestral state reconstruction and phylogenetic estimation in the analyses. The ratematrix package uses a novel pruning algorithm that significantly improve computational time. We also provide specific functions that facilitate users to conduct long MCMC analysis when computational resources are limited. © 2017 The Authors. Methods in Ecology and Evolution © 2017 British Ecological Society"
3,10.1007/s40725-017-0069-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043691127&doi=10.1007%2fs40725-017-0069-9&partnerID=40&md5=3cebbf9004a10284630970769090321f,"Purpose of review: Forest models are tools for analysis and prediction of productivity and other services. Model outputs can only be useful if possible errors in inputs and model structure are recognized. However, errors cannot be quantified directly, making uncertainty inevitable. In this paper, we aim to clarify terminological confusion around the concepts of error and uncertainty and review current methods for addressing uncertainty in forest modelling. Recent findings: Modellers increasingly recognize that all uncertainties—in data, model inputs and model structure—can be represented using probability distributions. This has stimulated the use of Bayesian methods for quantifying and reducing uncertainty and error in models of forests and other vegetation. The Achilles’ heel of Bayesian methods has always been their computational demand, but solutions are being found. Summary: We conclude that future work will likely include (1) more use of Bayesian methods, (2) more use of hierarchical modelling, (3) replacement of model spin-up by Bayesian calibration, (4) more use of ensemble modelling and Bayesian model averaging, (5) new ways to account for model structural error in calibration, (6) better software for Bayesian calibration of complex models, (7) faster Markov chain Monte Carlo algorithms, (8) more use of model emulators, (9) novel uncertainty visualization techniques, (10) more use of graphical modelling and (11) more use of risk analysis. © 2017, Springer International Publishing AG."
1,10.1186/s13662-017-1225-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021419419&doi=10.1186%2fs13662-017-1225-z&partnerID=40&md5=d27e3f291e8714756af4ad03d64a427a,"Ebola virus infection is a severe infectious disease with the highest case fatality rate which has become the global public health treat now. What makes the disease the worst of all is no specific effective treatment available, its dynamics is not much researched and understood. In this article a new mathematical model incorporating both vaccination and quarantine to study the dynamics of Ebola epidemic has been developed and comprehensively analyzed using fractional derivative in the sense of the Caputo derivative of order α∈ (0 , 1 ]. The existence as well as nonnegativity of the solution to the model is also verified and the basic reproduction number is calculated. Besides, stability conditions are also checked and finally simulation is done using both the Euler method and one of the top ten most influential algorithms known as Markov Chain Monte Carlo (MCMC) method. Different rates of vaccination to predict the effect of vaccination on the infected individual over time and that of quarantine are discussed. The results show that quarantine and vaccination are very effective ways to control Ebola epidemic. From our study it was also seen that there is less possibility of an individual for getting Ebola virus for the second time if they survived his/her first infection. Last but not least, real data has been fitted to the model, showing that it can be used to predict the dynamic of Ebola epidemic. © 2017, The Author(s)."
1,10.1038/s41598-017-17174-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036594534&doi=10.1038%2fs41598-017-17174-8&partnerID=40&md5=11f6bed699197248115d2ba862a142b2,"Epidemiological parameters for livestock diseases are often inferred from transmission experiments. However, there are several limitations inherent to the design of such experiments that limits the precision of parameter estimates. In particular, infection times and latent periods cannot be directly observed and infectious periods may also be censored. We present a Bayesian framework accounting for these features directly and employ Markov chain Monte Carlo techniques to provide robust inferences and quantify the uncertainty in our estimates. We describe the transmission dynamics using a susceptible-exposed-infectious-removed compartmental model, with gamma-distributed transition times. We then fit the model to published data from transmission experiments for foot-and-mouth disease virus (FMDV) and African swine fever virus (ASFV). Where the previous analyses of these data made various assumptions on the unobserved processes in order to draw inferences, our Bayesian approach includes the unobserved infection times and latent periods and quantifies them along with all other model parameters. Drawing inferences about infection times helps identify who infected whom and can also provide insights into transmission mechanisms. Furthermore, we are able to use our models to measure the difference between the latent periods of inoculated and contact-challenged animals and to quantify the effect vaccination has on transmission. © 2017 The Author(s)."
,10.1111/sjos.12279,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020494587&doi=10.1111%2fsjos.12279&partnerID=40&md5=d1ab75363501b55895bf0062c96addf8,"Estimation of time-average variance constant (TAVC), which is the asymptotic variance of the sample mean of a dependent process, is of fundamental importance in various fields of statistics. For frequentists, it is crucial for constructing confidence interval of mean and serving as a normalizing constant in various test statistics and so forth. For Bayesians, it is widely used for evaluating effective sample size and conducting convergence diagnosis in Markov chain Monte Carlo method. In this paper, by considering high-order corrections to the asymptotic biases, we develop a new class of TAVC estimators that enjoys optimal L2-convergence rates under different degrees of the serial dependence of stochastic processes. The high-order correction procedure is applicable to estimation of the so-called smoothness parameter, which is essential in determining the optimal bandwidth. Comparisons with existing TAVC estimators are comprehensively investigated. In particular, the proposed optimal high-order corrected estimator has the best performance in terms of mean squared error. © 2017 Board of the Foundation of the Scandinavian Journal of Statistics"
1,10.1371/journal.pone.0189605,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038438515&doi=10.1371%2fjournal.pone.0189605&partnerID=40&md5=e42ef1de5c5575f682c23a834cb1c2f9,"Objective Our previous 2005–2009 molecular epidemiological study in Mongolia identified a hot spot of HIV-1 transmission in men who have sex with men (MSM). To control the infection, we collaborated with NGOs to promote safer sex and HIV testing since mid-2010. In this study, we carried out the second molecular epidemiological survey between 2010 and 2016 to determine the status of HIV-1 infection in Mongolia. Methods The study included 143 new cases of HIV-1 infection. Viral RNA was extracted from stocked plasma samples and sequenced for the pol and the env regions using the Sanger method. Near-full length sequencing using MiSeq was performed in 3 patients who were suspected to be infected with recombinant HIV-1. Phylogenetic analysis was performed using the neighbor-joining method and Bayesian Markov chain Monte Carlo method. Results MSM was the main transmission route in the previous and current studies. However, heterosexual route showed a significant increase in recent years. Phylogenetic analysis documented three taxa; Mongolian B, Korean B, and CRF51_01B, though the former two were also observed in the previous study. CRF51_01B, which originated from Singapore and Malaysia, was confirmed by near-full length sequencing. Although these strains were mainly detected in MSM, they were also found in increasing numbers of heterosexual males and females. Bayesian phylogenetic analysis estimated transmission of CRF51_01B into Mongolia around early 2000s. An extended Bayesian skyline plot showed a rapid increase in the effective population size of Mongolian B cluster around 2004 and that of CRF51_01B cluster around 2011. Conclusions HIV-1 infection might expand to the general population in Mongolia. Our study documented a new cluster of HIV-1 transmission, enhancing our understanding of the epidemiological status of HIV-1 in Mongolia. © 2017 Jagdagsuren et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.12963/csd.17432,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040349902&doi=10.12963%2fcsd.17432&partnerID=40&md5=3e8d6c0cf4443a1e1f7217be9eed1c92,"Objectives: This study investigated predictors of word reading and spelling in first grade children with dyslexia. Methods: Twenty-four first graders with dyslexia participated in the study. In order to measure the children's reading and spelling abilities, a word decoding test, word recognition test, and spelling test were conducted. Other early literacy skills, including letter knowledge, phonological awareness, morphological awareness, orthographic awareness, rapid naming, working memory, and vocabulary were measured as predictors of reading and spelling abilities. Multiple regression and Markov Chain Monte Carlo (MCMC) analyses were performed to explore predictors of the children's word reading and spelling abilities. Results: The results of the regression analyses showed that the children's rapid naming score was the only significant predictor of decoding skill. For word recognition, letter knowledge was the only significant predictor among early literacy skills. Letter knowledge was also the only significant predictor of spelling ability as well. Because letter knowledge was found to be an important predictor of young dyslexic children's reading and spelling abilities, post-hoc analyses was performed. From the post-hoc analyses, it was revealed that letter name knowledge was an important contributor to word recognition skill, and that letter sound knowledge was an important contributor to spelling skill. Conclusion: The results of this study suggest that letter knowledge is a critical element for reading and spelling development in young children with dyslexia. In particular, letter names need to be taught explicitly to student who experience difficulty in reading words, and letter sounds need to be taught explicitly to students who have difficulty in spelling. © 2017 Korean Academy of Speech-Language Pathology and Audiology."
11,10.1016/j.ijpddr.2017.06.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021816687&doi=10.1016%2fj.ijpddr.2017.06.001&partnerID=40&md5=6a98159ae9f4c62217dbe1ef343b783a,"Control of human soil-transmitted helminths (STHs) relies on preventive chemotherapy of schoolchildren applying the benzimidazoles (BZ) albendazole or mebendazole. Anthelmintic resistance (AR) is a common problem in nematodes of veterinary importance but for human STHs, information on drug efficacy is limited and routine monitoring is rarely implemented. Herein, the efficacy of single dose albendazole (400 mg) was evaluated in 12 schools in the Huye district of Rwanda where Ascaris is the predominant STH. Ascaris eggs were detected by wet mount microscopy and the Mini-FLOTAC method to assess cure rate (CR) and faecal egg count reduction (FECR). Blood and faecal samples were analysed for co-infections with Plasmodium sp. and Giardia duodenalis, respectively. Ascaris positive samples collected before and after treatment were analysed for putatively BZ-resistance associated β-tubulin gene single nucleotide polymorphisms. The overall CR was 69.9% by Mini-FLOTAC and 88.6% by wet mount microscopy. The FECR was 75.4% and the 95% calculated confidence intervals were 50.4–87.8% using sample variance, 55.4–88.8% by bootstrapping, and 75.0–75.7% applying a Markov Chain Monte Carlo Bayesian approach. FECR varied widely between 0 and 96.8% for individual schools. No putative BZ-resistance associated polymorphisms were found in the four Ascaris β-tubulin isotype genes examined. Since FECRs <95% indicate reduced efficacy, these findings raise the suspicion of BZ resistance. In the absence of respective molecular evidence, heritable AR in the local Ascaris populations cannot be formally proven. However, since FECRs <95% indicate reduced efficacy, BZ resistance may be suspected which would be alarming and calls for further analyses and routine monitoring in preventive chemotherapy programs. © 2017 The Authors"
1,10.1214/17-AOAS1077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042670535&doi=10.1214%2f17-AOAS1077&partnerID=40&md5=ba2b18aa3e01d4d14b3ad0908d5aa724,"While genome-wide association studies (GWAS) have discovered thousands of risk loci for heritable disorders, so far even very large meta-analyses have recovered only a fraction of the heritability of most complex traits. Recent work utilizing variance components models has demonstrated that a larger fraction of the heritability of complex phenotypes is captured by the additive effects of SNPs than is evident only in loci surpassing genome-wide significance thresholds, typically set at a Bonferroni-inspired p ≤ 5 × 10−8. Procedures that control false discovery rate can be more powerful, yet these are still under-powered to detect the majority of nonnull effects from GWAS. The current work proposes a novel Bayesian semiparametric two-group mixture model and develops a Markov Chain Monte Carlo (MCMC) algorithm for a covariate-modulated local false discovery rate (cmfdr). The probability of being nonnull depends on a set of covariates via a logistic function, and the nonnull distribution is approximated as a linear combination of B-spline densities, where the weight of each B-spline density depends on a multinomial function of the covariates. The proposed methods were motivated by work on a large meta-analysis of schizophrenia GWAS performed by the Psychiatric Genetics Consortium (PGC).We show that the new cmfdr model fits the PGC schizophrenia GWAS test statistics well, performing better than our previously proposed parametric gamma model for estimating the nonnull density and substantially improving power over usual fdr. Using loci declared significant at cmfdr ≤ 0.20, we perform follow-up pathway analyses using the Kyoto Encyclopedia of Genes and Genomes (KEGG) Homo sapiens pathways database. We demonstrate that the increased yield from the cmfdr model results in an improved ability to test for pathways associated with schizophrenia compared to using those SNPs selected according to usual fdr. © Institute of Mathematical Statistics, 2017."
3,10.1007/s10980-017-0575-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030320439&doi=10.1007%2fs10980-017-0575-y&partnerID=40&md5=7657fa7ad9eb34558521c72642feedaf,"Context: Scientists face several theoretical and methodological challenges in appropriately describing fundamental wildlife-habitat relationships in models. The spatial scales of habitat relationships are often unknown, and are expected to follow a multi-scale hierarchy. Typical frequentist or information theoretic approaches often suffer under collinearity in multi-scale studies, fail to converge when models are complex or represent an intractable computational burden when candidate model sets are large. Objectives: Our objective was to implement an automated, Bayesian method for inference on the spatial scales of habitat variables that best predict animal abundance. Methods: We introduce Bayesian latent indicator scale selection (BLISS), a Bayesian method to select spatial scales of predictors using latent scale indicator variables that are estimated with reversible-jump Markov chain Monte Carlo sampling. BLISS does not suffer from collinearity, and substantially reduces computation time of studies. We present a simulation study to validate our method and apply our method to a case-study of land cover predictors for ring-necked pheasant (Phasianus colchicus) abundance in Nebraska, USA. Results: Our method returns accurate descriptions of the explanatory power of multiple spatial scales, and unbiased and precise parameter estimates under commonly encountered data limitations including spatial scale autocorrelation, effect size, and sample size. BLISS outperforms commonly used model selection methods including stepwise and AIC, and reduces runtime by 90%. Conclusions: Given the pervasiveness of scale-dependency in ecology, and the implications of mismatches between the scales of analyses and ecological processes, identifying the spatial scales over which species are integrating habitat information is an important step in understanding species-habitat relationships. BLISS is a widely applicable method for identifying important spatial scales, propagating scale uncertainty, and testing hypotheses of scaling relationships. © 2017, Springer Science+Business Media B.V."
1,10.1111/cob.12213,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049217183&doi=10.1111%2fcob.12213&partnerID=40&md5=5b2418b0cc81e30484e1ec30e3fd96a2,"In addition to weight loss, randomized controlled trials have shown improvement in glycaemic control in patients taking lorcaserin. The aim of this study aim was to compare adding lorcaserin or other glucose lowering medications to metformin on weight and glycaemic control. A systematic review and network meta-analysis of randomized controlled trials were conducted. Included studies (published 1990-2014) were of lorcaserin or glucose lowering medications in type 2 diabetic patients compared to placebo or different active treatments. Studies had to report ≥1 key outcome (change in weight or HbA1c, % HbA1c <7, hypoglycaemia). Direct meta-analysis was performed using DerSimonian and Laird random effects models, and network meta-analysis with Bayesian Markov-chain Monte Carlo random effects models; 6552 articles were screened and 41 included. Lorcaserin reduced weight significantly more than thiazolidinediones, glinides, sulphonylureas and dipeptidyl peptidase-4 inhibitors, some of which may have led to weight gain. There were no significant differences in weight change between lorcaserin and alpha-glucoside inhibitors, glucagon-like peptide-1 agonists and sodium/glucose cotransporter 2 inhibitors. Network meta-analysis showed lorcaserin was non-inferior to all other agents on HbA1c reduction and % achieving HbA1c of <7%. The risk of hypoglycaemia was not significantly different among studied agents except that sulphonylureas were associated with higher risk of hypoglycaemia than lorcaserin. Although additional studies are needed, this analysis suggests in a population of patients with a body mas index of ≥27 who do not achieve glycaemic control on a single agent, lorcaserin may be added as an alternative to an add-on glucose lowering medication. © 2017 World Obesity Federation."
,10.4067/S0717-65382017000200064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044606475&doi=10.4067%2fS0717-65382017000200064&partnerID=40&md5=c2bf429481e96acc6098f6ede16dc19a,"Phylogenetic study of the genera of South American Austral Trichopterygini (Lepidoptera: Geometridae, Larentiinae): a new classification. In this work, we evaluate the taxonomy of the Trichopterygini in Chile based on a phylogenetic analysis of the morphological attributes. In our analysis, we used Tatosoma and Sauris as outgroups. Two approaches were used to evaluate phylogenetic relationships: 1) parsimony criterion, and 2) Bayesian inference. Parsimony analysis was conducted in PAUP software, and Bayesian analysis with Markov chain Monte Carlo using the BayesPhylogenies software. Our results based on the phylogenetic hypothesis suggest a new taxonomic order for Trichopterygini of the Andean Region of Southern South America. The valid genera are: Arrayanaria Parra, Butleriana Parra, Danielaparra Kemal & Kocak, Fueguina Parra, Hoplosauris Butler, Lagynopteryx Berg, Llampidken Parra & Santos-Salas, Pachrophylla Blanchard, Parapachrophylla Parra, Rindgenaria Parra, Tomopteryx Philippi, Triptila Warren, Triptiloides Parra & Santos-Salas, Warrenaria Parra. The main changes with respect to the previous taxonomic order are: 1) the genus Lagynopteryx Berg is subordinated under the Trichopterygini; 2) Toxopaltes Warren is a junior synonym of Lagynopteryx; 3) Hoplosauris moesta is transferred to the genus Llampidken; 4) Llampidken valdiviana is a junior synonym of L. moesta; 5) Oparabia arenosa is newly combined with the genus Arrayanaria; 6) Danielaparra viridis is a junior synonym of D. fragmentata; 7) Lobophora imbricaria is newly combined with the genus Danielaparra; 8) Triptiloides fasciata is a junior synonym of T. randallae; and 9) Parapachrophylla michelleae Parra n. sp. is described. Andean Region species are more closely related to the genus Tatosoma from New Zealand, the synapomorphies that demonstrate this are: swollen metaepimeron and hypertrophy of the second abdominal segment. A checklist of the genera and species of the tribe in the region, and the figures of adults and genitalia of some species are included. © 2018, Universidad de Concepcion. All rights reserved."
2,10.1111/2041-210X.12842,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025082396&doi=10.1111%2f2041-210X.12842&partnerID=40&md5=d535f5a9e7e087ae88613972f7326937,"Occupancy models are widely used to describe the distribution of rare and cryptic species—those that occur on only a small portion of the landscape and cannot be detected reliably during a single survey. However, model estimates of occupancy (ψ) and detection probabilities (p) are often least accurate under these circumstances. Available sampling designs for occupancy surveys include standard design, wherein each of S sites is visited K times, and removal design, wherein S sites are visited K times each or until the species of interest is detected. We propose a new conditional design, wherein each of S sites is visited one time, and sites where the species of interest is encountered during the first survey are visited an additional (K−1) times to better estimate detection probability. We used large sample properties of maximum-likelihood estimators and Markov chain Monte Carlo (MCMC) simulations to characterise our proposed conditional design and compare it to standard and removal designs across a wide range of true occupancy and detection probabilities (ψ, p = 0.1 to 0.9 by 0.1 increments), maximum visits (K) and total sampling effort (E, the number of surveys accrued across all sites). The conditional design provided more accurate estimates (lower standard or root mean squared error) of occupancy than standard or removal designs in our calculations and simulations when species were rare (ψ ≤ 0.3) as well as more accurate estimates of detection probability over most combinations of ψ and p. These low-occupancy improvements are achieved by expending a greater proportion of effort at occupied sites, improving estimates of p and thus ψ. When species are common (ψ ≥ 0.5) the removal design generally provided the most accurate occupancy estimates, whereas the standard design performed best when ψ was intermediate and during MCMC simulations when p and K were low. We recommend the conditional design for surveys of rare species and pilot studies. For multi-species surveys that include mixtures of rare and common species, a hybrid standard-conditional design with 2–3 replicates at all sites and additional replicates at sites where rare species are detected improves occupancy estimates of rare species. © 2017 The Authors. Methods in Ecology and Evolution © 2017 British Ecological Society"
,10.1007/s10651-017-0387-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032660285&doi=10.1007%2fs10651-017-0387-x&partnerID=40&md5=bc70f5df112376f117a2fafedaaaa3cd,"We propose a model to consider data dependencies and assess spatial and temporal variability in land use specific floral coverage across landscapes. Data dependence arising from repeated measurements across the flowering season is taken into account using hierarchical Archimedean copulas, where the correlation is assumed to be stronger within seasonal periods than between periods. For each seasonal period, a bounded probability distribution is assigned to capture spatial variability in floral cover. The model uses a Bayesian approach and can assess land-use-specific floral covers by integrating experts judgments and field data. The model is applied to assess floral covers in four land use types in southern Sweden, where seasonal variability is captured by dividing the season into two periods according to winter oilseed rape flowering. Floral cover is updated using Markov Chain Monte Carlo sampling based on data from 16 landscapes and 2 years, with repeated measures available from each of the two seasonal periods. Our results indicate that considering data dependence improved the estimation of floral cover based on data observed during a season. Different copula families specifying multivariate probability distributions were tested, and no family had a consistently higher performance in the four tested land use types. Uncertainty in both mode and variability of floral cover was higher when data dependence were accounted for. Posterior modes of floral covers in semi-natural grassland were higher than in field edges, but both expert’s best guesses were higher than these estimates. This confirms previous findings in expert elicitation processes that experts may fail to discriminate extreme values on a bounded range. Floral cover in flower strips were estimated to be smaller/higher than semi-natural grasslands early/late in the season. The mode of floral cover in oil seed rape was estimated to be close to 100%, and higher than estimates provided by expert judgment. Floral covers for different land use classes are key parameters when quantifying floral resources at a landscape level whose assessments rely on both expert judgment and field measurements. © 2017, The Author(s)."
4,10.1002/sim.7431,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032985433&doi=10.1002%2fsim.7431&partnerID=40&md5=1d6f65532ec1452440b4d35104008c62,"When we synthesize research findings via meta-analysis, it is common to assume that the true underlying effect differs across studies. Total variability consists of the within-study and between-study variances (heterogeneity). There have been established measures, such as I2, to quantify the proportion of the total variation attributed to heterogeneity. There is a plethora of estimation methods available for estimating heterogeneity. The widely used DerSimonian and Laird estimation method has been challenged, but knowledge of the overall performance of heterogeneity estimators is incomplete. We identified 20 heterogeneity estimators in the literature and evaluated their performance in terms of mean absolute estimation error, coverage probability, and length of the confidence interval for the summary effect via a simulation study. Although previous simulation studies have suggested the Paule-Mandel estimator, it has not been compared with all the available estimators. For dichotomous outcomes, estimating heterogeneity through Markov chain Monte Carlo is a good choice if an informative prior distribution for heterogeneity is employed (eg, by published Cochrane reviews). Nonparametric bootstrap and positive DerSimonian and Laird perform well for all assessment criteria for both dichotomous and continuous outcomes. Hartung-Makambi estimator can be the best choice when the heterogeneity values are close to 0.07 for dichotomous outcomes and medium heterogeneity values (0.01, 0.05) for continuous outcomes. Hence, there are heterogeneity estimators (nonparametric bootstrap DerSimonian and Laird and positive DerSimonian and Laird) that perform better than the suggested Paule-Mandel. Maximum likelihood provides the best performance for both types of outcome in the absence of heterogeneity. Copyright © 2017 John Wiley & Sons, Ltd."
10,10.1017/9781316417041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039944772&doi=10.1017%2f9781316417041&partnerID=40&md5=5a93be232aaddc32d06b772d4a0fa113,"Over the past several decades, computational approaches to studying strongly-interacting systems have become increasingly varied and sophisticated. This book provides a comprehensive introduction to state-of-the-art quantum Monte Carlo techniques relevant for applications in correlated systems. Providing a clear overview of variational wave functions, and featuring a detailed presentation of stochastic samplings including Markov chains and Langevin dynamics, which are developed into a discussion of Monte Carlo methods. The variational technique is described, from foundations to a detailed description of its algorithms. Further topics discussed include optimisation techniques, real-time dynamics and projection methods, including Green's function, reptation and auxiliary-field Monte Carlo, from basic definitions to advanced algorithms for efficient codes, and the book concludes with recent developments on the continuum space. Quantum Monte Carlo Approaches for Correlated Systems provides an extensive reference for students and researchers working in condensed matter theory or those interested in advanced numerical methods for electronic simulation. © Federico Becca and Sandro Sorella 2017. All rights reserved."
2,10.1088/1361-6579/aa93a1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039065832&doi=10.1088%2f1361-6579%2faa93a1&partnerID=40&md5=f0b6cf699d30cacc6510b6e04d867b6e,"Objective: The instantaneous phase (IP) and instantaneous frequency (IF) of the electroencephalogram (EEG) are considered as notable complements for the EEG spectrum. The calculation of these parameters commonly includes narrow-band filtering, followed by the calculation of the signal's analytical form. The calculation of the IP and IF is highly susceptible to the filter parameters and background noise level, especially in low analytical signal amplitudes. The objective of this study is to propose a robust statistical framework for EEG IP/IF estimation and analysis. Approach: Herein, a Monte Carlo estimation scheme is proposed for the robust estimation of the EEG IP and IF. It is proposed that any EEG phase-related inference should be reported as an average with confidence intervals obtained by repeating the IP and IF estimation under infinitesimal variations (selected by an expert), in algorithmic parameters such as the filter's bandwidth, center frequency and background noise level. In the second part of the paper, a stochastic model consisting of the superposition of narrow-band foreground and background EEG is used to derive analytically probability density functions of the instantaneous envelope (IE) and IP of EEG signals, which justify the proposed Monte Carlo scheme. Main results: The instantaneous analytical envelope of the EEG, which has been empirically used in previous studies, is shown to have a fundamental impact on the accuracy of the EEG phase contents. It is rigorously shown that the IP/IF estimation quality highly depends on the IE and any phase/frequency interpretations in low IE are statistically unreliable and require a hypothesis test. Significance: The impact of the proposed method on previous studies, including time-domain phase synchrony, phase resetting, phase locking value and phase amplitude coupling are studied with examples. The findings of this research can set forth new standards for EEG phase/frequency estimation and analysis techniques. © 2017 Institute of Physics and Engineering in Medicine."
,10.1080/03610918.2016.1255972,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019122982&doi=10.1080%2f03610918.2016.1255972&partnerID=40&md5=cabc8e6dbc8047e5747368f1d2d083b6,"In this article, we perform Bayesian estimation of stochastic volatility models with heavy tail distributions using Metropolis adjusted Langevin (MALA) and Riemman manifold Langevin (MMALA) methods. We provide analytical expressions for the application of these methods, assess the performance of these methodologies in simulated data, and illustrate their use on two financial time series datasets. © 2017 Taylor & Francis Group, LLC."
2,10.1016/j.ecolmodel.2017.09.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029694021&doi=10.1016%2fj.ecolmodel.2017.09.008&partnerID=40&md5=e060e81a99a91d90dc33d0ad104c790c,"Forests are an important component of the global carbon (C) cycle: they can capture and retain large amounts of C annually, depending on stand characteristics, climate, and disturbance regimes. With climate and disturbance regimes shifting, it is important to be able to accurately represent the corresponding changes in forest C dynamics with well-calibrated models. The Carbon Budget Model of the Canadian Forest Sector (CBM-CFS3) is a widely used model for simulating C dynamics in managed forests at stand, regional, and national levels. Here, we use a Bayesian Markov Chain Monte Carlo (MCMC) technique to calibrate the parameters in the CBM-CFS3 by assimilating C stocks of six deadwood and soil pools estimated from data collected from 635 plots within the Canadian National Forest Inventory. Calibration led to most improvement in the simulation of C stocks in small and fine woody debris, reducing RMSE by 54.3%, followed by the snag stems (RMSE reduced by 23.2%), and coarse woody debris (13%). The calibrated parameters resulted in increased rates of C cycling in fine and coarse woody debris and the soil organic layer, distinct C dynamics in hardwood and softwood dominated stands, and increased temperature sensitivity of the C contained in the mineral soil. While parameter calibration considerably improved the simulation of the small and fine woody debris and snags stem pools, model representation of the branch snag, coarse woody debris, soil organic layer, and mineral soil pools were not substantially improved. Lack of substantial improvements in the calibrated model performance indicated the need for including additional processes in C dynamics simulation or a change in the modelling paradigm. We illustrate the potential need to include a lignin effect on deadwood decay and suggest further exploration of the effects of tree species, soil types, and mosses on performance of the CBM-CFS3. © 2017"
10,10.3390/su9112138,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035012219&doi=10.3390%2fsu9112138&partnerID=40&md5=7d7da81b33e94c3a0780933d36b2bb88,"This paper experiments an artificial neural networks model with Bayesian approach on a small real estate sample. The output distribution has been calculated operating a numerical integration on the weights space with the Markov Chain Hybrid Monte Carlo Method (MCHMCM). On the same real estate sample, MCHMCM has been compared with a neural networks model (NNs), traditional multiple regression analysis (MRA) and the Penalized Spline Semiparametric Method (PSSM). All four methods have been developed for testing the forecasting capacity and reliability of MCHMCM in the real estate field. The Markov Chain Hybrid Monte Carlo Method has proved to be the best model with an absolute average percentage error of 6.61%. © 2017 by the authors."
,10.1109/ASE.2017.8115657,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041448047&doi=10.1109%2fASE.2017.8115657&partnerID=40&md5=ffce6f560dcb5a7beb8f15424c192bb7,"In Online-to-Offline (O2O) commerce, customer services may need to be composed from online and offline services. Such composition is challenging, as it requires effective selection of appropriate services that, in turn, support optimal combination of both online and offline services. In this paper, we address this challenge by proposing an approach to O2O service composition which combines offline route planning and social collaboration to optimize service selection. We frame general O2O service composition problems using timed automata and propose an optimization procedure that incorporates: (1) a Markov Chain Monte Carlo (MCMC) algorithm to stochastically select a concrete composite service, and (2) a model checking approach to searching for an optimal collaboration plan with the lowest cost given certain time constraint. Our procedure has been evaluated using the simulation of a rich scenario on effectiveness and scalability. © 2017 IEEE."
,10.1109/PIERS-FALL.2017.8293580,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045335827&doi=10.1109%2fPIERS-FALL.2017.8293580&partnerID=40&md5=fd8a5b9099f0ebc945c6da399c9e077a,"In soil moisture remote sensing products ground verification, the soil moisture in-situ sampling is heterogeneous and not consistent with the remote sensing pixel scale. So, it is very important that extensive sampling soil moisture of the surface heterogeneous and effective upscaling multi-point in situ soil moisture to remote sensing pixel scale. However, it is very difficult to collect them extensively if the soil moisture signal is complicated or dynamically changing. Considering the sparsity of soil moisture and the effects of observation noise, this paper models the linear programming problem between the soil moisture remote sensing pixel and in-situ sampling data using hierarchical non-parametric Bayesian linear regression. This model does not assume that the specific distribution of regression parameters which will be learned adaptively by the nonparametric Bayesian method. In addition, we implement the Dirichlet process to exploit the spatial similarity of the in-situ sampling data of soil moisture, thus to improve the spatial upscaling accuracy. Due to the Dirichlet process without explicit mathematical expression, the model of the posteriori probability distribution is very hard to deal with. To this end, the Gibbs sampling scheme based on MCMC (Markov Chain Monte Carlo) is adopted to infer the optimal regression weighting coefficient, and the spatial scale of the soil moisture in situ sampling is effectively upscaled. The experimental results show that the spatial upscaling method of nonparametric Bayesian linear regression is closer to the observed remote sensing pixel scale and better than the state of the art Bayesian method and Kriging method. © 2018 Electromagnetics Academy. All rights reserved."
,10.3847/1538-4357/aa9658,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038024641&doi=10.3847%2f1538-4357%2faa9658&partnerID=40&md5=d38d131a0353a7eec12c54b178c4dee0,"In this erratum, we provide corrected sets of r01,10,02 difference ratio values and associated uncertainties, which were overestimated in the original paper (as noted by Roxburgh 2017) due to a missing trimming in the post-processing of the Markov chain Monte Carlo (MCMC) chains for these values. The typical reduction in the ratio uncertainties from performing the trimming is a factor of 10 (see Figure 3). Other parameters optimized in the peak-bagging (for instance, individual mode frequencies) are unaffected, as the trimming was performed for these in the original work (Lund et al. 2017). We also provide updated values for the Δ2ν values of l = 3 modes. We note that the values presented here, as with those presented in the original work, are obtained from a single peakbagging procedure (see Lund et al. 2017 for details) and have yet to be verified by independent analyses using the same input power spectra. Examples of the updated tables from the original paper are given in Tables 1-V3. We note that tables with individual mode parameters (Table 2) have been added for completeness, but the parameters in these tables are unchanged compared to the original paper. In addition to the corrected values mentioned above, we provide covariance matrices for the mode frequencies, frequency difference ratios (r01,10,02), and second differences (Δ2ν) for the LEGACY sample (Lund et al. 2017), which were not published with the original work. The values provided by this erratum will be available in the online version of the paper. (Figure Presented) (Table Presented). © 2017. The American Astronomical Society. All rights reserved."
2,10.3847/2041-8213/aa9704,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035344907&doi=10.3847%2f2041-8213%2faa9704&partnerID=40&md5=e3fef48fd891c1962d72536962fb352b,"In Hezaveh et al. we showed that deep learning can be used for model parameter estimation and trained convolutional neural networks to determine the parameters of strong gravitational-lensing systems. Here we demonstrate a method for obtaining the uncertainties of these parameters. We review the framework of variational inference to obtain approximate posteriors of Bayesian neural networks and apply it to a network trained to estimate the parameters of the Singular Isothermal Ellipsoid plus external shear and total flux magnification. We show that the method can capture the uncertainties due to different levels of noise in the input data, as well as training and architecture-related errors made by the network. To evaluate the accuracy of the resulting uncertainties, we calculate the coverage probabilities of marginalized distributions for each lensing parameter. By tuning a single variational parameter, the dropout rate, we obtain coverage probabilities approximately equal to the confidence levels for which they were calculated, resulting in accurate and precise uncertainty estimates. Our results suggest that the application of approximate Bayesian neural networks to astrophysical modeling problems can be a fast alternative to Monte Carlo Markov Chains, allowing orders of magnitude improvement in speed. © 2017. The American Astronomical Society. All rights reserved.."
,10.1088/1742-6596/921/1/012017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036463889&doi=10.1088%2f1742-6596%2f921%2f1%2f012017&partnerID=40&md5=696e503ffc08c546d94025f2284ae3b2,"The canonical technique for Monte Carlo simulations in statistical physics is importance sampling via a suitably constructed Markov chain. While such approaches are quite successful, they are not particularly well suited for parallelization as the chain dynamics is sequential, and if replicated chains are used to increase statistics each of them relaxes into equilibrium with an intrinsic time constant that cannot be reduced by parallel work. Population annealing is a sequential Monte Carlo method that simulates an ensemble of system replica under a cooling protocol. The population element makes it naturally well suited for massively parallel simulations, and bias can be systematically reduced by increasing the population size. We present an implementation of population annealing on graphics processing units and discuss its behavior for different systems undergoing continuous and first-order phase transitions. © Published under licence by IOP Publishing Ltd."
1,10.1080/02664763.2016.1266309,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006109972&doi=10.1080%2f02664763.2016.1266309&partnerID=40&md5=a085f4d63af4217328b803098d5f82be,"Typical joint modeling of longitudinal measurements and time to event data assumes that two models share a common set of random effects with a normal distribution assumption. But, sometimes the underlying population that the sample is extracted from is a heterogeneous population and detecting homogeneous subsamples of it is an important scientific question. In this paper, a finite mixture of normal distributions for the shared random effects is proposed for considering the heterogeneity in the population. For detecting whether the unobserved heterogeneity exits or not, we use a simple graphical exploratory diagnostic tool proposed by Verbeke and Molenberghs [34] to assess whether the traditional normality assumption for the random effects in the mixed model is adequate. In the joint modeling setting, in the case of evidence against normality (homogeneity), a finite mixture of normals is used for the shared random-effects distribution. A Bayesian MCMC procedure is developed for parameter estimation and inference. The methodology is illustrated using some simulation studies. Also, the proposed approach is used for analyzing a real HIV data set, using the heterogeneous joint model for this data set, the individuals are classified into two groups: a group with high risk and a group with moderate risk. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/02664763.2016.1259401,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997050393&doi=10.1080%2f02664763.2016.1259401&partnerID=40&md5=24f4178595b62f4a99c7bc2e258f56f1,"Foxhound training enclosures are facilities where wild-trapped foxes are placed into large fenced areas for dog training purposes. Although the purpose of these facilities is to train dogs without harming foxes, dog-related mortality has been reported to be an issue in some enclosures. Using data from a fox enclosure in Virginia, we investigate factors that influence fox survival in these dog training facilities and propose a set of policies to improve fox survival. In particular, a Bayesian hierarchical model is formulated to compute fox survival probabilities based on a fox's time in the enclosure and the number of dogs allowed in the enclosure at one time. These calculations are complicated by missing information on the number of dogs in the enclosure for many days during the study. We elicit expert knowledge for a prior on the number of dogs to account for the uncertainty in the missing data. Reversible jump Markov Chain Monte Carlo is used for model selection in the presence of missing covariates. We then use our model to examine possible changes to foxhound training enclosure policy and what effect those changes may have on fox survival. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
1,10.1080/03610926.2016.1260740,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026898790&doi=10.1080%2f03610926.2016.1260740&partnerID=40&md5=7825a86afd15d8818cdddf89d93ddb27,"Frailty models are used in the survival analysis to account for the unobserved heterogeneity in individual risks to disease and death. To analyze the bivariate data on related survival times (e.g., matched pairs experiments, twin, or family data), the shared frailty models were suggested. These models are based on the assumption that frailty acts multiplicatively to hazard rate. In this article, we assume that frailty acts additively to hazard rate. We introduce the shared inverse Gaussian frailty models with three different baseline distributions, namely the generalized log-logistic, the generalized Weibull, and exponential power distribution. We introduce the Bayesian estimation procedure using Markov chain Monte Carlo technique to estimate the parameters involved in these models. We apply these models to a real-life bivariate survival dataset of McGilchrist and Aisbett (1991) related to the kidney infection data, and a better model is suggested for the data. © 2017 Taylor & Francis Group, LLC."
,10.1142/S1793524518500018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034599362&doi=10.1142%2fS1793524518500018&partnerID=40&md5=f2060c03db060ab89de643dea55b4a84,"In this paper, we formulate and analyze a mathematical model to investigate the transmission dynamics of tomato bacterial wilt disease (TBWD) in Mukono district, Uganda. We derive the basic reproduction number (Formula presented.) and prove the existence of a disease-free equilibrium point which is globally stable if (Formula presented.) and an endemic equilibrium which exists if (Formula presented.). Model parameters are estimated using the Markov Chain Monte Carlo (MCMC) methods and robustness tested. The model parameters were observed to be identifiable. Numerical simulations show that soil solarization and sensitization of farmers can help to eliminate the disease in Uganda. A modified tomato bacterial wilt model with control terms is formulated. © 2018 World Scientific Publishing Company"
2,10.1109/UPEC.2016.8114092,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047745614&doi=10.1109%2fUPEC.2016.8114092&partnerID=40&md5=a8c4220f44392f66c5c5b0dba8032a94,"At present, energy distribution companies seek to improve service and implement alternatives to determine the capability of distribution system devices that allow to cover electric demand. This paper proposes a stochastic analysis to manage the demand response energy, depending on the voltage curve profiles established by historical measurements. The proposal is based on the stochastic prediction of energetic demand using Monte Carlo algorithms with Markov Chains (MCMC), from the analysis of the voltage profile as a deterministic variable. The analysis is associated with the prediction of the maximum power required to satisfy the peak demand period in distribution systems with predominance of residential load, also seeking the planning of the networks to increase efficiency, quality and reliability of power supply in peak hours, in order to reduce the contingencies in the operation of distribution networks in periods of peak demand, especially in systems where the residential load is predominant, which has been taking considerable growth, due to the insertion of electric cookers. © 2016 IEEE."
,10.1016/j.icarus.2017.06.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027223055&doi=10.1016%2fj.icarus.2017.06.028&partnerID=40&md5=bf128af2693513d623ebf565b654a8bc,"Estimates for asteroid masses are based on their gravitational perturbations on the orbits of other objects such as Mars, spacecraft, or other asteroids and/or their satellites. In the case of asteroid-asteroid perturbations, this leads to an inverse problem in at least 13 dimensions where the aim is to derive the mass of the perturbing asteroid(s) and six orbital elements for both the perturbing asteroid(s) and the test asteroid(s) based on astrometric observations. We have developed and implemented three different mass estimation algorithms utilizing asteroid-asteroid perturbations: the very rough ‘marching’ approximation, in which the asteroids’ orbital elements are not fitted, thereby reducing the problem to a one-dimensional estimation of the mass, an implementation of the Nelder–Mead simplex method, and most significantly, a Markov-chain Monte Carlo (MCMC) approach. We describe each of these algorithms with particular focus on the MCMC algorithm, and present example results using both synthetic and real data. Our results agree with the published mass estimates, but suggest that the published uncertainties may be misleading as a consequence of using linearized mass-estimation methods. Finally, we discuss remaining challenges with the algorithms as well as future plans. © 2017 Elsevier Inc."
2,10.1016/j.jcp.2017.08.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027524963&doi=10.1016%2fj.jcp.2017.08.005&partnerID=40&md5=c93325b8a674fda0b6f1abe9a6a7c019,"This paper is concerned with the characterization and the propagation of errors associated with data limitations in polynomial-chaos-based stochastic methods for uncertainty quantification. Such an issue can arise in uncertainty quantification when only a limited amount of data is available. When the available information does not suffice to accurately determine the probability distributions that must be assigned to the uncertain variables, the Bayesian method for assigning these probability distributions becomes attractive because it allows the stochastic model to account explicitly for insufficiency of the available information. In previous work, such applications of the Bayesian method had already been implemented by using the Metropolis–Hastings and Gibbs Markov Chain Monte Carlo (MCMC) methods. In this paper, we present an alternative implementation, which uses an alternative MCMC method built around an Itô stochastic differential equation (SDE) that is ergodic for the Bayesian posterior. We draw together from the mathematics literature a number of formal properties of this Itô SDE that lend support to its use in the implementation of the Bayesian method, and we describe its discretization, including the choice of the free parameters, by using the implicit Euler method. We demonstrate the proposed methodology on a problem of uncertainty quantification in a complex nonlinear engineering application relevant to metal forming. © 2017 Elsevier Inc."
,10.1109/ISWCS.2017.8108153,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041440460&doi=10.1109%2fISWCS.2017.8108153&partnerID=40&md5=25092513cf851174667ace03503a2ff7,"5G communication will bring a surge traffic in cellular network. The traffic in cellular network not only has strong variability by time, but also has strong spatio-temporal correlation, which brings large difficulty to predict. In order to make reasonable use of communication network resources, it is important to describe and predict the spatio-temporal information of traffic in cellular network. In this paper, we propose a traffic prediction algorithm based on Bayesian spatio-temporal model to predict the spatial distribution of traffic in cellular network at different moments via realistic traffic data of base stations (BSS). Firstly, we select Gaussian Predictive Process (GPP) as the basic model of the Bayesian spatio-temporal model and set proper prior distribution of the parameters. Secondly, we train the basic model by Gibbs sampling and the realistic traffic data to obtain the posterior distribution of the parameters. Then, we predict the spatio-temporal information of traffic in cellular network by Markov chain Monte Carlo (MCMC) computational techniques. Finally, we make theoretical analysis of prediction accuracy for the prediction results. The Index of Agreement (IA) of the prediction results in three different areas can reach 0.9 above, which indicate good prediction performance. The traffic prediction algorithm can be used to predict the spatio-temporal information of traffic in cellular network in different areas. © 2017 IEEE."
1,10.5194/amt-10-4341-2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034060852&doi=10.5194%2famt-10-4341-2017&partnerID=40&md5=18b21bb9b5426891dd432319853fdb32,"Optical particle counters (OPCs) are common tools for the in situ measurement of aerosol particle number size distributions. As the actual quantity measured by OPCs is the intensity of light scattered by individual particles, it is necessary to translate the distribution of detected scattering signals into the desired information, i.e., the distribution of particle sizes. A crucial part in this challenge is the modeling of OPC response and the calibration of the instrument - in other words, establishing the relation between instrument-specific particle scattering cross-section and measured signal amplitude. To date, existing methods lack a comprehensive parametrization of OPC response, particularly regarding the instrument-induced broadening of signal amplitude distributions. This deficiency can lead to significant size distribution biases. We introduce an advanced OPC response model including a simple parametrization of the broadening effect and a self-consistent way to evaluate calibration measurements using a Markov chain Monte Carlo (MCMC) method. We further outline how to consistently derive particle number size distributions with realistic uncertainty estimates within this new framework. Based on measurements of particle standards for two OPCs, the Grimm model 1.129 (SkyOPC) and the DMT Passive Cavity Aerosol Spectrometer Probe (PCASP), we demonstrate that residuals between measured and modeled response can be substantially reduced when using the new approach instead of existing methods. More importantly, for the investigated set of measurements only the new approach yields results that conform with the true size distributions within the range of model uncertainty. The presented innovations will help improving the accuracy of OPC-derived size distributions and the assessment of their precision. © Author(s) 2017."
,10.1109/NAPS.2017.8107304,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040580101&doi=10.1109%2fNAPS.2017.8107304&partnerID=40&md5=c4c9ff41a1fd98d045a106179a14d16f,"The following piece of work is dedicated to solving the unit commitment/optimal scheduling problem under high levels of wind power penetration and under the presence of a pumped hydro storage facility. This task has been divided into two parts; a) Baseline scheduling and b) Reserve scheduling, where baseline scheduling meets the day-ahead forecast of load and wind energy, while reserve scheduling meets the variation in load and wind energy from the baseline schedule. A Markov chain transition matrix has been used to quantify both upward and downward reserves. This paper reveals the results of considering different aspects (fuel cost, reliability, emissions and reservoir volume), in the objective function. Also a Monte-Carlo analysis of the results are performed to understand the probabilistic nature of the problem. Using frequency distribution of load and wind data, different cases have been analyzed to conclude a probabilistic (or fuzzy) schedule, which not only answers the question of ON/OFF status but can also imply time windows for maintenance scheduling and turn-down activities. © 2017 IEEE."
1,10.16285/j.rsm.2017.11.035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039073948&doi=10.16285%2fj.rsm.2017.11.035&partnerID=40&md5=caaf53aa5ba5368a8f1290c46ac4f353,"In the geotechnical engineering reliability analysis and design, it is very difficult to accurately select the random field parameters and the correlation function, and to accurately describe the spatial variability of soil parameters. Based on Bayesian theory, this paper presents a method to quantify the spatial variability of effective internal friction angle of sand. A proper correlation function using prior knowledge and cone penetration test (CPT) data are used to determine the random field parameters and the correlation function of the effective internal friction angle of sand by the method. This method takes reasonable account of the uncertainty of the empirical regression equation between the effective internal friction angle and the cone resistance. Markov chain Monte Carlo simulation (MCMCS) method is applied in this paper to generate random samples following the posterior distribution. The MCMCS samples are used to calculate the posterior distribution by a Gaussian Copula-based method. Then, the plausibility of a candidate correlation function is obtained and the most probable correlation function is selected. Finally, the proposed approaches are illustrated and validated by using real-life CPT data obtained from NGES at Texas A&M University. It is shown that the proposed approaches can, correctly and reasonably, determine the random field parameters and correlation function of sand effective friction angle by using the indirect CPT data. It is possible to accurately describe the spatial variability of sand effective friction angle. The correlation function of effective friction angle at the sand site of NGES at Texas A&M University is second-order Markov correlation function. © 2017, Science Press. All right reserved."
,10.1002/sim.7407,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030318228&doi=10.1002%2fsim.7407&partnerID=40&md5=8d26af6bc78c8ecda72115f390a41125,"Growth models used for describing the dynamics of body weight and height generally consider each trait independently. We proposed modeling height and weight trajectories jointly with a nonlinear heteroscedastic mixed model based on the Jenss-Bayley growth function with correlated individual random effects and using Bayesian inference techniques. Simulations showed that our model provides good estimates of the growth parameters. We illustrated how it can be used to assess the associations between maternal smoking during pregnancy, an early-life factor potentially involved in prenatal programming of obesity, and children's growth from birth to 5 years of age. We used real data from the EDEN study, a large French mother-child cohort study with a high number of height and weight measurements (a total of approximately 30 000 measurements for each of the 2 traits across the 1666 children). Our results supported the existence of a relationship between maternal smoking during pregnancy and growth from birth to 5 years of age. Children from mothers who smoked throughout pregnancy were shown to display a higher body mass index from the first few months of life onwards compared to children from nonsmokers. At 5 years of age, their mean body mass index was 0.21 kg/m2 higher than unexposed children. It was mainly explained by the fact that these children tended to be smaller at birth but rapidly exceeded the weight of children from nonsmokers postnatally. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1002/nme.5530,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017395475&doi=10.1002%2fnme.5530&partnerID=40&md5=ae731d7bd8a1e2a404f9b5a036ea6e78,"Advances in nondestructive material characterization are providing a wealth of information that could be exploited to gain insight into general aspects of material performance and, in particular, discover relationships between microstructure and thermo-mechanical properties in polycrystalline and other complex composite materials. In order to facilitate the integration of such measurements into existing models, as well as inform new physics-based predictions, we developed a C++/MPI computational framework for sensitivity analysis and parameter estimation. The framework utilizes a micro-mechanical modeling based on fast Fourier transforms, direct and adjoint formulations, and Markov chain Monte Carlo sampling techniques. We illustrate the characteristics of this framework and demonstrate its utility by computing the residual stresses arising from thermal expansion of an elastic composite and using data from simulated experiments. We show that the availability of nondestructive 3-D measurements is crucial to reduce the uncertainty in predictions, emphasizing the importance of an integrated experimental/modeling/data analysis approach for improved material characterization and design. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1088/1361-6420/aa9417,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038362414&doi=10.1088%2f1361-6420%2faa9417&partnerID=40&md5=59d4b053a222c652109298b7c7b659d1,"The major challenges in the Bayesian inverse problems arise from the need for repeated evaluations of the forward model, as required by Markov chain Monte Carlo (MCMC) methods for posterior sampling. Many attempts at accelerating Bayesian inference have relied on surrogates for the forward model, typically constructed through repeated forward simulations that are performed in an offline phase. Although such approaches can be quite effective at reducing computation cost, there has been little analysis of the approximation on posterior inference. In this work, we prove error bounds on the Kullback-Leibler (KL) distance between the true posterior distribution and the approximation based on surrogate models. Our rigorous error analysis show that if the forward model approximation converges at certain rate in the prior-weighted L 2 norm, then the posterior distribution generated by the approximation converges to the true posterior at least two times faster in the KL sense. The error bound on the Hellinger distance is also provided. To provide concrete examples focusing on the use of the surrogate model based methods, we present an efficient technique for constructing stochastic surrogate models to accelerate the Bayesian inference approach. The Christoffel least squares algorithms, based on generalized polynomial chaos, are used to construct a polynomial approximation of the forward solution over the support of the prior distribution. The numerical strategy and the predicted convergence rates are then demonstrated on the nonlinear inverse problems, involving the inference of parameters appearing in partial differential equations. © 2017 IOP Publishing Ltd."
1,10.1016/j.tcs.2016.11.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015672779&doi=10.1016%2fj.tcs.2016.11.017&partnerID=40&md5=94d21d6f713e09d0fa2a1c28a5345f97,"Motivated by a derandomization of Markov chain Monte Carlo (MCMC), this paper investigates a deterministic random walk, which is a deterministic process analogous to a random walk. There is some recent progress in the analysis of the vertex-wise discrepancy (i.e., L∞-discrepancy), while little is known about the total variation discrepancy (i.e., L1-discrepancy), which plays an important role in the analysis of an FPRAS based on MCMC. This paper investigates the L1-discrepancy between the expected number of tokens in a Markov chain and the number of tokens in its corresponding deterministic random walk. First, we give a simple but nontrivial upper bound O(mt⁎) of the L1-discrepancy for any ergodic Markov chains, where m is the number of edges of the transition diagram and t⁎ is the mixing time of the Markov chain. Then, we give a better upper bound O(mt⁎) for non-oblivious deterministic random walks, if the corresponding Markov chain is ergodic and lazy. We also present some lower bounds. © 2016 Elsevier B.V."
,10.1016/j.tcs.2017.02.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014102905&doi=10.1016%2fj.tcs.2017.02.006&partnerID=40&md5=3422d0e056a4d7d82a8632133312755b,"Statistical phylogenetic inference methods use tree rearrangement operations such as subtree–prune–regraft (SPR) to perform Markov chain Monte Carlo (MCMC) across tree topologies. The structure of the graph induced by tree rearrangement operations is an important determinant of the mixing properties of MCMC, motivating the study of the underlying SPR graph in greater detail. In this paper, we investigate the SPR graph of rooted trees (rSPR graph) in a new way: by calculating the Ricci–Ollivier curvature with respect to uniform and Metropolis–Hastings random walks. This value quantifies the degree to which a pair of random walkers from specified points move towards each other; negative curvature means that they move away from one another on average, while positive curvature means that they move towards each other. In order to calculate this curvature, we develop fast new algorithms for rSPR graph computation. We then develop formulas characterizing how the number of rSPR neighbors of a tree changes after an rSPR operation is applied to that tree. These give bounds on the curvature, as well as a flatness-in-the-limit theorem indicating that paths of small topology changes are easy to traverse. However, we find that large topology changes (i.e. moving a large subtree) give pairs of trees with negative curvature. We show using simulation that mean access time distributions depend on distance, degree, and curvature, demonstrating the relevance of these results to stochastic tree search. © 2017 The Authors"
,10.1145/3132847.3132928,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037327776&doi=10.1145%2f3132847.3132928&partnerID=40&md5=5edc79f7b5e9f22b12166e2f321b6e82,"In survival analysis, regression models are used to understand the effects of explanatory variables (e.g., age, sex, weight, etc.) to the survival probability. However, for sensitive survival data such as medical data, there are serious concerns about the privacy of individuals in the data set when medical data is used to fit the regression models. The closest work addressing such privacy concerns is the work on Cox regression which linearly projects the original data to a lower dimensional space. However, the weakness of this approach is that there is no formal privacy guarantee for such projection. In this work, we aim to propose solutions for the regression problem in survival analysis with the protection of differential privacy which is a golden standard of privacy protection in data privacy research. To this end, we extend the Output Perturbation and Objective Perturbation approaches which are originally proposed to protect differential privacy for the Empirical Risk Minimization (ERM) problems. In addition, we also propose a novel sampling approach based on the Markov Chain Monte Carlo (MCMC) method to practically guarantee differential privacy with better accuracy. We show that our proposed approaches achieve good accuracy as compared to the non-private results while guaranteeing differential privacy for individuals in the private data set. © 2017 Association for Computing Machinery."
,10.23919/EPE17ECCEEurope.2017.8099246,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042014959&doi=10.23919%2fEPE17ECCEEurope.2017.8099246&partnerID=40&md5=84def790616cb87eeeb8146555cb3b9f,"Constant random and wear-out failures of sub-modules (SM) within a Modular Multi-level Converter (MMC) are examined. Constant random failures are calculated using the binomial distribution formula, using a constant failure rate. Wear-out failures are calculated using discrete Markov chain modelling and Monte Carlo estimation, with the failure rate of a SM increasing with time. The results are compared with various maintenance intervals and different levels of availability to show the trends and characteristics. It is shown that for large gains in availability, relatively small increases in the number of redundant SMs are required. Additionally, the number of redundant SMs required to support longer maintenance intervals increase at less than a linear rate, making increasing maintenance intervals for applications such as offshore converter stations attractive. © assigned jointly to the European Power Electronics and Drives Association & the Institute of Electrical and Electronics Engineers (IEEE)."
,10.1109/ICDSP.2017.8096043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040317812&doi=10.1109%2fICDSP.2017.8096043&partnerID=40&md5=5ff1748ab96ef1ce75c3e8f389d7d764,"Monte Carlo (MC) methods are widely used for Bayesian inference in signal processing, machine learning and statistics. In this work, we introduce an adaptive importance sampler which mixes together the benefits of the Importance Sampling (IS) and Markov Chain Monte Carlo (MCMC) approaches. Different parallel MCMC chains provide the location parameters of the proposal probability density functions (pdfs) used in an IS method. The MCMC algorithms consider a tempered version of the posterior distribution as invariant density. We also provide an exhaustive theoretical support explaining why, in the presented technique, even an anti-tempering strategy (reducing the scaling of the posterior) can be beneficial. Numerical results confirm the advantages of the proposed scheme. © 2017 IEEE."
2,10.1109/ICDSP.2017.8096055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040315383&doi=10.1109%2fICDSP.2017.8096055&partnerID=40&md5=bfb4b7bd76ce92a74f0967ef8bd0444b,"We present a probabilistic framework for both (i) determining the initial settings of kernel adaptive filters (KAFs) and (ii) constructing fully-adaptive KAFs whereby in addition to weights and dictionaries, kernel parameters are learnt sequentially. This is achieved by formulating the estimator as a probabilistic model and defining dedicated prior distributions over the kernel parameters, weights and dictionary, enforcing desired properties such as sparsity. The model can then be trained using a subset of data to initialise standard KAFs or updated sequentially each time a new observation becomes available. Due to the nonlinear/non-Gaussian properties of the model, learning and inference is achieved using gradient-based maximum-aposteriori optimisation and Markov chain Monte Carlo methods, and can be confidently used to compute predictions. The proposed framework was validated on nonlinear time series of both synthetic and real-world nature, where it outperformed standard KAFs in terms of mean square error and the sparsity of the learnt dictionaries. © 2017 IEEE."
1,10.1080/10543406.2017.1293081,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015699143&doi=10.1080%2f10543406.2017.1293081&partnerID=40&md5=305a03bdd5e163af9b8a3227d94f8adb,"In literature, there are a few unified approaches to test proof of concept and estimate a target dose, including the multiple comparison procedure using modeling approach, and the permutation approach proposed by Klingenberg. We discuss and compare the operating characteristics of these unified approaches and further develop an alternative approach in a Bayesian framework based on the posterior distribution of a penalized log-likelihood ratio test statistic. Our Bayesian approach is much more flexible to handle linear or nonlinear dose–response relationships and is more efficient than the permutation approach. The operating characteristics of our Bayesian approach are comparable to and sometimes better than both approaches in a wide range of dose–response relationships. It yields credible intervals as well as predictive distribution for the response rate at a specific dose level for the target dose estimation. Our Bayesian approach can be easily extended to continuous, categorical, and time-to-event responses. We illustrate the performance of our proposed method with extensive simulations and Phase II clinical trial data examples. © 2017 Taylor & Francis Group, LLC."
1,10.3847/1538-3881/aa8d6f,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034439575&doi=10.3847%2f1538-3881%2faa8d6f&partnerID=40&md5=3bdc5d7d8ee2fee6bdab1a2554415fe5,"We present orbital elements and mass sums for 18 visual binary stars of spectral types B to K (five of which arenew orbits) with periods ranging from 20 to more than 500 yr. For two double-line spectroscopic binaries with noprevious orbits, the individual component masses, using combined astrometric and radial velocity data, have aformal uncertainty of ∼0.1 MO. Adopting published photometry and trigonometric parallaxes, plus our ownmeasurements, we place these objects on an H-R diagram and discuss their evolutionary status. These objects arepart of a survey to characterize the binary population of stars in the Southern Hemisphere using the SOAR 4 mtelescope+HRCAM at CTIO. Orbital elements are computed using a newly developed Markov chain Monte Carlo(MCMC) algorithm that delivers maximum-likelihood estimates of the parameters, as well as posterior probabilitydensity functions that allow us to evaluate the uncertainty of our derived parameters in a robust way. Forspectroscopic binaries, using our approach, it is possible to derive a self-consistent parallax for the system from thecombined astrometric and radial velocity data (""orbital parallax""), which compares well with the trigonometricparallaxes. We also present a mathematical formalism that allows a dimensionality reduction of the feature spacefrom seven to three search parameters (or from 10 to seven dimensions-including parallax-in the case ofspectroscopic binaries with astrometric data), which makes it possible to explore a smaller number of parameters ineach case, improving the computational efficiency of our MCMC code. © 2017. The American Astronomical Society. All rights reserved."
2,10.1002/qre.2113,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028643001&doi=10.1002%2fqre.2113&partnerID=40&md5=4d2ee96952f6fc22dfd26c55ed817d1e,"In this paper, a Cox proportional hazard model with error effect applied on the study of an accelerated life test is investigated. Statistical inference under Bayesian methods by using the Markov chain Monte Carlo techniques is performed in order to estimate the parameters involved in the model and predict reliability in an accelerated life testing. The proposed model is applied to the analysis of the knock sensor failure time data in which some observations in the data are censored. The failure times at a constant stress level are assumed to be from a Weibull distribution. The analysis of the failure time data from an accelerated life test is used for the posterior estimation of parameters and prediction of the reliability function as well as the comparisons with the classical results from the maximum likelihood estimation. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
4,10.1016/j.autcon.2017.07.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028540345&doi=10.1016%2fj.autcon.2017.07.003&partnerID=40&md5=e43130c94d17f90ead9a77f8d159caa5,"This paper proposes a Bayesian statistics-based analytical solution and a Markov Chain Monte Carlo (MCMC) method-based numerical solution to estimate the credible interval for fraction nonconforming. Both solutions provide a more accurate, reliable, and interpretable estimation of sampling uncertainty and can be used to improve the functionality of automated, nonconforming quality management systems. To reveal how the inherent mathematical mechanism functions for an analytical solution, a step-by-step proof with a calculation example is provided. For the numerical solution, a specialized Metropolis-Hastings algorithm and an illustrative simulation example are provided to elaborate the stochastic processes of the method. An industrial case study, from a pipe fabrication company in Alberta, Canada, is presented to demonstrate the feasibility and applicability of the proposed credible interval estimation methods. Results of the case study indicate that both solutions can accurately and reliably serve the nonconforming quality inference purpose. This research can be implemented as a decision-making tool for credible interval estimation and will provide valuable support for understanding and improving quality performance of automated, nonconforming quality control processes. © 2017 Elsevier B.V."
1,10.1002/esp.4189,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032702419&doi=10.1002%2fesp.4189&partnerID=40&md5=94d16ac2c54bd82ad183cfea6d6468ab,"Identifying sand provenance in depositional aeolian environments (e.g. dunefields) can elucidate sediment pathways and fluxes, and inform potential land management strategies where windblown sand and dust is a hazard to health and infrastructure. However, the complexity of these pathways typically makes this a challenging proposition, and uncertainties on the composition of mixed-source sediments are often not reported. This study demonstrates that a quantitative fingerprinting method within the Bayesian Markov Chain Monte Carlo (MCMC) framework offers great potential for exploring the provenance and uncertainties associated with aeolian sands. Eight samples were taken from dunes of the small (~58 km2) Ashkzar erg, central Iran, and 49 from three distinct potential sediment sources in the surrounding area. These were analyzed for 61 tracers including 53 geochemical elements (trace, major and rare earth elements (REE)) and eight REE ratios. Kruskal–Wallis H-tests and stepwise discriminant function analysis (DFA) allowed the identification of an optimum composite fingerprint based on six tracers (Rb, Sr, 87Sr, (La/Yb)n, Ga and δCe), and a Bayesian mixing model was applied to derive the source apportionment estimates within an uncertainty framework. There is substantial variation in the uncertainties in the fingerprinting results, with some samples yielding clear discrimination of components, and some with less clear fingerprints. Quaternary terraces and fans contribute the largest component to the dunes, but they are also the most extensive surrounding unit; clay flats and marls, however, contribute out of proportion to their small outcrop extent. The successful application of these methods to aeolian sediment deposits demonstrates their potential for providing quantitative estimates of aeolian sediment provenances in other mixed-source arid settings, and may prove especially beneficial where sediment is derived from multiple sources, or where other methods of provenance (e.g. detrital zircon U–Pb dating) are not possible due to mineralogical constraints. Copyright © 2017 John Wiley &amp; Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
2,10.1007/s00477-016-1344-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994297177&doi=10.1007%2fs00477-016-1344-1&partnerID=40&md5=214ee140397f72c993afe8cc72f486a6,"We focus on the Bayesian estimation of strongly heterogeneous transmissivity fields conditional on data sampled at a set of locations in an aquifer. Log-transmissivity, Y, is modeled as a stochastic Gaussian process, parameterized through a truncated Karhunen–Loève (KL) expansion. We consider Y fields characterized by a short correlation scale as compared to the size of the observed domain. These systems are associated with a KL decomposition which still requires a high number of parameters, thus hampering the efficiency of the Bayesian estimation of the underlying stochastic field. The distinctive aim of this work is to present an efficient approach for the stochastic inverse modeling of fully saturated groundwater flow in these types of strongly heterogeneous domains. The methodology is grounded on the construction of an optimal sparse KL decomposition which is achieved by retaining only a limited set of modes in the expansion. Mode selection is driven by model selection criteria and is conditional on available data of hydraulic heads and (optionally) Y. Bayesian inversion of the optimal sparse KLE is then inferred using Markov Chain Monte Carlo (MCMC) samplers. As a test bed, we illustrate our approach by way of a suite of computational examples where noisy head and Y values are sampled from a given randomly generated system. Our findings suggest that the proposed methodology yields a globally satisfactory inversion of the stochastic head and Y fields. Comparison of reference values against the corresponding MCMC predictive distributions suggests that observed values are well reproduced in a probabilistic sense. In a few cases, reference values at some unsampled locations (typically far from measurements) are not captured by the posterior probability distributions. In these cases, the quality of the estimation could be improved, e.g., by increasing the number of measurements and/or the threshold for the selection of KL modes. © 2016, Springer-Verlag Berlin Heidelberg."
2,10.1287/mnsc.2016.2529,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032570233&doi=10.1287%2fmnsc.2016.2529&partnerID=40&md5=054ee39fea96ccae8290e09d165d28ef,"This paper develops and estimates a structural model of two-sided markets with durable platform intermediaries and affiliated products. It models buyers' purchase decisions of platforms and affiliated products and sellers' decisions of price setting and entry, accounting for the dynamic interaction between the two distinct groups of platform participants. To estimate the proposed model, this paper develops a Bayesian Markov chain Monte Carlo estimation approach that incorporates nonparametric approximation and interpolation methods. The proposed model and estimation method are applied to the 32/64-bit generation of the U.S. video game industry. The results of counterfactual experiments show that the dynamic behavior of platform participants has significant impacts on platformadoption and the affiliated product market, and that a failed platform could have survived if it had priced the two sides properly in a dynamic two-sided market environment. © 2016 INFORMS."
1,10.1016/j.csda.2017.05.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020300016&doi=10.1016%2fj.csda.2017.05.005&partnerID=40&md5=bfae58cf2d7c9e9c11adb7b316c878f3,"Although the Markov Chain Monte Carlo (MCMC) is very popular in parameter inference, the alleviation of the burden of calculation is crucial due to the limit of processors, memory, and disk bottleneck. This is especially true in terms of handling big data. In recent years, researchers have developed a parallel MCMC algorithm, in which full data are partitioned into subdatasets. Samples are drawn from the subdatasets independently at different machines without communication. In the extant literature, all machines are deemed to be identical. However, due to the heterogeneity of the data put into different machines, and the random nature of MCMC, the assumption of “identical machines” is questionable. Here we propose a Powered Embarrassing Parallel MCMC (PEPMCMC) algorithm, in which the full data posterior density is the product of the sub-posterior densities (posterior densities of different subdatasets) raised by some constraint powers. This is proven to be equivalent to a weighted averaging procedure. In our work, the powers are determined based on a maximum likelihood criterion, which leads to finding a maximum likelihood point within the convex hull of the estimates from different machines. We prove the asymptotic exactness and apply it to several cases to verify its strength in comparison with the unparallel and unpowered parallel algorithms. Furthermore, the connection between normal kernel density and parametric density estimations under certain conditions is investigated. © 2017 Elsevier B.V."
2,10.3150/16-BEJ810,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019100236&doi=10.3150%2f16-BEJ810&partnerID=40&md5=5c983d401727dc860a7d782993af8b53,"Although Hamiltonian Monte Carlo has proven an empirical success, the lack of a rigorous theoretical understanding of the algorithm has in many ways impeded both principled developments of the method and use of the algorithm in practice. In this paper, we develop the formal foundations of the algorithm through the construction of measures on smooth manifolds, and demonstrate how the theory naturally identifies efficient implementations and motivates promising generalizations. © 2017 ISI/BS."
,10.1007/s11222-016-9699-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987660795&doi=10.1007%2fs11222-016-9699-1&partnerID=40&md5=1449cd79c574d7ff978357d7ea4a321b,"For big data analysis, high computational cost for Bayesian methods often limits their applications in practice. In recent years, there have been many attempts to improve computational efficiency of Bayesian inference. Here we propose an efficient and scalable computational technique for a state-of-the-art Markov chain Monte Carlo methods, namely, Hamiltonian Monte Carlo. The key idea is to explore and exploit the structure and regularity in parameter space for the underlying probabilistic model to construct an effective approximation of its geometric properties. To this end, we build a surrogate function to approximate the target distribution using properly chosen random bases and an efficient optimization process. The resulting method provides a flexible, scalable, and efficient sampling algorithm, which converges to the correct target distribution. We show that by choosing the basis functions and optimization process differently, our method can be related to other approaches for the construction of surrogate functions such as generalized additive models or Gaussian process models. Experiments based on simulated and real data show that our approach leads to substantially more efficient sampling algorithms compared to existing state-of-the-art methods. © 2016, Springer Science+Business Media New York."
3,10.1016/j.ejor.2017.04.019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018970493&doi=10.1016%2fj.ejor.2017.04.019&partnerID=40&md5=3e6697884dbed0f187284ea7c162492a,"New results are derived for the optimal preventive maintenance schedule of a single item over a finite horizon, based on Bayesian models of a failure rate function. Two types of failure rate functions—increasing and bathtub shapes—are considered. For both cases, optimality conditions and efficient algorithms to find an optimal maintenance schedule are given. A Bayesian parametric model for bathtub-shaped failure rate functions is used, while the class of increasing failure rate functions are tackled by an extended gamma process. We illustrate both approaches using real failure time data from the South Texas Project Nuclear Operating Company in Bay City, Texas. © 2017 Elsevier B.V."
9,10.1016/j.enbuild.2017.08.069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028925119&doi=10.1016%2fj.enbuild.2017.08.069&partnerID=40&md5=366bd453c54e919c881c5e7e8f6ed535,"Bayesian calibration as proposed by Kennedy and O'Hagan [22] has been increasingly applied to building energy models due to its ability to account for the discrepancy between observed values and model predictions. However, its application has been limited to calibration using monthly aggregated data because it is computationally inefficient when the dataset is large. This study focuses on improvements to the current implementation of Bayesian calibration to building energy simulation. This is achieved by: (1) using information theory to select a representative subset of the entire dataset for the calibration, and (2) using a more effective Markov chain Monte Carlo (MCMC) algorithm, the No-U-Turn Sampler (NUTS), which is an extension of Hamiltonian Monte Carlo (HMC) to explore the posterior distribution. The calibrated model was assessed by evaluating both accuracy and convergence. Application of the proposed method is demonstrated using two cases studies: (1) a TRNSYS model of a water-cooled chiller in a mixed-use building in Singapore, and (2) an EnergyPlus model of the cooling system of an office building in Pennsylvania, U.S.A. In both case studies, convergence was achieved for all parameters of the posterior distribution, with Gelman–Rubin statistics Rˆ within 1 ± 0.1. The coefficient of variation of the root mean squared error (CVRMSE) and normalized mean biased error (NMBE) were also within the thresholds set by ASHRAE Guideline 14 [1]. © 2017 Elsevier B.V."
2,10.1002/qre.2114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008498483&doi=10.1002%2fqre.2114&partnerID=40&md5=e36cea72f7f3236a26356d67b7d5ce67,"Degradation modeling might be an alternative to the conventional life test in reliability assessment for high quality products. This paper develops a Bayesian approach to the step-stress accelerated degradation test. Reliability inference of the population is made based on the posterior distribution of the underlying parameters with the aid of Markov chain Monte Carlo method. Further sequential reliability inference on individual product under normal condition is also proposed. Simulation study and an illustrative example are presented to show the appropriateness of the proposed method. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
4,10.1016/j.automatica.2017.07.053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027880897&doi=10.1016%2fj.automatica.2017.07.053&partnerID=40&md5=524aa1f0eee3cb5af9e5885da11384ea,"In this paper we introduce a novel method for linear system identification with quantized output data. We model the impulse response as a zero-mean Gaussian process whose covariance (kernel) is given by the recently proposed stable spline kernel, which encodes information on regularity and exponential stability. This serves as a starting point to cast our system identification problem into a Bayesian framework. We employ Markov Chain Monte Carlo methods to provide an estimate of the system. In particular, we design two methods based on the so-called Gibbs sampler that allow also to estimate the kernel hyperparameters by marginal likelihood maximization via the expectation–maximization method. Numerical simulations show the effectiveness of the proposed scheme, as compared to the state-of-the-art kernel-based methods when these are employed in system identification with quantized data. © 2017 Elsevier Ltd"
,10.1016/j.dss.2017.08.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029561760&doi=10.1016%2fj.dss.2017.08.005&partnerID=40&md5=e5443bc316f142f66331af2dd22dfa88,"We are dealing with mailing decisions of a direct marketing company and focus on assessing three alternative approaches to model unobserved heterogeneity, which are based on finite mixtures, continuous mixtures, and a mixture of Dirichlet processes (MDP), respectively. Models are estimated by Markov Chain Monte Carlo (MCMC) simulation. Based on Pseudo Bayes factors (PsBF), we find that a finite mixture model turns out to be superior both to models based on either a MDP or a continuous mixture. Whereas the MDP finds similar estimates compared to the finite mixture approach, estimates of the continuous mixture differ for some variables. According to the finite mixture, type of mailing has an effect on purchase behavior. In addition, some customers show supersaturation effects of mailings. Due to different coefficient estimates, managerial implications differ depending on which model they relate. In particular, a continuous mixture model would recommend more mailings than a finite mixture approach. © 2017 Elsevier B.V."
,10.1016/j.jhydrol.2017.09.035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029937114&doi=10.1016%2fj.jhydrol.2017.09.035&partnerID=40&md5=a4a29b80714f68c32e1f2fd313833ac9,"Hydrologic frequency analysis is commonly used by engineers and hydrologists to provide the basic information on planning, design and management of hydraulic and water resources systems under the assumption of stationarity. However, with increasing evidence of climate change, it is possible that the assumption of stationarity, which is prerequisite for traditional frequency analysis and hence, the results of conventional analysis would become questionable. In this study, we consider a framework for frequency analysis of extremes based on B-Spline quantile regression which allows to model data in the presence of non-stationarity and/or dependence on covariates with linear and non-linear dependence. A Markov Chain Monte Carlo (MCMC) algorithm was used to estimate quantiles and their posterior distributions. A coefficient of determination and Bayesian information criterion (BIC) for quantile regression are used in order to select the best model, i.e. for each quantile, we choose the degree and number of knots of the adequate B-spline quantile regression model. The method is applied to annual maximum and minimum streamflow records in Ontario, Canada. Climate indices are considered to describe the non-stationarity in the variable of interest and to estimate the quantiles in this case. The results show large differences between the non-stationary quantiles and their stationary equivalents for an annual maximum and minimum discharge with high annual non-exceedance probabilities. © 2017 Elsevier B.V."
4,10.1007/s10518-017-0169-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020747816&doi=10.1007%2fs10518-017-0169-8&partnerID=40&md5=7578f062e10a172d6fb7757912326fae,"Viscous dampers are dissipation devices widely employed for seismic structural control. To date, the performance of systems equipped with viscous dampers has been extensively analysed only by employing deterministic approaches. However, these approaches neglect the response dispersion due to the uncertainties in the input as well as the variability of the system properties. Some recent works have highlighted the important role of these seismic input uncertainties in the seismic performance of linear and nonlinear viscous dampers. This study analyses the effect of the variability of damper properties on the probabilistic system response and risk. In particular, the paper aims at evaluating the impact of the tolerance allowed in devices’ quality control and production tests in terms of variation of the exceedance probabilities of the Engineering Demand Parameters (EDPs) which are most relevant for the seismic performance. A preliminary study is carried out to relate the variability of the constitutive damper characteristics to the tolerance limit allowed in tests and to evaluate the consequences on the device’s dissipation properties. In the subsequent part of the study, the sensitivity of the dynamic response is analysed by harmonic analysis. Finally, the seismic response sensitivity is studied by evaluating the influence of the allowed variability of the constitutive damper characteristics on the response hazard curves, providing the exceedance probability per year of EDPs. A set of linear elastic systems with different dynamic properties, equipped with linear and nonlinear dampers, are considered in the analyses, and subset simulation is employed together with the Markov Chain Monte Carlo method to achieve a confident estimate of small exceedance probabilities. © 2017, Springer Science+Business Media B.V."
1,10.3390/s17112491,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032725427&doi=10.3390%2fs17112491&partnerID=40&md5=a95fe590309b70cb048759328edad14e,"Line scanning cameras, which capture only a single line of pixels, have been increasingly used in ground based mobile or robotic platforms. In applications where it is advantageous to directly georeference the camera data to world coordinates, an accurate estimate of the camera’s 6D pose is required. This paper focuses on the common case where a mobile platform is equipped with a rigidly mounted line scanning camera, whose pose is unknown, and a navigation system providing vehicle body pose estimates. We propose a novel method that estimates the camera’s pose relative to the navigation system. The approach involves imaging and manually labelling a calibration pattern with distinctly identifiable points, triangulating these points from camera and navigation system data and reprojecting them in order to compute a likelihood, which is maximised to estimate the 6D camera pose. Additionally, a Markov Chain Monte Carlo (MCMC) algorithm is used to estimate the uncertainty of the offset. Tested on two different platforms, the method was able to estimate the pose to within 0.06 m/1.05° and 0.18 m/2.39°. We also propose several approaches to displaying and interpreting the 6D results in a human readable way. © 2017 by the authors. Licensee MDPI, Basel, Switzerland."
,10.1080/03772063.2017.1379889,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041345421&doi=10.1080%2f03772063.2017.1379889&partnerID=40&md5=d9602808fa52847096cf9a56d689de9e,"In order to analyze non-stationary signals, like Electroencephalogram (EEG), it is sometimes easier to segment signals into pseudo-stationary segments. In this paper, the cascade of linear predictive coding (LPC) and non-linear Volterra filter is employed for modeling of noise in EEG signal and this methodology is applied to the procedure of change-point detection, for estimating the number of change-points and their exact location which is a powerful way to detect the change-points as precisely as possible. The earlier results are completed by constructing algorithms that use the cascade of LPC and non-linear Volterra filter for modeling the relation between noisy signal and noise in practical situations. In a Bayesian configuration, the posterior distribution of the change-point sequence is constructed and then Markov Chain Monte Carlo procedure is used for sampling this posterior distribution. The simulation results for segmentation of synthetic and real EEG data show that by applying our newly proposed methodology, the specificity and sensitivity of the segmentation are highly improved. In the case of synthetic data, the change-points are estimated completely precise (100% correct) in 70% of times and they are estimated with at least 98% accuracy in other times. © 2017 IETE"
,10.1007/s11222-016-9703-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988345859&doi=10.1007%2fs11222-016-9703-9&partnerID=40&md5=39958acabad14ae00525a570dd248a46,"Regression modelling beyond the mean of the response has found a lot of attention in the last years. Expectile regression is a special and computationally convenient case of this type of models where expectiles offer a quantile-like characterisation of the complete distribution and include the mean as a special case. In the frequentist framework, expectile regression could be combined with covariate effects of quite different forms and in particular nonlinear and spatial effects. We propose Bayesian expectile regression based on the asymmetric normal distribution as an auxiliary likelihood to allow for the additional inclusion of Bayesian regularisation priors for covariates with linear effects. Proposal densities based on iteratively weighted least squares updates for the resulting Markov chain Monte Carlo simulation algorithm are developed and evaluated in both simulations and an application. A special focus of the simulations lies on the evaluation of coverage properties of the Bayesian credible bands and the quantification of the detrimental effect arising from the misspecification of the auxiliary likelihood. © 2016, Springer Science+Business Media New York."
2,10.1177/1094342016649420,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035017087&doi=10.1177%2f1094342016649420&partnerID=40&md5=827d696d6bea02bb15d0435907e84ada,"A novel perturbative Monte Carlo mixed quantum mechanics (QM)/molecular mechanics (MM) approach has been recently developed to simulate molecular systems in complex environments. However, the required accuracy to efficiently simulate such complex molecular systems is usually granted at the cost of long executing times. To alleviate this problem, a new parallelization strategy of multi-level Monte Carlo molecular simulations is herein proposed for heterogeneous systems. It simultaneously exploits fine-grained (at the data level), coarse-grained (at the Markov chain level) and task-grained (pure QM, pure MM and QM/MM procedures) parallelism to ensure an efficient execution in heterogeneous systems composed of central processing units and multiple and possibly different graphical processing units. This is achieved by making use of the OpenCL library, together with appropriate dynamic load balancing schemes. From the conducted evaluation with real benchmarking data, a speed-up of 56x in the computational bottleneck part was observed, which results in a global speed-up of 38x for the whole simulation, reducing the time of a typical simulation from 80 hours to only 2 hours. © 2016, © The Author(s) 2016."
1,10.1007/s11222-016-9702-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990913361&doi=10.1007%2fs11222-016-9702-x&partnerID=40&md5=35a085ee560d90d6a6229be671c1c971,"Stationary time series models built from parametric distributions are, in general, limited in scope due to the assumptions imposed on the residual distribution and autoregression relationship. We present a modeling approach for univariate time series data, which makes no assumptions of stationarity, and can accommodate complex dynamics and capture non-standard distributions. The model for the transition density arises from the conditional distribution implied by a Bayesian nonparametric mixture of bivariate normals. This results in a flexible autoregressive form for the conditional transition density, defining a time-homogeneous, non-stationary Markovian model for real-valued data indexed in discrete time. To obtain a computationally tractable algorithm for posterior inference, we utilize a square-root-free Cholesky decomposition of the mixture kernel covariance matrix. Results from simulated data suggest that the model is able to recover challenging transition densities and non-linear dynamic relationships. We also illustrate the model on time intervals between eruptions of the Old Faithful geyser. Extensions to accommodate higher order structure and to develop a state-space model are also discussed. © 2016, Springer Science+Business Media New York."
,10.13196/j.cims.2017.11.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040539794&doi=10.13196%2fj.cims.2017.11.001&partnerID=40&md5=f502fe4c32239fc5027566b9b73afc90,"To choose the multiple dynamic quality control charts with more sensitive control performance, a performance analysis model for multiple dynamic control charts was established and the corresponding selection strategy was proposed. In Matlab environment, Markov chain method and Monte Carlo simulation method were used to solve the model, and the computational efficiency and precision of two methods were compared and analyzed when the process was in control and out of control. To solve the problem of too many conversions of optimal selection strategy, Variable Sampling Inlerval(VSI) and Variable Sampling Size and Interval(VSSI) multiple quality control charts should be given priority in the practical selection. If the size of off-target was small, VSI Multi-variate Cumulative Sum Control Chart(MCUSUM) control chart was optimal in practice; if 0.9&lt;δ&lt;1.9, VSSI Hotelling T2 control chart should be selected; if δ&gt;2, any kind of multivariate adaptive control chart would be suitable. © 2017, Editorial Department of CIMS. All right reserved."
3,10.1016/j.cpc.2017.06.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028032759&doi=10.1016%2fj.cpc.2017.06.020&partnerID=40&md5=8e4cf9b4fb25a56ec68d8333fd813f44,"Population annealing is a promising recent approach for Monte Carlo simulations in statistical physics, in particular for the simulation of systems with complex free-energy landscapes. It is a hybrid method, combining importance sampling through Markov chains with elements of sequential Monte Carlo in the form of population control. While it appears to provide algorithmic capabilities for the simulation of such systems that are roughly comparable to those of more established approaches such as parallel tempering, it is intrinsically much more suitable for massively parallel computing. Here, we tap into this structural advantage and present a highly optimized implementation of the population annealing algorithm on GPUs that promises speed-ups of several orders of magnitude as compared to a serial implementation on CPUs. While the sample code is for simulations of the 2D ferromagnetic Ising model, it should be easily adapted for simulations of other spin models, including disordered systems. Our code includes implementations of some advanced algorithmic features that have only recently been suggested, namely the automatic adaptation of temperature steps and a multi-histogram analysis of the data at different temperatures. Program summary Program Title: PAIsing Program Files doi: http://dx.doi.org/10.17632/sgzt4b7b3m.1 Licensing provisions: Creative Commons Attribution license (CC BY 4.0) Programming language: C, CUDA External routines/libraries: NVIDIA CUDA Toolkit 6.5 or newer Nature of problem: The program calculates the internal energy, specific heat, several magnetization moments, entropy and free energy of the 2D Ising model on square lattices of edge length L with periodic boundary conditions as a function of inverse temperature β. Solution method: The code uses population annealing, a hybrid method combining Markov chain updates with population control. The code is implemented for NVIDIA GPUs using the CUDA language and employs advanced techniques such as multi-spin coding, adaptive temperature steps and multi-histogram reweighting. Additional comments: Code repository at https://github.com/LevBarash/PAising. The system size and size of the population of replicas are limited depending on the memory of the GPU device used. For the default parameter values used in the sample programs, L=64, θ=100, β0=0, βf=1, Δβ=0.005, R=20000, a typical run time on an NVIDIA Tesla K80 GPU is 151 seconds for the single spin coded (SSC) and 17 seconds for the multi-spin coded (MSC) program (see Section 2 for a description of these parameters). © 2017 Elsevier B.V."
4,10.1002/stc.2004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013298141&doi=10.1002%2fstc.2004&partnerID=40&md5=524ed1878a416bd33e522815e18b9675,"Model updating based on vibration response measurements is a technique for reducing inherent modeling errors in finite element (FE) models that arise from simplifications, idealized connections, and uncertainties with regard to material properties. Updated FE models, which have relatively fewer discrepancies with their real structural counterparts, provide more in-depth predictions of the dynamic behaviors of those structures for future analysis. In this study, we develop a full-scale FE model of a major long-span bridge and update the model to improve an agreement between the identified modal properties of the real measured data and those from the FE model using a Bayesian model updating scheme. Sensitivity-based cluster analysis is performed to determine robust and efficient updating parameters, which include physical parameters having similar effects on targeted natural frequencies. The hybrid Monte Carlo method, one of the Markov chain Monte Carlo sampling methods, is used to obtain the posterior probability distributions of the updating parameters. Finally, the uncertainties of the updated parameters and the variability of the FE model's modal properties are evaluated. Copyright © 2017 John Wiley & Sons, Ltd."
2,10.5144/0256-4947.2017.433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042180444&doi=10.5144%2f0256-4947.2017.433&partnerID=40&md5=c57ea2b667018be34d5ffb0724fa9b6e,"BACKGROUND: Promising clinical and humanistic outcomes are associated with the use of new oral agents in the treatment of relapsing-remitting multiple sclerosis (RRMS). This is the first cost-effectiveness study comparing these medications in Saudi Arabia. OBJECTIVES: We aimed to compare the cost-effectiveness of fingolimod, teriflunomide, dimethyl fumarate, and interferon (IFN)-b1a products (Avonex and Rebif) as first-line therapies in the treatment of patients with RRMS from a Saudi payer perspective. DESIGN: Cohort Simulation Model (Markov Model). SETTING: Tertiary care hospital. METHODS: A hypothetical cohort of 1000 RRMS Saudi patients was assumed to enter a Markov model model with a time horizon of 20 years and an annual cycle length. The model was developed based on an expanded disability status scale (EDSS) to evaluate the cost-effectiveness of the five disease-modifying drugs (DMDs) from a healthcare system perspective. Data on EDSS progression and relapse rates were obtained from the literature; cost data were obtained from King Faisal Specialist Hospital and Research Centre, Riyadh, Saudi Arabia. Results were expressed as incremental cost-effectiveness ratios (ICERs) and net monetary benefits (NMB) in Saudi Riyals and converted to equivalent $US. The base-case willingness-to-pay (WTP) threshold was assumed to be $100000 (SAR375000). One-way sensitivity analysis and probabilistic sensitivity analysis were conducted to test the robustness of the model. MAIN OUTCOME MEASURES: ICERs and NMB. RESULTS: The base-case analysis results showed Rebif as the optimal therapy at a WTP threshold of $100000. Avonex had the lowest ICER value of $337282/QALY when compared to Rebif. One-way sensitivity analysis demonstrated that the results were sensitive to utility weights of health state three and four and the cost of Rebif. CONCLUSION: None of the DMDs were found to be cost-effective in the treatment of RRMS at a WTP threshold of $100000 in this analysis. The DMDs would only be cost-effective at a WTP above $300000. LIMITATIONS: The current analysis did not reflect the Saudi population preference in valuation of health states and did not consider the societal perspective in terms of cost."
6,10.1097/MD.0000000000008632,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037690111&doi=10.1097%2fMD.0000000000008632&partnerID=40&md5=e5c293d9ea67a183cc6a668dae704af4,"Background: Bariatric surgery has proved to be an effective strategy in treating obesity. However, randomized controlled trials (RCTs) of 3 most common bariatric surgery procedures, Roux-en-Y gastric bypass (RYGB), sleeve gastrectomy (SG), and laparoscopic adjustable gastric band (LAGB), reported inconsistent results. We performed a systematic review and network meta-analysis to synthesize evidence of effectiveness of the 3 common bariatric procedures from relevant RCTs. Methods: The present study was a systematic review and network meta-analysis of RCTs. All RCTs must meet the following criteria to be included in the analysis: patients with body mass index (BMI) ≥30 kg/m2, reported at least 1 outcome of interest, compared at least 2 of the 3 bariatric procedures, and had follow-ups of at least 1 year. Primary outcome was weight loss, expressed as differences in mean BMI reduction and percentage excess weight loss (%EWL) following 1 year after the surgery. Network meta-analysis was based on Bayesian framework with Markov Chain Monte Carlo simulation approach. Results: Eleven RCTs that met the criteria were included in the review. Of 9 trials (n = 765), the differences in mean BMI reduction were 0.76 (95% CI: 3.1 to 1.6) for RYGB versus SG, 5.8 (95% CI: 9.2 to 2.4) for RYGB versus LAGB, and 5.0 (95% CI: 9.0 to 1.0) for SG versus LAGB. Eight RCTs (n = 656) reported percentage excess weight-loss (%EWL), the mean differences between RYGB and SG, RYGB and LAGB, and SG and LAGB were 3.8% (95% CI: 8.5% to 13.8%), 22.2% (95% CI: 34.7% to 6.5%), and 26.0% (95% CI: 40.6% to 6.4%), respectively. The meta-analysis indicated low heterogeneity between studies, and the node splitting analysis showed that the studies were consistent between direct and indirect comparisons (P &gt; .05). Conclusion: The RYGB and SG yielded similar in weight-loss effect and both were superior to LAGB. Other factors such as complications and patient preference should be considered during surgical consultations. Copyright © 2017 the Author(s)."
,10.1371/journal.pone.0188660,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035755085&doi=10.1371%2fjournal.pone.0188660&partnerID=40&md5=482d3cce198e7a777983e9f90cdac9bd,"Horizontal behavior of highly migratory marine species is difficult to decipher because animals are wide-ranging, spend minimal time at the ocean surface, and utilize remote habitats. Satellite telemetry enables researchers to track individual movements, but population level inferences are rare due to data limitations that result from difficulty of capture and sporadic tag reporting. We introduce a Bayesian modeling framework to address population level questions with satellite telemetry data when data are sparse. We also outline an approach for identifying informative variables for use within the model. We tested our modeling approach using a large telemetry dataset for Shortfin Makos (Isurus oxyrinchus), which allowed us to assess the effects of various degrees of data paucity. First, a permuted Random Forest analysis is implemented to determine which variables are most informative. Next, a generalized additive mixed model is used to help define the relationship of each remaining variable with the response variable. Using jags and rjags for the analysis of Bayesian hierarchical models using Markov Chain Monte Carlo simulation, we then developed a movement model to generate parameter estimates for each of the variables of interest. By randomly reducing the tagging dataset by 25, 50, 75, and 90 percent and recalculating the parameter estimates, we demonstrate that the proposed Bayesian approach can be applied in data-limited situations. We also demonstrate how two commonly used linear mixed models with maximum likelihood estimation (MLE) can be similarly applied. Additionally, we simulate data from known parameter values to test each model’s ability to recapture those values. Despite performing similarly, we advocate using the Bayesian over the MLE approach due to the ability for later studies to easily utilize results of past study to inform working models, and the ability to use prior knowledge via informed priors in systems where such information is available. © 2017, Public Library of Science. All rights reserved. This is an open access article, free of all copyright, and may be freelyreproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication."
,10.1287/moor.2016.0834,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032943613&doi=10.1287%2fmoor.2016.0834&partnerID=40&md5=cb9482abb0bd3e839854e107ee32afbe,"Often in applications such as rare events estimation or optimal control it is required that one calculates the principal eigen-function and eigenvalue of a nonnegative integral kernel. Except in the finite-dimensional case, usually neither the principal eigenfunction nor the eigenvalue can be computed exactly. In this paper, we develop numerical approximations for these quantities. We show how a generic interacting particle algorithm can be used to deliver numerical approximations of the eigenquantities and the associated so-called ""twisted"" Markov kernel as well as how these approximations are relevant to the aforementioned applications. In addition, we study a collection of random integral operators underlying the algorithm, address some of their mean and pathwise properties, and obtain error estimates. Finally, numerical examples are provided in the context of importance sampling for computing tail probabilities of Markov chains and computing value functions for a class of stochastic optimal control problems. Copyright © 2017 INFORMS."
3,10.1016/j.cageo.2017.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017370156&doi=10.1016%2fj.cageo.2017.04.001&partnerID=40&md5=153833798a36de04fbf404af8905c5ca,"Bayesian inverse modeling techniques are computationally expensive because many forward simulations are needed when sampling the posterior distribution of the parameters. In this paper, we combine the implicit sampling method and generalized polynomial chaos expansion (gPCE) to significantly reduce the computational cost of performing Bayesian inverse modeling. There are three steps in this approach: (1) find the maximizer of the likelihood function using deterministic approaches; (2) construct a gPCE-based surrogate model using the results from a limited number of forward simulations; and (3) efficiently sample the posterior distribution of the parameters using implicit sampling method. The cost of constructing the gPCE-based surrogate model is further decreased by using sparse Bayesian learning to reduce the number of gPCE coefficients that have to be determined. We demonstrate the approach for a synthetic ponded infiltration experiment simulated with TOUGH2. The surrogate model is highly accurate with mean relative error that is <0.035% in predicting saturation and <0.25% in predicting the likelihood function. The posterior distribution of the parameters obtained using our proposed technique is nearly indistinguishable from the results obtained from either an implicit sampling method or a Markov chain Monte Carlo method utilizing the full model. © 2017 Elsevier Ltd"
2,10.1016/j.ress.2017.07.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026443552&doi=10.1016%2fj.ress.2017.07.007&partnerID=40&md5=92f80727c3d759342c43e85766247195,"This paper presents an efficient method for reliability-based design optimization (RBDO), which is robust to complex systems involving computationally expensive numerical models and/or a large number of random variables. This novel method belongs to a type of decoupling approaches in which the failure probability function (FPF) is approximated in the partitioned design space. In the setting of augmented reliability formulation, for a specific design configuration, the failure probability of a system is proportional to the probability density value of design variables conditioned on the failure event, thus transforming FPF approximation into a problem of density estimation. In this paper, we partition the design space into several subspaces and then estimate the density of failure samples in each subspace by binning and constructing regression functions. Sufficient failure samples are efficiently generated in each subspace using Markov Chain Monte Carlo method, which guarantees the accuracy of FPF approximation over there and ultimately over the entire design space. Three illustrative examples involving structural systems subjected to static or dynamic loadings are discussed to demonstrate the efficiency and accuracy of the proposed method. © 2017 Elsevier Ltd"
,10.1177/1060028017722007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031405541&doi=10.1177%2f1060028017722007&partnerID=40&md5=735cd4fe2ab6c4c463862ccefd85a5ac,"Background: Numerous economic models have been published evaluating treatment of chronic hepatitis C virus (HCV) infection, but none provide a comprehensive comparison among new antiviral agents. Objective: Evaluate the cost-effectiveness of all recommended therapies for treatment of genotypes 1 and 4 chronic HCV. Methods: Using data from clinical trials, observational analyses, and drug pricing databases, Markov decision models were developed for HCV genotypes 1 and 4 to compare all recommended drugs from the perspective of the third-party payer over a 5-, 10-, and 50-year time horizon. A probabilistic sensitivity analysis (PSA) was conducted by assigning distributions for clinical cure, age entering the model, costs for each health state, and quality-adjusted life years (QALYs) for each health state in a Monte Carlo simulation of 10 000 repetitions of the model. Results: In the lifetime model for genotype 1, effects ranged from 18.08 to 18.40 QALYs and total costs ranged from $88 107 to $184 636. The lifetime model of genotype 4 treatments had a range of effects from 18.23 to 18.43 QALYs and total costs ranging from $87 063 to $127 637. Grazoprevir/elbasvir was the optimal strategy followed by velpatasvir/sofosbuvir as the second-best strategy in most simulations for both genotypes 1 and 4, with drug costs and efficacy of grazoprevir/elbasvir as the primary model drivers. Conclusions: Grazoprevir/elbasvir was cost-effective compared with all strategies for genotypes 1 and 4. Effects for all strategies were similar with cost of drug in the initial year driving the results. © 2017, © The Author(s) 2017."
2,10.1016/j.mri.2017.06.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021101214&doi=10.1016%2fj.mri.2017.06.011&partnerID=40&md5=33d1f6192a7e57492c84803183b1c01d,"Purpose We applied our recently introduced Bayesian analytic method to achieve clinically-feasible in-vivo mapping of the proteoglycan water fraction (PgWF) of human knee cartilage with improved spatial resolution and stability as compared to existing methods. Materials and methods Multicomponent driven equilibrium single-pulse observation of T1 and T2 (mcDESPOT) datasets were acquired from the knees of two healthy young subjects and one older subject with previous knee injury. Each dataset was processed using Bayesian Monte Carlo (BMC) analysis incorporating a two-component tissue model. We assessed the performance and reproducibility of BMC and of the conventional analysis of stochastic region contraction (SRC) in the estimation of PgWF. Stability of the BMC analysis of PgWF was tested by comparing independent high-resolution (HR) datasets from each of the two young subjects. Results Unlike SRC, the BMC-derived maps from the two HR datasets were essentially identical. Furthermore, SRC maps showed substantial random variation in estimated PgWF, and mean values that differed from those obtained using BMC. In addition, PgWF maps derived from conventional low-resolution (LR) datasets exhibited partial volume and magnetic susceptibility effects. These artifacts were absent in HR PgWF images. Finally, our analysis showed regional variation in PgWF estimates, and substantially higher values in the younger subjects as compared to the older subject. Conclusions BMC-mcDESPOT permits HR in-vivo mapping of PgWF in human knee cartilage in a clinically-feasible acquisition time. HR mapping reduces the impact of partial volume and magnetic susceptibility artifacts compared to LR mapping. Finally, BMC-mcDESPOT demonstrated excellent reproducibility in the determination of PgWF. © 2016"
,10.1002/pst.1822,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028305033&doi=10.1002%2fpst.1822&partnerID=40&md5=00ed1a48f3049c432be81da83c3fe196,"Many new anticancer agents can be combined with existing drugs, as combining a number of drugs may be expected to have a better therapeutic effect than monotherapy owing to synergistic effects. Furthermore, to drive drug development and to reduce the associated cost, there has been a growing tendency to combine these as phase I/II trials. With respect to phase I/II oncology trials for the assessment of dose combinations, in the existing methodologies in which efficacy based on tumor response and safety based on toxicity are modeled as binary outcomes, it is not possible to enroll and treat the next cohort of patients unless the best overall response has been determined in the current cohort. Thus, the trial duration might be potentially extended to an unacceptable degree. In this study, we proposed a method that randomizes the next cohort of patients in the phase II part to the dose combination based on the estimated response rate using all the available observed data upon determination of the overall response in the current cohort. We compared the proposed method to the existing method using simulation studies. These demonstrated that the percentage of optimal dose combinations selected in the proposed method is not less than that in the existing method and that the trial duration in the proposed method is shortened compared to that in the existing method. The proposed method meets both ethical and financial requirements, and we believe it has the potential to contribute to expedite drug development. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1002/pst.1818,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022347587&doi=10.1002%2fpst.1818&partnerID=40&md5=d3dff189b27b61e23998ef0425c110ca,"The main purpose of dose-escalation trials is to identify the dose(s) that is/are safe and efficacious for further investigations in later studies. In this paper, we introduce dose-escalation designs that incorporate both the dose-limiting events and dose-limiting toxicities (DLTs) and indicative responses of efficacy into the procedure. A flexible nonparametric model is used for modelling the continuous efficacy responses while a logistic model is used for the binary DLTs. Escalation decisions are based on the combination of the probabilities of DLTs and expected efficacy through a gain function. On the basis of this setup, we then introduce 2 types of Bayesian adaptive dose-escalation strategies. The first type of procedures, called “single objective,” aims to identify and recommend a single dose, either the maximum tolerated dose, the highest dose that is considered as safe, or the optimal dose, a safe dose that gives optimum benefit risk. The second type, called “dual objective,” aims to jointly estimate both the maximum tolerated dose and the optimal dose accurately. The recommended doses obtained under these dose-escalation procedures provide information about the safety and efficacy profile of the novel drug to facilitate later studies. We evaluate different strategies via simulations based on an example constructed from a real trial on patients with type 2 diabetes, and the use of stopping rules is assessed. We find that the nonparametric model estimates the efficacy responses well for different underlying true shapes. The dual-objective designs give better results in terms of identifying the 2 real target doses compared to the single-objective designs. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.1016/j.neuroimage.2017.08.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027868620&doi=10.1016%2fj.neuroimage.2017.08.009&partnerID=40&md5=34c94764051098231caabedc67c626c8,"The dominant approach to neuroimaging data analysis employs the voxel as the unit of computation. While convenient, voxels lack biological meaning and their size is arbitrarily determined by the resolution of the image. Here, we propose a multivariate spatial model in which neuroimaging data are characterised as a linearly weighted combination of multiscale basis functions which map onto underlying brain nuclei or networks or nuclei. In this model, the elementary building blocks are derived to reflect the functional anatomy of the brain during the resting state. This model is estimated using a Bayesian framework which accurately quantifies uncertainty and automatically finds the most accurate and parsimonious combination of basis functions describing the data. We demonstrate the utility of this framework by predicting quantitative SPECT images of striatal dopamine function and we compare a variety of basis sets including generic isotropic functions, anatomical representations of the striatum derived from structural MRI, and two different soft functional parcellations of the striatum derived from resting-state fMRI (rfMRI). We found that a combination of ∼50 multiscale functional basis functions accurately represented the striatal dopamine activity, and that functional basis functions derived from an advanced parcellation technique known as Instantaneous Connectivity Parcellation (ICP) provided the most parsimonious models of dopamine function. Importantly, functional basis functions derived from resting fMRI were more accurate than both structural and generic basis sets in representing dopamine function in the striatum for a fixed model order. We demonstrate the translational validity of our framework by constructing classification models for discriminating parkinsonian disorders and their subtypes. Here, we show that ICP approach is the only basis set that performs well across all comparisons and performs better overall than the classical voxel-based approach. This spatial model constitutes an elegant alternative to voxel-based approaches in neuroimaging studies; not only are their atoms biologically informed, they are also adaptive to high resolutions, represent high dimensions efficiently, and capture long-range spatial dependencies, which are important and challenging objectives for neuroimaging data. © 2017 The Authors"
2,10.1016/j.ijnonlinmec.2017.08.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028728449&doi=10.1016%2fj.ijnonlinmec.2017.08.003&partnerID=40&md5=9e2521cd86e4fa95c00110dc4340b616,"The smooth and discontinuous oscillator with fractional derivative damping under combined harmonic and random excitations is investigated in this paper. The short memory principle is introduced so that the evolution process of the oscillator with fractional derivative damping can be described by the Markov chain. Then the stochastic generalized cell mapping method is used to obtain the steady-state probability density functions of the response. The stochastic response and bifurcation of the oscillator with fractional derivative damping are discussed in detail. We found that both the smoothness parameter, the noise intensity, the amplitude and frequency of the harmonic force can induce the occurrence of stochastic P-bifurcation in the system. Monte Carlo simulation verifies the effectiveness of the method we adopt in the paper. © 2017 Elsevier Ltd"
,10.1093/bioinformatics/btx407,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050446142&doi=10.1093%2fbioinformatics%2fbtx407&partnerID=40&md5=36f330bc4119e3a3cf91cd4b0f0a9fdf,"Results: We propose a novel method for inferring transcriptional regulation using a simple, yet biologically interpretable, model to find the logic by which a set of candidate genes and their associated transcription factors (TFs) regulate the transcriptional process of a gene of interest. Our dynamic model links the mRNA transcription rate of the target gene to the activation states of the TFs assuming that these interactions are consistent across multiple experiments and over time. A trans-dimensional Markov Chain Monte Carlo (MCMC) algorithm is used to efficiently sample the regulatory logic under different combinations of parents and rank the estimated models by their posterior probabilities. We demonstrate and compare our methodology with other methods using simulation examples and apply it to a study of transcriptional regulation of selected target genes of Arabidopsis Thaliana from microarray time series data obtained under multiple biotic stresses. We show that our method is able to detect complex regulatory interactions that are consistent under multiple experimental conditions.Availability and implementation: Programs are written in MATLAB and Statistics Toolbox Release 2016b, The MathWorks, Inc., Natick, Massachusetts, United States and are available on GitHub https://github.com/giorgosminas/TRS and at http://www2.warwick.ac.uk/fac/sci/systemsbiology/research/software.Contact: giorgos.minas@warwick.ac.uk or b.f.finkenstadt@warwick.ac.uk.Supplementary information: Supplementary data are available at Bioinformatics online.Motivation: The availability of more data of dynamic gene expression under multiple experimental conditions provides new information that makes the key goal of identifying not only the transcriptional regulators of a gene but also the underlying logical structure attainable. © The Author(s) 2017. Published by Oxford University Press."
3,10.1016/j.jclinepi.2017.07.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029498552&doi=10.1016%2fj.jclinepi.2017.07.006&partnerID=40&md5=83720b358066bcf95d4fe5ef2c1f7753,"Objectives The aim of the study was to identify the validity of effect estimates for serious rare adverse events in clinical study reports of antidepressants trials, across different meta-analysis methods. Study Design and Setting Four serious rare adverse events (all-cause mortality, suicidality, aggressive behavior, and akathisia) were meta-analyzed using different methods. The Yusuf-Peto odds ratio ignores studies with no events and was compared with the alternative approaches of generalized linear mixed models (GLMMs), conditional logistic regression, a Bayesian approach using Markov Chain Monte Carlo (MCMC), and a beta-binomial regression model. Results The estimates for the four outcomes did not change substantially across the different methods; the Yusuf-Peto method underestimated the treatment harm and overestimated its precision, especially when the estimated odds ratio deviated greatly from 1. For example, the odds ratio for suicidality for children and adolescents was 2.39 (95% confidence interval = 1.32–4.33), using the Yusuf-Peto method but increased to 2.64 (1.33–5.26) using conditional logistic regression, to 2.69 (1.19–6.09) using beta-binomial, to 2.73 (1.37–5.42) using the GLMM, and finally to 2.87 (1.42–5.98) using the MCMC approach. Conclusion The method used for meta-analysis of rare events data influences the estimates obtained, and the exclusion of double-zero event studies can give misleading results. To ensure reduction of bias and erroneous inferences, sensitivity analyses should be performed using different methods instead of the Yusuf-Peto approach, in particular the beta-binomial method, which was shown to be superior through a simulation study. © 2017 Elsevier Inc."
2,10.1002/bit.26379,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028574980&doi=10.1002%2fbit.26379&partnerID=40&md5=6eb446f643a417a3e53275c9d7702774,"13C Metabolic Fluxes Analysis (13C MFA) remains to be the most powerful approach to determine intracellular metabolic reaction rates. Decisions on strain engineering and experimentation heavily rely upon the certainty with which these fluxes are estimated. For uncertainty quantification, the vast majority of 13C MFA studies relies on confidence intervals from the paradigm of Frequentist statistics. However, it is well known that the confidence intervals for a given experimental outcome are not uniquely defined. As a result, confidence intervals produced by different methods can be different, but nevertheless equally valid. This is of high relevance to 13C MFA, since practitioners regularly use three different approximate approaches for calculating confidence intervals. By means of a computational study with a realistic model of the central carbon metabolism of E. coli, we provide strong evidence that confidence intervals used in the field depend strongly on the technique with which they were calculated and, thus, their use leads to misinterpretation of the flux uncertainty. In order to provide a better alternative to confidence intervals in 13C MFA, we demonstrate that credible intervals from the paradigm of Bayesian statistics give more reliable flux uncertainty quantifications which can be readily computed with high accuracy using Markov chain Monte Carlo. In addition, the widely applied chi-square test, as a means of testing whether the model reproduces the data, is examined closer. © 2017 Wiley Periodicals, Inc."
1,10.3150/16-BEJ814,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019069896&doi=10.3150%2f16-BEJ814&partnerID=40&md5=63a998dbecc45ff312987e5ca4001321,"This paper proposes a new exact simulation method, which simulates a realisation from a proposal density and then uses exact simulation of a Langevin diffusion to check whether the proposal should be accepted or rejected. Comparing to the existing coupling from the past method, the new method does not require constructing fast coalescence Markov chains. Comparing to the existing rejection sampling method, the new method does not require the proposal density function to bound the target density function. The new method is much more efficient than existing methods for certain problems. An application on exact simulation of the posterior of finite mixture models is presented. © 2017 ISI/BS."
,10.3150/16-BEJ827,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019141501&doi=10.3150%2f16-BEJ827&partnerID=40&md5=b94bdd6009123a1182cfcfc4b7686a85,"Strassen's classical martingale coupling theorem states that two random vectors are ordered in the convex (resp. increasing convex) stochastic order if and only if they admit a martingale (resp. submartingale) coupling. By analysing topological properties of spaces of probability measures equipped with a Wasserstein metric and applying a measurable selection theorem, we prove a conditional version of this result for random vectors conditioned on a random element taking values in a general measurable space. We provide an analogue of the conditional martingale coupling theorem in the language of probability kernels, and discuss how it can be applied in the analysis of pseudo-marginal Markov chain Monte Carlo algorithms.We also illustrate how our results imply the existence of a measurable minimiser in the context of martingale optimal transport. © 2017 ISI/BS."
3,10.1140/epjc/s10052-017-5274-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034632989&doi=10.1140%2fepjc%2fs10052-017-5274-y&partnerID=40&md5=943ddd5f29e59f1ec33f63097f029393,"We introduce ScannerBit, the statistics and sampling module of the public, open-source global fitting framework GAMBIT. ScannerBit provides a standardised interface to different sampling algorithms, enabling the use and comparison of multiple computational methods for inferring profile likelihoods, Bayesian posteriors, and other statistical quantities. The current version offers random, grid, raster, nested sampling, differential evolution, Markov Chain Monte Carlo (MCMC) and ensemble Monte Carlo samplers. We also announce the release of a new standalone differential evolution sampler, Diver, and describe its design, usage and interface to ScannerBit. We subject Diver and three other samplers (the nested sampler MultiNest, the MCMC GreAT, and the native ScannerBit implementation of the ensemble Monte Carlo algorithm T-Walk) to a battery of statistical tests. For this we use a realistic physical likelihood function, based on the scalar singlet model of dark matter. We examine the performance of each sampler as a function of its adjustable settings, and the dimensionality of the sampling problem. We evaluate performance on four metrics: optimality of the best fit found, completeness in exploring the best-fit region, number of likelihood evaluations, and total runtime. For Bayesian posterior estimation at high resolution, T-Walk provides the most accurate and timely mapping of the full parameter space. For profile likelihood analysis in less than about ten dimensions, we find that Diver and MultiNest score similarly in terms of best fit and speed, outperforming GreAT and T-Walk; in ten or more dimensions, Diver substantially outperforms the other three samplers on all metrics. © 2017, The Author(s)."
1,10.1007/s11222-016-9700-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987625575&doi=10.1007%2fs11222-016-9700-z&partnerID=40&md5=bb970f328a46a0a5501cb4123a51e882,"Markov chain Monte Carlo (MCMC) algorithms for Bayesian computation for Gaussian process-based models under default parameterisations are slow to converge due to the presence of spatial- and other-induced dependence structures. The main focus of this paper is to study the effect of the assumed spatial correlation structure on the convergence properties of the Gibbs sampler under the default non-centred parameterisation and a rival centred parameterisation (CP), for the mean structure of a general multi-process Gaussian spatial model. Our investigation finds answers to many pertinent, but as yet unanswered, questions on the choice between the two. Assuming the covariance parameters to be known, we compare the exact rates of convergence of the two by varying the strength of the spatial correlation, the level of covariance tapering, the scale of the spatially varying covariates, the number of data points, the number and the structure of block updating of the spatial effects and the amount of smoothness assumed in a Matérn covariance function. We also study the effects of introducing differing levels of geometric anisotropy in the spatial model. The case of unknown variance parameters is investigated using well-known MCMC convergence diagnostics. A simulation study and a real-data example on modelling air pollution levels in London are used for illustrations. A generic pattern emerges that the CP is preferable in the presence of more spatial correlation or more information obtained through, for example, additional data points or by increased covariate variability. © 2016, The Author(s)."
,10.29220/CSAM.2017.24.6.605,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044087590&doi=10.29220%2fCSAM.2017.24.6.605&partnerID=40&md5=8f625e6d35ea5b5f2d5b5ad9e5b1fcc7,"This paper presents proportional odds cure models to allow spatial correlations by including spatial frailty in the interval censored data setting. Parametric cure rate models with independent and dependent spatial frailties are proposed and compared. Our approach enables different underlying activation mechanisms that lead to the event of interest; in addition, the number of competing causes which may be responsible for the occurrence of the event of interest follows a Geometric distribution. Markov chain Monte Carlo method is used in a Bayesian framework for inferential purposes. For model comparison some Bayesian criteria were used. An influence diagnostic analysis was conducted to detect possible influential or extreme observations that may cause distortions on the results of the analysis. Finally, the proposed models are applied for the analysis of a real data set on smoking cessation. The results of the application show that the parametric cure model with frailties under the first activation scheme has better findings. © 2017 The Korean Statistical Society, and Korean International Statistical Society."
4,10.1016/j.hrtlng.2017.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029815526&doi=10.1016%2fj.hrtlng.2017.09.001&partnerID=40&md5=93b13e85fca99cfa7842ac863367fa9d,"Recent evidence challenges, the superiority of amiodarone, compared to other anti-arrhythmic medications, as the agent of choice in pulseless ventricular tachycardia (VT) or ventricular fibrillation (VF). We conducted Bayesian network and traditional meta-analyses to investigate the relative efficacies of amiodarone, lidocaine, magnesium (MgSO4) and placebo as treatments for pulseless VT or VF. Eleven studies [5200 patients, 7 randomized trials (4, 611 patients) and 4 non-randomized studies (589 patients)], were included in this meta-analysis. The search was conducted, from 1981 to February 2017, using MEDLINE, EMBASE and The Cochrane Library. Estimates were reported as odds ratio (OR) with 95% Credible Interval (CrI). Markov chain Monte Carlo (MCMC) modeling was used to estimate the relative ranking probability of each treatment group based on surface under cumulative ranking curve (SUCRA). Bayesian analysis demonstrated that lidocaine had superior effects on survival to hospital discharge, compared to amiodarone (OR, 2.18, 95% Cr.I 1.26–3.13), MgSO4 (OR, 2.03, 95% Cr.I 0.74–4.82) and placebo (OR, 2.42, 95% Cr.I 1.39–3.54). There were no statistical differences among treatment groups regarding survival to hospital admission/24 h (hrs) and return of spontaneous circulation (ROSC). Probability analysis revealed that lidocaine was the most effective therapy for survival to hospital discharge (SUCRA, 97%). We conclude that lidocaine may be the most effective anti-arrhythmic agent for survival to hospital discharge in patients with pulseless VT or VF. © 2017 Elsevier Inc."
,10.1007/s10260-017-0379-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016223080&doi=10.1007%2fs10260-017-0379-x&partnerID=40&md5=3505cc7a6e04affd624aea298e8f78c4,"In sample surveys, there is often insufficient sample size to obtain reliable direct estimates for parameters of interest for certain domains. Precision can be increased by introducing small area models which ‘borrow strength’ by connecting different areas through use of explicit linking models, area-specific random effects, and auxiliary covariate information. One consequence of the use of small area models is that small area estimates at a lower (for example, county) geographic level typically will not aggregate to the estimate at the corresponding higher (for example, state) geographic level. Benchmarking is the statistical procedure for reconciling these differences. This paper provides new perspectives for the benchmarking problem, especially for complex Bayesian small area models which require Markov Chain Monte Carlo estimation. Two new approaches to Bayesian benchmarking are introduced: one procedure based on minimum discrimination information, and another procedure for fully Bayesian self-consistent conditional benchmarking. Notably the proposed procedures construct adjusted posterior distributions whose first and higher order moments are consistent with the benchmarking constraints. It is shown that certain existing benchmarked estimators are special cases of the proposed methodology under normality, giving a distributional justification for the use of benchmarked estimates. Additionally, a ‘flexible’ benchmarking constraint is introduced, where the higher geographic level estimate is not considered fixed, and is simultaneously adjusted, along with lower level estimates. © 2017, Springer-Verlag Berlin Heidelberg."
,10.29220/CSAM.2017.24.6.561,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044029535&doi=10.29220%2fCSAM.2017.24.6.561&partnerID=40&md5=6686eafab30d87fa35a5281e353a97d2,"Bayesian statistics can play a key role in the design and analysis of clinical trials and this has been demonstrated for medical device trials. By 1995 Bayesian statistics had been well developed and the revolution in computing powers and Markov chain Monte Carlo development made calculation of posterior distributions within computational reach. The Food and Drug Administration (FDA) initiative of Bayesian statistics in medical device clinical trials, which began almost 20 years ago, is reviewed in detail along with some of the key decisions that were made along the way. Both Bayesian hierarchical modeling using data from previous studies and Bayesian adaptive designs, usually with a non-informative prior, are discussed. The leveraging of prior study data has been accomplished through Bayesian hierarchical modeling. An enormous advantage of Bayesian adaptive designs is achieved when it is accompanied by modeling of the primary endpoint to produce the predictive posterior distribution. Simulations are crucial to providing the operating characteristics of the Bayesian design, especially for a complex adaptive design. The 2010 FDA Bayesian guidance for medical device trials addressed both approaches as well as exchangeability, Type I error, and sample size. Treatment response adaptive randomization using the famous extracorporeal membrane oxygenation example is discussed. An interesting real example of a Bayesian analysis using a failed trial with an interesting subgroup as prior information is presented. The implications of the likelihood principle are considered. A recent exciting area using Bayesian hierarchical modeling has been the pediatric extrapolation using adult data in clinical trials. Historical control information from previous trials is an underused area that lends itself easily to Bayesian methods. The future including recent trends, decision theoretic trials, Bayesian benefit-risk, virtual patients, and the appalling lack of penetration of Bayesian clinical trials in the medical literature are discussed. © 2017 The Korean Statistical Society, and Korean International Statistical Society."
,10.1097/MD.0000000000008679,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036524934&doi=10.1097%2fMD.0000000000008679&partnerID=40&md5=3fbbfd8518deeedd14759ab2e89c9090,"Objective: This network meta-analysis aims to compare the efficacy and safety of 9 nonoperative regimens (placebo, pregabalin, GM-1 ganglioside, venlafaxine extended-release [venlafaxine XR], fampridine, conventional over-ground training [OT], body-weight-supported treadmill training [BWSTT], robotic-assisted gait training [RAGT]+OT and body-weight-supported over-ground training [BWSOT]) in treating spinal cord injury (SCI). Methods: Clinical controlled trials of 9 nonoperative regimens for SCI were retrieved in the electronic database. Traditional pairwise and Bayesian network meta-analyses were performed to compare the efficacy and safety of 9 nonoperative regimens for the treatment of SCI. Weighted mean difference (WMD), odds ratios (OR), and surface under the cumulative ranking curve (SUCRA) were calculated using the Markov Chain Monte Carlo engine Open BUGS (V.3.4.0) and R (V.3.2.1) package gemtc (V.0.6). Results: A total of 9 clinical controlled trials meeting the inclusion criteria were selected in this meta-analysis. On the aspect of efficacy, the results of pairwise meta-analysis indicated that the RAGT+OT and BWSOT might have the best efficacy in SCI patients in terms of a lower extremity motor score (LEMS) compared with conventional OT; the efficacy of RAGT+OT on SCI patients was relatively better than that of conventional OT in terms of walking index for spinal cord injury (WISCI). With the aspect of safety, the constipation rate of placebo on SCI patients was relatively higher than that of venlafaxine XR; however, with respect to headache and urinary tract infection, there was no significant difference in the safety of placebo, pregabalin, GM-1 ganglioside, venlafaxine XR, and fampridine on SCI patients. The results of SUCRA values suggested that BWSOT had the highest SUCRA value (75.25%) of LEMS; RAGT+OT had the highest SUCRA value (88.50%) of WISCI; venlafaxine XR had the highest SUCRA value (94.00%) of constipation; venlafaxine XR had the highest SUCRA value (80.00%) of headache; GM-1 ganglioside had the highest SUCRA value (87.75%) of urinary tract infection. Conclusion: Our results provide evidence that the RAGT+OT and BWSOT might have the best efficacy in the treatment of SCI, and the venlafaxine XR and GM-1 ganglioside showed adequate safety for SCI. © Copyright 2017 the Author(s). Published by Wolters Kluwer Health, Inc."
,10.1016/j.dam.2017.06.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026230759&doi=10.1016%2fj.dam.2017.06.003&partnerID=40&md5=9a8fdd9952b33108091ae6713d3ad932,"We consider a bipartite version of the color degree matrix problem. A bipartite graph G(U,V,E) is half-regular if all vertices in U have the same degree. We give necessary and sufficient conditions for a bipartite degree matrix (also known as demand matrix) to be the color degree matrix of an edge-disjoint union of half-regular graphs. We also give necessary and sufficient perturbations to transform realizations of a half-regular degree matrix into each other. Based on these perturbations, a Markov chain Monte Carlo method is designed in which the inverse of the acceptance ratios is polynomial bounded. Realizations of a half-regular degree matrix are generalizations of Latin squares, and they also appear in applied neuroscience. © 2017"
1,10.1109/HPEC.2017.8091050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041234620&doi=10.1109%2fHPEC.2017.8091050&partnerID=40&md5=0a05da6cfe522b0c23e7f1afa44af203,"The processing of graph data at large scale, though important and useful for real-world applications, continues to be challenging, particularly for problems such as graph partitioning. Optimal graph partitioning is NP-hard, but several methods provide approximate solutions in reasonable time. Yet scaling these approximate algorithms is also challenging. In this paper, we describe our efforts towards improving the scalability of one such technique, stochastic block partition, which is the baseline algorithm for the IEEE HPEC Graph Challenge [1]. Our key contributions are: Improvements to the parallelization of the baseline bottom-up algorithm, especially the Markov Chain Monte Carlo (MCMC) nodal updates for Bayesian inference; a new top-down divide and conquer algorithm capable of reducing the algorithmic complexity of static partitioning and also suitable for streaming partitioning; a parallel single-node multi-CPU implementation and a parallel multi-node MPI implementation. Although our focus is on algorithmic scalability, our Python implementation obtains a speedup of 1.65× over the fastest baseline parallel C++ run at a graph size of 100k vertices divided into 8 subgraphs on a multi-CPU single node machine. It achieves a speedup of 61× over itself on a cluster of 4 machines with 256 CPUs for a 20k node graph divided into 4 subgraphs, and 441× speedup over itself on a 50k node graph divided into 8 subgraphs on a multi-CPU single node machine. © 2017 IEEE."
6,10.1103/PhysRevX.7.041025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034428242&doi=10.1103%2fPhysRevX.7.041025&partnerID=40&md5=b716095b820661fa00aaa9c1fca37e59,"Pulsar timing and laser-interferometer gravitational-wave (GW) detectors are superb laboratories to study gravity theories in the strong-field regime. Here, we combine these tools to test the mono-scalartensor theory of Damour and Esposito-Farèse (DEF), which predicts nonperturbative scalarization phenomena for neutron stars (NSs). First, applying Markov-chain Monte Carlo techniques, we use the absence of dipolar radiation in the pulsar-timing observations of five binary systems composed of a NS and a white dwarf, and eleven equations of state (EOSs) for NSs, to derive the most stringent constraints on the two free parameters of the DEF scalar-tensor theory. Since the binary-pulsar bounds depend on the NS mass and the EOS, we find that current pulsar-timing observations leave scalarization windows, i.e., regions of parameter space where scalarization can still be prominent. Then, we investigate if these scalarization windows could be closed and if pulsar-timing constraints could be improved by laserinterferometer GW detectors, when spontaneous (or dynamical) scalarization sets in during the early (or late) stages of a binary NS (BNS) evolution. For the early inspiral of a BNS carrying constant scalar charge, we employ a Fisher-matrix analysis to show that Advanced LIGO can improve pulsar-timing constraints for some EOSs, and next-generation detectors, such as the Cosmic Explorer and Einstein Telescope, will be able to improve those bounds for all eleven EOSs. Using the late inspiral of a BNS, we estimate that for some of the EOSs under consideration, the onset of dynamical scalarization can happen early enough to improve the constraints on the DEF parameters obtained by combining the five binary pulsars. Thus, in the near future, the complementarity of pulsar timing and direct observations of GWs on the ground will be extremely valuable in probing gravity theories in the strong-field regime."
,10.1109/TCST.2017.2762288,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032742883&doi=10.1109%2fTCST.2017.2762288&partnerID=40&md5=80015703068665b0cd25551ea420bd5b,"Human-driven and autonomously driven cars of today act often reactively to the decisions of the cars they follow, which could lead to uncomfortable, inefficient, and sometimes unsafe situations in stop and go traffic. This paper proposes methods for probabilistic anticipation of the motion of the preceding vehicle and for the control of motion of the ego vehicle. We construct: 1) a Markov chain predictor based on the observed behavior of preceding vehicle and 2) a maximum likelihood motion predictor based on historical traffic speed at different locations and times. Heuristics are proposed for combining the two predictions to determine a probability distribution on the position of the preceding vehicle over a future planning horizon. A chance-constrained model predictive control framework is employed to optimize the motion of the ego vehicle, given the probabilistic prediction of motion of preceding vehicle. Effectiveness of the proposed approach is evaluated in multiple simulation scenarios. IEEE"
,10.1007/s10729-017-9420-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032505849&doi=10.1007%2fs10729-017-9420-8&partnerID=40&md5=de82bb61c82d4cbe35ab67057b64bfbf,"Markov models are commonly used for decision-making studies in many application domains; however, there are no widely adopted methods for performing sensitivity analysis on such models with uncertain transition probability matrices (TPMs). This article describes two simulation-based approaches for conducting probabilistic sensitivity analysis on a given discrete-time, finite-horizon, finite-state Markov model using TPMs that are sampled over a specified uncertainty set according to a relevant probability distribution. The first approach assumes no prior knowledge of the probability distribution, and each row of a TPM is independently sampled from the uniform distribution on the row’s uncertainty set. The second approach involves random sampling from the (truncated) multivariate normal distribution of the TPM’s maximum likelihood estimators for its rows subject to the condition that each row has nonnegative elements and sums to one. The two sampling methods are easily implemented and have reasonable computation times. A case study illustrates the application of these methods to a medical decision-making problem involving the evaluation of treatment guidelines for glycemic control of patients with type 2 diabetes, where natural variation in a patient’s glycated hemoglobin (HbA1c) is modeled as a Markov chain, and the associated TPMs are subject to uncertainty. © 2017 Springer Science+Business Media, LLC"
,10.5194/hess-21-5375-2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032494407&doi=10.5194%2fhess-21-5375-2017&partnerID=40&md5=71feb3c3cc178ee052088d277dbf690f,"A substantial interpretation of electromagnetic induction (EMI) measurements requires quantifying optimal model parameters and uncertainty of a nonlinear inverse problem. For this purpose, an adaptive Bayesian Markov chain Monte Carlo (MCMC) algorithm is used to assess multi-orientation and multi-offset EMI measurements in an agriculture field with non-saline and saline soil. In MCMC the posterior distribution is computed using Bayes' rule. The electromagnetic forward model based on the full solution of Maxwell's equations was used to simulate the apparent electrical conductivity measured with the configurations of EMI instrument, the CMD Mini-Explorer. Uncertainty in the parameters for the three-layered earth model are investigated by using synthetic data. Our results show that in the scenario of non-saline soil, the parameters of layer thickness as compared to layers electrical conductivity are not very informative and are therefore difficult to resolve. Application of the proposed MCMC-based inversion to field measurements in a drip irrigation system demonstrates that the parameters of the model can be well estimated for the saline soil as compared to the non-saline soil, and provides useful insight about parameter uncertainty for the assessment of the model outputs. © Author(s) 2017."
2,10.1080/02664763.2016.1258549,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996490528&doi=10.1080%2f02664763.2016.1258549&partnerID=40&md5=d5d6f69bfa2f2a213493e5a5d2fb8ea2,"In this paper, the statistical inference of the unknown parameters of a Burr Type III (BIII) distribution based on the unified hybrid censored sample is studied. The maximum likelihood estimators of the unknown parameters are obtained using the Expectation–Maximization algorithm. It is observed that the Bayes estimators cannot be obtained in explicit forms, hence Lindley's approximation and the Markov Chain Monte Carlo (MCMC) technique are used to compute the Bayes estimators. Further the highest posterior density credible intervals of the unknown parameters based on the MCMC samples are provided. The new model selection test is developed in discriminating between two competing models under unified hybrid censoring scheme. Finally, the potentiality of the BIII distribution to analyze the real data is illustrated by using the fracture toughness data of the three different materials namely silicon nitride (Si3N4), Zirconium dioxide (ZrO2) and sialon (Si6− xAlxOxN8− x). It is observed that for the present data sets, the BIII distribution has the better fit than the Weibull distribution which is frequently used in the fracture toughness data analysis. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/02664763.2016.1259400,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997605077&doi=10.1080%2f02664763.2016.1259400&partnerID=40&md5=d4b1ecfc996a18d1aff9ec2b1f73e63d,"This study is concerned with the extension of the Mallows–Bradley–Terry ranking model for one block comparison consisting of all the items of interest to situations which allow an expression of no preference. We consider a modification of the Mallows–Bradley–Terry ranking model by introducing an additional parameter, called an index of discrimination, in the model. This permits ties in the model. The maximum likelihood estimates of the parameters are found using a Maximization–Minimization algorithm: the evaluation of the mathematical expectations involved in the log-likelihood equation is obtained by generating samples of Monte Carlo Markov chain from the stationary distribution. In addition, a simulation study for asymptotic properties assessment has been made. The proposed method is applied to analyze data election. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
,10.1016/j.ecolmodel.2017.08.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028721202&doi=10.1016%2fj.ecolmodel.2017.08.016&partnerID=40&md5=7510745d051fcb27f1da689a060dceb9,"Ammonia-oxidizing bacteria and archaea (AOA and AOB) perform the rate-limiting step of nitrification, a biogeochemical process that controls the availability of inorganic nitrogen in terrestrial and aquatic ecosystems. We sought to investigate field values of AOA and AOB ammonia-uptake kinetics along with domain-level contributions to ammonia oxidation in temperate forest soils. To accomplish this goal, we constructed an ecosystem model that simulates ammonia oxidation in temperate forest soils based only on inorganic nitrogen pools and AOA and AOB population dynamics observed during in situ incubations. The model used Bayesian Markov chain Monte Carlo procedure to choose the most likely combination of in situ ammonia-uptake parameters for AOA and AOB, including Km,AOA, Km,AOB, Vmax,AOA, and Vmax,AOB. Domain-level contributions to ammonia oxidation were extracted from the best-fit solution and the model-selected values indicate that AOB was responsible for 70.0% of the simulated ammonia oxidation across sites, while AOA was responsible for the remaining 30.0%. We believe that the approach we demonstrate here can be applied to microbially-mediated biogeochemical fluxes in other elemental cycles as well. © 2017 Elsevier B.V."
2,10.23919/EUSIPCO.2017.8081197,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041423929&doi=10.23919%2fEUSIPCO.2017.8081197&partnerID=40&md5=f7d8a695fae72dee3b0c5e3b1deac948,"Monte Carlo (MC) methods are widely used for Bayesian inference and optimization in statistics, signal processing and machine learning. Two well-known class of MC methods are the Importance Sampling (IS) techniques and the Markov Chain Monte Carlo (MCMC) algorithms. In this work, we introduce the Group Importance Sampling (GIS) framework where different sets of weighted samples are properly summarized with one summary particle and one summary weight. GIS facilitates the design of novel efficient MC techniques. For instance, we present the Group Metropolis Sampling (GMS) algorithm which produces a Markov chain of sets of weighted samples. GMS in general outperforms other multiple try schemes as shown by means of numerical simulations. © EURASIP 2017."
1,10.23919/EUSIPCO.2017.8081191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040461650&doi=10.23919%2fEUSIPCO.2017.8081191&partnerID=40&md5=138bed2f674ed0bc9b011bd630771a14,"Gibbs sampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively used in signal processing, machine learning and statistics. The key point for the successful application of the Gibbs sampler is the ability to draw samples from the full-conditional probability density functions efficiently. In the general case this is not possible, so in order to speed up the convergence of the chain, it is required to generate auxiliary samples. However, such intermediate information is finally disregarded. In this work, we show that these auxiliary samples can be recycled within the Gibbs estimators, improving their efficiency with no extra cost. Theoretical and exhaustive numerical comparisons show the validity of the approach. © EURASIP 2017."
,10.1109/ICRAECT.2017.31,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040006730&doi=10.1109%2fICRAECT.2017.31&partnerID=40&md5=332bf2a2e506c588b4ccc58819743fe5,"The compilation of high quality traffic data is one of the basic concerns for real time traffic video surveillance. Since Video surveillance has figurative applications such as security, traffic monitoring and law enforcement. Detecting and recognizing the vehicle objects in a video is an imperative part of video surveillance systems, which is going to guide in tracking of the detected objects and gathering decisive information. Vehicular detection and classification is decisive as it can guide in traffic control and gathering of traffic info that can be used in intelligent transportation systems. Vehicular classification poses a thought-provoking problem as vehicles have high intra-class variation and comparatively low inter-class variation and it is time consuming, varying performance under different weather conditions etc. Hence there is a demand for employing innovative efficient data collection technique which will give the advantageous continuous data collection with acceptable level of accuracy. This paper surveys the prominent published literature of last few years to evaluate and summarize the work done so far on this field. The motivation for this paper is to provide a review of current methods employed for traffic data collection for Intelligent Transport System (ITS) available in the literature and estimate best one that accomplish all parameter conditions. © 2017 IEEE."
1,10.23919/EUSIPCO.2017.8081571,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041475372&doi=10.23919%2fEUSIPCO.2017.8081571&partnerID=40&md5=5d13b16acd17590714e3b26bcff27c45,"Many prediction studies using real life measure-ments such as wind speed, power, electricity load and rain-fall utilize linear autoregressive moving average (ARMA) based models due to their simplicity and general character. However, most of the real life applications exhibit nonlinear character and modelling them with linear time series may become problematic. Among nonlinear ARMA models, polynomial ARMA (PARMA) models belong to the class of linear-in-the-parameters. In this paper, we propose a reversible jump Markov chain Monte Carlo (RJMCMC) based complete model estimation method which estimates PARMA models with all their parameters including the nonlinearity degree. The proposed method is unique in the manner of estimating the nonlinearity degree and all other model orders and model coefficients at the same time. Moreover, in this paper, RJMCMC has been examined in an anomalous way by performing transitions between linear and nonlinear model spaces. © EURASIP 2017."
,10.23919/EUSIPCO.2017.8081205,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041530553&doi=10.23919%2fEUSIPCO.2017.8081205&partnerID=40&md5=8451cfd500cd005acabc00a50c2f66f9,"This paper studies a new Bayesian algorithm for the joint reconstruction and classification of reflectance confocal microscopy (RCM) images, with application to the identification of human skin lentigo. The proposed Bayesian approach takes advantage of the distribution of the multiplicative speckle noise affecting the true reflectivity of these images and of appropriate priors for the unknown model parameters. A Markov chain Monte Carlo (MCMC) algorithm is proposed to jointly estimate the model parameters and the image of true reflectivity while classifying images according to the distribution of their reflectivity. Precisely, a Metropolis-within-Gibbs sampler is investigated to sample the posterior distribution of the Bayesian model associated with RCM images and to build estimators of its parameters, including labels indicating the class of each RCM image. The resulting algorithm is applied to synthetic data and to real images from a clinical study containing healthy and lentigo patients. © EURASIP 2017."
,10.23919/EUSIPCO.2017.8081561,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041461269&doi=10.23919%2fEUSIPCO.2017.8081561&partnerID=40&md5=f593c37196068bd995810f60719756f7,"This paper considers the analysis of communication protocols in wireless networks implementing both cooperation and Hybrid Automatic Repeat reQuest (HARQ) for Type I decoder and Type II decoder with Chase Combining. Using an example of a three-node network, we show that the communication protocol can be modeled using Finite State Markov Chains. This model efficiently predicts the performance of the system. However, the complexity depends on the number of states, which increases very fast as the protocol gets more sophisticated. We then derive a simplified model using state aggregation, and obtain a compact description which can be used to predict the performance with a reduced complexity. Moreover, we show that the simplified model describes a probabilistic communication protocol on the same network. Monte Carlo simulations show that the theoretical predictions match the simulated performance. © EURASIP 2017."
1,10.1063/1.4995425,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026860225&doi=10.1063%2f1.4995425&partnerID=40&md5=bbe945ca056d8114611e90f7e99e306e,"We use first-principles density-functional theory to characterize the binding sites and diffusion mechanisms for a Ga adatom on the GaAs(001)β2(2 × 4) surface. Diffusion in this system is a complex process involving eleven unique binding sites and sixteen different hops between neighboring binding sites. Among the binding sites, we can identify four different superbasins such that the motion between binding sites within a superbasin is much faster than hops exiting the superbasin. To describe diffusion, we use a recently developed local superbasin kinetic Monte Carlo (LSKMC) method, which accelerates a conventional kinetic Monte Carlo (KMC) simulation by describing the superbasins as absorbing Markov chains. We find that LSKMC is up to 4300 times faster than KMC for the conditions probed in this study. We characterize the distribution of exit times from the superbasins and find that these are sometimes, but not always, exponential and we characterize the conditions under which the superbasin exit-time distribution should be exponential. We demonstrate that LSKMC simulations assuming an exponential superbasin exit-time distribution yield the same diffusion coefficients as conventional KMC. © 2017 Author(s)."
3,10.1063/1.4984932,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020382450&doi=10.1063%2f1.4984932&partnerID=40&md5=8f2cf12c0ef3003c9be42559db02dffb,"Markov state models (MSMs) and other related kinetic network models are frequently used to study the long-timescale dynamical behavior of biomolecular and materials systems. MSMs are often constructed bottom-up using brute-force molecular dynamics (MD) simulations when the model contains a large number of states and kinetic pathways that are not known a priori. However, the resulting network generally encompasses only parts of the configurational space, and regardless of any additional MD performed, several states and pathways will still remain missing. This implies that the duration for which the MSM can faithfully capture the true dynamics, which we term as the validity time for the MSM, is always finite and unfortunately much shorter than the MD time invested to construct the model. A general framework that relates the kinetic uncertainty in the model to the validity time, missing states and pathways, network topology, and statistical sampling is presented. Performing additional calculations for frequently-sampled states/pathways may not alter the MSM validity time. A new class of enhanced kinetic sampling techniques is introduced that aims at targeting rare states/pathways that contribute most to the uncertainty so that the validity time is boosted in an effective manner. Examples including straightforward 1D energy landscapes, lattice models, and biomolecular systems are provided to illustrate the application of the method. Developments presented here will be of interest to the kinetic Monte Carlo community as well. © 2017 Author(s)."
,10.1080/07474938.2017.1307548,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019740331&doi=10.1080%2f07474938.2017.1307548&partnerID=40&md5=4b8a716261003e887bf7d305f5df7ccc,"This paper develops tests of the null hypothesis of linearity in the context of autoregressive models with Markov-switching means and variances. These tests are robust to the identification failures that plague conventional likelihood-based inference methods. The approach exploits the moments of normal mixtures implied by the regime-switching process and uses Monte Carlo test techniques to deal with the presence of an autoregressive component in the model specification. The proposed tests have very respectable power in comparison with the optimal tests for Markov-switching parameters of Carrasco et al. (2014), and they are also quite attractive owing to their computational simplicity. The new tests are illustrated with an empirical application to an autoregressive model of USA output growth. © 2017 Taylor & Francis Group, LLC."
,10.1109/PHM.2017.8079119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039955320&doi=10.1109%2fPHM.2017.8079119&partnerID=40&md5=14efeba54c5fbc1b3bd2c58852e8027e,"Remaining useful life (RUL) prediction is one of the most critical procedures of the prognostics and health management (PHM). In the existing literature, most RUL prediction methods are under the assumption that there is no maintenance activity during the whole life time of the degrading system. However, most practical systems experience various kinds of maintenance activities when they are in operation. This article presents an approach to predict the RUL of a class of nonlinear degrading systems with stochastic maintenance. To predict the RUL for systems with stochastic maintenance, a wiener process based degradation model is proposed. The switches between states of normal operation and maintenance are described by a continuous time Markov chain (CTMC). In addition, the maximum likelihood estimation (MLE) is adopted to estimate both unknown parameters in the degradation model and the transition probability between normal operation and maintenance. The analytical form of first hitting time (FHT) of degradation process is difficult to derive with the presence of maintenance activities. To avoid complicated mathematical derivation of stochastic differential, Monte Carlo method is used to obtain a numerical result of the RUL distribution. A numerical study is presented to illustrate and validate the proposed method. © 2017 IEEE."
1,10.3389/fnins.2017.00586,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032028819&doi=10.3389%2ffnins.2017.00586&partnerID=40&md5=7dd98483e1a41f72422a5e07423c275a,"It is well-known that data from diffusion weighted imaging (DWI) follow the Rician distribution. The Rician distribution is also relevant for functional magnetic resonance imaging (fMRI) data obtained at high temporal or spatial resolution. We propose a general regression model for non-central χ (NC-χ) distributed data, with the heteroscedastic Rician regression model as a prominent special case. The model allows both parameters in the Rician distribution to be linked to explanatory variables, with the relevant variables chosen by Bayesian variable selection. A highly efficient Markov chain Monte Carlo (MCMC) algorithm is proposed to capture full model uncertainty by simulating from the joint posterior distribution of all model parameters and the binary variable selection indicators. Simulated regression data is used to demonstrate that the Rician model is able to detect the signal much more accurately than the traditionally used Gaussian model at low signal-to-noise ratios. Using a diffusion dataset from the Human Connectome Project, it is also shown that the commonly used approximate Gaussian noise model underestimates the mean diffusivity (MD) and the fractional anisotropy (FA) in the single-diffusion tensor model compared to the Rician model. © 2017 Wegmann, Eklund and Villani."
,10.1109/WHISPERS.2015.8075438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039167056&doi=10.1109%2fWHISPERS.2015.8075438&partnerID=40&md5=f881e077eda7ce2eb59f0ba363442c7f,"This paper presents a new Bayesian collaborative sparse regression method for linear unmixing of hyperspectral images. Our contribution is twofold; first, we propose a new Bayesian model for structured sparse regression in which the supports of the sparse abundance vectors are a priori spatially correlated across pixels. Secondly, we propose an advanced Markov chain Monte Carlo algorithm to estimate the posterior probabilities that materials are present or absent in each pixel, and, conditionally to the maximum marginal a posteriori configuration of this support, compute the MMSE estimates of the abundance vectors. A remarkable property of this algorithm is that it self-adjusts the values of the parameters of the Markov random field, thus relieving practitioners from setting regularisation parameters, namely by cross-validation. The proposed methodology is illustrated with real hyperspectral data. © 2015 IEEE."
,10.1109/WHISPERS.2014.8077636,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038574984&doi=10.1109%2fWHISPERS.2014.8077636&partnerID=40&md5=0d3215a768f0bd3b3b0d2a57e2b08223,"To reduce huge consumption of processing hyperspectral images(HSI), a novel Bayesian unmixing compressive sensing framework is proposed to compress and reconstruct HSI effectively, called structured sparse Bayesian umixing compressive sensing(SSBUCS). SSBUCS unites compressive sensing and hyperspectral linear mixed model in Bayesian framework. An HSI is decomposed as a linear combination of endmembers and abundance matrix. The abundance matrix is transformed to a structured sparse signal in the wavelet domain. Then, compressive sensing is employed on this sparse signal to produce a more compact result. To recover the HSI, a Markov chain Monte Carlo(MCMC) method based on Gibbs sampling is proposed, imposing structured sparse prior on abundance matrix. Experimental results verify the superiority of the proposed method over several state-of-art methods. © 2014 IEEE."
,10.1109/WHISPERS.2015.8075442,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039151806&doi=10.1109%2fWHISPERS.2015.8075442&partnerID=40&md5=790646ce8fdec8c022db8d97e12588f5,This paper presents an unsupervised Bayesian algorithm for hyper-spectral image unmixing accounting for endmember variability. This variability is obtained by assuming that each pixel is a linear combination of random endmembers weighted by their corresponDing abundances. An additive noise is also considered in the proposed model generalizing the normal compositional model. The proposed model is unsupervised since it estimates the abundances and both the mean and the covariance matrix of each endmember. A classification map indicating the class of each pixel is also obtained based on the estimated abundances. Simulations conducted on a real dataset show the potential of the proposed model in terms of unmixing performance for the analysis of hyperspectral images. © 2015 IEEE.
,10.1109/ICIEAM.2017.8076474,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039945044&doi=10.1109%2fICIEAM.2017.8076474&partnerID=40&md5=bb6ab38837b898b799cc4458aec0ba32,The paper dwells on the design of a diagnostic system and expert assessment of the significance of threats to the security of industrial networks. The proposed system is based on a new cyber-attacks classification and presupposes the existence of two structural blocks: the industrial network virtual model based on the scan selected nodal points and the generator of cyber-attacks sets. The diagnostic and expert assessment quality is improved by the use of the Markov chains or the Monte Carlo numerical method. The numerical algorithm of generating cyber-attacks sets is based on the LPτ-sequence. © 2017 IEEE.
,10.1109/WHISPERS.2016.8071782,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037529472&doi=10.1109%2fWHISPERS.2016.8071782&partnerID=40&md5=c5b22478f9bf21e090aa3da146bb6984,"This paper presents a new Bayesian spectral unmixing algorithm to analyse remote scenes sensed via multispectral Lidar measurements. To a first approximation, each Lidar waveform consists of the temporal signature of the observed target, which depends on the wavelength of the laser source considered and which is corrupted by Poisson noise. When the number of spectral bands is large enough, it becomes possible to identify and quantify the main materials in the scene, on top of the estimation of classical Lidar-based range profiles. Thanks to its anomaly detection capability, the proposed hierarchical Bayesian model, coupled with an efficient Markov chain Monte Carlo algorithm, allows robust estimation of depth images together with abundance and outlier maps associated with the observed 3D scene. The proposed methodology is illustrated via experiments conducted with real multispectral Lidar data. © 2016 IEEE."
,10.1080/03610926.2016.1235193,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024394960&doi=10.1080%2f03610926.2016.1235193&partnerID=40&md5=bbb0f33147db62f3f851bc7cd5f76455,"A stationary bilinear (SB) model can be used to describe processes with a time-varying degree of persistence that depends on past shocks. This study develops methods for Bayesian inference, model comparison, and forecasting in the SB model. Using monthly U.K. inflation data, we find that the SB model outperforms the random walk, first-order autoregressive AR(1), and autoregressive moving average ARMA(1,1) models in terms of root mean squared forecast errors. In addition, the SB model is superior to these three models in terms of predictive likelihood for the majority of forecast observations. © 2017 Taylor & Francis Group, LLC."
,10.1080/03610926.2016.1228961,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021820604&doi=10.1080%2f03610926.2016.1228961&partnerID=40&md5=a5822876372a108c94e6905e908bb2bb,"It is commonly asserted that the Gibbs sampler is a special case of the Metropolis–Hastings (MH) algorithm. While this statement is true for certain Gibbs samplers, it is not true in general for the version that is taught and used most often, namely, the deterministic scan Gibbs sampler. In this note, I prove that that there exist deterministic scan Gibbs samplers that do not exhibit detailed balance and hence cannot be considered MH samplers. The nuances of various Gibbs sampling schemes are discussed. © 2017 Taylor & Francis Group, LLC."
1,10.1002/2017GL075043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031326195&doi=10.1002%2f2017GL075043&partnerID=40&md5=47277884a8835ff40b24007c1f2865e0,"The yield strength of oceanic lithosphere determines the mode of mantle convection in a terrestrial planet, and low-temperature plasticity in olivine aggregates is generally believed to govern the plastic rheology of the stiffest part of lithosphere. Because, so far, proposed flow laws for this mechanism exhibit nontrivial discrepancies, we revisit the recent high-pressure deformation data of Mei et al. (2010) with a comprehensive inversion approach based on Markov chain Monte Carlo sampling. Our inversion results indicate that the uncertainty of the relevant flow law parameters is considerably greater than previously thought. Depending on the choice of flow law parameters, the strength of oceanic lithosphere would vary substantially, carrying different implications for the origin of plate tectonics on Earth. To reduce the flow law ambiguity, we suggest that it is important to establish a theoretical basis for estimating macroscopic stress in high-pressure experiments and also to better utilize marine geophysical observations. ©2017. American Geophysical Union. All Rights Reserved."
1,10.1002/sim.7385,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024919729&doi=10.1002%2fsim.7385&partnerID=40&md5=3ce95da4ecdba6194f15310c91f62749,"Motivated by a study measuring diabetes-related risk factors and complications, we postulate an extension to the standard formulation of joint models for longitudinal and survival outcomes, wherein the longitudinal outcome has a cumulative effect on the hazard of the event, weighted by recency. We focus on the relationship between the biomarker HbA1c and the development of sight threatening retinopathy, since the impact of the HbA1c marker on the risk of sight threatening retinopathy is expected to be cumulative, with the evolution of the HbA1c marker over time contributing to progressively greater damage to the vascular structure of the retina. Opting for a parametric approach, we propose the use of the normal and skewed normal probability density functions as weight functions, estimating the relevant parameters directly from the data. The use of the recency-weighted cumulative effect specification allows us to incorporate differences in the development of the longitudinal profile over time in the calculation of hazard ratios between subjects. The proposed functions provide us with parameters with clinically relevant interpretations while retaining a degree of flexibility. In addition, they also allow answering of important clinical questions regarding the relative importance of various segments of the biomarkers history in the estimation of the risk of the event. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1109/INAES.2017.8068537,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037155708&doi=10.1109%2fINAES.2017.8068537&partnerID=40&md5=322934369d145996ffaa365c99759031,"Hepatitis is one of the major health problems which can progress to chronic hepatitis and cancer. Currently, computer based diagnosis is commonly use among medical examination. The diagnosis has been examined by using the disease dataset as a reference to make the decisions. However, the dataset was incomplete because it contained many instances containing missing values. This situation can lead the results of the analysis to be biased. One method of handling missing values is Multiple Imputation. Hepatitis dataset has an arbitrary pattern of missing values. This pattern can be handled by using Markov Chain Monte Carlo (MCMC) and Fully Conditional Specification (FCS) as Multiple Imputation algorithms. The research conducted an experiment to compare combinations of Multiple Imputations algorithm and Principal Component Analysis (PCA) as instance selection. Instance selection applied to reduce data by selecting variables that contribute greatly to the dataset. The goal was to improve the accuracy of the analysis on data which had missing values with the arbitrary pattern. The results showed that FCS-PCA is the best performance with the higher accuracy (98.80%) and the lowest error rate (0.0116). © 2017 IEEE."
,10.1080/00949655.2017.1349131,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022050587&doi=10.1080%2f00949655.2017.1349131&partnerID=40&md5=cb784667c661a13ea0a09c31d5611320,"We introduce a multivariate heteroscedastic measurement error model for replications under scale mixtures of normal distribution. The model can provide a robust analysis and can be viewed as a generalization of multiple linear regression from both model structure and distribution assumption. An efficient method based on Markov Chain Monte Carlo is developed for parameter estimation. The deviance information criterion and the conditional predictive ordinates are used as model selection criteria. Simulation studies show robust inference behaviours of the model against both misspecification of distributions and outliers. We work out an illustrative example with a real data set on measurements of plant root decomposition. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/00949655.2017.1342824,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021411981&doi=10.1080%2f00949655.2017.1342824&partnerID=40&md5=e56521325855d9e655b28eebbdd1e932,"The max-stable process is a natural approach for modelling extrenal dependence in spatial data. However, the estimation is difficult due to the intractability of the full likelihoods. One approach that can be used to estimate the posterior distribution of the parameters of the max-stable process is to employ composite likelihoods in the Markov chain Monte Carlo (MCMC) samplers, possibly with adjustment of the credible intervals. In this paper, we investigate the performance of the composite likelihood-based MCMC samplers under various settings of the Gaussian extreme value process and the Brown–Resnick process. Based on our findings, some suggestions are made to facilitate the application of this estimator in real data. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
1,10.1080/00949655.2017.1349769,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022100759&doi=10.1080%2f00949655.2017.1349769&partnerID=40&md5=4f0a8773b4b5462e4ac95922bc7e8524,"A uniform shrinkage prior (USP) distribution on the unknown variance component of a random-effects model is known to produce good frequency properties. The USP has a parameter that determines the shape of its density function, but it has been neglected whether the USP can maintain such good frequency properties regardless of the choice for the shape parameter. We investigate which choice for the shape parameter of the USP produces Bayesian interval estimates of random effects that meet their nominal confidence levels better than several existent choices in the literature. Using univariate and multivariate Gaussian hierarchical models, we show that the USP can achieve its best frequency properties when its shape parameter makes the USP behave similarly to an improper flat prior distribution on the unknown variance component. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1016/j.vaccine.2017.09.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029580752&doi=10.1016%2fj.vaccine.2017.09.020&partnerID=40&md5=5bc627288e1c7addcad6576e59693d56,"Recently developed vaccines provide a new way of controlling rotavirus in sub-Saharan Africa. Models for the transmission dynamics of rotavirus are critical both for estimating current burden from imperfect surveillance and for assessing potential effects of vaccine intervention strategies. We examine rotavirus infection in the Maradi area in southern Niger using hospital surveillance data provided by Epicentre collected over two years. Additionally, a cluster survey of households in the region allows us to estimate the proportion of children with diarrhea who consulted at a health structure. Model fit and future projections are necessarily particular to a given model; thus, where there are competing models for the underlying epidemiology an ensemble approach can account for that uncertainty. We compare our results across several variants of Susceptible-Infectious-Recovered (SIR) compartmental models to quantify the impact of modeling assumptions on our estimates. Model-specific parameters are estimated by Bayesian inference using Markov chain Monte Carlo. We then use Bayesian model averaging to generate ensemble estimates of the current dynamics, including estimates of R0, the burden of infection in the region, as well as the impact of vaccination on both the short-term dynamics and the long-term reduction of rotavirus incidence under varying levels of coverage. The ensemble of models predicts that the current burden of severe rotavirus disease is 2.6–3.7% of the population each year and that a 2-dose vaccine schedule achieving 70% coverage could reduce burden by 39–42%. © 2017"
2,10.1186/s12872-017-0692-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031021427&doi=10.1186%2fs12872-017-0692-1&partnerID=40&md5=cb46f799b1cab3d10d43807270f21dba,"Background: Novel anticoagulations (NOACs) are increasingly prescribed for the prevention of stroke in premenopausal women with atrial fibrillation. Small studies suggest NOACs are associated with a higher risk of abnormal uterine bleeds than vitamin K antagonists (VKAs). Because there is no direct empirical evidence on the benefit/risk profile of rivaroxaban compared to VKAs in this subgroup, we synthesize available indirect evidence, estimate decision uncertainty on the treatments, and assess whether further research in premenopausal women is warranted. Methods: A Markov model with annual cycles and a lifetime horizon was developed comparing rivaroxaban (the most frequently prescribed NOAC in this population) and VKAs. Clinical event rates, associated quality adjusted life years, and health care costs were obtained from different sources and adjusted for gender, age, and history of stroke. A Monte Carlo simulation with 10,000 iterations was then performed for a hypothetical cohort of premenopausal women, estimated to be reflective of the population of premenopausal women with AF in The Netherlands. Results: In the simulation, rivaroxaban is the better treatment option for the prevention of ischemic strokes in premenopausal women in 61% of the iterations. Similarly, this is 98% for intracranial hemorrhages, 24% for major abnormal uterine bleeds, 1% for minor abnormal uterine bleeds, 9% for other major extracranial hemorrhages, and 23% for other minor extracranial hemorrhages. There is a 78% chance that rivaroxaban offers the most quality-adjusted life years. The expected value of perfect information in The Netherlands equals 122 quality-adjusted life years and 22 million Euros. Conclusions: There is a 22% risk that rivaroxaban offers a worse rather than a better benefit/risk profile than vitamin K antagonists in premenopausal women. Although rivaroxaban is preferred over VKAs in this population, further research is warranted, and should preferably take the shape of an internationally coordinated registry study including other NOACs. © 2017 The Author(s)."
,10.1145/3157737.3157750,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041106359&doi=10.1145%2f3157737.3157750&partnerID=40&md5=2bfe91e6432826a4c8b696c06e8744fd,"One of the most challenging parts of traffic modeling is how to model traffic behavior during traffic incidents. One of the possible approaches to this problem is to use historical data to identify typical incidents and use this knowledge to classify future time series. This classification can be utilized for example in prediction of traffic incident duration. This procedure requires solutions to several problems. The first problem is how can historical time series of traffic incidents be clustered, and these clusters can be parametrized. The second problem is how can time series of new ongoing incidents be classified to these existing clusters and how this classification can be utilized in the prediction of their length. The main aim of this article is to propose a solution to these problems. Methods utilized in addressing these problems are called Markov chains, Dynamic Time Warping, Bayesian classification and Monte Carlo simulation. © 2017 Association for Computing Machinery."
1,10.1021/acs.jpcb.7b08199,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031328582&doi=10.1021%2facs.jpcb.7b08199&partnerID=40&md5=de4b02eb6f4b0cc4f26c3ca633c3e032,"A wide range of cellular processes initiates upon recognizing and binding of proteins to specific DNA sites. Typically, the recognition process is incredibly fast owing to a complex mechanism that combines different 3D and 1D modes of translocation of the protein. While few studies performed on selected DNA topologies suggested that the DNA topology might alter the balance between these two modes and therefore the target search kinetics, its detailed role in the target search mechanism remains unclear. Here, we present a discrete-state stochastic approach that allows us to incorporate the topological information of DNA molecule explicitly and predict its role during the process when proteins search for their specific binding sites on DNA. Applying the theory to the closed loop and different supercoiled DNA topologies, we find that the target search efficiency of the protein is strongly influenced by the DNA topology. Furthermore, if the topology is such that it promotes juxtaposition of distant DNA sites, the number, position and relative distances between such juxtaposition sites play a crucial role in facilitating the search process by providing additional routes to approach the target site. Our predictions are validated through extensive Monte Carlo simulations. © 2017 American Chemical Society."
9,10.3847/1538-4357/aa8bb4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031930707&doi=10.3847%2f1538-4357%2faa8bb4&partnerID=40&md5=32fe81cae606b44dde01f44092114fa6,"Current and upcoming radio interferometric experiments are aiming to make a statistical characterization of the high-redshift 21 cm fluctuation signal spanning the hydrogen reionization and X-ray heating epochs of the universe. However, connecting 21 cm statistics to the underlying physical parameters is complicated by the theoretical challenge of modeling the relevant physics at computational speeds quick enough to enable exploration of the high-dimensional and weakly constrained parameter space. In this work, we use machine learning algorithms to build a fast emulator that can accurately mimic an expensive simulation of the 21 cm signal across a wide parameter space. We embed our emulator within a Markov Chain Monte Carlo framework in order to perform Bayesian parameter constraints over a large number of model parameters, including those that govern the Epoch of Reionization, the Epoch of X-ray Heating, and cosmology. As a worked example, we use our emulator to present an updated parameter constraint forecast for the Hydrogen Epoch of Reionization Array experiment, showing that its characterization of a fiducial 21 cm power spectrum will considerably narrow the allowed parameter space of reionization and heating parameters, and could help strengthen Planck's constraints on σg. We provide both our generalized emulator code and its implementation specifically for 21 cm parameter constraints as publicly available software. © 2017. The American Astronomical Society. All rights reserved."
,10.1109/RADAR.2016.8059259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034662060&doi=10.1109%2fRADAR.2016.8059259&partnerID=40&md5=1aeaaebd2be55c9ebe3766a3525fb0c8,"Group tracking based on Markov Chain Monte Carlo particle filter (MCMC-PF) algorithm has high accuracy of individual target tracking and group tracking, but the main problem of MCMC-PF algorithm is calculation burden, that's because it considers all kinds of hypotheses between state vector and measurement vector in the observation model. Therefore, in this paper, we originally use the results of data association as a prior to do the hypotheses management, which will reduce the variety of feasible hypotheses. We also raise a new method for the target state proposal based on group information and Kalman framework. In the end, simulation results demonstrate that the new algorithms can track the individual target and infer the correct group structure with less time consumption than previous work. © 2016 IEEE."
13,10.1103/PhysRevB.96.161102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037677955&doi=10.1103%2fPhysRevB.96.161102&partnerID=40&md5=f5d693804c1aea1ec5e32faa9ed752f0,"The recently introduced self-learning Monte Carlo method is a general-purpose numerical method that speeds up Monte Carlo simulations by training an effective model to propose uncorrelated configurations in the Markov chain. We implement this method in the framework of a continuous-time Monte Carlo method with an auxiliary field in quantum impurity models. We introduce and train a diagram generating function (DGF) to model the probability distribution of auxiliary field configurations in continuous imaginary time, at all orders of diagrammatic expansion. By using DGF to propose global moves in configuration space, we show that the self-learning continuous-time Monte Carlo method can significantly reduce the computational complexity of the simulation. © 2017 American Physical Society."
,10.1016/j.bpj.2017.08.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030656687&doi=10.1016%2fj.bpj.2017.08.015&partnerID=40&md5=e256cb764b1ab6f80c7bc1b412513267,"Rotary sequential hydrolysis of the metabolic machine F1-ATPase is a prominent manifestation of high coordination among multiple chemical sites in ring-shaped molecular machines, and it is also functionally essential for F1 to tightly couple chemical reactions and central γ-shaft rotation. High-speed AFM experiments have identified that sequential hydrolysis is maintained in the F1 stator ring even in the absence of the γ-rotor. To explore the origins of intrinsic sequential performance, we computationally investigated essential inter-subunit couplings on the hexameric ring of mitochondrial and bacterial F1. We first reproduced in stochastic Monte Carlo simulations the experimentally determined sequential hydrolysis schemes by kinetically imposing inter-subunit couplings and following subsequent tri-site ATP hydrolysis cycles on the F1 ring. We found that the key couplings to support the sequential hydrolysis are those that accelerate neighbor-site ADP and Pi release upon a certain ATP binding or hydrolysis reaction. The kinetically identified couplings were then examined in atomistic molecular dynamics simulations at a coarse-grained level to reveal the underlying structural mechanisms. To do that, we enforced targeted conformational changes of ATP binding or hydrolysis to one chemical site on the F1 ring and monitored the ensuing conformational responses of the neighboring sites using structure-based simulations. Notably, we found asymmetrical neighbor-site opening that facilitates ADP release upon enforced ATP binding. We also captured a complete charge-hopping process of the Pi release subsequent to enforced ATP hydrolysis in the neighbor site, confirming recent single-molecule analyses with regard to the role of ATP hydrolysis in F1. Our studies therefore elucidate both the coordinated chemical kinetics and structural dynamics mechanisms underpinning the sequential operation of the F1 ring. © 2017 Biophysical Society"
,10.1080/00401706.2017.1317290,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026548170&doi=10.1080%2f00401706.2017.1317290&partnerID=40&md5=6198aaff48dd816f9a33e8227546ae81,"The spatially varying coefficient process model is a nonstationary approach to explaining spatial heterogen-eity by allowing coefficients to vary across space. In this article, we develop a methodology for generalizing this model to accommodate geographically hierarchical data. This article considers two-level hierarchical structures and allow for the coefficients of both low-level and high-level units to vary over space. We assume that the spatially varying low-level coefficients follow the multivariate Gaussian process, and the spatially varying high-level coefficients follow the multivariate simultaneous autoregressive model that we develop by extending the standard simultaneous autoregressive model to incorporate multivariate data. We apply the proposed model to transaction data of houses sold in 2014 in a part of the city of Los Angeles. The results show that the proposed model predicts housing prices and fits the data effectively. © 2017 American Statistical Association and the American Society for Quality."
1,10.1109/INFOCOM.2017.8057071,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034040095&doi=10.1109%2fINFOCOM.2017.8057071&partnerID=40&md5=acce1e721273fd209f47bcd12ba492f5,"We study how the so-called Rao-Blackwellization, which is a variance reduction technique via 'conditioning' for Monte Carlo methods, can be judiciously applied for graph sampling through neighborhood exploration. Despite its popularity for Monte Carlo methods, it is little known for Markov chain Monte Carlo methods and has never been discussed for random walk-based graph sampling. We first propose two forms of Rao-Blackwellization that can be used as a swap-in replacement for virtually all (reversible) random-walk graph sampling methods, and prove that the 'Rao-Blackwellized' estimators reduce the (asymptotic) variances of their original estimators yet maintain their inherent unbiasedness. The variance reduction can translate into lowering the number of samples required to achieve a desired sampling accuracy. However, the sampling cost for neighborhood exploration, if required, may outweigh such improvement, even leading to higher total amortized cost. Considering this, we provide a generalization of Rao-Blackwellization, which allows one to choose a suitable extent of obtaining Rao-Blackwellized samples in order to achieve a right balance between sampling cost and accuracy. We finally provide simulation results via real-world datasets that confirm our theoretical findings. © 2017 IEEE."
1,10.1080/01621459.2016.1222291,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024502621&doi=10.1080%2f01621459.2016.1222291&partnerID=40&md5=1dd9e913b60c0c30c01f57fe135a628c,"We present an offline, iterated particle filter to facilitate statistical inference in general state space hidden Markov models. Given a model and a sequence of observations, the associated marginal likelihood L is central to likelihood-based inference for unknown statistical parameters. We define a class of “twisted” models: each member is specified by a sequence of positive functions ψ and has an associated ψ-auxiliary particle filter that provides unbiased estimates of L. We identify a sequence ψ* that is optimal in the sense that the ψ*-auxiliary particle filter’s estimate of L has zero variance. In practical applications, ψ* is unknown so the ψ*-auxiliary particle filter cannot straightforwardly be implemented. We use an iterative scheme to approximate ψ* and demonstrate empirically that the resulting iterated auxiliary particle filter significantly outperforms the bootstrap particle filter in challenging settings. Applications include parameter estimation using a particle Markov chain Monte Carlo algorithm. © 2017 The Author(s). Published with license by Taylor & Francis © 2017, © Pieralberto Guarniero, Adam M. Johansen, and Anthony Lee."
,10.1080/10618600.2017.1336444,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031411823&doi=10.1080%2f10618600.2017.1336444&partnerID=40&md5=ada6859caebd43168a73730075f59fe2,"We present a Bayesian framework for registration of real-valued functional data. At the core of our approach is a series of transformations of the data and functional parameters, developed under a differential geometric framework. We aim to avoid discretization of functional objects for as long as possible, thus minimizing the potential pitfalls associated with high-dimensional Bayesian inference. Approximate draws from the posterior distribution are obtained using a novel Markov chain Monte Carlo (MCMC) algorithm, which is well suited for estimation of functions. We illustrate our approach via pairwise and multiple functional data registration, using both simulated and real datasets. Supplementary material for this article is available online. © 2017 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
,10.1080/01621459.2016.1222288,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024504456&doi=10.1080%2f01621459.2016.1222288&partnerID=40&md5=9846f4e978700b939e9208a0488eeee5,"We introduce the Hamming ball sampler, a novel Markov chain Monte Carlo algorithm, for efficient inference in statistical models involving high-dimensional discrete state spaces. The sampling scheme uses an auxiliary variable construction that adaptively truncates the model space allowing iterative exploration of the full model space. The approach generalizes conventional Gibbs sampling schemes for discrete spaces and provides an intuitive means for user-controlled balance between statistical efficiency and computational tractability. We illustrate the generic utility of our sampling algorithm through application to a range of statistical models. Supplementary materials for this article are available online. © 2017 The Author(s). Published with license by Taylor & Francis © 2017, © Michalis K. Titsias and Christopher Yau."
,10.1080/15598608.2017.1305922,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018505121&doi=10.1080%2f15598608.2017.1305922&partnerID=40&md5=46e453252e8914bab5e3d5c4f52d5a4c,"Circular time series have received relatively little attention in statistics, and modeling complex circular time series using the state space approach is nonexistent in the literature. In this article we introduce a flexible Bayesian nonparametric approach to state-space modeling of observed circular time series where even the latent states are circular random variables. Crucially, we assume that the forms of the observational and evolutionary functions, both of which are circular in nature, are unknown and time-varying. We model these unknown circular functions by appropriate wrapped Gaussian processes having desirable properties. We develop an effective Markov-chain Monte Carlo strategy for implementing our Bayesian model by judiciously combining Gibbs sampling and Metropolis–Hastings methods. Validation of our ideas with a simulation study and two real bivariate circular time-series data sets, where we assume one of the variables to be unobserved, revealed very encouraging performance of our model and methods. We finally analyze a data set consisting of directions of whale migration, considering the unobserved ocean current direction as the latent circular process of interest. The results that we obtain are encouraging, and the posterior predictive distribution of the observed process correctly predicts the observed whale movement. © 2017 Grace Scientific Publishing, LLC."
,10.1080/10618600.2017.1322091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031090051&doi=10.1080%2f10618600.2017.1322091&partnerID=40&md5=ad1c6c7db93da9e651916005f15bf4c5,"We discuss efficient Bayesian estimation of dynamic covariance matrices in multivariate time series through a factor stochastic volatility model. In particular, we propose two interweaving strategies to substantially accelerate convergence and mixing of standard MCMC approaches. Similar to marginal data augmentation techniques, the proposed acceleration procedures exploit nonidentifiability issues which frequently arise in factor models. Our new interweaving strategies are easy to implement and come at almost no extra computational cost; nevertheless, they can boost estimation efficiency by several orders of magnitude as is shown in extensive simulation studies. To conclude, the application of our algorithm to a 26-dimensional exchange rate dataset illustrates the superior performance of the new approach for real-world data. Supplementary materials for this article are available online. © 2017 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
2,10.1080/16843703.2017.1304041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016126453&doi=10.1080%2f16843703.2017.1304041&partnerID=40&md5=b9b3b8355a32cc430e6989ffe9a89638,"Conventionally, a standard control chart implements fixed sample size in process monitoring. In this study, we propose an optimal statistical design for the variable sample size (VSS) multivariate exponentially weighted moving average (MEWMA) chart based on the median run-length (MRL). The proposal is based on the fact that the percentiles of the run-length distribution, especially the MRL, are more reflective and reliable for performance evaluation with respect to a skewed run-length distribution. The MRL for the VSS MEWMA chart computed using the Markov chain approach is verified with Monte Carlo simulation. For benchmarking purposes, the performance of the VSS MEWMA chart is compared against the standard MEWMA chart and the synthetic T2 chart, in terms of the MRL. The numerical results show that the VSS MEWMA chart performs better than the standard MEWMA chart and the synthetic T2 chart, in detecting shifts in the process mean vector. Finally, an application is provided as an illustration for the implementation of the VSS MEWMA chart based on the MRL. © 2017 International Chinese Association of Quantitative Management."
1,10.1109/INFOCOM.2017.8057077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025166811&doi=10.1109%2fINFOCOM.2017.8057077&partnerID=40&md5=33d905c76528c20c06708feef9cada67,"Device-to-device (D2D) communications for Long Term Evolution (LTE) networks relies on a discovery process to enable User Equipment (UE) to determine which D2D applications and services are supported by neighboring UEs. This is especially important for groups of UEs that operate outside the coverage area of any base station. The amount of time required for discovery information to reach every UE in a group depends on the number of UEs in the group and the dimensions of the discovery resource pool associated with the Physical Sidelink Discovery Channel (PSDCH); an additional factor is the half-duplex property of current UEs. In this paper, we use a Markov chain to characterize the performance of Mode 2 direct discovery. The resulting analytical model gives the distribution of the time for a UE to discover all other UEs in its group. We validate the model using Monte Carlo and network simulations. © 2017 IEEE."
,10.1080/10618600.2017.1328365,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030860564&doi=10.1080%2f10618600.2017.1328365&partnerID=40&md5=672bd4a7f9cf61f1607071dafb093f8a,"Stochastic epidemic models describe the dynamics of an epidemic as a disease spreads through a population. Typically, only a fraction of cases are observed at a set of discrete times. The absence of complete information about the time evolution of an epidemic gives rise to a complicated latent variable problem in which the state space size of the epidemic grows large as the population size increases. This makes analytically integrating over the missing data infeasible for populations of even moderate size. We present a data augmentation Markov chain Monte Carlo (MCMC) framework for Bayesian estimation of stochastic epidemic model parameters, in which measurements are augmented with subject-level disease histories. In our MCMC algorithm, we propose each new subject-level path, conditional on the data, using a time-inhomogenous continuous-time Markov process with rates determined by the infection histories of other individuals. The method is general, and may be applied to a broad class of epidemic models with only minimal modifications to the model dynamics and/or emission distribution. We present our algorithm in the context of multiple stochastic epidemic models in which the data are binomially sampled prevalence counts, and apply our method to data from an outbreak of influenza in a British boarding school. Supplementary material for this article is available online. © 2017 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
,10.1080/01621459.2017.1281811,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041103280&doi=10.1080%2f01621459.2017.1281811&partnerID=40&md5=0bd878ef21c1942c18ec550f98d45632,"This article considers the problem of analyzing associations between power spectra of multiple time series and cross-sectional outcomes when data are observed from multiple subjects. The motivating application comes from sleep medicine, where researchers are able to noninvasively record physiological time series signals during sleep. The frequency patterns of these signals, which can be quantified through the power spectrum, contain interpretable information about biological processes. An important problem in sleep research is drawing connections between power spectra of time series signals and clinical characteristics; these connections are key to understanding biological pathways through which sleep affects, and can be treated to improve, health. Such analyses are challenging as they must overcome the complicated structure of a power spectrum from multiple time series as a complex positive-definite matrix-valued function. This article proposes a new approach to such analyses based on a tensor-product spline model of Cholesky components of outcome-dependent power spectra. The approach flexibly models power spectra as nonparametric functions of frequency and outcome while preserving geometric constraints. Formulated in a fully Bayesian framework, a Whittle likelihood-based Markov chain Monte Carlo (MCMC) algorithm is developed for automated model fitting and for conducting inference on associations between outcomes and spectral measures. The method is used to analyze data from a study of sleep in older adults and uncovers new insights into how stress and arousal are connected to the amount of time one spends in bed. Supplementary materials for this article are available online. © 2017 American Statistical Association."
,10.1080/15598608.2017.1281180,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012882020&doi=10.1080%2f15598608.2017.1281180&partnerID=40&md5=5a06052261a24f8acad2535bd24c3787,"Nonparametric regression is an important tool for exploring the unknown relationship between a response variable and a set of explanatory variables also known as regressors. This article introduces the associated discrete kernel for multivariate nonparametric count regression estimation. We propose a Bayesian approach based upon likelihood cross-validation and a Monte Carlo Markov chain (MCMC) method for deriving the global optimal bandwidths. Through simulation and real count data, we point out the performance of binomial and triangular discrete kernels. A comparative study of the Bayesian approach and cross-validation technique is also presented. © 2017 Grace Scientific Publishing, LLC."
1,10.1002/mp.12508,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029544689&doi=10.1002%2fmp.12508&partnerID=40&md5=048fbc2186b0cb40c31cedc69d26c3db,"Purpose: High-dose-rate irradiation with 6 MV linac x rays is a wide-spread means to treat cancer tissue in radiotherapy. The treatment planning relies on a mathematical description of surviving fraction (SF), such as the linear-quadratic model (LQM) formula. However, even in the case of high-dose-rate treatment, the repair kinetics of DNA damage during dose-delivery time plays a function in predicting the dose-SF relation. This may call the SF model selection into question when considering the dose-delivery time or dose-rate effects (DREs) in radiotherapy and in vitro cell experiments. In this study, we demonstrate the importance of dose-delivery time at high-dose-rate irradiations used in radiotherapy by means of Bayesian estimation. Methods: To evaluate the model selection for SF, three types of models, the LQM and two microdosimetric-kinetic models with and without DREs (MKMDR and MKM) were applied to describe in vitroSF data (our work and references). The parameters in each model were evaluated by a Markov chain Monte Carlo (MCMC) simulation. Results: The MCMC analysis shows that the cell survival curve by the MKMDR fits the experimental data the best in terms of the deviance information criterion (DIC). In the fractionated regimen with 30 fractions to a total dose of 60 Gy, the final cell survival estimated by the MKMDR was higher than that by the LQM. This suggests that additional fractions are required for attaining the total dose equivalent to yield the same effect as the conventional regimen using the LQM in fractionated radiotherapy. Conclusions: Damage repair during dose-delivery time plays a key role in precisely estimating cell survival even at a high dose rate in radiotherapy. Consequently, it was suggested that the cell-killing model without repair factor during a short dose-delivery time may overestimate actual cell killing in fractionated radiotherapy. © 2017 American Association of Physicists in Medicine."
4,10.1016/j.cpc.2017.05.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021283147&doi=10.1016%2fj.cpc.2017.05.027&partnerID=40&md5=93be0cb991a148b392ac67139aee2ed3,"The analysis tool and software package Fast-NPS can be used to analyse smFRET data to obtain quantitative structural information about macromolecules in their natural environment. In the algorithm a Bayesian model gives rise to a multivariate probability distribution describing the uncertainty of the structure determination. Since Fast-NPS aims to be an easy-to-use general-purpose analysis tool for a large variety of smFRET networks, we established an MCMC based sampling engine that approximates the target distribution and requires no parameter specification by the user at all. For an efficient local exploration we automatically adapt the multivariate proposal kernel according to the shape of the target distribution. In order to handle multimodality, the sampler is equipped with a parallel tempering scheme that is fully adaptive with respect to temperature spacing and number of chains. Since the molecular surrounding of a dye molecule affects its spatial mobility and thus the smFRET efficiency, we introduce dye models which can be selected for every dye molecule individually. These models allow the user to represent the smFRET network in great detail leading to an increased localisation precision. Finally, a tool to validate the chosen model combination is provided. Programme summary Programme Title: Fast-NPS Programme Files doi: http://dx.doi.org/10.17632/7ztzj63r68.1 Licencing provisions: Apache-2.0 Programming language: GUI in MATLAB (The MathWorks) and the core sampling engine in C++ Nature of problem: Sampling of highly diverse multivariate probability distributions in order to solve for macromolecular structures from smFRET data. Solution method: MCMC algorithm with fully adaptive proposal kernel and parallel tempering scheme. © 2017 Elsevier B.V."
2,10.1371/journal.pone.0186391,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031756592&doi=10.1371%2fjournal.pone.0186391&partnerID=40&md5=95e2de423b715030477ae322501028d9,"The progressive character of tooth formation records aspects of mammalian life history, diet, seasonal behavior and climate. Tooth mineralization occurs in two stages: secretion and maturation, which overlap to some degree. Despite decades of study, the spatial and temporal pattern of elemental incorporation during enamel mineralization remains poorly characterized. Here we use synchrotron X-ray microtomography and Markov Chain Monte Carlo sampling to estimate mineralization patterns from an ontogenetic series of sheep molars (n = 45 M1s, 18 M2s). We adopt a Bayesian approach that posits a general pattern of maturation estimated from individual- and population-level mineral density variation over time. This approach converts static images of mineral density into a dynamic model of mineralization, and demonstrates that enamel secretion and maturation waves advance at nonlinear rates with distinct geometries. While enamel secretion is ordered, maturation geometry varies within a population and appears to be driven by diffusive processes. Our model yields concrete expectations for the integration of physiological and environmental signals, which is of particular significance for paleoseasonality research. This study also provides an avenue for characterizing mineralization patterns in other taxa. Our synchrotron imaging data and model are available for application to multiple disciplines, including health, material science, and paleontological research. © 2017 Green et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
,10.16383/j.aas.2017.c160200,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035008344&doi=10.16383%2fj.aas.2017.c160200&partnerID=40&md5=0bebfb41c1964f61ee9529a04d0f9442,"When learning from imbalanced datasets, the traditional fuzzy systems have a low rate of identification over the minority class. Firstly, in the antecedent parameter learning stage, a new clustering method, called Bayesian fuzzy clustering based on competitive learning (BFCCL), is proposed to partition the input space for the antecedents of if-then rules. BFCCL considers the repulsed force of clustering prototypes between different classes, and uses an alternating iterative strategy to obtain the optimal model parameters by Markov chain Monte Carlo method. Secondly, in the consequent parameter learning stage, based on the maximum separation strategy and by keeping the distance between the minority class and the classification hyperplane larger than the distance between the majority class and the hyperplane, the method can effectively correct the skewness of the classification hyperplane. Based on the above ideas, a zero-order-Takagi-Sugeno-Kang fuzzy system for imbalanced data classification (0-TSK-IDC) is proposed. Experimental results on artificial and real-world medicine datasets illustrate the effectiveness of 0-TSK-IDC on both minority and majority classes in imbalanced data classification, as well as its good robustness and interpretability. Copyright © 2017 Acta Automatica Sinica. All rights reserved."
2,10.1107/S1600576717012742,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030572405&doi=10.1107%2fS1600576717012742&partnerID=40&md5=120a98509a37977f644a5dc4782e23d6,"Laterally periodic nanostructures have been investigated with grazing-incidence small-angle X-ray scattering (GISAXS) by using the diffraction patterns to reconstruct the surface shape. To model visible light scattering, rigorous calculations of the near and far field by numerical solution of Maxwell's equations with a finite-element method are well established. The application of this technique to X-rays is still challenging, owing to the discrepancy between the incident wavelength and the finite-element size. This drawback vanishes for GISAXS because of the small angles of incidence, the conical scattering geometry and the periodicity of the surface structures, which allows a rigorous computation of the diffraction efficiencies with sufficient numerical precision. To develop metrology tools based on GISAXS, lamellar gratings with line widths down to 55anm were produced by state-of-the-art electron-beam lithography and then etched into silicon. The high surface sensitivity of GISAXS in conjunction with a Maxwell solver allows the detailed reconstruction of the grating line shape for thick non-homogeneous substrates as well. The reconstructed geometric line-shape models are statistically validated by applying a Markov chain Monte Carlo sampling technique which reveals that GISAXS is able to reconstruct critical parameters like the widths of the lines with sub-nanometre uncertainty.The shallow incidence angles in grazing-incidence small-angle X-ray scattering (GISAXS) allow the use of a rigorous Maxwell solver in combination with the finite-element method for the reconstruction of nanometre-sized periodic surface structures. © 2017 International Union of Crystallography."
1,10.1007/s00170-017-0567-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019726364&doi=10.1007%2fs00170-017-0567-8&partnerID=40&md5=9d2af8826731e7b5a6c88ac47ac143ce,"Predicting process forces in micromilling is difficult due to complex interaction between the cutting edge and the work material, size effect, and process dynamics. This study describes the application of Bayesian inference to identify force coefficients in the micromilling process. The Metropolis-Hastings (MH) algorithm Markov chain Monte Carlo (MCMC) approach has been used to identify probability distributions of cutting, edge, and ploughing force coefficients based on experimental measurements and a mechanistic model of micromilling. The Bayesian inference scheme allows for predicting the upper and lower limits of micromilling forces, providing useful information about stability boundary calculations and robust process optimization. In the first part of the paper, micromilling experiments are performed to investigate the influence of micromilling process parameters on machining forces, tool edge condition, and surface texture. Under the experimental conditions used in this study, built-up edge formation is observed to have a significant influence on the process outputs in micromilling of titanium alloy Ti6Al4V. In the second part, Bayesian inference was explained in detail and applied to model micromilling force prediction. The force predictions are validated with the experimental measurements. The paper concludes with a discussion of the effectiveness of employing Bayesian inference in micromilling force modeling considering special machining cases. © 2017, Springer-Verlag London."
,10.1016/j.jmarsys.2017.05.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019351720&doi=10.1016%2fj.jmarsys.2017.05.004&partnerID=40&md5=8e4668c0f39f4c0df235a3d62206b4df,"Methods for extracting empirically and theoretically sound parameter values are urgently needed in aquatic ecosystem modelling to describe key flows and their variation in the system. Here, we compare three Bayesian formulations for mechanistic model parameterization that differ in their assumptions about the variation in parameter values between various datasets: 1) global analysis - no variation, 2) separate analysis - independent variation and 3) hierarchical analysis - variation arising from a shared distribution defined by hyperparameters. We tested these methods, using computer-generated and empirical data, coupled with simplified and reasonably realistic plankton food web models, respectively. While all methods were adequate, the simulated example demonstrated that a well-designed hierarchical analysis can result in the most accurate and precise parameter estimates and predictions, due to its ability to combine information across datasets. However, our results also highlighted sensitivity to hyperparameter prior distributions as an important caveat of hierarchical analysis. In the more complex empirical example, hierarchical analysis was able to combine precise identification of parameter values with reasonably good predictive performance, although the ranking of the methods was less straightforward. We conclude that hierarchical Bayesian analysis is a promising tool for identifying key ecosystem-functioning parameters and their variation from empirical datasets. © 2017 Elsevier B.V."
2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032947988&partnerID=40&md5=9a3535d9a75d8a04f890d44b2a3a4d8f,"This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a data set is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Monte Carlo sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks. © 2017 Leonard Hasenclever, Stefan Webb, Thibaut Lienart, Sebastian Vollmer, Balaji Lakshminarayanan, Charles Blundell and Yee Whye Teh."
,10.3390/e19100561,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031898257&doi=10.3390%2fe19100561&partnerID=40&md5=d68ad7f914e4ea33dafa6ae1490e9e8d,"Markov chain Monte Carlo sampling propagators, including numerical integrators for stochastic dynamics, are central to the calculation of thermodynamic quantities and determination of structure for molecular systems. Efficiency is paramount, and to a great extent, this is determined by the integrated autocorrelation time (IAcT). This quantity varies depending on the observable that is being estimated. It is suggested that it is the maximum of the IAcT over all observables that is the relevant metric. Reviewed here is a method for estimating this quantity. For reversible propagators (which are those that satisfy detailed balance), the maximum IAcT is determined by the spectral gap in the forward transfer operator, but for irreversible propagators, the maximum IAcT can be far less than or greater than what might be inferred from the spectral gap. This is consistent with recent theoretical results (not to mention past practical experience) suggesting that irreversible propagators generally perform better if not much better than reversible ones. Typical irreversible propagators have a parameter controlling the mix of ballistic and diffusive movement. To gain insight into the effect of the damping parameter for Langevin dynamics, its optimal value is obtained here for a multidimensional quadratic potential energy function. © 2017 by the authors."
1,10.1016/j.csda.2017.04.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019005042&doi=10.1016%2fj.csda.2017.04.005&partnerID=40&md5=26f3664eb3d3db8faa088b6c063ea986,"The application of Bayesian methods often requires Metropolis–Hastings or related algorithms to sample from an intractable posterior distribution. In especially challenging cases, such as with strongly correlated parameters or multimodal posteriors, exotic forms of Metropolis–Hastings are preferred for generating samples within a reasonable time. These algorithms require nontrivial and often prohibitive tuning, with little or no performance guarantees. In light of this difficulty, a new, parallelizable algorithm called weighted particle tempering is introduced. Weighted particle tempering is easily tuned and suitable for a broad range of applications. The algorithm works by running multiple random walk Metropolis chains directed at a tempered version of the target distribution, weighting the iterates and resampling. The algorithm's performance monotonically improves with more of these underlying chains, a feature that simplifies tuning. Through the use of simulation studies, weighted particle tempering is shown to outperform two similar methods: parallel tempering and parallel hierarchical sampling. In addition, two case studies are explored: breast cancer classification and graphical models for financial data. © 2017 Elsevier B.V."
2,10.1109/TIFS.2017.2705629,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021666887&doi=10.1109%2fTIFS.2017.2705629&partnerID=40&md5=a04fa3d123c66d3c0ec5c393636a9006,"We analyze sets of intrusion detection records observed on the networks of several large, nonresidential organizations protected by a form of intrusion detection and prevention service. Our analyses reveal that the process of intrusion detection in these networks exhibits a significant degree of burstiness as well as strong memory, with burstiness and memory properties that are comparable to those of natural processes driven by threshold effects, but different from bursty human activities. We explore time-series models of these observable network security incidents based on partially observed data using a hidden Markov model with restricted hidden states, which we fit using Markov Chain Monte Carlo techniques. We examine the output of the fitted model with respect to its statistical properties and demonstrate that the model adequately accounts for intrinsic ""bursting"" within observed network incidents as a result of alternation between two or more stochastic processes. While our analysis does not lead directly to new detection capabilities, the practical implications of gaining better understanding of the observed burstiness are significant, and include opportunities for quantifying a network's risks and defensive efforts. © 2017 IEEE."
2,10.1145/3132704,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031324940&doi=10.1145%2f3132704&partnerID=40&md5=906f59676758f9db027ae32b6d8737af,"We study Markov Chain Monte Carlo (MCMC) methods operating in primary sample space and their interactions with multiple sampling techniques.We observe that incorporating the sampling technique into the state of the Markov Chain, as done in Multiplexed Metropolis Light Transport,impedes the ability of the chain to properly explore the path space,as transitions between sampling techniques lead to disruptive alterations of path samples. To address this issue, we reformulate Multiplexed MLT in the Reversible Jump MCMC framework (RJMCMC) and introduce inverse sampling techniques that turn light paths into the random numbers that would produce them. This allows us to formulate a novel perturbation that can locally transition between sampling techniques without changing the geometry of the path, and we derive the correct acceptance probability using RJMCMC. We investigate how to generalize this concept to noninvertible sampling techniques commonly found in practice, and introduce probabilistic inverses that extend our perturbation to cover most sampling methods found in light transport simulations. Our theory reconciles the inverses with RJMCMC yielding an unbiased algorithm, which we call Reversible Jump MLT. We verify the correctness of our implementation in canonical and practical scenarios and demonstrate improved temporal coherence,decrease in structured artifacts, and faster convergence on a wide variety of scenes. © 2017 ACM."
5,10.1177/0962280215596186,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031687121&doi=10.1177%2f0962280215596186&partnerID=40&md5=5d813932d4ff5ac6c4cbff8b6281e329,"In this paper, we extend the spatially explicit survival model for small area cancer data by allowing dependency between space and time and using accelerated failure time models. Spatial dependency is modeled directly in the definition of the survival, density, and hazard functions. The models are developed in the context of county level aggregated data. Two cases are considered: the first assumes that the spatial and temporal distributions are independent; the second allows for dependency between the spatial and temporal components. We apply the models to prostate cancer data from the Louisiana SEER cancer registry. © The Author(s) 2017."
2,10.1162/NECO_a_01000,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029290123&doi=10.1162%2fNECO_a_01000&partnerID=40&md5=5c1bc66bb377465f72a88c0f911d8912,"Cluster analysis of functional magnetic resonance imaging (fMRI) data is often performed using gaussian mixture models, but when the time series are standardized such that the data reside on a hypersphere, this modeling assumption is questionable. The consequences of ignoring the underlying sphericalmanifold are rarely analyzed, in part due to the computational challenges imposed by directional statistics. In this letter, we discuss a Bayesian von Mises-Fisher (vMF) mixture model for data on the unit hypersphere and present an efficient inference procedure based on collapsed Markov chain Monte Carlo sampling. Comparing the vMF and gaussian mixture models on synthetic data, we demonstrate that the vMF model has a slight advantage inferring the true underlying clustering when compared to gaussian-based models on data generated from both a mixture of vMFs and a mixture of gaussians subsequently normalized. Thus, when performing model selection, the two models are not in agreement. Analyzing multisubjectwhole brain resting-state fMRI data from healthy adult subjects, we find that the vMF mixture model is considerably more reliable than the gaussian mixture model when comparing solutions across models trained on different groups of subjects, and again we find that the two models disagree on the optimal number of components. The analysis indicates that the fMRI data support more than a thousand clusters, and we confirm this is not a result of overfitting by demonstrating better prediction on data from held-out subjects. Our results highlight the utility of using directional statistics to model standardized fMRI data and demonstrate that whole brain segmentation of fMRI data requires a very large number of functional units in order to adequately account for the discernible statistical patterns in the data. © 2017 Massachusetts Institute of Technology."
6,10.1016/j.ress.2016.11.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009210567&doi=10.1016%2fj.ress.2016.11.020&partnerID=40&md5=ceebbf7b38feed209615a6b6696ee7da,"We consider a three-state continuous-time semi-Markov process with Weibull-distributed transition times to model the degradation mechanism of an industrial equipment. To build this model, an original combination of techniques is proposed for building a semi-Markov degradation model based on expert knowledge and few field data within the Bayesian statistical framework. The issues addressed are: i) the prior elicitation of the model parameters values from experts, avoiding possible information commitment; ii) the development of a Markov-Chain Monte Carlo algorithm for sampling from the posterior distribution; iii) the posterior inference of the model parameters values and, on this basis, the estimation of the time-dependent state probabilities and the prediction of the equipment remaining useful life. The developed Bayesian model offers the possibility of updating the system reliability estimation every time a new evidence is gathered. The application of the modeling framework is illustrated by way of a real industrial case study concerning the degradation of diaphragms installed in a production line of a biopharmaceutical industry. © 2016 Elsevier Ltd"
,10.1111/rssa.12321,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032957010&doi=10.1111%2frssa.12321&partnerID=40&md5=7eaa81a14655956539da344dd881eaf1,"Area level models, such as the Fay–Herriot model, aim to improve direct survey estimates for small areas by borrowing strength from related covariates and from direct estimates across all areas. In their multivariate form, where related population characteristics are jointly modelled, area level models allow for inference about functions of two or more characteristics and may exploit dependence between the response variables to improve small area predictions. When model covariates are observed with random error, such as those drawn from another survey, it is important to account for this error in the modelling. We present a Bayesian analysis of a multivariate Fay–Herriot model with functional measurement error, allowing for both joint modelling of related characteristics and accounting for random observation error in some of the covariates. We apply it to modelling 2010 and 2011 poverty rates of school-aged children for US counties, for predicting 2011 poverty rates and the 2010–2011 changes. For this application, the measurement error model results in great improvements in prediction when compared with the direct estimates, and ignoring the measurement error results in uncertainty estimates that are misleading. We propose a computational approach to implementing this model via an independence chain Markov chain Monte Carlo algorithm and prove the propriety of the posterior distribution under a class of non-informative priors. © 2017 The Authors, Royal Statistical Society. This article has been contributed to by US Government employees and their work is in the public domain in the USA."
,10.3390/e19100524,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031914866&doi=10.3390%2fe19100524&partnerID=40&md5=855814e7ef1cba94726e593f739a1e5a,"The Information Geometry of extended exponential families has received much recent attention in a variety of important applications, notably categorical data analysis, graphical modelling and, more specifically, log-linear modelling. The essential geometry here comes from the closure of an exponential family in a high-dimensional simplex. In parallel, there has been a great deal of interest in the purely Fisher Riemannian structure of (extended) exponential families, most especially in the Markov chain Monte Carlo literature. These parallel developments raise challenges, addressed here, at a variety of levels: both theoretical and practical-relatedly, conceptual and methodological. Centrally to this endeavour, this paper makes explicit the underlying geometry of these two areas via an analysis of the limiting behaviour of the fundamental geodesics of Information Geometry, these being Amari's (+1) and (0)-geodesics, respectively. Overall, a substantially more complete account of the Information Geometry of extended exponential families is provided than has hitherto been the case. We illustrate the importance and benefits of this novel formulation through applications. © 2017 by the authors."
,10.1111/rssa.12301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021354639&doi=10.1111%2frssa.12301&partnerID=40&md5=054cf41089d5f69fcd78ad2c4288d370,"The Brazilian Institute of Geography and Statistics performs an annual service survey that focuses on segments of the tertiary sector. Sample estimates for some economic activities in the north, north-east and midwest regions of Brazil have low precision due to the sample design. Furthermore, one of the main variables of interest is considerably skewed with potential outliers. To overcome this problem, skew normal and skew t-models are proposed to produce model-based estimates. The small domain estimation models relate operating revenue variables to potential auxiliary variables (the number of employed people and wages) obtained from a business register. The models proposed are compared with the usual Fay–Herriot model under the assumptions of known and unknown sampling variances and its transformed log-version under the assumption of known variances. The evaluation studies with real business survey data show that the models proposed seem to be more efficient for small area predictions under skewed data than the customarily employed Fay–Herriot model, as well as its log-normal version. © 2017 Royal Statistical Society"
1,10.1016/j.jbankfin.2017.06.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025829564&doi=10.1016%2fj.jbankfin.2017.06.010&partnerID=40&md5=a68c6049145b8b7a09189a8a7c0dc02e,"This paper analyzes a wide range of flexible drift and diffusion specifications of stochastic-volatility jump–diffusion models for daily S&P 500 index returns. We find that model performance is driven almost exclusively by the specification of the diffusion component whereas the drift specifications is of second-order importance. Further, the variance dynamics of non-affine models resemble popular non-parametric high-frequency estimates of variance, and their outperformance is mainly accumulated during turbulent market regimes. Finally, we show that jump diffusion models yield more reliable estimates for the expected return of variance swap contracts. © 2017 Elsevier B.V."
1,10.1007/s11004-017-9693-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021879749&doi=10.1007%2fs11004-017-9693-y&partnerID=40&md5=00b8e42729ac2e7fdb7ae3851caebc31,"Seismic inverse modeling, which transforms appropriately processed geophysical data into the physical properties of the Earth, is an essential process for reservoir characterization. This paper proposes a work flow based on a Markov chain Monte Carlo method consistent with geology, well-logs, seismic data, and rock-physics information. It uses direct sampling as a multiple-point geostatistical method for generating realizations from the prior distribution, and Metropolis sampling with adaptive spatial resampling to perform an approximate sampling from the posterior distribution, conditioned to the geophysical data. Because it can assess important uncertainties, sampling is a more general approach than just finding the most likely model. However, since rejection sampling requires a large number of evaluations for generating the posterior distribution, it is inefficient and not suitable for reservoir modeling. Metropolis sampling is able to perform an equivalent sampling by forming a Markov chain. The iterative spatial resampling algorithm perturbs realizations of a spatially dependent variable, while preserving its spatial structure by conditioning to subset points. However, in most practical applications, when the subset conditioning points are selected at random, it can get stuck for a very long time in a non-optimal local minimum. In this paper it is demonstrated that adaptive subset sampling improves the efficiency of iterative spatial resampling. Depending on the acceptance/rejection criteria, it is possible to obtain a chain of geostatistical realizations aimed at characterizing the posterior distribution with Metropolis sampling. The validity and applicability of the proposed method are illustrated by results for seismic lithofacies inversion on the Stanford VI synthetic test sets. © 2017, International Association for Mathematical Geosciences."
1,10.1016/j.autcon.2017.06.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024876897&doi=10.1016%2fj.autcon.2017.06.010&partnerID=40&md5=2236ebd1111046d5487e129360df8e0d,"The importance of constructability is often understated in the design phase of a construction project. Issues regarding constructability often do not receive sufficient attention until it is too late for design changes. This paper proposes a quantitative measure of truss structural system design that can be used as an index of constructability in the aspect of standardization and repetition elements at early design stages. Construction can be regarded as a co-operative undertaking among architects, who specify the form of a building, and builders, who implement construction tasks. The designed form alone does not provide enough information to specify feasible processes for actual building. Sufficient fabrication information is needed so that fabricators can be completely certain concerning how to actually build the form. The goal of this study is to establish a model for estimating the amount of information needed for construction based on Shannon's information theory. This amount of information is based on uncertainty concerning assembly construction in the topological graph of the designed form, and the knowledge base shared by the designer and the builder in a design-build communication model. In this paper, the entropy of uncertainty is shown as being quantified as an index of constructability assessment for the attributes of standardization and repetition. Markov chain, Monte Carlo method and symmetry-group theories are also used in our model to analyze different levels of assembly. The scope of this research is currently focused on analyzing the constructability of truss structure systems. The assessment of constructability is demonstrated for six different types of truss structural systems. The five alternative designs are assessed for a schoolhouse project in Cambodia requiring easy construction, identified by our theoretical model. The results show that our methodology can help architects to design easily constructed truss structural systems and explore important design principles for improving constructability. The limitations and future works are discussed in the final two sections. © 2017 Elsevier B.V."
,10.1016/j.compbiolchem.2017.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024896892&doi=10.1016%2fj.compbiolchem.2017.07.002&partnerID=40&md5=dabeff4be3b74f4aab01e0f3641199af,"Objective To explore the disturbed molecular functions and pathways in clear cell renal cell carcinoma (ccRCC) using Gibbs sampling. Methods Gene expression data of ccRCC samples and adjacent non-tumor renal tissues were recruited from public available database. Then, molecular functions of expression changed genes in ccRCC were classed to Gene Ontology (GO) project, and these molecular functions were converted into Markov chains. Markov chain Monte Carlo (MCMC) algorithm was implemented to perform posterior inference and identify probability distributions of molecular functions in Gibbs sampling. Differentially expressed molecular functions were selected under posterior value more than 0.95, and genes with the appeared times in differentially expressed molecular functions ≥5 were defined as pivotal genes. Functional analysis was employed to explore the pathways of pivotal genes and their strongly co-regulated genes. Results In this work, we obtained 396 molecular functions, and 13 of them were differentially expressed. Oxidoreductase activity showed the highest posterior value. Gene composition analysis identified 79 pivotal genes, and survival analysis indicated that these pivotal genes could be used as a strong independent predictor of poor prognosis in patients with ccRCC. Pathway analysis identified one pivotal pathway − oxidative phosphorylation. Conclusions We identified the differentially expressed molecular functions and pivotal pathway in ccRCC using Gibbs sampling. The results could be considered as potential signatures for early detection and therapy of ccRCC. © 2017 Elsevier Ltd"
2,10.1061/(ASCE)HE.1943-5584.0001571,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025120060&doi=10.1061%2f%28ASCE%29HE.1943-5584.0001571&partnerID=40&md5=9a462d8485c71919f956e39cb0cb2a13,"The present study analyzes the various uncertainties and nonstationarity in the streamflow projections over the Wainganga River Basin, India, under representative concentration pathways (RCPs) 4.5 and 8.5 using the 3-layer variable infiltration capacity (VIC-3L) model. The uncertainties associated with the multiple climate models, parameters, and return levels were modeled using reliability ensemble averaging (REA), Bayesian analysis, and the delta method, respectively. With the recent development of extreme value theory (EVT), theannual maximum flows for the past and future were modeled with nonstationary assumption and validated using the Akaike information criterion (AIC) value and likelihood ratio test. The results obtained from the study indicate that the stationary assumption is a good fit for the observed and stabilized radioactive forcing scenarios (RCP4.5); whereas, for the highest greenhouse gas emission scenarios (RCP8.5), nonstationary modeling is more suitable. The obtained future flood quantiles under RCP4.5 and 8.5 are not likely to be critical in the coming century for both stationary and nonstationary assumptions. However, the nonstationary estimate of the return levels under lower return periods will be more useful to design low-capacity hydraulic structures. Further analysis of nonstationary return levels revealed that the change detection in the return levels under a lower return period was much earlier than the higher return period. The uncertainty analysis of the return levels showed larger uncertainty bound in the case of RCP8.5 rather than the RCP4.5. Furthermore, the quantification of the uncertainty between the stationary and nonstationary assumptions using Bayesian analysis with Markov chain Monte Carlo (MCMC) simulation provided a high uncertainty range in the case of nonstationary assumption compared with stationary assumption. © 2017 American Society of Civil Engineers."
5,10.1038/s41559-017-0280-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031925206&doi=10.1038%2fs41559-017-0280-x&partnerID=40&md5=b88137c61d3c08036305913f2ed026e6,"Bayesian methods have become very popular in molecular phylogenetics due to the availability of user-friendly software for running sophisticated models of evolution. However, Bayesian phylogenetic models are complex, and analyses are often carried out using default settings, which may not be appropriate. Here we summarize the major features of Bayesian phylogenetic inference and discuss Bayesian computation using Markov chain Monte Carlo (MCMC) sampling, the diagnosis of an MCMC run, and ways of summarizing the MCMC sample. We discuss the specification of the prior, the choice of the substitution model and partitioning of the data. Finally, we provide a list of common Bayesian phylogenetic software packages and recommend appropriate applications. © 2017 The Author(s)."
1,10.1371/journal.pone.0185910,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031768433&doi=10.1371%2fjournal.pone.0185910&partnerID=40&md5=5afd4ba10beaba3d2d4d646ab5d37861,"We consider a continuous-time Markov chain model of SIR disease dynamics with two levels of mixing. For this so-called stochastic households model, we provide two methods for inferring the model parameters—governing within-household transmission, recovery, and between-household transmission—from data of the day upon which each individual became infectious and the household in which each infection occurred, as might be available from First Few Hundred studies. Each method is a form of Bayesian Markov Chain Monte Carlo that allows us to calculate a joint posterior distribution for all parameters and hence the household reproduction number and the early growth rate of the epidemic. The first method performs exact Bayesian inference using a standard data-augmentation approach; the second performs approximate Bayesian inference based on a likelihood approximation derived from branching processes. These methods are compared for computational efficiency and posteriors from each are compared. The branching process is shown to be a good approximation and remains computationally efficient as the amount of data is increased. © 2017 Walker et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
2,10.1093/biostatistics/kxx007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032448364&doi=10.1093%2fbiostatistics%2fkxx007&partnerID=40&md5=79408acbc1e0deaf2c761f40b8e18edb,"Combined inference for heterogeneous high-dimensional data is critical in modern biology, where clinical and various kinds of molecular data may be available from a single study. Classical genetic association studies regress a single clinical outcome on many genetic variants one by one, but there is an increasing demand for joint analysis of many molecular outcomes and genetic variants in order to unravel functional interactions. Unfortunately, most existing approaches to joint modeling are either too simplistic to be powerful or are impracticable for computational reasons. Inspired by Richardson and others (2010, Bayesian Statistics 9), we consider a sparse multivariate regression model that allows simultaneous selection of predictors and associated responses. As Markov chain Monte Carlo (MCMC) inference on such models can be prohibitively slow when the number of genetic variants exceeds a few thousand, we propose a variational inference approach which produces posterior information very close to that of MCMC inference, at a much reduced computational cost. Extensive numerical experiments show that our approach outperforms popular variable selection methods and tailored Bayesian procedures, dealing within hours with problems involving hundreds of thousands of genetic variants and tens to hundreds of clinical or molecular outcomes. © The Author 2017. Published by Oxford University Press. All rights reserved."
5,10.1002/cncr.30863,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021424388&doi=10.1002%2fcncr.30863&partnerID=40&md5=571f17287f72eb07e2fbb9a860bc4949,"BACKGROUND: Regorafenib, a multikinase inhibitor, has demonstrated prolonged survival by 2.8 months as a second-line agent in patients with hepatocellular carcinoma (HCC) who progress on sorafenib therapy. The objective of the current study was to examine the cost effectiveness of regorafenib for the treatment of HCC. METHODS: The authors constructed a Markov simulation model of patients with unresectable HCC and Child-Pugh A cirrhosis who received treatment with regorafenib versus best supportive care. Model inputs for regorafenib effectiveness and rates of adverse events in patients with HCC were based on published clinical trial data and literature review. Quality-adjusted life years (QALYs) were calculated along with the incremental cost-effectiveness ratio (ICER) of regorafenib therapy. One-way sensitivity analyses also were conducted simultaneously on all model parameters and on various Monte-Carlo simulation parameters, and the regorafenib cost threshold at which cost effectiveness would be achieved was determined. RESULTS: Regorafenib provided an increase of 0.18 QALYs at a cost of $47,112. The ICER for regorafenib, compared with best supportive care, was $224,362. In 1-way sensitivity analyses, there were no scenarios in which regorafenib was cost effective. In cost threshold analysis, regorafenib would have to be priced at or below $67 per pill to be cost effective at an ICER of $100,000. CONCLUSIONS: Regorafenib is not cost effective as a second-line agent in the treatment of HCC, with a marginal increase in QALYs at a high cost. Lowering the cost of regorafenib or improving the selection of patients who can achieve maximal survival benefit would improve its value as a second-line treatment option for patients with HCC. Cancer 2017;123:3725–3731. © 2017 American Cancer Society. © 2017 American Cancer Society"
1,10.1177/0272989X17696996,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028646517&doi=10.1177%2f0272989X17696996&partnerID=40&md5=4997a7e6caf91af9f60da670771c689e,"Background. New cancer biomarkers are being discovered at a rapid pace; however, these tests vary in their predictive performance characteristics, and it is unclear how best to use them. Methods. We investigated 2-stage biomarker-based screening strategies in the context of prostate cancer using a partially observable Markov model to simulate patients' progression through prostate cancer states to mortality from prostate cancer or other causes. Patients were screened every 2 years from ages 55 to 69. If the patient's serum prostate-specific antigen (PSA) was over a specified threshold in the first stage, a second stage biomarker test was administered. We evaluated design characteristics for these 2-stage strategies using 7 newly discovered biomarkers as examples. Monte Carlo simulation was used to estimate the number of screening biopsies, prostate cancer deaths, and quality-adjusted life-years (QALYs) per 1000 men. Results. The all-cancer biomarkers significantly underperformed the high-grade cancer biomarkers in terms of QALYs. The screening strategy that used a PSA threshold of 2 ng/mL and a second biomarker test with high-grade sensitivity and specificity of 0.86 and 0.62, respectively, maximized QALYs. This strategy resulted in a prostate cancer death rate within 1% of using PSA alone with a threshold of 2 ng/mL, while reducing the number of biopsies by 20%. Sensitivity analysis suggests that the results are robust with respect to variation in model parameters. Conclusions. Two-stage biomarker screening strategies using new biomarkers with risk thresholds optimized for high-grade cancer detection may increase quality-adjusted survival and reduce unnecessary biopsies. © 2017 Author(s)."
,10.1134/S1995423917040024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042731035&doi=10.1134%2fS1995423917040024&partnerID=40&md5=12c7ede8cf2a58f59b33194897512af5,"In Part 1 of this paper we consider the web-page ranking problem, also known as the problem of finding the PageRank vector, or the Google problem. We discuss the link between this problem and the ergodic theorem and describe different numerical methods to solve this problem together with their theoretical background, such asMarkov chain Monte Carlo and equilibrium in a macrosystem. © 2017, Pleiades Publishing, Ltd."
,10.1177/0962280217708671,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031675034&doi=10.1177%2f0962280217708671&partnerID=40&md5=ebeb79dde157f5637fbfab62b41cf90d,"Frailty models provide a convenient way of modeling unobserved dependence and heterogeneity in survival data which, if not accounted for duly, would result incorrect inference. Gamma frailty models are commonly used for this purpose, but alternative continuous distributions are possible as well. However, with cure rate being present in survival data, these continuous distributions may not be appropriate since individuals with long-term survival times encompass zero frailty. So, we propose here a flexible probability distribution induced by a discrete frailty, and then present some special discrete probability distributions. We specifically focus on a special hyper-Poisson distribution and then develop the corresponding Bayesian simulation, influence diagnostics and an application to real dataset by means of intensive Markov chain Monte Carlo algorithm. These illustrate the usefulness of the proposed model as well as the inferential results developed here. © The Author(s) 2017."
7,10.1002/bjs.10582,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023625408&doi=10.1002%2fbjs.10582&partnerID=40&md5=3152ed8d932998e2cc5434714490bb87,"Background: Intraoperative nerve monitoring (IONM) provides dynamic neural information and is recommended for high-risk thyroid surgery. In this analysis, the cost-effectiveness of IONM in preventing bilateral recurrent laryngeal nerve (RLN) injury was investigated. Methods: A Markov chain model was constructed based on IONM use. The base-case patient was defined as a 40-year-old woman presenting with a 4·1-cm left-sided papillary thyroid cancer who developed RLN injury with loss of monitoring signal during planned bilateral thyroidectomy. It was hypothesized that, if the surgeon had used IONM, the RLN injury would have been detected and the operation would have been concluded as a thyroid lobectomy to avoid the risk of contralateral RLN injury. Cost in US dollars was converted to euros; probabilities and utility scores were identified from the literature and government resources. Length of follow-up was set as 20 years, and willingness-to-pay (WTP) as €38 000 (US $50 000) per quality-adjusted life-year (QALY). Results: At the end of year 20, the not using IONM strategy accrued €163 995·40 (US $215 783·43) and an effectiveness of 14·15 QALYs, whereas use of the IONM strategy accrued €170 283·68 (US $224 057·48) and an effectiveness of 14·33 QALYs. The incremental cost–effectiveness ratio, comparing use versus no use of IONM, was €35 285·26 (US $46 427·97) per QALY, which is below the proposed WTP, indicating that IONM is the preferred and cost-effective management plan. A Monte Carlo simulation test that considered variability of the main study factors in a hypothetical sample of 10 000 patients showed IONM to be the preferred strategy in 85·8 per cent of the population. Conclusion: Use of IONM is cost-effective in patients undergoing bilateral thyroid surgery. © 2017 BJS Society Ltd Published by John Wiley & Sons Ltd"
,10.7589/2016-09-220,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031049194&doi=10.7589%2f2016-09-220&partnerID=40&md5=49857247f71328cb623426e206d39db8,"Infection with Brucella spp., long known as a cause of abortion, infertility, and reproductive loss in domestic livestock, has increasingly been documented in marine mammals over the past two decades. We report molecular evidence of Brucella infection in Asian sea otters (Enhydra lutris lutris). Brucella DNA was detected in 3 of 78 (4%) rectal swab samples collected between 2004 and 2006 on Bering Island, Russia. These 78 animals had previously been documented to have a Brucella seroprevalence of 28%, markedly higher than the prevalence documented in sea otters (Enhydra lutris) in North America. All of the DNA sequences amplified were identical to one or more previously isolated Brucella spp. including strains from both terrestrial and marine hosts. Phylogenetic analysis of this sequence suggested that one animal was shedding Brucella spp. DNA with a sequence matching a Brucella abortus strain, whereas two animals yielded a sequence matching a group of strains including isolates classified as Brucella pinnipedialis and Brucella melitensis. Our results highlight the diversity of Brucella spp. within a single sea otter population. © Wildlife Disease Association 2017."
4,10.1007/s11999-017-5437-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021914885&doi=10.1007%2fs11999-017-5437-z&partnerID=40&md5=b1fa1370c5ffb1abf42cefd730ef6081,"Background: Failure of hip preservation to alleviate symptoms potentially subjects the patient to reoperation or conversion surgery to THA, adding recovery time, risk, and cost. A risk calculator using an algorithm that can predict the likelihood that a patient who undergoes arthroscopic hip surgery will undergo THA within 2 years would be helpful, but to our knowledge, no such tool exists. Questions: (1) Are there preoperative and intraoperative variables at the time of hip arthroscopy associated with subsequent conversion to THA? (2) Can these variables be used to develop a predictive tool for conversion to THA? Materials and Methods: All patients undergoing arthroscopy from January 2009 through December 2011 were registered in our longitudinal database. Inclusion criteria for the study group were patients undergoing hip arthroscopy for a labral tear, who eventually had conversion surgery to THA. Patients were compared with a control group of patients who underwent hip arthroscopy for a labral tear but who did not undergo conversion surgery to THA during the same study period. Of the 893 who underwent surgery during that time, 792 (88.7%) were available for followup at a minimum of 2 years (mean, 31.1 ± 8.1 years) and so were considered in this analysis. Multivariate regression analyses of 41 preoperative and intraoperative variables were performed. Using the results of the multivariate regression, we developed a simplified calculator that may be helpful in counseling a patient regarding the risk of conversion to THA after hip arthroscopy. Results: Variables simultaneously associated with conversion to THA in this model were older age (rate ratio, 1.06; 95% CI, 1.03–1.08; p < 0.0001), lower preoperative modified Harris hip score (rate ratio [RR], 0.98; 95% CI, 0.96–0.99; p = 0.0003), decreased femoral anteversion (RR, 0.97; 95% CI, 0.94–0.99; p = 0.0111), revision surgery (RR, 2.4; 95% CI, 1.15–5.01; p = 0.0193), femoral Outerbridge Grades II to IV (Grade II: RR, 2.23 [95% CI, 1.11–4.46], p = 0.023; Grade III: RR, 2.17, [95% CI, 1.11–4.23], p = 0.024; Grade IV: RR, 2.96 [95% CI, 1.34–6.52], p = 0.007), performance of acetabuloplasty (RR, 1.83; 95% CI, 1.03–3.24; p = 0.038), and lack of performance of femoral osteoplasty (RR, 0.62; 95% CI, 0.36–1.06; p = 0.081). Using the results of the multivariate regression, we developed a simplified calculator that may be helpful in counseling a patient regarding the risk of conversion surgery to THA after hip arthroscopy. Conclusion: Multiple risk factors have been identified as possible risk factors for conversion to THA after hip arthroscopy. A weighted calculator based on our data is presented here and may be useful for predicting failure after hip arthroscopy for labral treatment. Determining the best candidates for hip preservation remains challenging; careful attention to long-term followup and identifying characteristics associated with successful outcomes should be the focus of further study. Level of Evidence: Level III, therapeutic study. © 2017, The Association of Bone and Joint Surgeons®."
,10.1037/xlm0000404,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017399357&doi=10.1037%2fxlm0000404&partnerID=40&md5=b7a118155163fe2be9c2dbc695ac3b50,"While researchers have often sought to understand the learning curve in terms of multiple component processes, few studies have measured and mathematically modeled these processes on a complex task. In particular, there remains a need to reconcile how abrupt changes in strategy use can co-occur with gradual changes in task completion time. Thus, the current study aimed to assess the degree to which strategy change was abrupt or gradual, and whether strategy aggregation could partially explain gradual performance change. It also aimed to show how Bayesian methods could be used to model the effect of practice on strategy use. To achieve these aims, 162 participants completed 15 blocks of practice on a complex computer-based task-the Wynton-Anglim booking (WAB) task. The task allowed for multiple component strategies (i.e., memory retrieval, information reduction, and insight) that could also be aggregated to a global measure of strategy use. Bayesian hierarchical models were used to compare abrupt and gradual functions of component and aggregate strategy use. Task completion time was well-modeled by a power function, and global strategy use explained substantial variance in performance. Change in component strategy use tended to be abrupt, whereas change in global strategy use was gradual and well-modeled by a power function. Thus, differential timing of component strategy shifts leads to gradual changes in overall strategy efficiency, and this provides one reason for why smooth learning curves can co-occur with abrupt changes in strategy use. © 2017 American Psychological Association."
3,10.1111/ecog.02194,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010450404&doi=10.1111%2fecog.02194&partnerID=40&md5=fb21e0c028accdfce2a91e9dff20f2ef,"The documentation of biological invasions is often incomplete with records lagging behind the species’ actual spread to a spatio-temporally heterogeneous extent. Such imperfect observation bears the risk of underestimating the already realised distribution of the invading species, misguiding management efforts and misjudging potential future impacts. In this paper, we develop a hierarchical modelling framework which disentangles the determinants of the invasion and observation processes, models spatio-temporal heterogeneity in detection patterns, and infers the actual, yet partly undocumented distribution of the species at any particular time. We illustrate the model with a case study application to the invasion of common ragweed Ambrosia artemisiifolia in Austria. The invasion part of the model reconstructs the historical spread of this species across a grid of ∼ 6 × 6 km2 cells as driven by spatio-temporal variation in physical site conditions, propagule production, dispersal, and ‘background’ introductions from unknown sources. The observation part models the detection of the species’ occurrences based on heterogeneous sampling efforts, human population density, and estimated local invasion level. We fitted the hierarchical model using a Bayesian inference approach with parameters estimated by Markov chain Monte Carlo (MCMC). The actual spread of A. artemisiifolia concentrated on the climatically well-suited lowlands and was mainly driven by spatio-temporal propagule pressure from source cells with long-distance dispersal occurring rather frequently. Annual detection probabilities were estimated to vary between about 1 and up to 28%, depending mainly on sampling intensity. The model suggested that by 2005 about half of the actual distribution of the species was not yet documented. Our hierarchical model offers a flexible means to account for imperfect observation and spatio-temporal variability in detection efficiency. Inferences can be used to disentangle aspects of the invasion dynamics itself from patterns of data collection, develop improved future surveying schemes, and design more efficient invasion management strategies. © 2016 The Authors"
,10.3310/hta21580,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033402252&doi=10.3310%2fhta21580&partnerID=40&md5=22e282c2fa5f7a1a965a77ad9cbbe717,"Background: Real-time modelling is an essential component of the public health response to an outbreak of pandemic influenza in the UK. A model for epidemic reconstruction based on realistic epidemic surveillance data has been developed, but this model needs enhancing to provide spatially disaggregated epidemic estimates while ensuring that real-time implementation is feasible. Objectives: To advance state-of-the-art real-time pandemic modelling by (1) developing an existing epidemic model to capture spatial variation in transmission, (2) devising efficient computational algorithms for the provision of timely statistical analysis and (3) incorporating the above into freely available software. Methods: Markov chain Monte Carlo (MCMC) sampling was used to derive Bayesian statistical inference using 2009 pandemic data from two candidate modelling approaches: (1) a parallel-region (PR) approach, splitting the pandemic into non-interacting epidemics occurring in spatially disjoint regions; and (2) a meta-region (MR) approach, treating the country as a single meta-population with long-range contact rates informed by census data on commuting. Model discrimination is performed through posterior mean deviance statistics alongside more practical considerations. In a real-time context, the use of sequential Monte Carlo (SMC) algorithms to carry out real-time analyses is investigated as an alternative to MCMC using simulated data designed to sternly test both algorithms. SMC-derived analyses are compared with ‘gold-standard’ MCMC-derived inferences in terms of estimation quality and computational burden. Results: The PR approach provides a better and more timely fit to the epidemic data. Estimates of pandemic quantities of interest are consistent across approaches and, in the PR approach, across regions (e.g. R0 is consistently estimated to be 1.76-1.80, dropping by 43-50% during an over-summer school holiday). A SMC approach was developed, which required some tailoring to tackle a sudden ‘shock’ in the data resulting from a pandemic intervention. This semi-automated SMC algorithm outperforms MCMC, in terms of both precision of estimates and their timely provision. Software implementing all findings has been developed and installed within Public Health England (PHE), with key staff trained in its use. Limitations: The PR model lacks the predictive power to forecast the spread of infection in the early stages of a pandemic, whereas the MR model may be limited by its dependence on commuting data to describe transmission routes. As demand for resources increases in a severe pandemic, data from general practices and on hospitalisations may become unreliable or biased. The SMC algorithm developed is semi-automated; therefore, some statistical literacy is required to achieve optimal performance. Conclusions: Following the objectives, this study found that timely, spatially disaggregate, real-time pandemic inference is feasible, and a system that assumes data as per pandemic preparedness plans has been developed for rapid implementation. Future work recommendations: Modelling studies investigating the impact of pandemic interventions (e.g. vaccination and school closure); the utility of alternative data sources (e.g. internet searches) to augment traditional surveillance; and the correct handling of test sensitivity and specificity in serological data, propagating this uncertainty into the real-time modelling. © NETSCC 2017."
1,10.1016/j.ijforecast.2017.06.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026913671&doi=10.1016%2fj.ijforecast.2017.06.006&partnerID=40&md5=b09085c1fc3972dc4abf2941e9dd9521,"This paper develops vector autoregressive models with infinite hidden Markov structures, motivated by the recent empirical success of hierarchical Dirichlet process mixture models in financial and macroeconomic applications. We begin by developing a new Markov chain Monte Carlo (MCMC) method that is built upon precision-based algorithms, in order to improve the computational efficiency. We then investigate the forecast performances of these infinite hidden Markov switching models. Our forecasting results suggest that (1) models with separate infinite hidden Markov processes for the VAR coefficients and the volatilities generally forecast better than other specifications of infinite hidden Markov switching models; (2) using a single infinite hidden Markov process to govern all model parameters tends to result in poor forecasts; (3) most of the gains obtained when forecasting the inflation rate and GDP growth seem to come from allowing for time-variation in the volatilities rather than in the conditional mean coefficients. In contrast, when forecasting the short-term interest rate it is important to allow time-variation in all model parameters. © 2017 International Institute of Forecasters"
,10.1016/j.engstruct.2017.06.071,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030479775&doi=10.1016%2fj.engstruct.2017.06.071&partnerID=40&md5=e5cd30d52653fdfbc4382d0bd182cd8c,"In this paper, a Bayesian approach is developed to conduct uncertainty quantification on a single breathing crack in a beam structure using nonlinear forced responses. The proposed methodology not only determines the breathing crack characteristics but also quantifies associated uncertainties of the inferred values. Such information is important for fatigue crack monitoring and remaining life prediction in cracked beam structures. First, a single degree of freedom model is developed to characterize the nonlinear behavior of the cracked beam. The Modified Homotopy Perturbation Method (MHPM) is applied to determine analytical approximate solutions. Then, a Bayesian inference approach is proposed by applying Markov chain Monte Carlo (MCMC) technique, in which the Random Walk Metropolis algorithm is employed. The objective is to estimate crack size or location from the nonlinear vibration responses, in which noise is added to represent actual measurement data. Finally, the proposed probabilistic damage detection approach is successfully demonstrated and the breathing crack status is quantified with associated uncertainties. This leads to a new way of detecting a single breathing crack in beam structures. © 2017 Elsevier Ltd"
8,10.1002/qj.3138,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031114463&doi=10.1002%2fqj.3138&partnerID=40&md5=12ba21467ae926a0d3501ca64a51dd3e,"Inverse modelling of the emissions of atmospheric species and pollutants has significantly progressed over the past 15 years. However, in spite of seemingly reliable estimates, the retrievals are rarely accompanied by an objective estimate of their uncertainty, except when Gaussian statistics are assumed for the errors, which is often an unrealistic assumption. Here, we assess rigorous techniques meant to compute this uncertainty in the context of the inverse modelling of the time emission rates – the so-called source term – of a point-wise atmospheric tracer. Log-normal statistics are used for the positive source term prior and possibly the observation errors; this precludes simple Gaussian statistics-based solutions. Firstly, through the so-called empirical Bayesian approach, parameters of the error statistics – the hyperparameters – are first estimated by maximizing their likelihood via an expectation–maximization algorithm. This enables a robust estimation of a source term. Then, the uncertainties attached to the retrieved source rates and total emission are estimated using four Monte Carlo techniques: (i) an importance sampling based on a Laplace proposal, (ii) a naive randomize-then-optimize (RTO) sampling approach, (iii) an unbiased RTO sampling approach, and (iv) a basic Markov chain Monte Carlo (MCMC) simulation. Secondly, these methods are compared to a more thorough hierarchical Bayesian approach, using an MCMC based on a transdimensional representation of the source term to reduce the computational cost. Those methods, and improvements thereof, are applied to the estimation of the atmospheric caesium-137 source terms from the Chernobyl nuclear power plant accident in April and May 1986 and Fukushima Daiichi nuclear power plant accident in March 2011. This study provides the first consistent and rigorous quantification of the uncertainty of these best estimates. © 2017 Royal Meteorological Society"
2,10.1371/journal.pcbi.1005836,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032731576&doi=10.1371%2fjournal.pcbi.1005836&partnerID=40&md5=9943256b5dbb910eaa0e642313da80b2,"New architectures of multilayer artificial neural networks and new methods for training them are rapidly revolutionizing the application of machine learning in diverse fields, including business, social science, physical sciences, and biology. Interpreting deep neural networks, however, currently remains elusive, and a critical challenge lies in understanding which meaningful features a network is actually learning. We present a general method for interpreting deep neural networks and extracting network-learned features from input data. We describe our algorithm in the context of biological sequence analysis. Our approach, based on ideas from statistical physics, samples from the maximum entropy distribution over possible sequences, anchored at an input sequence and subject to constraints implied by the empirical function learned by a network. Using our framework, we demonstrate that local transcription factor binding motifs can be identified from a network trained on ChIP-seq data and that nucleosome positioning signals are indeed learned by a network trained on chemical cleavage nucleosome maps. Imposing a further constraint on the maximum entropy distribution also allows us to probe whether a network is learning global sequence features, such as the high GC content in nucleosome-rich regions. This work thus provides valuable mathematical tools for interpreting and extracting learned features from feed-forward neural networks. © 2017 Finnegan, Song."
7,10.1534/genetics.117.300151,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030666936&doi=10.1534%2fgenetics.117.300151&partnerID=40&md5=9e5368fcfa5b9957373cc54a050d9ffe,"Evolutionary transitions between male and female heterogamety are common in both vertebrates and invertebrates. Theoretical studies of these transitions have found that, when all genotypes are equally fit, continuous paths of intermediate equilibria link the two sex chromosome systems. This observation has led to a belief that neutral evolution along these paths can drive transitions, and that arbitrarily small fitness differences among sex chromosome genotypes can determine the system to which evolution leads. Here, we study stochastic evolutionary dynamics along these equilibrium paths. We find non-neutrality, both in transitions retaining the ancestral pair of sex chromosomes, and in those creating a new pair. In fact, substitution rates are biased in favor of dominant sex determining chromosomes, which fix with higher probabilities than mutations of no effect. Using diffusion approximations, we show that this non-neutrality is a result of “drift-induced selection” operating at every point along the equilibrium paths: stochastic jumps off the paths return with, on average, a directional bias in favor of the dominant segregating sex chromosome. Our results offer a novel explanation for the observed preponderance of dominant sex determining genes, and hint that drift-induced selection may be a common force in standard population genetic systems. © 2017 by the Genetics Society of America."
,10.1007/s11424-017-6010-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018370077&doi=10.1007%2fs11424-017-6010-2&partnerID=40&md5=66645dc91c4937f887bec2786f19c5d0,"This paper proposes a Bayesian semiparametric accelerated failure time model for doubly censored data with errors-in-covariates. The authors model the distributions of the unobserved covariates and the regression errors via the Dirichlet processes. Moreover, the authors extend the Bayesian Lasso approach to our semiparametric model for variable selection. The authors develop the Markov chain Monte Carlo strategies for posterior calculation. Simulation studies are conducted to show the performance of the proposed method. The authors also demonstrate the implementation of the method using analysis of PBC data and ACTG 175 data. © 2017, Institute of Systems Science, Academy of Mathematics and Systems Science, CAS and Springer-Verlag Berlin Heidelberg."
4,10.1016/j.advwatres.2016.10.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006056338&doi=10.1016%2fj.advwatres.2016.10.004&partnerID=40&md5=99267a7112d02ae159fd60526e5081c9,"Although treatment for cholera is well-known and cheap, outbreaks in epidemic regions still exact high death tolls mostly due to the unpreparedness of health care infrastructures to face unforeseen emergencies. In this context, mathematical models for the prediction of the evolution of an ongoing outbreak are of paramount importance. Here, we test a real-time forecasting framework that readily integrates new information as soon as available and periodically issues an updated forecast. The spread of cholera is modeled by a spatially-explicit scheme that accounts for the dynamics of susceptible, infected and recovered individuals hosted in different local communities connected through hydrologic and human mobility networks. The framework presents two major innovations for cholera modeling: the use of a data assimilation technique, specifically an ensemble Kalman filter, to update both state variables and parameters based on the observations, and the use of rainfall forecasts to force the model. The exercise of simulating the state of the system and the predictive capabilities of the novel tools, set at the initial phase of the 2010 Haitian cholera outbreak using only information that was available at that time, serves as a benchmark. Our results suggest that the assimilation procedure with the sequential update of the parameters outperforms calibration schemes based on Markov chain Monte Carlo. Moreover, in a forecasting mode the model usefully predicts the spatial incidence of cholera at least one month ahead. The performance decreases for longer time horizons yet allowing sufficient time to plan for deployment of medical supplies and staff, and to evaluate alternative strategies of emergency management. © 2016 The Authors"
1,10.1016/j.spl.2017.05.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020426936&doi=10.1016%2fj.spl.2017.05.011&partnerID=40&md5=60dfe6002b4bdee06f2565c1c2639f60,"Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in probability theory in general and in machine learning in particular. A Markov chain is devised so that its stationary distribution is some probability distribution of interest. Then one samples from the given distribution by running the Markov chain for a “long time” until it appears to be stationary and then collects the sample. However these chains are often very complex and there are no theoretical guarantees that stationarity is actually reached. In this paper we study the Gibbs sampler of the posterior distribution of a very simple case of Latent Dirichlet Allocation, an attractive Bayesian unsupervised learning model for text generation and text classification. It turns out that in some situations, the mixing time of the Gibbs sampler is exponential in the length of documents and so it is practically impossible to properly sample from the posterior when documents are sufficiently long. © 2017"
1,10.1016/j.irfa.2017.08.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029424910&doi=10.1016%2fj.irfa.2017.08.004&partnerID=40&md5=f879f6bd654079b1f1d64f840d89d08b,"Parameter estimation risk is non-trivial in both asset pricing and risk management. We adopt a Bayesian estimation paradigm supported by the Markov Chain Monte Carlo inferential techniques to incorporate parameter estimation risk in financial modelling. In option pricing activities, we find that the Merton's Jump-Diffusion (MJD) model outperforms the Black-Scholes (BS) model both in-sample and out-of-sample. In addition, the construction of Bayesian posterior option price distributions under the two well-known models offers a robust view to the influence of parameter estimation risk on option prices as well as other quantities of interest in finance such as probabilities of default. We derive a VaR-type parameter estimation risk measure for option pricing and we show that parameter estimation risk can bring significant impact to Greeks' hedging activities. Regarding the computation of default probabilities, we find that the impact of parameter estimation risk increases with gearing level, and could alter important risk management decisions. © 2017 Elsevier Inc."
,10.22034/APJCP.2017.18.10.2709,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031934745&doi=10.22034%2fAPJCP.2017.18.10.2709&partnerID=40&md5=ab296ad2b7a2aedbdc10b82d070716c7,"Background: There has been no previous study to classify malignant breast tumor in details based on Markov Chain Monte Carlo (MCMC) convergence in Western, Nigeria. This study therefore aims to profile patients living with benign and malignant breast tumor in two different hospitals among women of Western Nigeria, with a focus on prognostic factors and MCMC convergence. Materials and Methods: A hospital-based record was used to identify prognostic factors for malignant breast cancer among women of Western Nigeria. This paper describes Bayesian inference and demonstrates its usage to estimation of parameters of the logistic regression via Markov Chain Monte Carlo (MCMC) algorithm. The result of the Bayesian approach is compared with the classical statistics. Results: The mean age of the respondents was 42.2 ±16.6 years with 52% of the women aged between 35-49 years. The results of both techniques suggest that age and women with at least high school education have a significantly higher risk of being diagnosed with malignant breast tumors than benign breast tumors. The results also indicate a reduction of standard errors is associated with the coefficients obtained from the Bayesian approach. In addition, simulation result reveal that women with at least high school are 1.3 times more at risk of having malignant breast lesion in western Nigeria compared to benign breast lesion. Conclusion: We concluded that more efforts are required towards creating awareness and advocacy campaigns on how the prevalence of malignant breast lesions can be reduced, especially among women. The application of Bayesian produces precise estimates for modeling malignant breast cancer."
,10.3847/1538-4357/aa88bf,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031094155&doi=10.3847%2f1538-4357%2faa88bf&partnerID=40&md5=2eb75d3d978b04ff6a912545387825ab,"The cosmological origin of carbon, the fourth most abundant element in the universe, is not well known and a matter of heavy debate. We investigate the behavior of C/O to O/H in order to constrain the production mechanism of carbon. We measured emission-line intensities in the spectral range from 1600 to 10000 on Space Telescope Imaging Spectrograph (STIS) long-slit spectra of 18 starburst galaxies in the local universe. We determined chemical abundances through traditional nebular analysis, and we used a Markov Chain Monte Carlo method to determine where our carbon and oxygen abundances lie in the parameter space. We conclude that our C and O abundance measurements are sensible. We analyzed the behavior of our sample in the [C/O] versus [O/H] diagram with respect to other objects such as DLAs, neutral ISM measurements, and disk and halo stars, finding that each type of object seems to be located in a specific region of the diagram. Our sample shows a steeper C/O versus O/H slope with respect to other samples, suggesting that massive stars contribute more to the production of C than N at higher metallicities, only for objects where massive stars are numerous; otherwise, intermediate-mass stars dominate the C and N production. © 2017. The American Astronomical Society. All rights reserved."
7,10.1093/mnras/stx1531,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041370662&doi=10.1093%2fmnras%2fstx1531&partnerID=40&md5=a6f1aea3ee258799a21ea8f6d83bdfc2,"There is a subset of short gamma-ray bursts (SGRBs) that exhibit a rebrightening in their highenergy light curves known as extended emission. These bursts have the potential to discern between various models proposed to describe SGRBs as any model needs to account for extended emission. In this paper, we combine fallback accretion into the magnetar propeller model and investigate the morphological changes fallback accretion has on model light curves and fit to the afterglows of 15 SGRBs exhibiting extended emission from the Swift archive. We have parametrized the fallback in terms of existing parameters within the propeller model and solved for the disc mass and angular frequency of the magnetar over time. We then apply a Markov chain Monte Carlo routine to produce fits to the data. We present fits to our extended emission SGRB sample that are morphologically and energetically consistent with the data provided by Swift Burst Alert Telescope and X-ray Telescope. The parameters derived from these fits are consistent with predictions formagnetar properties and fallback accretionmodels. Fallback accretion provides a noticeable improvement to the fits of the light curves of SGRBs with extended emission when compared to previous work and could play an important role in explaining features such as variability, flares and long dipole plateaux. © 2017 The Authors."
1,10.1089/aid.2017.0091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031115661&doi=10.1089%2faid.2017.0091&partnerID=40&md5=eb70e4a53c10ee1fc471f044045277cc,"CRF07-BC was originally formed in Yunnan province of China in 1980s and spread quickly in injecting drug users (IDUs). In recent years, it has been introduced into men who have sex with men (MSM) and become the most dominant strain in China. In this study, we performed a comprehensively phylodynamic analysis of CRF07-BC sequences from China. All CRF07-BC sequences identified in China were retrieved from database. More sequences obtained in our laboratory were added to make the dataset more representative. A maximum-likelihood (ML) tree was constructed with PhyML3.0. Maximum clade credibility (MCC) tree and effective population size were predicted by using Markov Chains Monte Carlo sampling method with Beast software. A total of 610 CRF07-BC sequences coving 1,473 bp of the gag gene (from 817 to 2,289 according to HXB2 calculator) were included into the dataset. Three epidemic clusters were identified; two clusters comprised sequences from IDUs, while one cluster mainly contained sequences from MSMs. The time of the most recent common ancestor of clusters that composed of sequences from MSMs was estimated to be in 2000. Two rapid spreading waves of effective population size of CRF07-BC infections were identified in the skyline plot. The second wave coincided with the expanding of MSM cluster. The results indicated that the control of CRF07-BC infections in MSMs would help to decrease its epidemic in China. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
1,10.1007/s11295-017-1177-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027588966&doi=10.1007%2fs11295-017-1177-1&partnerID=40&md5=72408c71e5ce58a1a49f5b13ac60ebc5,"Despite the interest foresters have for inter-specific hybrid trees, still little is known about their quantitative genetics. This is especially true for the hybrid (HL) between Larix decidua (EL) and L. kaempferi (JL). Long-term, well-designed, multi-site experiments are necessary to estimate the parameters required for HL breeding programs. This paper presents the results from a diallel mating trial between nine EL and nine JL, set up in three contrasted sites. Growth traits (height, circumference), quality traits (wood density, stem form, heartwood proportion), and bud flush were measured from plantation to up to 18 years after plantation. Wood density and heartwood proportion were assessed using increment cores. We did a spatial analysis to take into account environmental heterogeneity at the tree level, and we fitted a multi-trait, Bayesian MCMC (Markov chain Monte Carlo) genetic model. Our study confirmed, in most situations, that HL expressed heterosis over its best parent for growth traits taking advantage of an early faster growth, with no loss in wood density. However, growth traits showed low levels of heritability. On the other hand, bud flush and stem flexuosity had high heritabilities, and wood density was clearly under JL control. Site-dependent heritabilities were expressed by EL. Additive genetic correlations were presented. The traits with high heritabilities showed high correlation between their performances in pure species and in hybridization, as well as high across-site correlations. The discussion focused on the interest of these genetic parameters for the hybrid larch breeding programs. © 2017, Springer-Verlag GmbH Germany."
,10.7511/jslx201705003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040171435&doi=10.7511%2fjslx201705003&partnerID=40&md5=3ac8379125caea9b1b4735b140ca2b1f,"Traditional models for critical diagonal crack angle of shear-critical reinforced concrete (RC) column are generally deterministic and exhibit low accuracy and large scatter, due to the fact that they do not consider the uncertainties of various influential factors, such as material parameters, geometric dimension and boundary constraints and so on. In order to overcome the above limitations, a probabilistic model for critical diagonal crack angle of shear-critical RC column was established in this study. The deterministic model for critical diagonal crack angle of shear-critical RC column was proposed based on the variable angle truss model first. Then, a probabilistic model for critical diagonal crack angle of shear-critical RC column which takes into account the influence of both epistemic and aleatory uncertainties was developed by combining with the Bayesian theory and the Markov Chain Monte Carlo (MCMC) method. Moreover, analytical expressions of the mean and variance of probabilistic critical diagonal crack angle were also derived. The accuracy and efficiency of the proposed probabilistic model were validated by comparing with the experimental data and existing deterministic models, which indicates that the proposed probabilistic model could describe the probabilistic characteristic of critical diagonal crack angle of the shear-critical RC column reasonably. Most importantly, the proposed probabilistic model provides not only a benchmark to calibrate the confidence level of traditional deterministic models, but also an efficient way to determine the characteristic values of critical diagonal crack angles of shear-critical RC column with different confidence levels. © 2017, Editorial Office of Chinese Journal of Computational Mechanics. All right reserved."
1,10.1016/j.rse.2017.08.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026904414&doi=10.1016%2fj.rse.2017.08.012&partnerID=40&md5=522dd723cb291f5a717a2959d165088b,"A thorough understanding of full waveform (FW) LiDAR data processing and associated uncertainty is critical to vegetation applications such as retrieving forest structure variables and estimating forest biomass. This paper applies the Bayesian non-linear modeling concept to process small-footprint FW LiDAR data (the Bayesian decomposition) collected at a study site of the National Ecological Observatory Network (NEON) to investigate its potential for waveform decomposition and uncertainty estimation. Specifically, several possible models suitable for fitting waveforms were assessed within the Bayesian framework, and the Gaussian model was selected to perform the Bayesian decomposition. Subsequently, we conducted performance evaluation and uncertainty analysis at the parameter, derived point cloud and surface model levels. Results of the model reasonableness show that the Gaussian model is superior to alternative models with respect to uncertainty, physical meaning and processing efficiency. After converting waveforms to discrete points, the model comparisons demonstrate that the Bayesian decomposition can be utilized for FW LiDAR data processing, and its results are comparable to the direct decomposition (DD), Gold and RL (Richardson–Lucy) approaches in terms of the root mean squared error (RMSE < 0.93 m) of the point distances between the waveform-based point cloud and the reference point cloud. Additionally, more points can be extracted from FW LiDAR data with these methods than discrete-return LiDAR data, especially at the mid-story of vegetation based on the results of height bins, percentile heights and canopy LiDAR density at the individual tree level. Moreover, uncertainty estimates from the Bayesian method enhance the credibility of decomposition results in a probabilistic sense to capture the true error of estimates and trace the uncertainty propagation along the processing steps. For example, results of the surface model yield larger RMSE values (1.38 m vs. 0.65 m) with a wider credible interval than quantile point clouds with a more compact distribution. In contrast to commonly used deterministic approaches, the Bayesian decomposition method can produce an ensemble of reasonable parameter estimates with probability through Markov Chain Monte Carlo (MCMC) sampling from the posterior distribution of model parameters. These parameter estimates and corresponding derived products can be queried to provide meaningful interpretation of results and associated uncertainty. Both the flat priors and empirical priors can achieve good performance of the decomposition while the empirical priors tend to significantly speed up the model convergence. The Bayesian approach also renders an important insight into the uncertainty of the model performance evaluation using field data by generating reasonable prediction intervals to reduce inherent errors of field measurements. © 2017"
,10.1002/sim.7384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021697702&doi=10.1002%2fsim.7384&partnerID=40&md5=f9f3fde854ced1c9d2f5bfcfcd275310,"In systems biology, it is of great interest to identify new genes that were not previously reported to be associated with biological pathways related to various functions and diseases. Identification of these new pathway-modulating genes does not only promote understanding of pathway regulation mechanisms but also allow identification of novel targets for therapeutics. Recently, biomedical literature has been considered as a valuable resource to investigate pathway-modulating genes. While the majority of currently available approaches are based on the co-occurrence of genes within an abstract, it has been reported that these approaches show only sub-optimal performances because 70% of abstracts contain information only for a single gene. To overcome such limitation, we propose a novel statistical framework based on the concept of ontology fingerprint that uses gene ontology to extract information from large biomedical literature data. The proposed framework simultaneously identifies pathway-modulating genes and facilitates interpreting functions of these new genes. We also propose a computationally efficient posterior inference procedure based on Metropolis–Hastings within Gibbs sampler for parameter updates and the poor man's reversible jump Markov chain Monte Carlo approach for model selection. We evaluate the proposed statistical framework with simulation studies, experimental validation, and an application to studies of pathway-modulating genes in yeast. The R implementation of the proposed model is currently available at https://dongjunchung.github.io/bayesGO/. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.1002/sim.7374,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021305386&doi=10.1002%2fsim.7374&partnerID=40&md5=36a81bd8385c0c7ef50f3b8825394fa9,"Subgroup identification (clustering) is an important problem in biomedical research. Gene expression profiles are commonly utilized to define subgroups. Longitudinal gene expression profiles might provide additional information on disease progression than what is captured by baseline profiles alone. Therefore, subgroup identification could be more accurate and effective with the aid of longitudinal gene expression data. However, existing statistical methods are unable to fully utilize these data for patient clustering. In this article, we introduce a novel clustering method in the Bayesian setting based on longitudinal gene expression profiles. This method, called BClustLonG, adopts a linear mixed-effects framework to model the trajectory of genes over time, while clustering is jointly conducted based on the regression coefficients obtained from all genes. In order to account for the correlations among genes and alleviate the high dimensionality challenges, we adopt a factor analysis model for the regression coefficients. The Dirichlet process prior distribution is utilized for the means of the regression coefficients to induce clustering. Through extensive simulation studies, we show that BClustLonG has improved performance over other clustering methods. When applied to a dataset of severely injured (burn or trauma) patients, our model is able to identify interesting subgroups. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
4,10.5194/bg-14-4295-2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030465062&doi=10.5194%2fbg-14-4295-2017&partnerID=40&md5=913daced06ed31e7e3675c6162de0802,"Calibration of terrestrial ecosystem models is important but challenging. Bayesian inference implemented by Markov chain Monte Carlo (MCMC) sampling provides a comprehensive framework to estimate model parameters and associated uncertainties using their posterior distributions. The effectiveness and efficiency of the method strongly depend on the MCMC algorithm used. In this work, a differential evolution adaptive Metropolis (DREAM) algorithm is used to estimate posterior distributions of 21 parameters for the data assimilation linked ecosystem carbon (DALEC) model using 14 years of daily net ecosystem exchange data collected at the Harvard Forest Environmental Measurement Site eddy-flux tower. The calibration of DREAM results in a better model fit and predictive performance compared to the popular adaptive Metropolis (AM) scheme. Moreover, DREAM indicates that two parameters controlling autumn phenology have multiple modes in their posterior distributions while AM only identifies one mode. The application suggests that DREAM is very suitable to calibrate complex terrestrial ecosystem models, where the uncertain parameter size is usually large and existence of local optima is always a concern. In addition, this effort justifies the assumptions of the error model used in Bayesian calibration according to the residual analysis. The result indicates that a heteroscedastic, correlated, Gaussian error model is appropriate for the problem, and the consequent constructed likelihood function can alleviate the underestimation of parameter uncertainty that is usually caused by using uncorrelated error models. © 2017 Author(s)."
1,10.1016/B978-0-12-813968-4.00008-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048016788&doi=10.1016%2fB978-0-12-813968-4.00008-0&partnerID=40&md5=7982b28bbd666bc54c476582b196e778,"This paper is motivated by imaging genetics studies with the goal to perform feature selection among multivariate phenotypes and ultrahigh dimensional genotypes. Specifically, we propose a novel multilevel sequential selection procedure under a Bayesian multivariate response regression model to select informative features among multivariate responses and ultrahigh dimensional predictors. We treat the identification of nonzero elements in the sparse coefficient matrix into a hierarchical feature selection problem by first selecting potential nonzero rows among the matrix (genotype selection) and then localizing the nonzero elements within the marked rows (phenotype selection). The genotypewise selection is accomplished by constructing multilevel auxiliary selection models under different scales with the actual scale auxiliary model treated as another level for the ultimate phenotypewise selection. We apply the method to the Alzheimer's Disease Neuroimaging Initiative with biologically meaningful results obtained. © 2018 Elsevier Inc. All rights reserved."
,10.11990/jheu.201605025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035147825&doi=10.11990%2fjheu.201605025&partnerID=40&md5=b183919c62976e67ed9243e9fda3187a,"The dynamic system simulation of effective ship embarking or disembarking at harbors is necessary to quantitatively analyze and identify risk evolution in ship pilotage in specific waters for safety control. In this study, the working process of the pilotage operation of a ship in port waters is analyzed and the risk degrees of different ship pilotage operation tasks are determined. The calculation result shows that the transfer of pilotage state complies with Markov steady-state characteristics and remains stable after tide-type fluctuations. In addition, the transferring variable remarkably affects the whole ship flow in the harbor. The dynamic system simulation model with the Markov Chain Monte Carlo algorithm is suitable for the risk analysis of maritime traffic risk and can provide the basic law for the evolution of risk in the safety management of marine traffic. © 2017, Editorial Department of Journal of HEU. All right reserved."
1,10.7527/S1000-6893.2017.220832,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037540479&doi=10.7527%2fS1000-6893.2017.220832&partnerID=40&md5=9631180125f74bc65eae413286bd8472,"To quantify the uncertainties in the model for low cycle fatigue life prediction, the classic model calibration method is applied using Bayesian theory, and the error term was verified by the normality test. Posterior distribution of the model parameter samples is obtained by Markov Chain-Monte Carlo (MCMC) simulation. An application is presented where a 95% interval of fatigue life prediction well describes the dispersity in real tests with small data samples. Correlation analysis of the samples of parameters is conducted to establish the heteroscedastic regression model. Comparison of the two models shows that the heteroscedastic regression model is questionable in uncertainty quantification performance. Morris global sensitivity analysis method is applied to quantify the sensitivity of the parameters in Manson-Coffin model, indicating that the non-informative prior is reasonable if posterior distribution is sensitive to the prior. © 2017, Press of Chinese Journal of Aeronautics. All right reserved."
2,10.1016/j.ecolmodel.2017.07.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026211612&doi=10.1016%2fj.ecolmodel.2017.07.011&partnerID=40&md5=be7b383646d864c2fdf96058e64afccd,"A Bayesian method involving Markov Chain Monte Carlo (MCMC) technique was implemented into a pesticide fate and transport model to estimate the best input parameter ranges while considering uncertainties included in both the observed pesticide concentrations and in the model. The methodology used for integrating the MCMC technique into a pollutant fate and transport models was detailed. The uncertainties encompassed in the dissolution rate and in the adsorption coefficient of the herbicide mefenacet were greatly reduced by the MCMC simulations. In addition, an optimal set of input parameters extracted from the MCMC simulations accurately reproduced mefenacet concentrations in paddy water and paddy soil as compared to the original published dataset. Consequently, by simultaneously optimizing multiple parameters of environmental models and conducting uncertainty analysis, MCMC technique exhibits powerful capability for improving the reliability and accuracy of computer models. The main strengths of the MCMC methodology are: (1) the consideration of uncertainties from both input parameters and observations and (2) the prior distributions of the input parameters which can be reformulate when additional knowledge is available. © 2017 Elsevier B.V."
,10.1109/CLUSTER.2017.68,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032646282&doi=10.1109%2fCLUSTER.2017.68&partnerID=40&md5=1cd17d14999add8d1c21f5ccc1d97f84,"Markov Chain Monte Carlo methods provide a tool for tackling high dimensional problems. With many-core systems readily available today, it is no surprise that leveraging parallelism in these samplers has been a subject of recent research. The focus has been on solutions for shared-memory architectures, however these perform poorly in a distributed-memory environment. This paper introduces a fully decentralized version of an affine invariant sampler. By observing that a pseudorandom number generator makes stochastic algorithms deterministic, communication is both minimized and hidden by computation. Two cases at opposite ends of the communication-To-computation ratio spectrum are used during evaluation against the currently available master-slave solution, where a more than tenfold reduction in execution time is measured. © 2017 IEEE."
3,10.1103/PhysRevE.96.032312,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029828746&doi=10.1103%2fPhysRevE.96.032312&partnerID=40&md5=df56d5d7b12e56b1ea95cadb062babb7,"Simplicial complexes are now a popular alternative to networks when it comes to describing the structure of complex systems, primarily because they encode multinode interactions explicitly. With this new description comes the need for principled null models that allow for easy comparison with empirical data. We propose a natural candidate, the simplicial configuration model. The core of our contribution is an efficient and uniform Markov chain Monte Carlo sampler for this model. We demonstrate its usefulness in a short case study by investigating the topology of three real systems and their randomized counterparts (using their Betti numbers). For two out of three systems, the model allows us to reject the hypothesis that there is no organization beyond the local scale. © 2017 American Physical Society."
,10.1080/00949655.2017.1341887,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021297390&doi=10.1080%2f00949655.2017.1341887&partnerID=40&md5=60f65c02b3d500d6028c8f65579e363e,"Screening procedures play an important role in data analysis, especially in high-throughput biological studies where the datasets consist of more covariates than independent subjects. In this article, a Bayesian screening procedure is introduced for the binary response models with logit and probit links. In contrast to many screening rules based on marginal information involving one or a few covariates, the proposed Bayesian procedure simultaneously models all covariates and uses closed-form screening statistics. Specifically, we use the posterior means of the regression coefficients as screening statistics; by imposing a generalized g-prior on the regression coefficients, we derive the analytical form of their posterior means and compute the screening statistics without Markov chain Monte Carlo implementation. We evaluate the utility of the proposed Bayesian screening method using simulations and real data analysis. When the sample size is small, the simulation results suggest improved performance with comparable computational cost. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1088/1742-6596/890/1/012146,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030709760&doi=10.1088%2f1742-6596%2f890%2f1%2f012146&partnerID=40&md5=53ed0226cebf029098055d0da235ca61,"Analysis of flood trends is vital since flooding threatens human living in terms of financial, environment and security. The data of annual maximum river flows in Sabah were fitted into generalized extreme value (GEV) distribution. Maximum likelihood estimator (MLE) raised naturally when working with GEV distribution. However, previous researches showed that MLE provide unstable results especially in small sample size. In this study, we used different Bayesian Markov Chain Monte Carlo (MCMC) based on Metropolis-Hastings algorithm to estimate GEV parameters. Bayesian MCMC method is a statistical inference which studies the parameter estimation by using posterior distribution based on Bayes' theorem. Metropolis-Hastings algorithm is used to overcome the high dimensional state space faced in Monte Carlo method. This approach also considers more uncertainty in parameter estimation which then presents a better prediction on maximum river flow in Sabah. © Published under licence by IOP Publishing Ltd."
6,10.1109/ECOC.2017.8346217,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046950338&doi=10.1109%2fECOC.2017.8346217&partnerID=40&md5=3de64b5d686d5ecf601b1ecb4a3fcd5e,"We model and experimentally demonstrate a self-learning abstraction process based on statistical assessment of the real-time monitoring data, both amplifier and non-linear noise parameters are periodically updated which further enables an accurate QoT estimator. © 2017 IEEE."
,10.1186/s12918-017-0453-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029765475&doi=10.1186%2fs12918-017-0453-x&partnerID=40&md5=551ad59866992c800252f98b4342e489,"Background: Phylogenetic analysis is a key way to understand current research in the biological processes and detect theory in evolution of natural selection. The evolutionary relationship between species is generally reflected in the form of phylogenetic trees. Many methods for constructing phylogenetic trees, are based on the optimization criteria. We extract the biological data via modeling features, and then compare these characteristics to study the biological evolution between species. Results: Here, we use maximum likelihood and Bayesian inference method to establish phylogenetic trees; multi-chain Markov chain Monte Carlo sampling method can be used to select optimal phylogenetic tree, resolving local optimum problem. The correlation model of phylogenetic analysis assumes that phylogenetic trees are built on homogeneous data, however there exists a large deviation in the presence of heterogeneous data. We use conscious detection to solve compositional heterogeneity. Our method is evaluated on two sets of experimental data, a group of bacterial 16S ribosomal RNA gene data, and a group of genetic data with five homologous species. Conclusions: Our method can obtain accurate phylogenetic trees on the homologous data, and also detect the compositional heterogeneity of experimental data. We provide an efficient method to enhance the accuracy of generated phylogenetic tree. © 2017 The Author(s)."
3,10.1093/mnras/stx1402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023772587&doi=10.1093%2fmnras%2fstx1402&partnerID=40&md5=36f12fd9ed801678bccb2b5bec7b86c1,"We present the γ-ray observations of the flat-spectrum radio quasar PKS 1441+25 (z=0.939), using the Fermi large Area Telescope data accumulated during 2015 January - December. A γ -ray flare was observed in January 24, when the flux increased up to (2.22 ± 0.38) × 10-6 photon cm-2 s-1 with the flux-doubling time-scale being as short as ~1.44 d. The spectral analysis shows that from 2015 April 13 to April 28, the MeV-to-GeV photon index has hardened and changes in the range of Γ = (1.73 - 1.79) for most of the time. The hardest photon index of Γ = 1.54 ± 0.16 has been observed on MJD 57 131.46 with 11.8s which is not common for flat-spectrum radio quasars. For the same period the γ -ray spectrum shows a possible deviation from a simple power-law shape, indicating a spectral cutoff at Ecut = 17.7 ± 8.9 GeV. The spectral energy distributions during quiescent and flaring states are modelled using one-zone leptonic models that include the synchrotron, synchrotron self Compton and external inverse Compton processes; the model parameters are estimated using the Markov Chain Monte Carlo method. The emission in the flaring states can be modelled assuming that either the bulk Lorentz factor or the magnetic field has increased. The modelling shows that there is a hint of hardening of the low-energy index (~1.98) of the underlying non-thermal distribution of electrons responsible for the emission in 2015 April. Such hardening agrees with the γ-ray data, which pointed out a significant γ-ray photon index hardening on 2015 April 13 to 28. © 2017 The Authors Published by Oxford University Press."
,10.1109/ICTIS.2017.8047838,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032824341&doi=10.1109%2fICTIS.2017.8047838&partnerID=40&md5=fdcbf70bd6c2f89d24a04f14b81e56c0,"In order to control or reduce the risk of water transportation, it is necessary to study the evolution trend of marine traffic risk. A new risk simulation model based on Markov Chain-Monte Carlo algorithm (MCMC) is proposed for marine traffic risk analysis near land. First, the spatial state transfer model of ship traffic risk in the waters adjacent to land was established. Second, the spatial transfer hypothesis matrix was conducted for ships operating in offshore, coastal and port waters; third, the risk value of traffic waters under stochastic process was obtained by using MCMC method to carry out random sampling. Finally, combined with typical coastal marine traffic data, the risk occurrence law and its changing trend were analyzed in different traffic waters through simulation. Empirical data shows that ship traffic risks are 0.193%, 0.563%, and 0.560% respectively in offshore, coastal and port waters, and the risk is high and unstable in restricted waters. MCMC simulation method can be applied to study the characteristics of traffic risk in waters adjacent to land, which provides a theoretical reference for improving safety and accident prevention in certain marine traffic waters. © 2017 IEEE."
,10.1109/ICTIS.2017.8047839,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032806648&doi=10.1109%2fICTIS.2017.8047839&partnerID=40&md5=3848165370acb1df7f19bf929422aedc,"In order to quantitatively analyze and identify the risk evolution on ship pilotage in the specific waters, it is necessary for simulation of the dynamic system to effectively carry out the ship in-and-out the harbor for the purpose of safety control. Firstly, the degree of risk on different pilotage tasks was established and ship transition equation between Fairway, berth or in the vicinity, anchorage or roadsteads was proposed through the harbor pilotage workflow analysis. Secondly, with Markov chain transition matrix, risk simulation model of pilotage dynamic system was constructed under a state of stochastic transition and trends of dynamic risk in different pilotage tasks were analyzed. Thirdly, with the help of data binding scenarios on harbor pilotage status, risk simulation of harbor pilotage dynamic systems was built. Furthermore, examples verification was carried out through harbor pilotage samples in eastern China. The illustrative example shows that the transition of pilotage state has the steady state of Markov characteristics, which keeps stable after fluctuations. The non-symmetric flow of ship in-and-out the harbor causes pilotage risk fluctuation with the time change. The overall situation risk of ship pilotage remains a steady value of 0.41% while the individual risk of ship pilotage remains a steady value of 0.67%. Dynamic system simulation model using the Markov Chain Monte Carlo algorithm is suitable for maritime traffic risk analysis, and turns out the basic characteristic for the evolution of risk on marine traffic safety control. © 2017 IEEE."
,10.1088/1742-6596/888/1/012216,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032442360&doi=10.1088%2f1742-6596%2f888%2f1%2f012216&partnerID=40&md5=9a33a8533f11cd44d31e8b6b39cb525c,"We present details of the first T2K neutrino and antineutrino oscillation results, in which data collected using both a muon neutrino-enhanced neutrino beam and a muon antineutrino-enhanced neutrino beam are analysed, equating to 7.002×1020 protons on target (POT) and 7.471×1020 POT respectively. Both disappearance and appearance data are analysed using a Bayesian Markov Chain Monte Carlo method, providing the first ever sensitivity to the CP-violating phase δCP from T2K data alone. The T2K data favour near-maximal mixing, with sin2 θ23 and consistent with previous T2K measurements, a value of sin2 θ13 consistent with measurements by reactor experiments, and δCP close to -π/2. When fitting with T2K data alone, the 90% credible interval for δCP disfavours values around π/2: δCP ∉ [0.38, 2.60] rad. When using a prior on sin2 2θ13 from reactor measurements, the 90% credible interval contains δCP ∉ [-3.10, -0.17] rad, disfavouring the CP-conserving values 0 and ±π. The effect on this result of the δCP prior is also investigated and presented. © Published under licence by IOP Publishing Ltd."
,10.1080/03610926.2016.1205619,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020725414&doi=10.1080%2f03610926.2016.1205619&partnerID=40&md5=1eb2c300a4b07e462d1ca0e5b85a34b6,"Three linear prediction methods of a single missing value for a stationary first order multiplicative spatial autoregressive model are proposed based on the quarter observations, observations in the first neighborhood, and observations in the nearest neighborhood. Three different types of innovations including Gaussian (symmetric and thin tailed), exponential (skew to right), and asymmetric Laplace (skew and heavy tailed) are considered. In each case, the proposed predictors are compared based on the two well-known criteria: mean square prediction and Pitman's measure of closeness. Parameter estimation is performed by maximum likelihood, least square errors, and Markov chain Monte Carlo (MCMC). © 2017 Taylor & Francis Group, LLC."
5,10.1016/j.ymssp.2017.02.042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016606537&doi=10.1016%2fj.ymssp.2017.02.042&partnerID=40&md5=a34688901eba5c4ae0a4fb8edf8455a0,"Imprecise probability based methods are developed in this study for the parameter estimation, in finite element model updating for concrete structures, when the measurements are imprecisely defined. Bayesian analysis using Metropolis Hastings algorithm for parameter estimation is generalized to incorporate the imprecision present in the prior distribution, in the likelihood function, and in the measured responses. Three different cases are considered (i) imprecision is present in the prior distribution and in the measurements only, (ii) imprecision is present in the parameters of the finite element model and in the measurement only, and (iii) imprecision is present in the prior distribution, in the parameters of the finite element model, and in the measurements. Procedures are also developed for integrating the imprecision in the parameters of the finite element model, in the finite element software Abaqus. The proposed methods are then verified against reinforced concrete beams and prestressed concrete beams tested in our laboratory as part of this study. © 2017 Elsevier Ltd"
1,10.1016/j.engstruct.2017.05.063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033376517&doi=10.1016%2fj.engstruct.2017.05.063&partnerID=40&md5=67bb6e2cc0bfdda77ee65bfc47e4d252,"This study investigates the failure probability of steel frame structures against terrorist attack from Vehicle Borne Improvised Explosive Device (VBIED). A two-step approach is used to evaluate the collapse potential of structures against blast loads. In the first step, the damage degree and responses of structural members under blast loads are determined based on an equivalent single–degree of freedom system. In the second step, the post-blast collapse behavior of steel frame structures is investigated using a 3-D nonlinear macro-based numerical model. To improve the computational efficiency, the failure probability is calculated using subset simulation method cooperated with an advanced Delayed Rejection Adaptive Markov Chain Monte Carlo simulation algorithm. The variability of blast load, vertical gravity load and structural material properties are considered. The computational framework is applied to a prototype 10-story steel frame to study the failure risk against VBIED. The results show that the reliability assessment framework used in this study provides an accurate and more efficient prediction of failure risk of structures against blast loads compared with the direct Monte Carlo simulation method. The framework also presents an approach for determination of effective measures in protecting structures against blast loads. © 2017 Elsevier Ltd"
1,10.1088/1741-4326/aa8387,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034456169&doi=10.1088%2f1741-4326%2faa8387&partnerID=40&md5=4e9450c6e931e6d26d7f15aa0354e413,"It remains an open question to explain the dramatic change in intrinsic rotation induced by slight changes in electron density (White et al 2013 Phys. Plasmas 20 056106). One proposed explanation is that momentum transport is sensitive to the second derivatives of the temperature and density profiles (Lee et al 2015 Plasma Phys. Control. Fusion 57 125006), but it is widely considered to be impossible to measure these higher derivatives. In this paper, we show that it is possible to estimate second derivatives of electron density and temperature using a nonparametric regression technique known as Gaussian process regression. This technique avoids over-constraining the fit by not assuming an explicit functional form for the fitted curve. The uncertainties, obtained rigorously using Markov chain Monte Carlo sampling, are small enough that it is reasonable to explore hypotheses which depend on second derivatives. It is found that the differences in the second derivatives of and between the peaked and hollow rotation cases are rather small, suggesting that changes in the second derivatives are not likely to explain the experimental results. © 2017 IAEA, Vienna."
1,10.1016/j.prevetmed.2017.07.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024843002&doi=10.1016%2fj.prevetmed.2017.07.005&partnerID=40&md5=73ceed1c06c314889095e3aabab52257,"Accurate information on the geographic distribution of domestic animal populations helps biosecurity authorities to efficiently prepare for and rapidly eradicate exotic diseases, such as Foot and Mouth Disease (FMD). Developing and maintaining sufficiently high-quality data resources is expensive and time consuming. Statistical modelling of population density and distribution has only begun to be applied to farm animal populations, although it is commonly used in wildlife ecology. We developed zero-inflated Poisson regression models in a Bayesian framework using environmental and socioeconomic variables to predict the counts of livestock units (LSUs) and of cattle on spatially referenced farm polygons in a commercially available New Zealand farm database, Agribase. Farm-level counts of cattle and of LSUs varied considerably by region, because of the heterogeneous farming landscape in New Zealand. The amount of high quality pasture per farm was significantly associated with the presence of both cattle and LSUs. Internal model validation (predictive performance) showed that the models were able to predict the count of the animal population on groups of farms that were located in randomly selected 3 km zones with a high level of accuracy. Predicting cattle or LSU counts on individual farms was less accurate. Predicted counts were statistically significantly more variable for farms that were contract grazing dry stock, such as replacement dairy heifers and dairy cattle not currently producing milk, compared with other farm types. This analysis presents a way to predict numbers of LSUs and cattle for farms using environmental and socio-economic data. The technique has the potential to be extrapolated to predicting other pastoral based livestock species. © 2017 Elsevier B.V."
6,10.1016/j.ymssp.2017.02.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016649492&doi=10.1016%2fj.ymssp.2017.02.023&partnerID=40&md5=e813c995f39df89af5e06326df3283e0,"This paper introduces the Bayesian regularization applied to the Force Analysis Technique (FAT), a method for identifying vibration sources from displacement measurements. The FAT is based on the equation of motion of a structure instead of a transfer matrix as it is the case for most of inverse problems. This particularity allows the estimation of vibration sources without the need of boundary conditions. Nevertheless, this method is highly sensitive to noise perturbations and needs a careful regularization. Two Bayesian approaches are thus presented. Firstly, the empirical Bayesian regularization which shows better robustness than L-curve and GCV regularizations while keeping a low numerical cost. Secondly, a fully Bayesian procedure using a Markov Chain Monte Carlo (MCMC) algorithm which provides credible intervals on variables of interest besides the automatically regularized vibration source field. In particular, measurement quality can be evaluated by the noise variance estimation and the uncertainties over the source level are quantified for a wide frequency range, with only a unique measurement scan. © 2017 Elsevier Ltd"
4,10.1016/j.virusres.2017.03.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017410681&doi=10.1016%2fj.virusres.2017.03.020&partnerID=40&md5=1451b729bd8d5b34623c12bae11b9ab5,"Grapevine red blotch-associated virus (GRBaV), the causative agent of red blotch disease, is a member of the genus Grablovirus, in the family Geminiviridae and the first known geminivirus of Vitis spp. Limited information is available on the epidemiology of red blotch disease. A 2-hectare Vitis vinifera cv. ‘Cabernet franc’ vineyard in Napa County, California, USA was selected for monitoring GRBaV spread over a three-year period (2014–2016) based on an initially low disease incidence and an aggregation of symptomatic vines at the edge of the vineyard proximal to a wooded riparian area. The incidence of diseased plants increased by 1–2% annually. Spatial analysis of diseased plants in each year using ordinary runs analysis within rows and Spatial Analysis by Distance IndicEs (SADIE) demonstrated aggregation. Spatiotemporal analysis between consecutive years within the association function of SADIE revealed a strong overall association among all three years (X = 0.874–0.945). Analysis of epidemic spread fitting a stochastic spatiotemporal model using the Monte Carlo Markov Chain method identified strong evidence for localized (within vineyard) spread. A spatial pattern consisting of a combination of strongly aggregated and randomly isolated symptomatic vines within 8-years post-planting suggested unique epidemic attributes compared to those of other grapevine viruses vectored by mealybugs and soft scales or by dagger nematodes for which typical within-row spread and small-scale autocorrelation are well documented. These findings are consistent with the existence of a new type of vector for a grapevine virus. © 2017 Elsevier B.V."
1,10.1080/15325008.2017.1404660,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041107739&doi=10.1080%2f15325008.2017.1404660&partnerID=40&md5=0cf42e5c3fbf483cc9f4e10d4a774dfa,"Probabilistic adequacy evaluation of allocated spinning reserve is beneficial to economically regulating this foremost auxiliary service to counterbalance unforeseen generation-demand mismatches. As the time horizon for a probabilistic spinning reserve adequacy investigation task may vary from several to dozens of minutes, adaptive importance sampling methods, such as the classical cross-entropy method and its variants, are appealing instead of the classical non-sequential Monte Carlo to estimate desired reliability indices due to the rareness of demand-not-supplied contingencies. In this article, a new adaptive cross-entropy method is proposed, particularly, nesting a specially optimized partially collapsed Gibbs sampler to help in avoidance of locally trapped Markov chain samples which may be encountered by traditional cross-entropy methods. RTS-79 is utilized for illustrating the superiority of the proposed method, termed E-MICEM, against its parent method, i.e., the Markov chain Monte Carlo-integrated cross-entropy method. Two traditional indices including loss of load probability and expected demand not supplied are comparatively evaluated and the simulation results suggest that the E-MICEM is superior in the efficiency of estimating the two indices. Some advices are also given on the build-in-parameter regulation for the E-MICEM applicable to the systems of different dimensions. © 2017, Copyright © Taylor & Francis Group, LLC."
1,10.1109/EMBC.2017.8037422,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032222132&doi=10.1109%2fEMBC.2017.8037422&partnerID=40&md5=84fdeed99c6205fdaf0ef25a93b0096d,"Paraquat (N, N'-dimethyl-4,4'-bipyridium dichloride) is a potent and widely used herbicide in agricultural countries, including Thailand. The presence of this chemical in the body can lead to toxic effects in the liver, kidney, and lung. Pulmonary toxicity has been identified as the main cause of acute toxicity in animals and humans. Chronic exposure to paraquat is associated with Parkinson's disease in humans. Paraquat is transported into the lungs by neutral amino acid transporter. Therefore, a physiologically based pharmacokinetic (PBPK) model of paraquat was developed with a description of the protein transporter mechanism. To develop a PBPK model of paraquat, a pharmacokinetic study of paraquat in rats was selected from the ThaiLIS and Pubmed database. The selected study contained tissue-specific concentration-time course information such as paraquat concentration levels in liver, kidney and lung. Physiologic parameters were acquired from the literature or determined using a Markov-Chain Monte Carlo (MCMC) technique. The developed PBPK model consisted of 5 organ compartments (i.e. kidney, liver, slowly perfused organs, richly perfuse organs and lung), featuring an incorporation of neutral amino acid transporter in the lung. Our model simulations could explain the data from the literature and adequately describe pharmacokinetics of paraquat in the rats. This developed PBPK model may be able help in understanding of paraquat-induced Parkinson's disease as well as in risk assessment of paraquat. © 2017 IEEE."
1,10.1051/epjconf/201714602007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030471352&doi=10.1051%2fepjconf%2f201714602007&partnerID=40&md5=3f79b5d8fdb008afb5fe9cefdfd1eda0,"As model parameters, necessary ingredients of theoretical models, are not always predicted by theory, a formal mathematical framework associated to the evaluation work is needed to obtain the best set of parameters (resonance parameters, optical models, fission barrier, average width, multigroup cross sections) with Bayesian statistical inference by comparing theory to experiment. The formal rule related to this methodology is to estimate the posterior density probability function of a set of parameters by solving an equation of the following type: pdf(posterior) ∼ pdf(prior) × a likelihood function. A fitting procedure can be seen as an estimation of the posterior density probability of a set of parameters (referred as x→) knowing a prior information on these parameters and a likelihood which gives the probability density function of observing a data set knowing x→. To solve this problem, two major paths could be taken: add approximations and hypothesis and obtain an equation to be solved numerically (minimum of a cost function or Generalized least Square method, referred as GLS) or use Monte-Carlo sampling of all prior distributions and estimate the final posterior distribution. Monte Carlo methods are natural solution for Bayesian inference problems. They avoid approximations (existing in traditional adjustment procedure based on chi-square minimization) and propose alternative in the choice of probability density distribution for priors and likelihoods. This paper will propose the use of what we are calling Bayesian Monte Carlo (referred as BMC in the rest of the manuscript) in the whole energy range from thermal, resonance and continuum range for all nuclear reaction models at these energies. Algorithms will be presented based on Monte-Carlo sampling and Markov chain. The objectives of BMC are to propose a reference calculation for validating the GLS calculations and approximations, to test probability density distributions effects and to provide the framework of finding global minimum if several local minimums exist. Application to resolved resonance, unresolved resonance and continuum evaluation as well as multigroup cross section data assimilation will be presented. © The Authors, published by EDP Sciences, 2017."
,10.5194/isprs-archives-XLII-2-W7-647-2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031029033&doi=10.5194%2fisprs-archives-XLII-2-W7-647-2017&partnerID=40&md5=3ebd5264e25063ca6b164790a42d9690,"For the image segmentation method based on Gaussian Mixture Model (GMM), there are some problems: 1) The number of component was usually a fixed number, i.e., fixed class and 2) GMM is sensitive to image noise. This paper proposed a RS image segmentation method that combining GMM with reversible jump Markov Chain Monte Carlo (RJMCMC). In proposed algorithm, GMM was designed to model the distribution of pixel intensity in RS image. Assume that the number of component was a random variable. Respectively build the prior distribution of each parameter. In order to improve noise resistance, used Gibbs function to model the prior distribution of GMM weight coefficient. According to Bayes' theorem, build posterior distribution. RJMCMC was used to simulate the posterior distribution and estimate its parameters. Finally, an optimal segmentation is obtained on RS image. Experimental results show that the proposed algorithm can converge to the optimal number of class and get an ideal segmentation results."
4,10.1093/mnras/stx1219,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023757611&doi=10.1093%2fmnras%2fstx1219&partnerID=40&md5=6661028197a719820be1bef6e855380f,"Dwarf galaxies, among the most dark matter dominated structures of our Universe, are excellent test-beds for dark matter theories. Unfortunately, mass modelling of these systems suffers from the well-documented mass-velocity anisotropy degeneracy. For the case of spherically symmetric systems, we describe a method for non-parametric modelling of the radial and tangential velocity moments. The method is a numerical velocity anisotropy ""inversion"", with parametric mass models, where the radial velocity dispersion profile, σ2π, is modelled as a B-spline, and the optimization is a three-step process that consists of (i) an evolutionary modelling to determine the mass model form and the best B-spline basis to represent σ2π; (ii) an optimization of the smoothing parameters and (iii) a Markov chain Monte Carlo analysis to determine the physical parameters. The mass-anisotropy degeneracy is reduced into mass model inference, irrespective of kinematics. We test our method using synthetic data. Our algorithm constructs the best kinematic profile and discriminates between competing dark matter models. We apply our method to the Fornax dwarf spheroidal galaxy. Using a King brightness profile and testing various dark matter mass models, our model inference favours a simple mass-follows-light system. We find that the anisotropy profile of Fornax is tangential (β(r) &lt; 0) and we estimate a total mass of Mtot = 1.613+0.050-0.075 × 108M⊙, and a mass-to-light ratio of υV = 8.93+0.32-0.47 (M⊙/L⊙). The algorithm we present is a robust and computationally inexpensive method for non-parametric modelling of spherical clusters independent of the mass-anisotropy degeneracy. © 2017 The Authors. Published by Oxford University Press on behalf of the Royal Astronomical Society."
,10.1016/j.ecolmodel.2017.05.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021203611&doi=10.1016%2fj.ecolmodel.2017.05.009&partnerID=40&md5=3ea839f210be052500c439ce86b31381,"Black leaf streak disease (BLSD), caused by the fungal pathogen Mycosphaerella fijiensis, is considered as the most destructive foliar disease of banana. To advance our knowledge of the dynamics of the disease at plant scale as well as of the components of varietal resistance, we designed, calibrated and evaluated a mechanistic model to simulate the disease on a banana plant. The model runs in discrete time at plant scale and describes plant growth and pathogen dynamics under optimal epidemiological conditions. The model is divided into two modules: a deterministic plant sub-model that simulates the simplified architecture and growth of the banana, and a pathogen sub-model that simulates the detailed life cycle of the pathogen including infection, lesion growth, asexual and sexual sporulation, and the dispersal of spores on the plant. The three most influential epidemiological parameters identified by sensitivity analysis of the model (lesion growth rate, infection efficiency, and incubation period) were estimated in a Bayesian framework using Markov chain Monte Carlo methods and acquired data on the dynamics of leaf lesions under natural conditions. The posterior densities provided precise knowledge on pathogen life history traits. The evaluation of the model using an independent data set confirmed the good quality of the predictions. Simulations allowed us to evaluate the impact of host resistance components, auto-infection at plant scale, and of the leaf emergence rate, which is linked with cropping practices, on the severity of BLSD. This foliar disease simulation model will help design new methods of controlling BLSD. © 2017 Elsevier B.V."
1,10.1002/sim.7347,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020121164&doi=10.1002%2fsim.7347&partnerID=40&md5=47836f62fa3f4dc52f8d1363903cc0e1,"Multilevel item response theory (MLIRT) models have been widely used to analyze the multivariate longitudinal data of mixed types (e.g., categorical and continuous) in clinical studies. The MLIRT models often have unidimensional assumption, that is, the multiple outcomes are clinical manifestations of a univariate latent variable. However, the unidimensional assumption may be unrealistic because some diseases may be heterogeneous and characterized by multiple impaired domains with variable clinical symptoms and disease progressions. We relax this assumption and propose a multidimensional latent trait linear mixed model (MLTLMM) to allow multiple latent variables and within-item multidimensionality (one outcome can be a manifestation of more than one latent variable). We conduct extensive simulation studies to assess the unidimensional MLIRT model and the proposed MLTLMM model. The simulation studies suggest that the MLTLMM model outperforms unidimensional model when the multivariate longitudinal outcomes are manifested by multiple latent variables. The proposed model is applied to two motivating studies of amyotrophic lateral sclerosis: a clinical trial of ceftriaxone and the Pooled Resource Open-Access ALS Clinical Trials database. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
,10.3847/1538-4357/aa844f,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029516709&doi=10.3847%2f1538-4357%2faa844f&partnerID=40&md5=52942e751c0c1a096a7436649fd10fc8,"At a distance of ∼2 pc, our nearest brown dwarf neighbor, Luhman 16 AB, has been extensively studied since its discovery 3 years ago, yet its most fundamental parameter - the masses of the individual dwarfs - has not been constrained with precision. In this work, we present the full astrometric orbit and barycentric motion of Luhman 16 AB and the first precision measurements of the individual component masses. We draw upon archival observations spanning 31 years from the European Southern Observatory (ESO) Schmidt Telescope, the Deep Near-Infrared Survey of the Southern Sky (DENIS), public FORS2 data on the Very Large Telescope (VLT), and new astrometry from the Gemini South Multiconjugate Adaptive Optics System (GeMS). Finally, we include three radial velocity measurements of the two components from VLT/CRIRES, spanning one year. With this new data sampling a full period of the orbit, we use a Markov chain Monte Carlo algorithm to fit a 16-parameter model incorporating mutual orbit and barycentric motion parameters and constrain the individual masses to be for the T dwarf and for the L dwarf. Our measurements of Luhman 16 AB's mass ratio and barycentric motion parameters are consistent with previous estimates in the literature utilizing recent astrometry only. The GeMS-derived measurements of the Luhman 16 AB separation in 2014-2015 agree closely with Hubble Space Telescope (HST) measurements made during the same epoch, and the derived mutual orbit agrees with those measurements to within the HST uncertainties of 0.3-0.4 mas. © 2017. The American Astronomical Society. All rights reserved."
6,10.1016/j.ajhg.2017.08.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028057012&doi=10.1016%2fj.ajhg.2017.08.002&partnerID=40&md5=f2ad37ff5c0f2ea8585038b1a58323b2,"Genome-wide association studies (GWASs) have identified many complex loci. However, most loci reside in noncoding regions and have unknown biological functions. Integrative analysis that incorporates known functional information into GWASs can help elucidate the underlying biological mechanisms and prioritize important functional variants. Hence, we develop a flexible Bayesian variable selection model with efficient computational techniques for such integrative analysis. Different from previous approaches, our method models the effect-size distribution and probability of causality for variants with different annotations and jointly models genome-wide variants to account for linkage disequilibrium (LD), thus prioritizing associations based on the quantification of the annotations and allowing for multiple associated variants per locus. Our method dramatically improves both computational speed and posterior sampling convergence by taking advantage of the block-wise LD structures in human genomes. In simulations, our method accurately quantifies the functional enrichment and performs more powerfully for prioritizing the true associations than alternative methods, where the power gain is especially apparent when multiple associated variants in LD reside in the same locus. We applied our method to an in-depth GWAS of age-related macular degeneration with 33,976 individuals and 9,857,286 variants. We find the strongest enrichment for causality among non-synonymous variants (54× more likely to be causal, 1.4× larger effect sizes) and variants in transcription, repressed Polycomb, and enhancer regions, as well as identify five additional candidate loci beyond the 32 known AMD risk loci. In conclusion, our method is shown to efficiently integrate functional information in GWASs, helping identify functional associated-variants and underlying biology. © 2017"
20,10.1103/PhysRevLett.119.101301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029695760&doi=10.1103%2fPhysRevLett.119.101301&partnerID=40&md5=1ff5a6529969c19eb307f8f907b89d51,"We compute the Bayesian evidence for models considered in the main analysis of Planck cosmic microwave background data. By utilizing carefully defined nearest-neighbor distances in parameter space, we reuse the Monte Carlo Markov chains already produced for parameter inference to compute Bayes factors B for many different model-data set combinations. The standard 6-parameter flat cold dark matter model with a cosmological constant (ΛCDM) is favored over all other models considered, with curvature being mildly favored only when cosmic microwave background lensing is not included. Many alternative models are strongly disfavored by the data, including primordial correlated isocurvature models (lnB=-7.8), nonzero scalar-to-tensor ratio (lnB=-4.3), running of the spectral index (lnB=-4.7), curvature (lnB=-3.6), nonstandard numbers of neutrinos (lnB=-3.1), nonstandard neutrino masses (lnB=-3.2), nonstandard lensing potential (lnB=-4.6), evolving dark energy (lnB=-3.2), sterile neutrinos (lnB=-6.9), and extra sterile neutrinos with a nonzero scalar-to-tensor ratio (lnB=-10.8). Other models are less strongly disfavored with respect to flat ΛCDM. As with all analyses based on Bayesian evidence, the final numbers depend on the widths of the parameter priors. We adopt the priors used in the Planck analysis, while performing a prior sensitivity analysis. Our quantitative conclusion is that extensions beyond the standard cosmological model are disfavored by Planck data. Only when newer Hubble constant measurements are included does ΛCDM become disfavored, and only mildly, compared with a dynamical dark energy model (lnB∼+2). © 2017 American Physical Society."
2,10.5194/tc-11-2089-2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028864578&doi=10.5194%2ftc-11-2089-2017&partnerID=40&md5=d8e3bce00c6bcbe1f92ba70f8c313960,"Quantitative characterization of soil organic carbon (OC) content is essential due to its significant impacts on surface-subsurface hydrological-Thermal processes and microbial decomposition of OC, which both in turn are important for predicting carbon-climate feedbacks. While such quantification is particularly important in the vulnerable organic-rich Arctic region, it is challenging to achieve due to the general limitations of conventional core sampling and analysis methods, and to the extremely dynamic nature of hydrological-Thermal processes associated with annual freeze-Thaw events. In this study, we develop and test an inversion scheme that can flexibly use single or multiple datasets-including soil liquid water content, temperature and electrical resistivity tomography (ERT) data-to estimate the vertical distribution of OC content. Our approach relies on the fact that OC content strongly influences soil hydrological-Thermal parameters and, therefore, indirectly controls the spatiotemporal dynamics of soil liquid water content, temperature and their correlated electrical resistivity. We employ the Community Land Model to simulate nonisothermal surface-subsurface hydrological dynamics from the bedrock to the top of canopy, with consideration of land surface processes (e.g., solar radiation balance, evapotranspiration, snow accumulation and melting) and ice-liquid water phase transitions. For inversion, we combine a deterministic and an adaptive Markov chain Monte Carlo (MCMC) optimization algorithm to estimate a posteriori distributions of desired model parameters. For hydrological-Thermal-To-geophysical variable transformation, the simulated subsurface temperature, liquid water content and ice content are explicitly linked to soil electrical resistivity via petrophysical and geophysical models. We validate the developed scheme using different numerical experiments and evaluate the influence of measurement errors and benefit of joint inversion on the estimation of OC and other parameters. We also quantify the propagation of uncertainty from the estimated parameters to prediction of hydrological-Thermal responses. We find that, compared to inversion of single dataset (temperature, liquid water content or apparent resistivity), joint inversion of these datasets significantly reduces parameter uncertainty. We find that the joint inversion approach is able to estimate OC and sand content within the shallow active layer (top 0.3ĝ€m of soil) with high reliability. Due to the small variations of temperature and moisture within the shallow permafrost (here at about 0.6ĝ€m depth), the approach is unable to estimate OC with confidence. However, if the soil porosity is functionally related to the OC and mineral content, which is often observed in organic-rich Arctic soil, the uncertainty of OC estimate at this depth remarkably decreases. Our study documents the value of the new surface-subsurface, deterministic-stochastic inversion approach, as well as the benefit of including multiple types of data to estimate OC and associated hydrological-Thermal dynamics. © Author(s) 2017."
1,10.1103/PhysRevE.96.033301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029889631&doi=10.1103%2fPhysRevE.96.033301&partnerID=40&md5=85ed4769d68445b14a1457ba17d5252d,"Herdeiro and Doyon [Phys. Rev. E 94, 043322 (2016)2470-004510.1103/PhysRevE.94.043322] introduced a numerical recipe, dubbed uv sampler, offering precise estimations of the conformal field theory (CFT) data of the planar two-dimensional (2D) critical Ising model. It made use of scale invariance emerging at the critical point in order to sample finite sublattice marginals of the infinite plane Gibbs measure of the model by producing holographic boundary distributions. The main ingredient of the Markov chain Monte Carlo sampler is the invariance under dilation. This paper presents a generalization to higher dimensions with the critical 3D Ising model. This leads to numerical estimations of a subset of the CFT data - scaling weights and structure constants - through fitting of measured correlation functions. The results are shown to agree with the recent most precise estimations from numerical bootstrap methods [Kos, Poland, Simmons-Duffin, and Vichi, J. High Energy Phys. 08 (2016) 03610.1007/JHEP08(2016)036]. © 2017 American Physical Society."
1,10.1186/s13059-017-1297-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028857221&doi=10.1186%2fs13059-017-1297-9&partnerID=40&md5=f40051e29f13ca35b43aa1d4e3e85ea3,"Single-molecule RNA fluorescence in situ hybridization (smFISH) provides unparalleled resolution in the measurement of the abundance and localization of nascent and mature RNA transcripts in fixed, single cells. We developed a computational pipeline (BayFish) to infer the kinetic parameters of gene expression from smFISH data at multiple time points after gene induction. Given an underlying model of gene expression, BayFish uses a Monte Carlo method to estimate the Bayesian posterior probability of the model parameters and quantify the parameter uncertainty given the observed smFISH data. We tested BayFish on synthetic data and smFISH measurements of the neuronal activity-inducible gene Npas4 in primary neurons. © 2017 The Author(s)."
,10.1080/10543406.2016.1226323,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989942901&doi=10.1080%2f10543406.2016.1226323&partnerID=40&md5=bebf9df5be0b5b683595224a1b3f4139,"The agreement of different measurement methods is an important issue in several disciplines like, for example, Medicine, Metrology, and Engineering. In this article, some agreement measures, common in the literature, were analyzed from a Bayesian point of view. Posterior inferences for such agreement measures were obtained based on well-known Bayesian inference procedures for the bivariate normal distribution. As a consequence, a general, simple, and effective method is presented, which does not require Markov Chain Monte Carlo methods and can be applied considering a great variety of prior distributions. Illustratively, the method was exemplified using five objective priors for the bivariate normal distribution. A tool for assessing the adequacy of the model is discussed. Results from a simulation study and an application to a real dataset are also reported. © 2016, © 2016 Taylor & Francis."
,10.1061/(ASCE)EM.1943-7889.0001316,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021692750&doi=10.1061%2f%28ASCE%29EM.1943-7889.0001316&partnerID=40&md5=e509eaa3e3643313e542a5c96329db66,"Uncertain changes in spatial distribution of structural parameters, caused by deterioration or damage, may weaken the structure and result in unexpected losses of properties or casualties. In recent decades, to identify spatial distribution of parameters, various system identification (SI) methods have been developed based on optimization algorithms employing various regularization techniques. However, such optimization-based SI methods may suffer from ill-posedness of the optimization problem under uncertain measurement noises. Moreover, depending on boundary and traction conditions, the accuracy and robustness of SI methods may differ. In this paper, to overcome these technical challenges in identification of spatial distribution, a new SI method is developed by modifying the transitional Markov chain Monte Carlo (m-TMCMC). In addition to the modifications introduced to the sampling algorithm, the proposed method enhances robustness of the SI results by exploiting the results by the maximum likelihood estimation and finite-element updating. To identify general shapes of spatial distribution with a reasonable number of parameters, a spatial deterioration model is proposed based on the modes obtained based on a random field model called Karhunen-Loeve expansion. The proposed SI method is tested and demonstrated through numerical examples of steel plate and B-pillar structure, in which the effects of random measurement errors are also considered. The numerical examples demonstrate accuracy and robustness of the proposed method. © 2017 American Society of Civil Engineers."
,10.1080/03772063.2017.1369909,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029904715&doi=10.1080%2f03772063.2017.1369909&partnerID=40&md5=f4f229d94865d26b68ada34e74b5f14b,"In this paper, a target-tracking algorithm based on improved unscented particle filter with the Markov chain Monte Carlo (MCMC) is proposed. In the proposed method, the improved unscented Kalman filter (UKF) is used to generate the proposal distribution, and particle swarm optimization (PSO) integrates into the UKF proposal. Moreover, the sample impoverishment created by resampling step is restrained with MCMC move step after the resampling. Experiments are presented to evaluate the performance of the proposed algorithm. The results show that the proposed algorithm has more significant advantages in tracking accuracy than other classical algorithms. © 2017 IETE"
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030651197&partnerID=40&md5=ad0902ed623575f9403089129351565e,"We develop a new Bayesian Markov Chain Monte Carlo algorithm for Euler-discretized Feller square-root stochastic volatility models and demonstrate the performance of our algorithm through simulations and empirical analyses. Specifically, our algorithm use the Laplace approximation of the posterior density of conditional variance, which is the probability kernel of the generalized inverse gaussian distribution, derived from the joint density of return and conditional variance so that it can be easily applied to the extended stochastic volatility models with such as fat-tailed distributions or Lévy jump processes. In addition, we conduct the simulation experiment investigating and comparing the size and power of the parametric specification tests checking certain finite-dimensional moment conditions without correction for parameter estimation uncertainty with that of the nonparametric Hong and Li (2005)’s omnibus test which is not affected by parameter estimation uncertainty. The parametric and nonparametric tests are based on the probability integral transform of the prediction densities of returns obtained using auxiliary particle filter algorithms. Our experiment result shows that the classical parametric specification test may have no worse size distortion and better power than Hong and Li (2005)’s test. © 2017, Korean Econometric Society. All rights reserved."
,10.1061/(ASCE)EM.1943-7889.0001066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020649483&doi=10.1061%2f%28ASCE%29EM.1943-7889.0001066&partnerID=40&md5=7c7ac29d093f78b117df0e9d56f99785,[No abstract available]
,10.1061/(ASCE)EM.1943-7889.0001066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020660282&doi=10.1061%2f%28ASCE%29EM.1943-7889.0001066&partnerID=40&md5=47cafc79887c647a43e5f3d989961fa3,[No abstract available]
,10.1061/(ASCE)EM.1943-7889.0001325,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021860481&doi=10.1061%2f%28ASCE%29EM.1943-7889.0001325&partnerID=40&md5=4246792c84a1fe08ee67f4b42770926c,"In the typesetting process, the expressions prior belief, prior distribution, and prior PDF were transformed to previous belief, previous distribution, and previous PDF, which are misnomers. All expressions of previous (expect for two occurrences) should be replaced with prior. ASCE regrets this error. © ASCE."
5,10.1016/j.ymssp.2016.12.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015268280&doi=10.1016%2fj.ymssp.2016.12.023&partnerID=40&md5=08e65fc1b47c6e2e878f668e254a2bdf,"In this paper the authors present a method which facilitates computationally efficient parameter estimation of dynamical systems from a continuously growing set of measurement data. It is shown that the proposed method, which utilises Sequential Monte Carlo samplers, is guaranteed to be fully parallelisable (in contrast to Markov chain Monte Carlo methods) and can be applied to a wide variety of scenarios within structural dynamics. Its ability to allow convergence of one's parameter estimates, as more data is analysed, sets it apart from other sequential methods (such as the particle filter). © 2016 Elsevier Ltd"
2,10.1016/j.anucene.2017.04.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018723689&doi=10.1016%2fj.anucene.2017.04.020&partnerID=40&md5=628c11e9fe02ddf4ff93aaefc1e6b4c9,"Quantifying the uncertainty contributors of Best Estimate (BE) Thermal Hydraulic (TH) codes has been getting more and more attention in safety analysis of nuclear industry during recent decades. Yet for evaluation of intrinsic physical models which may not be readily measured, the quantification process is usually subjective and inaccurate. This paper investigates the statistical methodology in order to get the probability density function (pdf) of model parameters more objectively based on observed experimental responses. The simplification of mathematical model is described for the parameter estimation, and the solution using Markov Chain Monte Carlo (MCMC) algorithm is demonstrated. As the direct evaluations are computationally intensive, surrogate models using Radial Basis Function (RBF) are constructed to substitute the complex forward calculations. And to efficiently improve the accuracy of the surrogate model, an adaptive approach based on cross-entropy minimization to densify training samples at space of posterior pdf is applied. As an application, uncertainties of model parameters related to reflood phenomena implemented in RELAP5 code are quantified. It is indicated that the developed method which is independent of BE codes is feasible and efficient to apply. Through the check of uncertainty propagation, it proves that the uncertainty bands can envelope most of the experiment measurements with an advantage of accuracy. The model calibration by posterior mean value also presents a good improvement of calculations. © 2017 Elsevier Ltd"
,10.1109/DT.2017.8024325,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030110812&doi=10.1109%2fDT.2017.8024325&partnerID=40&md5=ec17f2ad87b7ddbd7d6b4370f6b77029,"In this paper, the parameters and reliability characteristics of the mixture of the failure time distribution are estimated based on a complete sample using both Markov chain Monte Carlo (MCMC) method and maximum likelihood estimation via cross-entropy (CE) algorithm. While maximum likelihood estimation is the most frequently used method for parameter estimation, MCMC has recently emerged as a good alternative. The most popular MCMC method, called the Metropolis-Hastings algorithm, is used to provide a complete analysis of the concerned posterior distribution. A simulation study is provided to compare MCMC with CE, and differences between the estimates obtained by the two approaches are evaluated. © 2017 IEEE."
4,10.1016/j.ress.2017.04.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018561948&doi=10.1016%2fj.ress.2017.04.004&partnerID=40&md5=e81f765c3ade927d14f7b41864e76baf,"Mathematical numerical models are increasingly employed to simulate system behavior and identify sequences of events or configurations of the system's design and operational parameters that can lead the system to extreme conditions (Critical Region, CR). However, when a numerical model is: i) computationally expensive, ii) high-dimensional, and iii) complex, these tasks become challenging. In this paper, we propose an adaptive framework for efficiently tackling this problem: i) a dimensionality reduction technique is employed for identifying the factors and variables that most affect the system behavior; ii) a meta-model is sequentially trained to replace the computationally expensive model with a computationally cheap one; iii) an adaptive exploration algorithm based on Markov Chain Monte Carlo is introduced for exploring the system state-space using the meta-model; iv) clustering and other techniques for the visualization of high dimensional data (e.g., parallel coordinates plot) are employed to summarize the retrieved information. The method is employed to explore a power network model involving 20 inputs. The CRs are properly identified with a limited computational cost, compared to another exploration technique of literature (i.e., Latin Hypercube Sampling). © 2017 Elsevier Ltd"
2,10.1016/j.ijnonlinmec.2017.03.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016710749&doi=10.1016%2fj.ijnonlinmec.2017.03.012&partnerID=40&md5=c80872c1f3094a32cf74c51eeea76741,"Bayesian approaches to statistical inference and system identification became practical with the development of effective sampling methods like Markov Chain Monte Carlo (MCMC). However, because the size and complexity of inference problems has dramatically increased, improved MCMC methods are required. Dynamical systems based samplers are an effective extension of traditional MCMC methods. These samplers treat the posterior probability distribution as the potential energy function of a dynamical system, enabling them to better exploit the structure of the inference problem. We present an algorithm, Second-Order Langevin MCMC (SOL-MC), a stochastic dynamical system based MCMC algorithm, which uses the damped second-order Langevin stochastic differential equation (SDE) to sample a posterior distribution. We design the SDE such that the desired posterior probability distribution is its stationary distribution. Since this method is based upon an underlying dynamical system, we can utilize existing work to develop, implement, and optimize the sampler's performance. As such, we can choose parameters which speed up the convergence to the stationary distribution and reduce temporal state and energy correlations in the samples. We then apply this sampler to a system identification problem for a non-linear hysteretic structure model to investigate this method under globally identifiable and unidentifiable conditions. © 2017 Elsevier Ltd"
,10.1016/j.eneco.2017.08.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031743138&doi=10.1016%2fj.eneco.2017.08.029&partnerID=40&md5=3116d709417c32dc84ff536e98cda148,"Partial linear models provide an intuitively appealing way to examine gasoline demand because one can examine how response to price varies according to the price level and people's income. However, despite their intuitive appeal, partial linear models have tended to produce implausible and/or erratic price effects. Blundell et al. (2012) propose a solution to this problem that involves using Slutsky shape restrictions to improve the precision of the nonparametric estimate of the demand function. They propose estimating a constrained partially linear model through three steps, where the weights are optimized by minimizing an objective function under the Slutsky constraint, bandwidths are selected through least squares cross-validation, and linear coefficients are estimated using profile least squares. A limitation of their three-step estimation method is that bandwidths are selected based on pre-estimated parameters. We improve on the Blundell et al. (2012) solution in that we derive a posterior and develop a posterior simulation algorithm to simultaneously estimate the linear coefficients, bandwidths in the kernel estimator and the weights imposed by the Slutsky condition. With our proposed sampling algorithm, we estimate a constrained partially linear model of household gasoline demand employing household survey data for the United States for 1991 and 2001 and for Canada for 2006–2009 and find plausible price effects. © 2017 Elsevier B.V."
,10.1109/TR.2017.2713760,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023596317&doi=10.1109%2fTR.2017.2713760&partnerID=40&md5=417fad98cbdf7561b7dc47e175a5be87,"Damage diagnosis and prognosis play an important role in ensuring the safety of mechanical, aerospace, and civil structures. Most existing structural damage estimation methods are limited in only providing an estimate of the damage magnitude at the current time instance. Revealing the evolving path of structural damage is highly desirable in practice for prognosis and remaining useful life prediction. In this paper, we propose a dynamic data-driven hierarchical Bayesian degradation model, which takes advantage of both the physical finite element model and the data-driven Bayesian framework, to tackle the structural damage growth prediction. The damage growth trend can be efficiently and accurately estimated by Gibbs sampling. Systematic case analyses are performed to validate and demonstrate the effectiveness of the proposed method. © 2017 IEEE."
2,10.1016/j.jsr.2017.06.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022100358&doi=10.1016%2fj.jsr.2017.06.005&partnerID=40&md5=2b478f7f5e6b9a02f5d7e9bd5b25387d,"Introduction Safety performance functions (SPFs) are essential tools for highway agencies to predict crashes, identify hotspots and assess safety countermeasures. In the Highway Safety Manual (HSM), a variety of SPFs are provided for different types of roadway facilities, crash types and severity levels. Agencies, lacking the necessary resources to develop own localized SPFs, may opt to apply the HSM's SPFs for their jurisdictions. Yet, municipalities that want to develop and maintain their regional SPFs might encounter the issue of the small sample bias. Bayesian inference is being conducted to address this issue by combining the current data with prior information to achieve reliable results. It follows that the essence of Bayesian statistics is the application of informative priors, obtained from other SPFs or experts’ experiences. Method In this study, we investigate the applicability of informative priors for Bayesian negative binomial SPFs for rural divided multilane highway segments in Florida and California. An SPF with non-informative priors is developed for each state and its parameters’ distributions are assigned to the other state's SPF as informative priors. The performances of SPFs are evaluated by applying each state's SPFs to the other state. The analysis is conducted for both total (KABCO) and severe (KAB) crashes. Results, conclusions and practical applications As per the results, applying one state's SPF with informative priors, which are the other state's SPF independent variable estimates, to the latter state's conditions yields better goodness of fit (GOF) values than applying the former state's SPF with non-informative priors to the conditions of the latter state. This is for both total and severe crash SPFs. Hence, for localities where it is not preferred to develop own localized SPFs and adopt SPFs from elsewhere to cut down on resources, application of informative priors is shown to facilitate the process. © 2017 National Safety Council and Elsevier Ltd"
3,10.1016/j.dss.2017.06.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021049186&doi=10.1016%2fj.dss.2017.06.001&partnerID=40&md5=702d5c5cd3aae4d0cebcb3839022ebe1,"The broad aim of this paper is to answer the following query: is the relationship between social media sentiments and stock returns time-varying? To provide a satisfactory response, a novel methodology—a symbiosis of Bayesian Dynamic Linear Models and Seemingly Unrelated Regressions —is introduced. Two sets of Dow Jones Industrial Average stock data and corresponding social media data from Yahoo! Finance stock message boards are used in a comprehensive empirical study. Some key findings are: (a) Affirmative response to the above question; (b) Models with only social media sentiments and market returns perform at least as well as models that include Fama-French and Momentum factors; (c) There are significant correlations between stocks, ranging from −0.8 to 0.6 in both data sets. © 2017 Elsevier B.V."
2,10.1007/s00477-016-1260-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966716017&doi=10.1007%2fs00477-016-1260-4&partnerID=40&md5=1739aa6c99b3b3ba8aa1dccd1b9b142b,"Seismic intensity, measured through the Mercalli–Cancani–Sieberg (MCS) scale, provides an assessment of ground shaking level deduced from building damages, any natural environment changes and from any observed effects or feelings. Generally, moving away from the earthquake epicentre, the effects are lower but intensities may vary in space, as there could be areas that amplify or reduce the shaking depending on the earthquake source geometry, geological features and local factors. Currently, the Istituto Nazionale di Geofisica e Vulcanologia analyzes, for each seismic event, intensity data collected through the online macroseismic questionnaire available at the web-page www.haisentitoilterremoto.it. Questionnaire responses are aggregated at the municipality level and analyzed to obtain an intensity defined on an ordinal categorical scale. The main aim of this work is to model macroseismic attenuation and obtain an intensity prediction equation which describes the decay of macroseismic intensity as a function of the magnitude and distance from the hypocentre. To do this we employ an ordered probit model, assuming that the intensity response variable is related through the link probit function to some predictors. Differently from what it is commonly done in the macroseismic literature, this approach takes properly into account the qualitative and ordinal nature of the macroseismic intensity as defined on the MCS scale. Using Markov chain Monte Carlo methods, we estimate the posterior probability of the intensity at each site. Moreover, by comparing observed and estimated intensities we are able to detect anomalous areas in terms of residuals. This kind of information can be useful for a better assessment of seismic risk and for promoting effective policies to reduce major damages. © 2016, Springer-Verlag Berlin Heidelberg."
,10.1515/sagmb-2016-0014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028889632&doi=10.1515%2fsagmb-2016-0014&partnerID=40&md5=d8bdeb4ee091f975bd56e68a6135c2c2,"An important topic in bioinformatics is the protein structure alignment. Some statistical methods have been proposed for this problem, but most of them align two protein structures based on the global geometric information without considering the effect of neighbourhood in the structures. In this paper, we provide a Bayesian model to align protein structures, by considering the effect of both local and global geometric information of protein structures. Local geometric information is incorporated to the model through the partial Procrustes distance of small substructures. These substructures are composed of β-carbon atoms from the side chains. Parameters are estimated using a Markov chain Monte Carlo (MCMC) approach. We evaluate the performance of our model through some simulation studies. Furthermore, we apply our model to a real dataset and assess the accuracy and convergence rate. Results show that our model is much more efficient than previous approaches. © 2017 Walter de Gruyter GmbH, Berlin/Boston."
2,10.1016/j.nimb.2017.04.060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018770654&doi=10.1016%2fj.nimb.2017.04.060&partnerID=40&md5=46d5a10fef0d6c682825342c41a74b98,"We use the classical Monte Carlo transport model of electrons moving near the surface and inside solids to reproduce the measured reflection electron energy-loss spectroscopy (REELS) spectra. With the combination of the classical transport model and the Markov chain Monte Carlo (MCMC) sampling of oscillator parameters the so-called reverse Monte Carlo (RMC) method was developed, and used to obtain optical constants of Ni in this work. A systematic study of the electronic and optical properties of Ni has been performed in an energy loss range of 0–200 eV from the measured REELS spectra at primary energies of 1000 eV, 2000 eV and 3000 eV. The reliability of our method was tested by comparing our results with the previous data. Moreover, the accuracy of our optical data has been confirmed by applying oscillator strength-sum rule and perfect-screening-sum rule. © 2017 Elsevier B.V."
1,10.1002/asmb.2210,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996508252&doi=10.1002%2fasmb.2210&partnerID=40&md5=d6c514880bb04327c51cfb2ce8ad3f86,"We present a Bayesian decision theoretic approach for developing replacement strategies. In so doing, we consider a semiparametric model to describe the failure characteristics of systems by specifying a nonparametric form for cumulative intensity function and by taking into account effect of covariates by a parametric form. Use of a gamma process prior for the cumulative intensity function complicates the Bayesian analysis when the updating is based on failure count data. We develop a Bayesian analysis of the model using Markov chain Monte Carlo methods and determine replacement strategies. Adoption of Markov chain Monte Carlo methods involves a data augmentation algorithm. We show the implementation of our approach using actual data from railroad tracks. Copyright © 2016 John Wiley & Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd."
1,10.1016/j.jhydrol.2017.05.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021663319&doi=10.1016%2fj.jhydrol.2017.05.002&partnerID=40&md5=d124a099bd0dc0ed4a7258829f936c64,"The diversity of vegetation in semi-arid, ephemeral wetlands is determined by niche availability and species competition, both of which are influenced by changes in water availability and salinity. Here, we hypothesise that ignoring physiological differences and competition between species when managing wetland hydrologic regimes can lead to a decrease in vegetation diversity, even when the overall wetland carrying capacity is improved. Using an ecohydrological model capable of resolving water-vegetation-salt feedbacks, we investigate why water surface and groundwater management interventions to combat vegetation decline have been more beneficial to Casuarina obesa than to Melaleuca strobophylla, the co-dominant tree species in Lake Toolibin, a salt-affected wetland in Western Australia. The simulations reveal that in trying to reduce the negative effect of salinity, the management interventions have created an environment favouring C. obesa by intensifying the climate-induced trend that the wetland has been experiencing of lower water availability and higher root-zone salinity. By testing alternative scenarios, we show that interventions that improve M. strobophylla biomass are possible by promoting hydrologic conditions that are less specific to the niche requirements of C. obesa. Modelling uncertainties were explored via a Markov Chain Monte Carlo (MCMC) algorithm. Overall, the study demonstrates the importance of including species differentiation and competition in ecohydrological models that form the basis for wetland management. © 2017 Elsevier B.V."
,10.1002/etep.2366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018436954&doi=10.1002%2fetep.2366&partnerID=40&md5=c04567d202d3217b3c1cdf127cad1f25,"With the prevalence of renewable energy source in power system, it is necessary to appraise the voltage stability of the integration system by probabilistic methods. The traditional Markov Chain Monte Carlo (MCMC) simulation could show great calculation precision for the probabilistic assessment, but it is always involved with complicated sampling iterations because of the Gibbs sampling method currently used in MCMC simulation. Instead of Gibbs sampling method, this paper presents the application of slice sampling in MCMC simulation for the voltage stability probabilistic assessment of the power system with renewable source. Firstly, the probabilistic models of renewable source generation are constructed. Then, the sample space of renewable source outputs is obtained by slice sampling, and the samples from the sample space are calculated by power flow. Finally, the voltage stability margin is obtained by the result of the power flow calculation, and the probabilistic assessment of the voltage stability is implemented. Furthermore, the MCMC simulations using Gibbs sampling and slice sampling are compared by Gelman-Rubin diagnostic and Kullback-Leibler divergence tests on IEEE 14-bus system and IEEE 39-bus system, respectively. The results show that the slice sampling method is simpler and more efficient than Gibbs sampling method in the voltage stability probabilistic assessment. Copyright © 2017 John Wiley & Sons, Ltd."
3,10.1007/s13253-017-0286-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021114864&doi=10.1007%2fs13253-017-0286-5&partnerID=40&md5=a0c2afd41ae493127ae6e6888b7db978,"Mechanistic modelling of animal movement is often formulated in discrete time despite problems with scale invariance, such as handling irregularly timed observations. A natural solution is to formulate in continuous time, yet uptake of this has been slow. This lack of implementation is often excused by a difficulty in interpretation. Here we aim to bolster usage by developing a continuous-time model with interpretable parameters, similar to those of popular discrete-time models that use turning angles and step lengths. Movement is defined by a joint bearing and speed process, with parameters dependent on a continuous-time behavioural switching process, creating a flexible class of movement models. Methodology is presented for Markov chain Monte Carlo inference given irregular observations, involving augmenting observed locations with a reconstruction of the underlying movement process. This is applied to well-known GPS data from elk (Cervus elaphus), which have previously been modelled in discrete time. We demonstrate the interpretable nature of the continuous-time model, finding clear differences in behaviour over time and insights into short-term behaviour that could not have been obtained in discrete time. © 2017, The Author(s)."
,10.12693/APhysPolA.132.1112,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033394863&doi=10.12693%2fAPhysPolA.132.1112&partnerID=40&md5=10d754a80dad5a3c817d8d9aa0c4c6ac,"Markov chain Monte Carlo methods (MCMC) are iterative algorithms that are used in many Bayesian simulation studies, where the inference cannot be easily obtained directly through the defined model. Reversible jump MCMC methods belong to a special type of MCMC methods, in which the dimension of parameters can change in each iteration. In this study, we suggest Gibbs sampling in place of RJMCMC, to decrease the computational demand of the calculation of high dimensional systems. We evaluate the performance of the suggested algorithm in three real benchmark datasets, by comparing the accuracy and the computational demand with its strong alternatives, namely, birth-death MCMC, RJMCMC and QUIC algorithms. From the comparative analyses, we detect that Gibbs sampling improves the computational cost of RJMCMC without losing the accuracy."
,10.1142/S0578563417500140,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031289391&doi=10.1142%2fS0578563417500140&partnerID=40&md5=2c06a89558153db42620a1db44d8ff2d,"This study proposes a safety evaluation process for a prevention structure against tsunamis, in which the most updated guideline (i.e. FEMA P-646) is used as the deterministic analysis and a probabilistic approach is adopted to consider uncertainties involved. To overcome the incomplete data on Tsunamis, a Markov chain Monte Carlo (MCMC) simulation is developed to increase the quantity of historical data from Taiwan followed by the use of the least squares support vector machine (LS-SVM) to estimate the probability density function (PDF) of random variables. Based on the fragility analyses of the superstructure, substructure and entire system, if the wall thickness is below 4.45m, the wall thickness is more likely to be the governing factor compared with the pile diameter. Conversely, the pile diameter would more likely be the dominating factor. When the pile size decreases from 75cm to 65cm, the threshold value will shift from 4.45m to 3.85m. In addition, when the wall thickness and height are greater, there is a greater likelihood of the failure probability being governed by the substructure. Based on historical records only, the probability of failure of the seawall is extremely low. Nevertheless, results shown here are in line with the engineering judgement and the computation procedure established in the present study can be used as a reference for performing safety analysis on tsunami structures with insufficient data. © 2017 World Scientific Publishing Company."
2,10.1016/j.jhydrol.2017.07.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026254077&doi=10.1016%2fj.jhydrol.2017.07.021&partnerID=40&md5=e0e485caae2b44329c382852218b73d3,"The common practice of infrequent (e.g., monthly) stream water quality sampling for state of the environment monitoring may, when combined with high resolution stream flow data, provide sufficient information to accurately characterise the dominant nutrient transfer pathways and predict annual catchment yields. In the proposed approach, we use the spatially lumped catchment model StreamGEM to predict daily stream flow and nitrate concentration (mg L−1 NO3-N) in four contrasting mesoscale headwater catchments based on four years of daily rainfall, potential evapotranspiration, and stream flow measurements, and monthly or daily nitrate concentrations. Posterior model parameter distributions were estimated using the Markov Chain Monte Carlo sampling code DREAMZS and a log-likelihood function assuming heteroscedastic, t-distributed residuals. Despite high uncertainty in some model parameters, the flow and nitrate calibration data was well reproduced across all catchments (Nash-Sutcliffe efficiency against Log transformed data, NSL, in the range 0.62–0.83 for daily flow and 0.17–0.88 for nitrate concentration). The slight increase in the size of the residuals for a separate validation period was considered acceptable (NSL in the range 0.60–0.89 for daily flow and 0.10–0.74 for nitrate concentration, excluding one data set with limited validation data). Proportions of flow and nitrate discharge attributed to near-surface, fast seasonal groundwater and slow deeper groundwater were consistent with expectations based on catchment geology. The results for the Weida Stream in Thuringia, Germany, using monthly as opposed to daily nitrate data were, for all intents and purposes, identical, suggesting that four years of monthly nitrate sampling provides sufficient information for calibration of the StreamGEM model and prediction of catchment dynamics. This study highlights the remarkable effectiveness of process based, spatially lumped modelling with commonly available monthly stream sample data, to elucidate high resolution catchment function, when appropriate calibration methods are used that correctly handle the inherent uncertainties. © 2017 Elsevier B.V."
2,10.1111/biom.12644,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009787615&doi=10.1111%2fbiom.12644&partnerID=40&md5=cf23ef3cc16468aa6184d6870901a766,"To assess the compliance of air quality regulations, the Environmental Protection Agency (EPA) must know if a site exceeds a pre-specified level. In the case of ozone, the level for compliance is fixed at 75 parts per billion, which is high, but not extreme at all locations. We present a new space-time model for threshold exceedances based on the skew-t process. Our method incorporates a random partition to permit long-distance asymptotic independence while allowing for sites that are near one another to be asymptotically dependent, and we incorporate thresholding to allow the tails of the data to speak for themselves. We also introduce a transformed AR(1) time-series to allow for temporal dependence. Finally, our model allows for high-dimensional Bayesian inference that is comparable in computation time to traditional geostatistical methods for large data sets. We apply our method to an ozone analysis for July 2005, and find that our model improves over both Gaussian and max-stable methods in terms of predicting exceedances of a high level. © 2017, The International Biometric Society"
3,10.1093/biomet/asx033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037128794&doi=10.1093%2fbiomet%2fasx033&partnerID=40&md5=b1e099107704822a6409cee5fd92c121,"Standard posterior sampling algorithms, such as Markov chain Monte Carlo procedures, face major challenges in scaling up to massive datasets. We propose a simple and general posterior interval estimation algorithm to rapidly and accurately estimate quantiles of the posterior distributions for one-dimensional functionals. Our algorithm runs Markov chain Monte Carlo in parallel for subsets of the data, and then averages quantiles estimated from each subset. We provide strong theoretical guarantees and show that the credible intervals from our algorithm asymptotically approximate those from the full posterior in the leading parametric order. Our algorithm has a better balance of accuracy and efficiency than its competitors across a variety of simulations and a real-data example. © 2017 Biometrika Trust."
1,10.5351/CSAM.2017.24.5.507,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044066457&doi=10.5351%2fCSAM.2017.24.5.507&partnerID=40&md5=45424b13ad4c7e5311fa75a93025bd8d,"Volatility plays a crucial role in theory and applications of asset pricing, optimal portfolio allocation, and risk management. This paper proposes a combined model of autoregressive moving average (ARFIMA), generalized autoregressive conditional heteroscedasticity (GRACH), and skewed-t error distribution to accommodate important features of volatility data; long memory, heteroscedasticity, and asymmetric error distribution. A fully Bayesian approach is proposed to estimate the parameters of the model simultaneously, which yields parameter estimates satisfying necessary constraints in the model. The approach can be easily implemented using a free and user-friendly software JAGS to generate Markov chain Monte Carlo samples from the joint posterior distribution of the parameters. The method is illustrated by using a daily volatility index from Chicago Board Options Exchange (CBOE). JAGS codes for model specification is provided in the Appendix. © 2017 The Korean Statistical Society, and Korean International Statistical Society."
5,10.1214/17-AOAS1046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031498677&doi=10.1214%2f17-AOAS1046&partnerID=40&md5=4a059382364e5a05f4257d81ad2bd5cb,"Bayesian methods for large-scale multiple regression provide attractive approaches to the analysis of genome-wide association studies (GWAS). For example, they can estimate heritability of complex traits, allowing for both polygenic and sparse models; and by incorporating external genomic data into the priors, they can increase power and yield new biological insights. However, these methods require access to individual genotypes and phenotypes, which are often not easily available. Here we provide a framework for performing these analyses without individual-level data. Specifically, we introduce a “Regression with Summary Statistics” (RSS) likelihood, which relates the multiple regression coefficients to univariate regression results that are often easily available. The RSS likelihood requires estimates of correlations among covariates (SNPs), which also can be obtained from public databases. We perform Bayesian multiple regression analysis by combining the RSS likelihood with previously proposed prior distributions, sampling posteriors by Markov chain Monte Carlo. In a wide range of simulations RSS performs similarly to analyses using the individual data, both for estimating heritability and detecting associations. We apply RSS to a GWAS of human height that contains 253,288 individuals typed at 1.06 million SNPs, for which analyses of individual-level data are practically impossible. Estimates of heritability (52%) are consistent with, but more precise, than previous results using subsets of these data. We also identify many previously unreported loci that show evidence for association with height in our analyses. Software is available at https://github.com/stephenslab/rss. © Institute of Mathematical Statistics, 2017."
,10.1007/s00180-017-0730-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020751350&doi=10.1007%2fs00180-017-0730-6&partnerID=40&md5=22f56a556a6d13f4102794428809225a,"Understanding how viruses offer protection against closely related emerging strains is vital for creating effective vaccines. For many viruses, multiple serotypes often co-circulate and testing large numbers of vaccines can be infeasible. Therefore the development of an in silico predictor of cross-protection between strains is important to help optimise vaccine choice. Here we present a sparse hierarchical Bayesian model for detecting relevant antigenic sites in virus evolution (SABRE) which can account for the experimental variability in the data and predict antigenic variability. The method uses spike and slab priors to identify sites in the viral protein which are important for the neutralisation of the virus. Using the SABRE method we are able to identify a number of key antigenic sites within several viruses, as well as providing estimates of significant changes in the evolutionary history of the serotypes. We show how our method outperforms alternative established methods; standard mixed effects models, the mixed effects LASSO, and the mixed effects elastic nets. We also propose novel proposal mechanisms for the Markov chain Monte Carlo simulations, which improve mixing and convergence over that of the established component-wise Gibbs sampler. © 2017, The Author(s)."
,10.1214/17-AOAS1054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031490784&doi=10.1214%2f17-AOAS1054&partnerID=40&md5=7041ed978352c676b280d86070ff7a85,"Förster resonance energy transfer (FRET) is a quantum-physical phenomenon where energy may be transferred from one molecule to a neighbor molecule if the molecules are close enough. Using fluorophore molecule marking of proteins in a cell, it is possible to measure in microscopic images to what extent FRET takes place between the fluorophores. This provides indirect information of the spatial distribution of the proteins. Questions of particular interest are whether (and if so to which extent) proteins of possibly different types interact or whether they appear independently of each other. In this paper we propose a new likelihood-based approach to statistical inference for FRET microscopic data. The likelihood function is obtained from a detailed modeling of the FRET data-generating mechanism conditional on a protein configuration. We next follow a Bayesian approach and introduce a spatial point process prior model for the protein configurations depending on hyperparameters quantifying the intensity of the point process. Posterior distributions are evaluated using Markov chain Monte Carlo. We propose to infer microscope-related parameters in an initial step from reference data without interaction between the proteins. The new methodology is applied to simulated and real datasets. © Institute of Mathematical Statistics, 2017."
1,10.21314/JCF.2017.329,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030092584&doi=10.21314%2fJCF.2017.329&partnerID=40&md5=f528fe0ef43e03acbc12a34295bf79db,"Risk-based asset allocation models have received considerable attention in recent years. This increased popularity is due in part to the difficulty in estimating expected returns, as well as to the 2008 financial crisis, which helped reinforce the key role of risk in asset allocation.We propose a generalized risk budgeting (GRB) approach to portfolio construction. In a GRB portfolio, assets are grouped into possibly overlapping subsets, and each subset is allocated a prespecified risk budget. Minimum variance, risk parity and risk budgeting portfolios are all special instances of a GRB portfolio. The GRB portfolio optimization problem is to find a GRB portfolio with an optimal risk–return profile, where risk is measured using any positively homogeneous risk measure. When the subsets form a partition, the assets all have the same expected return, and we restrict ourselves to long-only portfolios; then, the GRB problem can in fact be solved as a convex optimization problem. In general, however, the GRB problem is a constrained nonconvex problem, for which we propose two solution approaches. The first approach uses a semidefinite programming relaxation to obtain an (upper) bound on the optimal objective function value. In the second approach, we develop a numerical algorithm that integrates augmented Lagrangian and Markov chain Monte Carlo methods in order to find a point in the vicinity of a very good local optimum. This point is then supplied to a standard nonlinear optimization routine with the goal of finding this local optimum. The merit of this second approach is in its generic nature: in particular, it provides a strategy for choosing a starting point for any nonlinear optimization algorithm. © 2017 Infopro Digital Risk (IP) Limited."
2,10.1093/biomet/asx031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037129236&doi=10.1093%2fbiomet%2fasx031&partnerID=40&md5=80890f5775994f646d15a4a0cdcee639,"We consider a pseudo-marginal Metropolis-Hastings kernel Pm that is constructed using an average of m exchangeable random variables, and an analogous kernel Ps that averages s &lt; m of these same random variables. Using an embedding technique to facilitate comparisons, we provide a lower bound for the asymptotic variance of any ergodic average associated with Pm in terms of the asymptotic variance of the corresponding ergodic average associated with Ps.We showthat the bound is tight and disprove a conjecture that when the random variables to be averaged are independent, the asymptotic variance under Pm is never less than s/m times the variance under Ps. The conjecture does, however, hold for continuous-time Markov chains. These results imply that if the computational cost of the algorithm is proportional to m, it is often better to set m = 1. We provide intuition as to why these findings differ so markedly from recent results for pseudo-marginal kernels employing particle filter approximations. Our results are exemplified through two simulation studies; in the first the computational cost is effectively proportional to m and in the second there is a considerable start-up cost at each iteration. © 2017 Biometrika Trust."
,10.1088/1674-4527/17/9/94,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030848262&doi=10.1088%2f1674-4527%2f17%2f9%2f94&partnerID=40&md5=435c443260e75da4187aefc771e28c3c,"We carried out new photometric observations of asteroid (106) Dione at three apparitions (2004, 2012 and 2015) to understand its basic physical properties. Based on a new brightness model, new photometric observational data and published data of (106) Dione were analyzed to characterize the morphology of Dione's photometric phase curve. In this brightness model, a cellinoid ellipsoid shape and three-parameter (H,G1,G2) magnitude phase function system were involved. Such a model can not only solve the phase function system parameters of (106) Dione by considering an asymmetric shape of an asteroid, but also can be applied to more asteroids, especially those without enough photometric data to solve the convex shape. Using aMarkov ChainMonte Carlo (MCMC) method, Dione's absolutemagnitude of H = 7.66+0.03 -0.03 mag, and phase function parameters G1= 0.682+0.077 -0.077 and G2 = 0.081+0.042 -0.042 were obtained. Simultaneously, Dione's simplistic shape, orientation of pole and rotation period were also determined preliminarily. © 2017 National Astronomical Observatories, CAS and IOP Publishing Ltd."
,10.2166/washdev.2017.118,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029917831&doi=10.2166%2fwashdev.2017.118&partnerID=40&md5=a5b57c34dbb9657b935ef24e1499423f,"Mortality rates of some diseases are affected by water quality. This research examines the roles of two factors related to water quality, namely the quality of drinking water termed ‘water’ and the quality of sanitation termed ‘sanitation’. Two age-related diseases, cardiovascular disease and diabetes (CDD) and chronic respiratory conditions (CRC) are considered while adjusting for personal health issues, environmental and geographical factors. The dataset consists of worldwide mortality rates of adults for the mentioned diseases in 195 countries. These countries are clustered within continents geographically and literature shows the importance of considering the geographical effect of a continent. Furthermore, the two diseases were highly related to each other. Accordingly, the multivariate multilevel model was fitted to the dataset. The results indicated that when the usage of improved drinking water sources and sanitation facilities decreases, the chance of mortality from the two diseases increases. Furthermore, the difference in the risk of the diseases was statistically significant between the continents. It showed that North America and Europe had a lower risk of having CDD and CRC compared to Asia and Oceania. Therefore, the results revealed that the factors ‘water’ and ‘sanitation’ play important roles for this macro geographical variation of CDD and CRC. © IWA Publishing 2017."
1,10.1109/TSIPN.2017.2731160,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049506334&doi=10.1109%2fTSIPN.2017.2731160&partnerID=40&md5=0235fd2ce71281ddb680a4d1408fbee2,"Understanding the process by which a contagion disseminates throughout a network is of great importance in many real-world applications. The required sophistication of the inference approach depends on the type of information we want to extract as well as the number of observations that are available to us. We analyze scenarios in which not only the underlying network structure (parental relationships and link strengths) needs to be detected, but also the infection times must be estimated. We assume that our only observation of the diffusion process is a set of time series, one for each node of the network, which exhibit changepoints when an infection occurs. After formulating a model to describe the contagion, and selecting appropriate prior distributions, we seek to find the set of model parameters that best explains our observations. Modeling the problem in a Bayesian framework, we exploit Monte Carlo Markov Chain, sequential Monte Carlo, and time series analysis techniques to develop batch and online inference algorithms. We evaluate the performance of our proposed algorithms via numerical simulations of synthetic network contagions and analysis of real-world datasets. © 2015 IEEE."
3,10.1017/apr.2017.22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029620205&doi=10.1017%2fapr.2017.22&partnerID=40&md5=7cfd4f9f0d1b8cea1a67115e6f613a45,"Markov chain Monte Carlo (MCMC) methods provide an essential tool in statistics for sampling from complex probability distributions. While the standard approach to MCMC involves constructing discrete-time reversible Markov chains whose transition kernel is obtained via the Metropolis-Hastings algorithm, there has been recent interest in alternative schemes based on piecewise deterministic Markov processes (PDMPs). One such approach is based on the zig-zag process, introduced in Bierkens and Roberts (2016), which proved to provide a highly scalable sampling scheme for sampling in the big data regime; see Bierkens et al. (2016). In this paper we study the performance of the zig-zag sampler, focusing on the one-dimensional case. In particular, we identify conditions under which a central limit theorem holds and characterise the asymptotic variance. Moreover, we study the influence of the switching rate on the diffusivity of the zig-zag process by identifying a diffusion limit as the switching rate tends to. Based on our results we compare the performance of the zig-zag sampler to existing Monte Carlo methods, both analytically and through simulations. © Copyright Applied Probability Trust 2017."
,10.1134/S2070048217050088,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029755683&doi=10.1134%2fS2070048217050088&partnerID=40&md5=58318e79efe6c66857d17f5ad7686154,"The simplest stochastic lattice model of an excitable medium is considered. Each lattice cell can be in one of three states: excited, refractory, or quiescent. Transitions between different cell states occur with the prescribed probabilities. The model is designed for studying the transfer of excitation in the cardiac muscle and nerve fiber at the cellular and subcellular levels, and also for modeling the spreading of epidemics. Elementary events on the lattice are simulated by the kinetic Monte Carlo method, which consists in constructing a Markov chain of the lattice states corresponding to the solution of the master equation. An effective algorithm for implementing the Kinetic Monte Carlo simulations is suggested. The number of the arithmetic operations at each time step of the proposed algorithm is practically independent of the lattice size, which enables making calculations on two- and three-dimensional lattices of a very large size (more than 109 cells). It is shown that the model reproduces the basic spatiotemporal structures (solitary traveling pulses, pulse trains, concentric and spiral waves, and spiral turbulence) characteristic of an excitable medium. The basic properties of traveling pulses and spiral waves for the considered stochastic lattice model are studied and compared with the known properties of deterministic equations of the reaction-diffusion type, which are usually employed for modeling excitable media. © 2017, Pleiades Publishing, Ltd."
,10.1016/j.radonc.2017.08.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028645982&doi=10.1016%2fj.radonc.2017.08.015&partnerID=40&md5=61a7f2d47bc9de353d8bb5ddcd636995,"We propose a Bayesian hierarchical model applicable to the calibration of the linear-quadratic model of radiation dose–response. Experimental data used in model calibration were taken from a clonogenic survival assay conducted on human breast cancer cells (MDA-MB-231) across a range of radiation doses (0–6 Gy). Employing Markov-chain Monte Carlo methods, we calibrated the proposed Bayesian hierarchical model, computed posterior distributions for the model parameters and survival fraction dose–response probability densities. Key contributions include the proposal of a model that incorporates multiple sources of inter- and intra-experiment variability commonly neglected in the standard frequentist approach and its subsequent application to in vitro experimental data. © 2017 Elsevier B.V."
2,10.1007/s40328-016-0183-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027515916&doi=10.1007%2fs40328-016-0183-3&partnerID=40&md5=3e0b37913877e6d916cf924a7ed6348d,"At present, the satellite SAR persistent scatterer interferometry can already estimate surface changes with a near to 1 mm theoretical precision limit. However, the ascending and descending acquisitions of available SAR services cannot provide three-dimensional changes routinely, though the slow deformation processes are basically three-dimensional (3D). In this paper the geometric features of ascending and descending SAR data and possible fusion with geodetic data are summarised. All the geometric equations are introduced, which are necessary to derive the two characteristic changes in the observation plain defined by ascending and descending unit vectors pointing to SAR satellite positions. The unambiguously derivable characteristic changes can be transformed into vertical and east changes, but they may be biased by possible north displacement. The geometric features of symmetric and asymmetric acquisitions are also investigated. Monte-Carlo simulation is used to investigate the precision of two estimated components. It is experienced that the precisions are not sensitive to one degree standard deviations of positional angles. The Gauss–Markov model of least square adjustment method is used to derive only the statistical properties of reasonable data fusion which can contribute to the 3D applications. Although complementary satellites, which are already proposed in the literature, could provide precise autonomous solutions, in the practise GNSS and levelling data can be used for direct data fusion. Whereas, even errorless levelled high changes cannot contribute to the proper estimation of northern components, GNSS derived changes are the best candidates, which can be interpolated or measured directly. Moreover, these two techniques can properly compensate the weaknesses of each other. The interferometric SAR techniques are not sensitive enough to the north changes, but can contribute to the precision of height estimation, which are the weakest components of the GNSS technique. This statement is valid if the standard deviations of combined data are comparable. For test computations the geometric parameters of available Sentinel-1A images are used, which cover the area of the Széchenyi István Geophysical Observatory, where experimental integrated geodetic benchmark is located combining ascending and descending backscatterers with the possibility of the GNSS, gravimetric and traditional geodetic measurements, as well. © 2016, Akadémiai Kiadó."
1,10.1007/s11277-017-4241-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018295822&doi=10.1007%2fs11277-017-4241-0&partnerID=40&md5=06ccb648a3d545fa42697be96105da87,"The Time Slotted Channel Hopping (TSCH) mechanism is created in the IEEE 802.15.4e amendment, to meet the need of Industrial Wireless Sensor Networks. It combines time slotted access and channel hopping with deterministic behavior. The mechanism offers two types of links: dedicated links and shared links. In order to reduce the probability of repeated collisions in shared links, the mechanism implemented a retransmission backoff algorithm, named TSCH Collision Avoidance (TSCH CA). In this article, we develop a two dimensional Markov chain model for the IEEE 802.15.4e TSCH CA mechanism, we take into account the deterministic behavior of this mechanism. In order to evaluate its performances, we estimate the stationary distribution of this chain. Then, we derive theoretical expressions of: collision probability, data packet loss rate, reliability, energy consumption, throughput, delay and jitter. Then, we analyze the impact of the number of devices sharing the link for a fixed network size under different traffic conditions. Finally, the accuracy of our theoretical analysis is validated by Monte Carlo simulation. It is shown that the performances of the IEEE 802.15.4e TSCH parameters are strongly related to the number of devices sharing the link. © 2017, Springer Science+Business Media New York."
4,10.5061/dryad.r6j80,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039042971&doi=10.5061%2fdryad.r6j80&partnerID=40&md5=bc4876ca7015280708d0206f8304caf9,"Individuals in a population vary in their growth due to hidden and observed factors such as age, genetics, environment, disease, and carryover effects from past environments. Because size affects fitness, growth trajectories scale up to affect population dynamics. However, it can be difficult to estimate growth in data fromwild populations with missing observations and observation error. Previous work has shown that linear mixed models (LMMs) underestimate hidden individual heterogeneity when more than 25%of repeatedmeasures are missing. Here we demonstrate a flexible and robust way to model growth trajectories. We show that state-space models (SSMs), fit using R package growmod, are far less biased than LMMs when fit to simulated data sets with missing repeated measures and observation error. This method is much faster than Markov chain Monte Carlo methods, allowing more models to be tested in a shorter time. For the scenarios we simulated, SSMs gave estimates with little bias when up to 87.5% of repeated measuresweremissing. We use thismethod to quantify growth of Soay sheep, using data froma long-termmark-recapture study, and demonstrate that growth decreased with age, population density, weather conditions, and when individuals are reproductive. The method improves our ability to quantify how growth varies among individuals in response to their attributes and the environments they experience, with particular relevance for wild populations. © 2017 by The University of Chicago."
,10.1109/ISCC.2017.8024556,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030535955&doi=10.1109%2fISCC.2017.8024556&partnerID=40&md5=0a30b4b1a77905bb861287f1a811d330,"In this paper we revisit the problem of deriving the expected number of transmissions for multicasting random linear coded (RLC) packets on single-hop wireless channels. We show by deriving the closed form expression for an instance of the problem that previous analytical formulation does not accurately model the true expected number of transmissions, especially for smaller finite field size. Our understanding is that similar to the wireless multi-hop network, the problem of deriving an exact closed form expression for the expected number of transmissions for a single-hop RLC wireless network is a complex open problem. As it is unknown whether a scalable closed form expression for the problem exists, we then propose a computationally efficient Monte Carlo method to derive a good approximation of the expected number of transmissions. © 2017 IEEE."
4,10.1002/jrsm.1236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018588715&doi=10.1002%2fjrsm.1236&partnerID=40&md5=0a424ccc212934ecde3fbe34b6bf3647,"Meta-analysis can necessitate the combination of parallel and cross-over trial designs. Because of the differences in the trial designs and potential biases notably associated with the crossover trials, one often combines trials of the same designs only, which decreases the power of the meta-analysis. To combine results of clinical trials from parallel and cross-over designs, we extend the method proposed in an accompanying study to account for random effects. We propose here a hierarchical mixed model allowing the combination of the 2 types of trial designs and accounting for additional covariates where random effects can be introduced to account for heterogeneity in trial, treatment effect, and interactions. We introduce a multilevel model and a Bayesian hierarchical model for combined trial design meta-analysis. The analysis of the models by restricted iterative generalised least square and Monte Carlo Markov Chain is presented. Methods are compared in a combined design meta-analysis model on salt reduction. Both models and their respective advantages in the perspective of meta-analysis are discussed. However, the access to the trial data, in particular sequence and period data in cross-over trials, remains a major limitation to the meta-analytic combination of trial designs. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1166/jmihi.2017.2137,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027327113&doi=10.1166%2fjmihi.2017.2137&partnerID=40&md5=7c1dd7114ce559e288711eef16581f70,"According to medical image characteristics, an fMCMC algorithm (the fuzzy entropy measurement and Markov Chain and Monte Carlo based medical image segmentation algorithm) is proposed based on stochastic Markov chain model combining with the fuzzy entropy measurement. To address these challenges by taking the complexity and uncertainty of the medical images, the fuzzy entropy edge measurement of curves is designed via fuzzy entropy to describe the features of medical image firstly. Secondly, the random sequence closed curves are generated as Markov chains by shifting probability. Following, the Monte Carlo method is utilized to simulate and accelerate the convergence of the designed image segmentation model. Lastly, the ideal boundary curve is adopted as the closed region edge with maximal edge probability value. The experiment results demonstrate the validity of the presented algorithm. Moreover, it also indicates that the segmentation algorithm has higher ability of anti-noise and can achieve accurate medical image segmentation more quickly and exactly. © Copyright 2017 American Scientific Publishers All rights reserved."
3,10.1007/s12094-017-1647-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016104499&doi=10.1007%2fs12094-017-1647-9&partnerID=40&md5=d25a7843db641122efbac45a73af9a3a,"Purpose: Second-line chemotherapy has been shown to benefit patients with advanced gastric cancer (AGC), extending the overall survival (OS) and progression-free survival (PFS). This study aimed to assess the efficacy and cost-effectiveness of second-line treatment for elderly patients with AGC. Methods: Medical records and follow-up information of elderly patients (≥70 years) with AGC who received second-line chemotherapy were collected. A Markov model comprising three health states PFS, progressive disease and death was developed to simulate the process of AGC. Cost was calculated from the perspective of Chinese society. Sensitivity analyses were applied to explore the impact of essential variables. Results: Forty-three elderly patients with AGC receiving second-line chemotherapy were included in our study. The median OS was 6.0 months (95% confidence interval (CI) 3.90–8.10) and PFS was 3.1 months (95% CI 1.38–4.82). No treatment-related death occurred. The most frequently drug-related grade 3/4 AEs were diarrhea (2.3%), leukopenia (16.3%) and nausea (7.0%). The incremental cost-effective ratio was $18,223.75/QALY for second-line chemotherapy versus BSC, which was below the threshold of 3× the per capita GDP of China, $23,970.00. Conclusion: Second-line chemotherapy was an optimal strategy for elderly AGC patients in China from the efficacy and cost-effectiveness perspective. © 2017, Federación de Sociedades Españolas de Oncología (FESEO)."
8,10.3390/nu9090983,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029168230&doi=10.3390%2fnu9090983&partnerID=40&md5=9e463d08c3ba49f7633fb0748181fb7c,"Interventions targeting portion size and energy density of food and beverage products have been identified as a promising approach for obesity prevention. This study modelled the potential cost-effectiveness of: a package size cap on single-serve sugar sweetened beverages (SSBs) >375 mL (package size cap), and product reformulation to reduce energy content of packaged SSBs (energy reduction). The cost-effectiveness of each intervention was modelled for the 2010 Australia population using a multi-state life table Markov model with a lifetime time horizon. Long-term health outcomes were modelled from calculated changes in body mass index to their impact on Health-Adjusted Life Years (HALYs). Intervention costs were estimated from a limited societal perspective. Cost and health outcomes were discounted at 3%. Total intervention costs estimated in AUD 2010 were AUD 210 million. Both interventions resulted in reduced mean body weight (package size cap: 0.12 kg; energy reduction: 0.23 kg); and HALYs gained (package size cap: 73, 883; energy reduction: 144, 621). Cost offsets were estimated at AUD 750.8 million (package size cap) and AUD 1.4 billion (energy reduction). Cost-effectiveness analyses showed that both interventions were “dominant”, and likely to result in long term cost savings and health benefits. A package size cap and kJ reduction of SSBs are likely to offer excellent “value for money” as obesity prevention measures in Australia. © 2017 by the authors."
4,10.1007/s13253-017-0285-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020114691&doi=10.1007%2fs13253-017-0285-6&partnerID=40&md5=acdd6c5d3dd9d1a42222afff6cab8e59,"When data streams are observed without error and at regular time intervals, discrete-time hidden Markov models (HMMs) have become immensely popular for the analysis of animal location and auxiliary biotelemetry data. However, measurement error and temporally irregular data are often pervasive in telemetry studies, particularly in marine systems. While relatively small amounts of missing data that are missing-completely-at-random are not typically problematic in HMMs, temporal irregularity can result in few (if any) observations aligning with the regular time steps required by HMMs. Fitting HMMs that explicitly account for uncertainty attributable to location measurement error, temporally irregular observations, or other forms of missing data typically requires computationally demanding techniques, such as Markov chain Monte Carlo (MCMC). Using simulation and a real-world bearded seal (Erignathus barbatus) example, I investigate a practical alternative to incorporating measurement error and temporally irregular observations into HMMs based on multiple imputation of the position process drawn from a single-state continuous-time movement model. This two-stage approach is relatively simple, performed with existing software using efficient maximum likelihood methods, and completely parallelizable. I generally found the approach to perform well across a broad range of simulated measurement error and irregular sampling rates, with latent states and locations reliably recovered in nearly all simulated scenarios. However, high measurement error coupled with low sampling rates often induced bias in both the estimated probability distributions of data streams derived from the imputed position process and the estimated effects of spatial covariates on state transition probabilities. Results from the two-stage analysis of the bearded seal data were similar to a more computationally intensive single-stage MCMC analysis, but the two-stage analysis required much less computation time and no custom model-fitting algorithms. I thus found the two-stage multiple-imputation approach to be promising in terms of its ease of implementation, computation time, and performance. Code for implementing the approach using the R package “momentuHMM” is provided. Supplementary materials accompanying this paper appear online. © 2017, The Author(s)."
1,10.1007/s11356-017-9720-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023769939&doi=10.1007%2fs11356-017-9720-z&partnerID=40&md5=209abe02abd9dff5b8e1addb222ec1c1,"The objective of this paper is to provide an efficient framework for effluent trading in river systems. The proposed framework consists of two pessimistic and optimistic decision-making models to increase the executability of river water quality trading programs. The models used for this purpose are (1) stochastic fallback bargaining (SFB) to reach an agreement among wastewater dischargers and (2) stochastic multi-criteria decision-making (SMCDM) to determine the optimal treatment strategy. The Monte-Carlo simulation method is used to incorporate the uncertainty into analysis. This uncertainty arises from stochastic nature and the errors in the calculation of wastewater treatment costs. The results of river water quality simulation model are used as the inputs of models. The proposed models are used in a case study on the Zarjoub River in northern Iran to determine the best solution for the pollution load allocation. The best treatment alternatives selected by each model are imported, as the initial pollution discharge permits, into an optimization model developed for trading of pollution discharge permits among pollutant sources. The results show that the SFB-based water pollution trading approach reduces the costs by US$ 14,834 while providing a relative consensus among pollutant sources. Meanwhile, the SMCDM-based water pollution trading approach reduces the costs by US$ 218,852, but it is less acceptable by pollutant sources. Therefore, it appears that giving due attention to stability, or in other words acceptability of pollution trading programs for all pollutant sources, is an essential element of their success. © 2017, Springer-Verlag GmbH Germany."
3,10.1051/0004-6361/201730587,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028765998&doi=10.1051%2f0004-6361%2f201730587&partnerID=40&md5=6867e5d7b82ac5440caa30e426ad0556,"Context. Current constraints on models of galaxy evolution rely on morphometric catalogs extracted from multi-band photometric surveys. However, these catalogs are altered by selection effects that are difficult to model, that correlate in non trivial ways, and that can lead to contradictory predictions if not taken into account carefully. Aims. To address this issue, we have developed a new approach combining parametric Bayesian indirect likelihood (pBIL) techniques and empirical modeling with realistic image simulations that reproduce a large fraction of these selection effects. This allows us to perform a direct comparison between observed and simulated images and to infer robust constraints on model parameters. Methods. We use a semi-empirical forward model to generate a distribution of mock galaxies from a set of physical parameters. These galaxies are passed through an image simulator reproducing the instrumental characteristics of any survey and are then extracted in the same way as the observed data. The discrepancy between the simulated and observed data is quantified, and minimized with a custom sampling process based on adaptive Markov chain Monte Carlo methods. Results. Using synthetic data matching most of the properties of a Canada-France-Hawaii Telescope Legacy Survey Deep field, we demonstrate the robustness and internal consistency of our approach by inferring the parameters governing the size and luminosity functions and their evolutions for different realistic populations of galaxies. We also compare the results of our approach with those obtained from the classical spectral energy distribution fitting and photometric redshift approach. Conclusions. Our pipeline infers efficiently the luminosity and size distribution and evolution parameters with a very limited number of observables (three photometric bands). When compared to SED fitting based on the same set of observables, our method yields results that are more accurate and free from systematic biases. © ESO, 2017."
7,10.1161/CIRCULATIONAHA.117.027067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023759129&doi=10.1161%2fCIRCULATIONAHA.117.027067&partnerID=40&md5=a006af85ba0936287a9f97cc40d6aad2,"BACKGROUND: Statins are effective in the primary prevention of atherosclerotic cardiovascular disease. The 2013 American College of Cardiology/American Heart Association (ACC/AHA) guideline expands recommended statin use, but its cost-effectiveness has not been compared with other guidelines. METHODS: We used the Cardiovascular Disease Policy Model to estimate the cost-effectiveness of the ACC/AHA guideline relative to current use, Adult Treatment Panel III guidelines, and universal statin use in all men 45 to 74 years of age and women 55 to 74 years of age over a 10-year horizon from 2016 to 2025. Sensitivity analyses varied costs, risks, and benefits. Main outcomes were incremental cost-effectiveness ratios and numbers needed to treat for 10 years per quality-adjusted life-year gained. RESULTS: Each approach produces substantial benefits and net cost savings relative to the status quo. Full adherence to the Adult Treatment Panel III guideline would result in 8.8 million more statin users than the status quo, at a number needed to treat for 10 years per quality-adjusted life-year gained of 35. The ACC/AHA guideline would potentially result in up to 12.3 million more statin users than the Adult Treatment Panel III guideline, with a marginal number needed to treat for 10 years per quality-adjusted life-year gained of 68. Moderate-intensity statin use in all men 45 to 74 years of age and women 55 to 74 years of age would result in 28.9 million more statin users than the ACC/AHA guideline, with a marginal number needed to treat for 10 years per quality-adjusted life-year gained of 108. In all cases, benefits would be greater in men than women. Results vary moderately with different risk thresholds for instituting statins and statin toxicity estimates but depend greatly on the disutility caused by daily medication use (pill burden). CONCLUSIONS: At a population level, the ACC/AHA guideline for expanded statin use for primary prevention is projected to treat more people, to save more lives, and to cost less compared with Adult Treatment Panel III in both men and women. Whether individuals benefit from long-term statin use for primary prevention depends more on the disutility associated with pill burden than their degree of cardiovascular risk. © 2017 American Heart Association, Inc."
1,10.1097/EDE.0000000000000680,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028830774&doi=10.1097%2fEDE.0000000000000680&partnerID=40&md5=9476c0dbf3987eb0065ceaa01bace7fe,[No abstract available]
,10.1097/EDE.0000000000000679,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028866964&doi=10.1097%2fEDE.0000000000000679&partnerID=40&md5=5c739cfd2fb0cf67403c70f8f374a6eb,[No abstract available]
1,10.1007/s10198-016-0851-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001055589&doi=10.1007%2fs10198-016-0851-9&partnerID=40&md5=839c4b986bddbb0e39272be5baf144fa,"Background: Policymakers need to know the cost-effectiveness of interventions to prevent type 2 diabetes (T2D). The objective of this study was to estimate the cost-effectiveness of a T2D prevention initiative targeting weight reduction, increased physical activity and healthier diet in persons in pre-diabetic states by comparing a hypothetical intervention versus no intervention in a Swedish setting. Methods: A Markov model was used to study the cost-effectiveness of a T2D prevention program based on lifestyle change versus a control group where no prevention was applied. Analyses were done deterministically and probabilistically based on Monte Carlo simulation for six different scenarios defined by sex and age groups (30, 50, 70 years). Cost and quality adjusted life year (QALY) differences between no intervention and intervention and incremental cost-effectiveness ratios (ICERs) were estimated and visualized in cost-effectiveness planes (CE planes) and cost-effectiveness acceptability curves (CEA curves). Results: All ICERs were cost-effective and ranged from 3833 €/QALY gained (women, 30 years) to 9215 €/QALY gained (men, 70 years). The CEA curves showed that the probability of the intervention being cost-effective at the threshold value of 50,000 € per QALY gained was very high for all scenarios ranging from 85.0 to 91.1%. Discussion/conclusion: The prevention or the delay of the onset of T2D is feasible and cost-effective. A small investment in healthy lifestyle with change in physical activity and diet together with weight loss are very likely to be cost-effective. © 2016, The Author(s)."
,10.1007/s00180-017-0710-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010973222&doi=10.1007%2fs00180-017-0710-x&partnerID=40&md5=b5f765f1844ed6c8982700163b5861bb,"Frequentist standard errors are a measure of uncertainty of an estimator, and the basis for statistical inferences. Frequestist standard errors can also be derived for Bayes estimators. However, except in special cases, the computation of the standard error of Bayesian estimators requires bootstrapping, which in combination with Markov chain Monte Carlo can be highly time consuming. We discuss an alternative approach for computing frequentist standard errors of Bayesian estimators, including importance sampling. Through several numerical examples we show that our approach can be much more computationally efficient than the standard bootstrap. © 2017, Springer-Verlag Berlin Heidelberg."
,10.4103/jrms.JRMS_926_16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035083631&doi=10.4103%2fjrms.JRMS_926_16&partnerID=40&md5=c6ea82589be869fb679584050e606a29,"Background: In this study, we aimed to determine comprehensive maternal characteristics associated with birth weight using Bayesian modeling. Materials and Methods: A total of 526 participants were included in this prospective study. Nutritional status, supplement consumption during the pregnancy, demographic and socioeconomic characteristics, anthropometric measures, physical activity, and pregnancy outcomes were considered as effective variables on the birth weight. Bayesian approach of complex statistical models using Markov chain Monte Carlo approach was used for modeling the data considering the real distribution of the response variable. Results: There was strong positive correlation between infant birth weight and the maternal intake of Vitamin C, folic acid, Vitamin B3, Vitamin A, selenium, calcium, iron, phosphorus, potassium, magnesium as micronutrients, and fiber and protein as macronutrients based on the 95% high posterior density regions for parameters in the Bayesian model. None of the maternal characteristics had statistical association with birth weight. Conclusion: Higher maternal macro- and micro-nutrient intake during pregnancy was associated with a lower risk of delivering low birth weight infants. These findings support recommendations to expand intake of nutrients during pregnancy to high level. © 2017 Journal of Research in Medical Sciences."
,10.5351/CSAM.2017.24.5.457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044064924&doi=10.5351%2fCSAM.2017.24.5.457&partnerID=40&md5=d812694681f0195ca8c7385420b7c476,"We develop a random partition procedure based on a Dirichlet process prior with Laplace distribution. Gibbs sampling of a Laplace mixture of linear mixed regressions with a Dirichlet process is implemented as a random partition model when the number of clusters is unknown. Our approach provides simultaneous partitioning and parameter estimation with the computation of classification probabilities, unlike its counterparts. A full Gibbssampling algorithm is developed for an efficient Markov chain Monte Carlo posterior computation. The proposed method is illustrated with simulated data and one real data of the energy efficiency of Tsanas and Xifara (Energy and Buildings, 49, 560-567, 2012). © 2017 The Korean Statistical Society, and Korean International Statistical Society."
2,10.1007/s11222-016-9681-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978877426&doi=10.1007%2fs11222-016-9681-y&partnerID=40&md5=77c230c65f9dfca4bb75e0293383aa15,"Latent feature models are a powerful tool for modeling data with globally-shared features. Nonparametric distributions over exchangeable sets of features, such as the Indian Buffet Process, offer modeling flexibility by letting the number of latent features be unbounded. However, current models impose implicit distributions over the number of latent features per data point, and these implicit distributions may not match our knowledge about the data. In this work, we demonstrate how the restricted Indian buffet process circumvents this restriction, allowing arbitrary distributions over the number of features in an observation. We discuss several alternative constructions of the model and apply the insights to develop Markov Chain Monte Carlo and variational methods for simulation and posterior inference. © 2016, Springer Science+Business Media New York."
,10.1002/cjs.11318,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018613525&doi=10.1002%2fcjs.11318&partnerID=40&md5=2ec5696a8f0ff2d4e93166cb4f65fc9a,"This work proposes a Bayesian approach for the analysis of a semiparametric density ratio model, a model useful for the integration of data from multiple sources. The proposed Bayesian analysis uses a non-parametric likelihood and a transformed Gaussian prior for the “non-parametric part” of the model. The former choice guarantees the validity of the Bayesian analysis in contrast to some semiparametric Bayesian analyses that rely on empirical likelihoods whereas the latter choice allows the representation of an expected smoothness property. We describe a Markov chain Monte Carlo algorithm to fit this model which was found to empirically display good convergence behaviour. The model is illustrated with the analysis of motor vibration data obtained from three different locations on a motor. The Canadian Journal of Statistics 45: 274–289; 2017 © 2017 Statistical Society of Canada. © 2017 Statistical Society of Canada"
1,10.1142/S2010132517500213,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023205841&doi=10.1142%2fS2010132517500213&partnerID=40&md5=4dc8d33b3da850373cb798dea64102ec,"The number of occupants in a space can significantly affect ventilation control. Using neural network and Bayesian Markov chain Monte Carlo (MCMC) methods, this study estimates the number of occupants based on CO2 concentration in a room. The abilities of both methods to recognize the input-parameter characteristics are compared under certain circumstances, and the parameters are optimized to improve the estimation accuracy. The neural network trains an input dataset of CO2 concentrations, ventilation rates, and occupancy patterns with tapped delay lines. Meanwhile, the Bayesian MCMC calculates the given CO2 data by a mathematical model based on a statistical approach. The present space model is a single-office room in which the CO2 concentration is determined through several simulation schemes and experiments. The estimation accuracy of the neural network depends on the complexity of the input parameters (i.e., CO2 concentration and ventilation rate), whereas the Bayesian MCMC is influenced by uncertainty in the CO2 concentration. Both methods produce acceptable estimates under certain treatments. © 2017 World Scientific Publishing Company."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030033071&partnerID=40&md5=1a78d6931253767836f7e1e874e83402,"Statistical modeling and analysis of social networks has evolved into an important area of research related to the complex relationships among social entities. In this paper, a new monitoring method based on Exponential Random Graph Model (ERGM) is presented and control charts are used to detect out-of-control states in networks. The Markov Chain Monte Carlo (MCMC) algorithm is used for parameter estimation purposes. A monitoring statistics based on the Generalized Likelihood Ratio Test (GLRT) is applied to detect departures to anomalies in networks. A case study in social network is used to depict the properties and benefits of the proposed methodology, and the results show that GLR chart has better performance in average run length (ARL) as compared to the CUSUM chart in detecting large shifts, while the CUSUM chart is better for detecting small shifts."
,10.4169/college.math.j.48.4.265,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033473938&doi=10.4169%2fcollege.math.j.48.4.265&partnerID=40&md5=27a7c79f13e764f8bd4a9a97554a9a7b,"Learning about probability can be hard and frustrating for many students. However, learning about probability through examples with board games can make this task more interesting and fun. We present a sequence of increasingly difficult probability problems derived from the popular board game Carcassonne. Each question is appropriate either for a college classroom or for undergraduate research, with topics including basic counting problems, expected value, Bayes’s theorem, Markov chains, and Monte Carlo simulation. Some problems have solutions, but other questions are left open for the reader to explore. © THE MATHEMATICAL ASSOCIATION OF AMERICA."
1,10.1002/rra.3162,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019868728&doi=10.1002%2frra.3162&partnerID=40&md5=fa643fe1b8df0fcfe3a34f28052a4a73,"Warm or cool ambient temperatures during winter generate plasticity in the within-year timing of oviposition or breeding phenology shifts through the use of a reaction norm in many riverine ectotherms, but within-year timing of oviposition in Hynobius kimurae (Caudata: Hynobiidae) is predicted to be constant at different water temperatures of a mountain stream. To clarify this difference, by experimentally controlling water temperatures from fall to spring and thus changing durations of aquatic hibernation and dates of oviposition, we determined whether oviposition is more advanced when water temperatures increase earlier than usual or is more delayed when water temperatures increase later than usual. Oviposition was more delayed than usual with earlier exposure to high water temperatures during spring and was earlier than usual with exposing to higher water temperatures during hibernation. Days from submergence to oviposition were responsive to cumulative daily water temperatures from submergence to oviposition within a population. A rate of increase in days from submergence to oviposition was different between populations but was constant within a population even if minimal water temperatures varied each year. When running a program of a Markov chain Monte Carlo generalized linear mixed model for Bayesian computation to interpret confounding effects, neither different treatments nor different years statistically affected days from submergence to oviposition. These results suggest that a biological clock exits for days from submergence to oviposition in H. kimurae, which could allow survival in situations with either warmer or cooler environments. Copyright © 2017 John Wiley & Sons, Ltd."
4,10.3847/1538-4357/aa845e,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029066981&doi=10.3847%2f1538-4357%2faa845e&partnerID=40&md5=8aeab2056ceb9ca47c8d54fc94fbd029,"Dwarf galaxies are known to have remarkably low star formation efficiency due to strong feedback. Adopting the dwarf galaxies of the Milky Way (MW) as a laboratory, we explore a flexible semi-analytic galaxy formation model to understand how the feedback processes shape the satellite galaxies of the MW. Using Markov Chain Monte Carlo, we exhaustively search a large parameter space of the model and rigorously show that the general wisdom of strong outflows as the primary feedback mechanism cannot simultaneously explain the stellar mass function and the mass-metallicity relation of the MW satellites. An extended model that assumes that a fraction of baryons is prevented from collapsing into low-mass halos in the first place can be accurately constrained to simultaneously reproduce those observations. The inference suggests that two different physical mechanisms are needed to explain the two different data sets. In particular, moderate outflows with weak halo mass dependence are needed to explain the mass-metallicity relation, and prevention of baryons falling into shallow gravitational potentials of low-mass halos (e.g., ""pre-heating"") is needed to explain the low stellar mass fraction for a given subhalo mass. © 2017. The American Astronomical Society. All rights reserved."
2,10.1111/ppa.12666,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026772438&doi=10.1111%2fppa.12666&partnerID=40&md5=2098cb4679ac93219e5514e118c8dd0c,"Phytophthora sojae is a destructive soilborne pathogen causing seedling damping-off and root rot of soybean (Glycine max). The goal of this study was to determine the genetic structure of P. sojae populations in Fujian, China. Nine microsatellite markers were used to investigate the genetic variation in 19 P. sojae populations, sampled from Fujian Province and northeastern China (Jilin and Heilongjiang Provinces) between 2002 and 2013. Overall, a low genetic diversity, Hardy–Weinberg disequilibrium, and an (Formula presented.) index (an index of association) that was significantly different from zero were detected in populations; these results were consistent with self-fertilization and clonal modes of reproduction for this pathogen. However, using Bayesian Markov chain Monte Carlo approach, principal component analysis and neighbour joining (NJ) algorithm, the Fujian P. sojae populations clustered into three distinct groups, one of which included most isolates of the northeast populations. What is more, significant estimates of pairwise fixation indices (FST) were detected between most populations, especially in different clusters. It is hypothesized that the cropping system used, the limited dispersal ability, and human-mediated gene flow may account for the observed genetic structure of P. sojae populations in Fujian, China. In addition, a high virulence frequency of the pathogen on different cultivars carrying known major R genes for resistance, and a rapid increase in virulence frequency, indicated that these major R genes should not be used to manage seedling damping-off and root rot diseases of soybean (Glycine max). © 2017 British Society for Plant Pathology"
,10.1051/0004-6361/201527852e,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029756704&doi=10.1051%2f0004-6361%2f201527852e&partnerID=40&md5=4a2a8348bf02757f17f6a6d88e3638c0,"Ghelfi et al. (2016) performed a global analysis of top-of-atmosphere data (with the recent PAMELA, BESS, and AMS-02 data) to obtain H and He interstellar (IS) fluxes and their uncertainties. A simple parametric formula was provided for the IS fluxes (shown in their Fig. 5) because they are of interest for a wide range of astrophysics problems. However, the parametric form of Eq. (8) contains an error and all coeficients c12 and c0 in Table 3 are incorrect. The new Eq. (1) and Table 1 below replace Eq. (8) and Table 3 of Ghelfi et al.(Formula Presented). (2016): (Formula Presented). As discussed in Ghelfi et al. (2016), the first five columns of Table 1 are coeficients for the median and 1- and 2 credible intervals from the Markov chain Monte Carlo (MCMC) analysis (without Voyager data), to be considered as a high estimate of the H and He IS fluxes (not valid below 400 MeV/n). The last column provides the best-fit including Voyager data (assumed insterstellar), to be considered as a low estimate of the IS H and He fluxes. (Table Presented). © ESO 2017."
3,10.1016/j.dark.2017.07.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027236190&doi=10.1016%2fj.dark.2017.07.003&partnerID=40&md5=999bd8dbba9c41a7127df3be7cacc62d,"We extend an alternative, phenomenological approach to inflation by means of an equation of state and a sound speed, both of them functions of the number of e-folds and four phenomenological parameters. This approach captures a number of possible inflationary models, including those with non-canonical kinetic terms or scale-dependent non-gaussianities. We perform Markov Chain Monte Carlo analyses using the latest cosmological publicly available measurements, which include Cosmic Microwave Background (CMB) data from the Planck satellite. Within this parameterization, we discard scale invariance with a significance of about 10σ, and the running of the spectral index is constrained as αs=−0.60−0.10 +0.08×10−3 (68% CL errors). The limit on the tensor-to-scalar ratio is r&lt;0.005 at 95% CL from CMB data alone. We find no significant evidence for this alternative parameterization with present cosmological observations. The maximum amplitude of the equilateral non-gaussianity that we obtain, |fNL equil|&lt;1, is much smaller than the current Planck mission errors, strengthening the case for future high-redshift, all-sky surveys, which could reach the required accuracy on equilateral non-gaussianities. © 2017 Elsevier B.V."
4,10.1214/17-AOAS1027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031498647&doi=10.1214%2f17-AOAS1027&partnerID=40&md5=75622bbc8fd91e301ca57d5d5fc03b04,"The gravitational field of a galaxy can act as a lens and deflect the light emitted by a more distant object such as a quasar. Strong gravitational lensing causes multiple images of the same quasar to appear in the sky. Since the light in each gravitationally lensed image traverses a different path length from the quasar to the Earth, fluctuations in the source brightness are observed in the several images at different times. The time delay between these fluctuations can be used to constrain cosmological parameters and can be inferred from the time series of brightness data or light curves of each image. To estimate the time delay, we construct a model based on a state-space representation for irregularly observed time series generated by a latent continuous-time Ornstein–Uhlenbeck process. We account for microlensing, an additional source of independent long-term extrinsic variability, via a polynomial regression. Our Bayesian strategy adopts a Metropolis–Hastings within Gibbs sampler. We improve the sampler by using an ancillarity-sufficiency interweaving strategy and adaptive Markov chain Monte Carlo. We introduce a profile likelihood of the time delay as an approximation of its marginal posterior distribution. The Bayesian and profile likelihood approaches complement each other, producing almost identical results; the Bayesian method is more principled but the profile likelihood is simpler to implement. We demonstrate our estimation strategy using simulated data of doubly- and quadruply-lensed quasars, and observed data from quasars Q0957+561 and J1029+2623. © Institute of Mathematical Statistics, 2017."
13,10.1093/sysbio/syw119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019776034&doi=10.1093%2fsysbio%2fsyw119&partnerID=40&md5=034ae7b8afcf75b179a15a06ed1c8598,"We develop a Bayesian method for inferring the species phylogeny under the multispecies coalescent (MSC) model. To improve the mixing properties of the Markov chain Monte Carlo (MCMC) algorithm that traverses the space of species trees, we implement two efficient MCMC proposals: The first is based on the Subtree Pruning and Regrafting (SPR) algorithm and the second is based on a node-slider algorithm. Like the Nearest-Neighbor Interchange (NNI) algorithm we implemented previously, both new algorithms propose changes to the species tree, while simultaneously altering the gene trees at multiple genetic loci to automatically avoid conflicts with the newly proposed species tree. The method integrates over gene trees, naturally taking account of the uncertainty of gene tree topology and branch lengths given the sequence data. A simulation study was performed to examine the statistical properties of the new method. The method was found to show excellent statistical performance, inferring the correct species tree with near certainty when 10 loci were included in the dataset. The prior on species trees has some impact, particularly for small numbers of loci. We analyzed several previously published datasets (both real and simulated) for rattlesnakes and Philippine shrews, in comparison with alternative methods. The results suggest that the Bayesian coalescent-based method is statistically more efficient than heuristic methods based on summary statistics, and that our implementation is computationally more efficient than alternative full-likelihood methods under theMSC. Parameter estimates for the rattlesnake data suggest drastically different evolutionary dynamics between the nuclear and mitochondrial loci, even though they support largely consistent species trees. We discuss the different challenges facing the marginal likelihood calculation and transmodel MCMC as alternative strategies for estimating posterior probabilities for species trees. © The Author(s) 2017."
7,10.1371/journal.pcbi.1005697,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030468714&doi=10.1371%2fjournal.pcbi.1005697&partnerID=40&md5=65ea16fcbc9935a25a7dec6b33744f25,"Heterogeneities in contact networks have a major effect in determining whether a pathogen can become epidemic or persist at endemic levels. Epidemic models that determine which interventions can successfully prevent an outbreak need to account for social structure and mixing patterns. Contact patterns vary across age and locations (e.g. home, work, and school), and including them as predictors in transmission dynamic models of pathogens that spread socially will improve the models’ realism. Data from population-based contact diaries in eight European countries from the POLYMOD study were projected to 144 other countries using a Bayesian hierarchical model that estimated the proclivity of age-and-location-specific contact patterns for the countries, using Markov chain Monte Carlo simulation. Household level data from the Demographic and Health Surveys for nine lower-income countries and socio-demographic factors from several on-line databases for 152 countries were used to quantify similarity of countries to estimate contact patterns in the home, work, school and other locations for countries for which no contact data are available, accounting for demographic structure, household structure where known, and a variety of metrics including workforce participation and school enrolment. Contacts are highly assortative with age across all countries considered, but pronounced regional differences in the age-specific contacts at home were noticeable, with more inter-generational contacts in Asian countries than in other settings. Moreover, there were variations in contact patterns by location, with work-place contacts being least assortative. These variations led to differences in the effect of social distancing measures in an age structured epidemic model. Contacts have an important role in transmission dynamic models that use contact rates to characterize the spread of contact-transmissible diseases. This study provides estimates of mixing patterns for societies for which contact data such as POLYMOD are not yet available. © 2017 Prem et al."
1,10.1016/j.fishres.2017.04.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018352174&doi=10.1016%2fj.fishres.2017.04.004&partnerID=40&md5=486d42fe89ffc84ff59f0740d0c2746a,"Age and growth estimates based on growth band counts of sectioned vertebrae have been produced for longnose skate (Raja rhina) and big skate (Beringraja binoculata [formerly Raja binoculata]) populations in the Gulf of Alaska, British Columbia and California. Previous growth studies involving estimates from different laboratories in the USA (Alaska Fisheries Science Center, AFSC; Pacific Shark Research Center, PSRC at Moss Landing Marine Laboratories) and Canada (Fisheries and Oceans Canada, DFO) have produced dissimilar results for either species, highlighting the need for development of a consistent age determination protocol and more importantly an age validation study. Archived large specimens of longnose skate and big skate collected in Monterey Bay, CA, in 1980 and 1981 had minimum preliminary age estimates from vertebral growth band counts old enough to suggest that radiocarbon (14C) signals from bomb testing conducted in the late-1960s could be used to establish dates of growth band formation. To this end, we micro-milled skate vertebral thin sections, measured Δ14C using mass spectrometry, and estimated year of growth band formation based on the estimated age from growth band counts using both unstained and stained preparation methods Non-linear random effects modeling, implemented in a Markov Chain Monte Carlo (MCMC) simulation, was used to compare the skate Δ14C data set to a marine teleost otolith reference chronology for the California Current System. Results showed Δ14C measurements for big skate were non-informative as none of the archived samples were old enough for comparison to the reference curve, hence validation of the age estimation approach was not possible. However, for longnose skate, Δ14C data were more informative to fit pulse function models and compare results to the reference chronology. Modeling results indicated longnose skate age estimates based on unstained vertebral thin sections were less biased (overestimated to a smaller degree) than estimates based on stained vertebral thin sections. The degree of bias depended on agency ageing criteria, with the least biased age estimates produced by age readers from the AFSC. The AFSC age estimates had about a 70% probability that age estimates of longnose skate was within +/− 2 years of the expected age based on the radiocarbon assays. We were able to validate the age estimation methodology for longnose skate and establish criteria for growth band counts, which should now be useful to generate region-specific accurate growth and life history parameters required for more reliable stock assessment approaches. © 2017 Elsevier B.V."
,10.1007/s10958-017-3498-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026833638&doi=10.1007%2fs10958-017-3498-x&partnerID=40&md5=096e1dd15a02c7511cfe27665625e72c,"We consider a system of two independent alternating renewal processes with states 0 and 1 and an initial shift t0 of one process relative to the other one. An integral equation with respect to the expectation of time T (the first time when both processes have state 0) is derived. To derive this equation, we use the method of so-called minimal chains of overlapping 1-intervals. Such a chain generates some breaking semi-Markov process of intervals composing the interval (0, T). A solution of the integral equation is obtained for the case where the lengths of 1-intervals have exponential distributions and lengths of 0-intervals have arbitrary distributions. For more general distributions of 1-intervals, the Monte Carlo method is applied when both processes are simulated numerically by a computer. A histogram for estimates of the expectation of T as a function of t0 is demonstrated. Bibliography: 4 titles. © 2017, Springer Science+Business Media, LLC."
4,10.1016/j.enggeo.2017.06.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021456942&doi=10.1016%2fj.enggeo.2017.06.014&partnerID=40&md5=c0e36b5f71d41cfc9293841d05a16de5,"One of the essential tasks in the excavation of tunnels with TBM is the reliable estimation of its performance needed for the planning, cost control and other decision making on the feasibility of the tunneling project. The current study aims at predicting the rate of penetration (RoP) of TBM on the basis of the rock mass parameters including the uniaxial compressive strength (UCS), intact rock brittleness (BI), the angle between the plane of weakness and the TBM driven direction (α) and the distance between planes of weakness (DPW). To this end, datasets from the Queens Water Tunnel No. 3 project, New York City, are compiled and used to establish the models. The Bayesian inference approach is implemented to identify the most appropriate models for estimating the RoP among eight (8) candidate models that have been proposed. The selected TBM empirical models are fitted to field data. The unknown parameters of the models are considered as random variables. The WinBUGS software which uses Bayesian analysis of complex statistical models and Markov chain Monte Carlo (MCMC) techniques is employed to compute the posterior predictive distributions. The mean values of the model parameters obtained via MCMC simulations are considered for the model prediction performance evaluation. Meanwhile, the deviance information criterion (DIC) is used as the main prediction accuracy indicator and therefore, to rank the models taking into account both their fit and complexity. Overall, the results indicate that the proposed RoP model possesses satisfactory predictive performance. © 2017 Elsevier B.V."
,10.1109/SNPD.2017.8022746,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030845149&doi=10.1109%2fSNPD.2017.8022746&partnerID=40&md5=e01a1e86249a4c2c4d1abea1f166bc02,"Latent Dirichlet allocation (LDA) based topic inference is a data classification method, that is used efficiently for extremely large data sets. However, the processing time is very large due to the serial computational behavior of the Markov Chain Monte Carlo method used for the topic inference. We propose a pipelined hardware architecture and memory allocation scheme to accelerate LDA using parallel processing. The proposed architecture is implemented on a reconfigurable hardware called FPGA (field programmable gate array), using OpenCL design environment. According to the experimental results, we achieved maximum speed-up of 2.38 times, while maintaining the same quality compared to the conventional CPU-based implementation. © 2017 IEEE."
,10.11743/ogg20170419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029852717&doi=10.11743%2fogg20170419&partnerID=40&md5=b0e6c645d5e01f62dbc582aace67f921,"Carbonate karst reservoirs of the Lower-Middle Ordovician in Tahe oilfield feature in large burial depth, irregular distribution, complex reservoir types and strong heterogeneity, thus reservoir type identification is always a challenging during oil and gas exploration and development in the area. This paper documented a new impedance inversion method, i. e. waveform-indication-based inversion, based on Bayesian discriminant theory and Markov Chain Monte Carlo sampling algorithm. The method, based on reservoir type identification, seismic reflection characteristics and petrophysical analyses, can be used to optimize sample wells by observing seismic waveform similarities, and establish an initial impedance model by referring to sampling spacing and curve characteristics. The relationship between proposed distribution and prior information is firstly established to achieve efficient sampling of a priori solution space. A Metropolis-Hastings sampling algorithm is then used to sample the posterior probability distribution, so as to obtain the maximum posterior probability solution. The application of the method in identifying the types of the Ordovician karst reservoirs in Tahe Oilfield shows significant improvement of inversion accuracy and a better utilization of horizontal variation of seismic waveforms to reveal effectively the types and spatial and lateral distribution of the reservoirs, and provides fine characterization of the reservoirs. © 2017, Editorial Office of Oil and Gas Geology. All right reserved."
3,10.1109/ACCESS.2017.2746141,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028697217&doi=10.1109%2fACCESS.2017.2746141&partnerID=40&md5=2e1cb392925e5a2a0a68623b22f77f5b,"In this paper, we propose a novel probabilistic localization approach that relies on a Metropolis-Hastings (MH) algorithm-based Bayesian approach to visible light communication (VLC) systems. Due to the usage of the MH algorithm from Markov chain Monte Carlo methods, the positioning capability of the proposed approach becomes more robust against varying channel propagation conditions and measurement uncertainties. The validity of the proposed approach is demonstrated by numerical analyses based on simulations in 3-D indoor environments in a comparative manner with the least square (LS) and the differential LS algorithms-based localization solutions, while circumventing the shortcomings of LS-based approaches. Addressing the short range challenge in the VLC-based positioning system, an efficient hybrid localization framework is also developed for multi-tier heterogeneous networks (HetNets), jointly considering VLC and radio frequency networks. Our methodology mainly considers independent positioning solution branches that each estimate the target location by utilizing the MH-based Bayesian approach. Based on simulation results, the proposed framework for multi-tier HetNets provides a robust performance. Overall, we show that with the new VLC localization scheme, the performance in the short range is enhanced, while with HetNets the effectiveness of the localization in the long range is improved. © 2013 IEEE."
,10.1186/s12711-017-0339-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028339369&doi=10.1186%2fs12711-017-0339-9&partnerID=40&md5=1ef81b83b23a78e34af35b009dc4d56d,"Background: The rapid adoption of genomic selection is due to two key factors: availability of both high-throughput dense genotyping and statistical methods to estimate and predict breeding values. The development of such methods is still ongoing and, so far, there is no consensus on the best approach. Currently, the linear and non-linear methods for genomic prediction (GP) are treated as distinct approaches. The aim of this study was to evaluate the implementation of an iterative method (called GBC) that incorporates aspects of both linear [genomic-best linear unbiased prediction (G-BLUP)] and non-linear (Bayes-C) methods for GP. The iterative nature of GBC makes it less computationally demanding similar to other non-Markov chain Monte Carlo (MCMC) approaches. However, as a Bayesian method, GBC differs from both MCMC- and non-MCMC-based methods by combining some aspects of G-BLUP and Bayes-C methods for GP. Its relative performance was compared to those of G-BLUP and Bayes-C. Methods: We used an imputed 50 K single-nucleotide polymorphism (SNP) dataset based on the Illumina Bovine50K BeadChip, which included 48,249 SNPs and 3244 records. Daughter yield deviations for somatic cell count, fat yield, milk yield, and protein yield were used as response variables. Results: GBC was frequently (marginally) superior to G-BLUP and Bayes-C in terms of prediction accuracy and was significantly better than G-BLUP only for fat yield. On average across the four traits, GBC yielded a 0.009 and 0.006 increase in prediction accuracy over G-BLUP and Bayes-C, respectively. Computationally, GBC was very much faster than Bayes-C and similar to G-BLUP. Conclusions: Our results show that incorporating some aspects of G-BLUP and Bayes-C in a single model can improve accuracy of GP over the commonly used method: G-BLUP. Generally, GBC did not statistically perform better than G-BLUP and Bayes-C, probably due to the close relationships between reference and validation individuals. Nevertheless, it is a flexible tool, in the sense, that it simultaneously incorporates some aspects of linear and non-linear models for GP, thereby exploiting family relationships while also accounting for linkage disequilibrium between SNPs and genes with large effects. The application of GBC in GP merits further exploration. © 2017 The Author(s)."
,10.1103/PhysRevE.96.022413,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028653166&doi=10.1103%2fPhysRevE.96.022413&partnerID=40&md5=5b851eaec798c7ecd547869871a4acc3,"Rapid experimental advances now enable simultaneous electrophysiological recording of neural activity at single-cell resolution across large regions of the nervous system. Models of this neural network activity will necessarily increase in size and complexity, thus increasing the computational cost of simulating them and the challenge of analyzing them. Here we present a method to approximate the activity and firing statistics of a general firing rate network model (of the Wilson-Cowan type) subject to noisy correlated background inputs. The method requires solving a system of transcendental equations and is fast compared to Monte Carlo simulations of coupled stochastic differential equations. We implement the method with several examples of coupled neural networks and show that the results are quantitatively accurate even with moderate coupling strengths and an appreciable amount of heterogeneity in many parameters. This work should be useful for investigating how various neural attributes qualitatively affect the spiking statistics of coupled neural networks. © 2017 American Physical Society."
2,10.7554/eLife.25062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029232429&doi=10.7554%2feLife.25062&partnerID=40&md5=d445c7ea41522d09cf49d1f1a8deac76,"Background: Exercise-induced cognitive improvements have traditionally been observed following aerobic exercise interventions; that is, sustained sessions of moderate intensity. Here, we tested the effect of a 6 week high-intensity training (HIT) regimen on measures of cognitive control and working memory in a multicenter, randomized (1:1 allocation), placebocontrolled trial. Methods: 318 children aged 7-13 years were randomly assigned to a HIT or an active control group matched for enjoyment and motivation. In the primary analysis, we compared improvements on six cognitive tasks representing two cognitive constructs (N = 305). Secondary outcomes included genetic data and physiological measurements. Results: The 6-week HIT regimen resulted in improvements on measures of cognitive control [BFM = 3.38, g = 0.31 (0.09, 0.54)] and working memory [BFM = 5233.68, g = 0.54 (0.31, 0.77)], moderated by BDNF genotype, with met66 carriers showing larger gains post-exercise than val66 homozygotes. Conclusion: This study suggests a promising alternative to enhance cognition, via short and potent exercise regimens. Clinical Trial Registration: Protocol #015078, University of Auckland. © Moreau et al."
3,10.1016/j.tecto.2017.04.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019374596&doi=10.1016%2fj.tecto.2017.04.027&partnerID=40&md5=fd143279f470f264c055155154e31c24,"Faults are one of the building-blocks for subsurface modeling studies. Incomplete observations of subsurface fault networks lead to uncertainty pertaining to location, geometry and existence of faults. In practice, gaps in incomplete fault network observations are filled based on tectonic knowledge and interpreter's intuition pertaining to fault relationships. Modeling fault network uncertainty with realistic models that represent tectonic knowledge is still a challenge. Although methods that address specific sources of fault network uncertainty and complexities of fault modeling exists, a unifying framework is still lacking. In this paper, we propose a rigorous approach to quantify fault network uncertainty. Fault pattern and intensity information are expressed by means of a marked point process, marked Strauss point process. Fault network information is constrained to fault surface observations (complete or partial) within a Bayesian framework. A structural prior model is defined to quantitatively express fault patterns, geometries and relationships within the Bayesian framework. Structural relationships between faults, in particular fault abutting relations, are represented with a level-set based approach. A Markov Chain Monte Carlo sampler is used to sample posterior fault network realizations that reflect tectonic knowledge and honor fault observations. We apply the methodology to a field study from Nankai Trough & Kumano Basin. The target for uncertainty quantification is a deep site with attenuated seismic data with only partially visible faults and many faults missing from the survey or interpretation. A structural prior model is built from shallow analog sites that are believed to have undergone similar tectonics compared to the site of study. Fault network uncertainty for the field is quantified with fault network realizations that are conditioned to structural rules, tectonic information and partially observed fault surfaces. We show the proposed methodology generates realistic fault network models conditioned to data and a conceptual model of the underlying tectonics. © 2017 Elsevier B.V."
,10.1103/PhysRevE.96.022410,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028713802&doi=10.1103%2fPhysRevE.96.022410&partnerID=40&md5=bd3974e4b8c948b31e189fe31e616419,"Mechanosensitive channels are ion channels which act as cells' safety valves, opening when the osmotic pressure becomes too high and making cells avoid damage by releasing ions. They are found on the cellular membrane of a large number of organisms. They interact with each other by means of deformations they induce in the membrane. We show that collective dynamics arising from the interchannel interactions lead to first- and second-order phase transitions in the fraction of open channels in equilibrium relating to the formation of channel clusters. We show that this results in a considerable delay of the response of cells to osmotic shocks, and to an extreme cell-to-cell stochastic variations in their response times, despite the large numbers of channels present in each cell. We discuss how our results are relevant for E. coli. © 2017 American Physical Society."
1,10.3847/1538-4357/aa81ca,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028760046&doi=10.3847%2f1538-4357%2faa81ca&partnerID=40&md5=075e9ebf8dec38d358688cba6aa26912,"We present the first kinematic study of extraplanar diffuse ionized gas (eDIG) in the nearby, face-on disk galaxy M83 using optical emission-line spectroscopy from the Robert Stobie Spectrograph on the Southern African Large Telescope. We use a Markov Chain Monte Carlo method to decompose the [N ii] 6548, 6583, Hα, and [S ii] 6717, 6731 emission lines into H ii region and diffuse ionized gas emission. Extraplanar, diffuse gas is distinguished by its emission-line ratios ([N ii]λ6583/Hα ) and its rotational velocity lag with respect to the disk ( km s-1 in projection). With interesting implications for isotropy, the velocity dispersion of the diffuse gas, km s-1, is a factor of a few higher in M83 than in the Milky Way and nearby, edge-on disk galaxies. The turbulent pressure gradient is sufficient to support the eDIG layer in dynamical equilibrium at an electron scale height of kpc. However, this dynamical equilibrium model must be finely tuned to reproduce the rotational velocity lag. There is evidence of local bulk flows near star-forming regions in the disk, suggesting that the dynamical state of the gas may be intermediate between a dynamical equilibrium and a galactic fountain flow. As one of the first efforts to study eDIG kinematics in a face-on galaxy, this study demonstrates the feasibility of characterizing the radial distribution, bulk velocities, and vertical velocity dispersions in low-inclination systems. © 2017. The American Astronomical Society. All rights reserved.."
13,10.1146/annurev-astro-082214-122339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027980732&doi=10.1146%2fannurev-astro-082214-122339&partnerID=40&md5=ad94b5c83c4b8efda559938de48d68bf,"Markov chain Monte Carlo-based Bayesian data analysis has now become the method of choice for analyzing and interpreting data in almost all disciplines of science. In astronomy, over the past decade, we have also seen a steady increase in the number of papers that employ Monte Carlo-based Bayesian analysis. New, efficient Monte Carlo-based methods are continuously being developed and explored. In this review, we first explain the basics of Bayesian theory and discuss how to set up data analysis problems within this framework. Next, we provide an overview of various Monte Carlo-based methods for performing Bayesian data analysis. Finally, we discuss advanced ideas that enable us to tackle complex problems and thus hold great promise for the future. We also distribute downloadable computer software (https:github.comsanjibsbmcmc) Python that implements some of the algorithms and examples discussed here. © Copyright 2017 by Annual Reviews. All rights reserved."
,10.1016/j.jsv.2017.05.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019689912&doi=10.1016%2fj.jsv.2017.05.005&partnerID=40&md5=2027e16af0a990b88c1fc9dfcc9d7f02,"The dynamic properties of viscoelastic materials show highly frequency-temperature dependency and numerical methods for structural systems containing this type of material require accurate mathematical models to describe their dynamical behaviour. The material behaviour here is modelled using a constitutive equation based on fractional derivative operators and considering the temperature dependence of the material under the thermorheologically simple postulate. The quest for information about the constitutive model parameters is phrased as a statistical inverse problem under the Bayesian framework. A Markov Chain Monte Carlo (MCMC) method is used to explore the posterior density of model parameters using measured data from dynamic tests at different temperatures. The agreement between measured data and the predictive capabilities of sixteen models were quantitatively assessed using two validation metrics. Based on the validation metrics analysis it is possible to conclude that the range of temperature of the calibration data set is a key-point into the implementation of the Frequency Temperature Superposition Principle (FTSP). This was verified defining some scenarios for assessing the agreement of model predictions and the set of available experimental data. The results are quite compelling due to the fact that the proposed approach is easy-handed. Furthermore, this approach could be applied on any generic constitutive model using the FTSP. © 2017 Elsevier Ltd"
,10.1080/02664763.2016.1238058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990172626&doi=10.1080%2f02664763.2016.1238058&partnerID=40&md5=017dd3b257a23cf2000bc4decbc2ca27,"Sinh-normal/independent distributions are a class of symmetric heavy-tailed distributions that include the sinh-normal distribution as a special case, which has been used extensively in Birnbaum–Saunders regression models. Here, we explore the use of Markov Chain Monte Carlo methods to develop a Bayesian analysis in nonlinear regression models when Sinh-normal/independent distributions are assumed for the random errors term, and it provides a robust alternative to the sinh-normal nonlinear regression model. Bayesian mechanisms for parameter estimation, residual analysis and influence diagnostics are then developed, which extend the results of Farias and Lemonte [Bayesian inference for the Birnbaum-Saunders nonlinear regression model, Stat. Methods Appl. 20 (2011), pp. 423-438] who used the Sinh-normal/independent distributions with known scale parameter. Some special cases, based on the sinh-Student-t (sinh-St), sinh-slash (sinh-SL) and sinh-contaminated normal (sinh-CN) distributions are discussed in detail. Two real datasets are finally analyzed to illustrate the developed procedures. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/08120099.2017.1362663,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028531341&doi=10.1080%2f08120099.2017.1362663&partnerID=40&md5=87a715416e3f4fb45cb867ceea8ef2f0,"Borehole temperature data have the potential to record historical variations in ground and air surface temperature, yet very few reliable, purpose-drilled, boreholes are available to explore such impacts, particularly in the southern hemisphere. The 400-m deep Tynong-1 borehole, approximately 65 km ESE of Melbourne, Australia, was drilled specifically to determine conductive heat flow and provides a unique dataset for evaluating ground surface temperature history in southeastern Australia. Steady-state conductive heat flow of 87 ± 1 mW m−2 was determined in the deeper borehole sections, with measured temperature profiles clearly demonstrating a progressive divergence of the observed temperature profile from the equilibrium model in the upper ∼150 m of the hole. We applied a Bayesian method employing a reverse jump Markov chain Monte Carlo search algorithm to explore the origins of this variation. Our results indicate a 2°C increase in ground surface temperature since 1800, after at least 500 years of relatively stable ground surface temperature. The inversion results are consistent with the trend of surface air temperature recorded in southeast Victoria by historical meteorological data since 1950. The inferred increase in ground surface temperature evident prior to 1950 is likely a cumulative effect of land clearing and a rise in surface air temperature. © 2017 Geological Society of Australia."
,10.1080/00036846.2016.1270414,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011562602&doi=10.1080%2f00036846.2016.1270414&partnerID=40&md5=7a52f7553e0aba92a6045c5a72d432bd,"Some empirical studies have attempted to clarify the mechanism of illegal dumping by examining the degree to which per-bag pricing plays a role. However, previous research on the behaviour of avoiding paying a charge for waste collection has tended to neglect so-called ‘immoral disposal,’ which is less risky than illegal dumping because there is no legal penalty. In this study, we define immoral disposal as the dumping of waste in a manner that is immoral but not illegal. To detect the existence of immoral disposal, we apply a spatial econometric approach, namely an extended panel spatial Durbin model, to identify the actual spillover effect of garbage pricing in neighbouring municipalities on immoral disposal from the total waste. A major finding of this study is that immoral disposal exists in unit-based pricing, two-tiered pricing, and fixed pricing. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
1,10.1002/sim.7301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018797427&doi=10.1002%2fsim.7301&partnerID=40&md5=248fe6f1ca28febc20dc75a785faa1f6,"Using original data that we have collected on referral relations between 110 hospitals serving a large regional community, we show how recently derived Bayesian exponential random graph models may be adopted to illuminate core empirical issues in research on relational coordination among healthcare organisations. We show how a rigorous Bayesian computation approach supports a fully probabilistic analytical framework that alleviates well-known problems in the estimation of model parameters of exponential random graph models. We also show how the main structural features of interhospital patient referral networks that prior studies have described can be reproduced with accuracy by specifying the system of local dependencies that produce – but at the same time are induced by – decentralised collaborative arrangements between hospitals. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.1186/s12864-017-4030-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027507554&doi=10.1186%2fs12864-017-4030-x&partnerID=40&md5=0c52f031dd688fb56791f5ed4cd5a3bf,"Background: Using whole genome sequence data might improve genomic prediction accuracy, when compared with high-density SNP arrays, and could lead to identification of casual mutations affecting complex traits. For some traits, the most accurate genomic predictions are achieved with non-linear Bayesian methods. However, as the number of variants and the size of the reference population increase, the computational time required to implement these Bayesian methods (typically with Monte Carlo Markov Chain sampling) becomes unfeasibly long. Results: Here, we applied a new method, HyB_BR (for Hybrid BayesR), which implements a mixture model of normal distributions and hybridizes an Expectation-Maximization (EM) algorithm followed by Markov Chain Monte Carlo (MCMC) sampling, to genomic prediction in a large dairy cattle population with imputed whole genome sequence data. The imputed whole genome sequence data included 994,019 variant genotypes of 16,214 Holstein and Jersey bulls and cows. Traits included fat yield, milk volume, protein kg, fat% and protein% in milk, as well as fertility and heat tolerance. HyB_BR achieved genomic prediction accuracies as high as the full MCMC implementation of BayesR, both for predicting a validation set of Holstein and Jersey bulls (multi-breed prediction) and a validation set of Australian Red bulls (across-breed prediction). HyB_BR had a ten fold reduction in compute time, compared with the MCMC implementation of BayesR (48 hours versus 594 hours). We also demonstrate that in many cases HyB_BR identified sequence variants with a high posterior probability of affecting the milk production or fertility traits that were similar to those identified in BayesR. For heat tolerance, both HyB_BR and BayesR found variants in or close to promising candidate genes associated with this trait and not detected by previous studies. Conclusions: The results demonstrate that HyB_BR is a feasible method for simultaneous genomic prediction and QTL mapping with whole genome sequence in large reference populations. © 2017 The Author(s)."
5,10.1186/s12942-017-0104-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027460122&doi=10.1186%2fs12942-017-0104-x&partnerID=40&md5=b107acb46bcbc2c01d4d3ee3bd94df75,"Background: Dengue is a high incidence arboviral disease in tropical countries around the world. Colombia is an endemic country due to the favourable environmental conditions for vector survival and spread. Dengue surveillance in Colombia is based in passive notification of cases, supporting monitoring, prediction, risk factor identification and intervention measures. Even though the surveillance network works adequately, disease mapping techniques currently developed and employed for many health problems are not widely applied. We select the Colombian city of Bucaramanga to apply Bayesian areal disease mapping models, testing the challenges and difficulties of the approach. Methods: We estimated the relative risk of dengue disease by census section (a geographical unit composed approximately by 1-20 city blocks) for the period January 2008 to December 2015. We included the covariates normalized difference vegetation index (NDVI) and land surface temperature (LST), obtained by satellite images. We fitted Bayesian areal models at the complete period and annual aggregation time scales for 2008-2015, with fixed and space-varying coefficients for the covariates, using Markov Chain Monte Carlo simulations. In addition, we used Cohen's Kappa agreement measures to compare the risk from year to year, and from every year to the complete period aggregation. Results: We found the NDVI providing more information than LST for estimating relative risk of dengue, although their effects were small. NDVI was directly associated to high relative risk of dengue. Risk maps of dengue were produced from the estimates obtained by the modeling process. The year to year risk agreement by census section was sligth to fair. Conclusion: The study provides an example of implementation of relative risk estimation using Bayesian models for disease mapping at small spatial scale with covariates. We relate satellite data to dengue disease, using an areal data approach, which is not commonly found in the literature. The main difficulty of the study was to find quality data for generating expected values as input for the models. We remark the importance of creating population registry at small spatial scale, which is not only relevant for the risk estimation of dengue but also important to the surveillance of all notifiable diseases. © 2017 The Author(s)."
2,10.1063/1.4998163,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027284466&doi=10.1063%2f1.4998163&partnerID=40&md5=122a6f9ddd87262196e574ace52a89d3,"The time and frequency dependence of the electric field-induced phase transition in BaTiO3-BiZn1/2Ti1/2O3 was studied using in situ X-ray diffraction. The kinetics of the field-induced phase transition between cubic and tetragonal phases was described using a modified Kolmogorov-Avrami-Ishibashi (KAI) equation. Unlike previous works, for which some assumptions (e.g., unimodal and Gaussian) on the distribution of transition rates are needed, the present work utilized Bayesian inference and a Markov chain Monte Carlo algorithm to obtain the distribution of transition rates empirically without a priori assumption on the distribution. The results show that the transition rate coefficient increases as the frequency of applied field increases. The mean value of exponent n in the modified-KAI equation was close to 1, implying that the field-induced phase transition is site saturated and the growth of the induced phase occurred primarily from the surface. © 2017 Author(s)."
2,10.1145/3097983.3098100,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029038319&doi=10.1145%2f3097983.3098100&partnerID=40&md5=9869f2357fed15ffc58afe42a7da1b22,"Diabetes is a serious disease affecting a large number of people. Although there is no cure for diabetes, it can be managed. Especially, with advances in sensor technology, lots of data may lead to the improvement of diabetes management, if properly mined. However, there usually exists noise or errors in the observed behavioral data which poses challenges in extracting meaningful knowledge. To overcome this challenge, we learn the latent state which represents the patient's condition. Such states should be inferred from the behavioral data but unknown a priori. In this paper, we propose a novel framework to capture the trajectory of latent states for patients from behavioral data while exploiting their demographic differences and similarities to other patients. We conduct a hypothesis test to illustrate the importance of the demographic data in diabetes management, and validate that each behavioral feature follows an exponential or a Gaussian distribution. Integrating these aspects, we use a Demographic feature restricted hidden Markov model (DfrHMM) to estimate the trajectory of latent states by integrating the demographic and behavioral data. In DfrHMM, the latent state is mainly determined by the previous state and the demographic features in a nonlinear way. Markov Chain Monte Carlo techniques are used for model parameter estimation. Experiments on synthetic and real datasets show that DfrHMM is effective in diabetes management. © 2017 ACM."
,10.1080/00949655.2017.1332196,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019674624&doi=10.1080%2f00949655.2017.1332196&partnerID=40&md5=4fc0ddd90ac2d9228d25a8f6ff5165fe,"In practice, survival data are often collected over geographical regions. Shared spatial frailty models have been used to model spatial variation in survival times, which are often implemented using the Bayesian Markov chain Monte Carlo method. However, this method comes at the price of slow mixing rates and heavy computational cost, which may render it impractical for data-intensive application. Alternatively, a frailty model assuming an independent and identically distributed (iid) random effect can be easily and efficiently implemented. Therefore, we used simulations to assess the bias and efficiency loss in the estimated parameters, if residual spatial correlation is present but using an iid random effect. Our simulations indicate that a shared frailty model with an iid random effect can estimate the regression coefficients reasonably well, even with residual spatial correlation present, when the percentage of censoring is not too high and the number of clusters and cluster size are not too low. Therefore, if the primary goal is to assess the covariate effects, one may choose the frailty model with an iid random effect; whereas if the goal is to predict the hazard, additional care needs to be given due to the efficiency loss in the parameter(s) for the baseline hazard. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1142/S0217751X17501330,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027167560&doi=10.1142%2fS0217751X17501330&partnerID=40&md5=5b685f152498e75d7c2395024cda9842,"Motivated by the physics of strings and branes, we develop a class of Markov chain Monte Carlo (MCMC) algorithms involving extended objects. Starting from a collection of parallel Metropolis-Hastings (MH) samplers, we place them on an auxiliary grid, and couple them together via nearest neighbor interactions. This leads to a class of ""suburban samplers"" (i.e. spread out Metropolis). Coupling the samplers in this way modifies the mixing rate and speed of convergence for the Markov chain, and can in many cases allow a sampler to more easily overcome free energy barriers in a target distribution. We test these general theoretical considerations by performing several numerical experiments. For suburban samplers with a fluctuating grid topology, performance is strongly correlated with the average number of neighbors. Increasing the average number of neighbors above zero initially leads to an increase in performance, though there is a critical connectivity with effective dimension deff ∼ 1, above which ""groupthink"" takes over, and the performance of the sampler declines. © 2017 World Scientific Publishing Company."
1,10.1080/03610918.2016.1152365,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014568342&doi=10.1080%2f03610918.2016.1152365&partnerID=40&md5=97e1db80acbd226f0d8543df224de0dc,"In this article, we propose to evaluate and compare Markov chain Monte Carlo (MCMC) methods to estimate the parameters in a generalized extreme value model. We employed the Bayesian approach using traditional Metropolis-Hastings methods, Hamiltonian Monte Carlo (HMC), and Riemann manifold HMC (RMHMC) methods to obtain the approximations to the posterior marginal distributions of interest. Applications to real datasets and simulation studies provide evidence that the extra analytical work involved in Hamiltonian Monte Carlo algorithms is compensated by a more efficient exploration of the parameter space. © 2017 Taylor & Francis Group, LLC."
,10.1088/1361-6382/aa8124,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028338627&doi=10.1088%2f1361-6382%2faa8124&partnerID=40&md5=53292c763d521fc5b56e5c9131ed2985,"We design a direct test of the local position invariance (LPI) in post-Newtonian gravity, using the timing observation of the triple pulsar, PSR J0337 + 1715. The test takes advantage of the large gravitational acceleration exerted by the outer white dwarf to the inner neutron star-white dwarf binary. Using machineprecision three-body simulations and dedicated Markov-chain Monte Carlo (MCMC) techniques with various sampling strategies and noise realizations, we estimate that the Whitehead's parameter could have already been limited to |ξ| ≲ 0.4 (95% CL), with the published timing data spanning from January 2012 to May 2013. The constraint is still orders of magnitude looser than the best limit, yet it is able to independently falsify Whitehead's gravity theory where ξ = 1. In addition, the new test is immune to extra assumptions and involves full dynamics of a three-body system with a strongly self-gravitating neutron star. © 2017 IOP Publishing Ltd."
1,10.1186/s12913-017-2502-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026912219&doi=10.1186%2fs12913-017-2502-y&partnerID=40&md5=fc8552013cb5a5b81fb18648f91c778e,"Background: The UK National Health Service provides Stop Smoking Services for pregnant women (SSSP) but there is a lack of evidence concerning how these are best organised. This study investigates influences on services' effectiveness and also on their propensity to engage pregnant smokers with support in stopping smoking. Methods: Survey data collected from 121/141 (86%) of SSSP were augmented with data from Hospital Episode Statistics and the 2011 UK National Census. 'Reach' or propensity to engage smokers with support was defined as the percentage of pregnant smokers setting a quit date with SSSP support, and 'Effectiveness' as the percentage of women who set a quit date who also reported abstinence at four weeks later. A bivariate (i.e. two outcome variable) response Markov Chain Monte Carlo model was used to identify service-level factors associated with the Reach and Effectiveness of SSSP. Results: Beta coefficients represent a percentage change in Reach and Effectiveness by the covariate. Providing the majority of one-to-one contacts in a clinic rather than at home increased both Reach (%) (β: 6.97, 95% CI: 3.34, 10.60) and Effectiveness (%) (β: 7.37, 95% CI: 3.03, 11.70). Reach of SSSP was also increased when the population served was more deprived (β for increase in Reach with a one unit increase in IMD score: 0.55, 95% CI: 0.25, 0.85), had a lower proportion of people with dependent children (β: -2.52, 95% CI: -3.82, -1.22), and a lower proportion of people in managerial or professional occupations (β: -0.31, 95% CI: -0.59, -0.03). The Effectiveness of SSSP was decreased in those areas that had a greater percentage of people >16 years with no educational qualifications (β: -0.51, 95% CI: -0.95, -0.07). Conclusions: To engage pregnant smokers and to encourage them to quit, it may be more efficient for SSSP support to be focussed around clinics, rather than women's homes. Reach of SSSP is inversely associated with disadvantage and efforts should be made to contact these women as they are less likely to achieve abstinence in the short and longer term. © 2017 The Author(s)."
2,10.1145/3077136.3080766,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029392129&doi=10.1145%2f3077136.3080766&partnerID=40&md5=cdae50635c63235009edbc88f85aad56,"Using classical statistical significance tests, researchers can only discuss PD+jH, the probability of observing the data D at hand or something more extreme, under the assumption that the hypothesis H is true (i.e., the p-value). But what we usually want is PHjD, the probability that a hypothesis is true, given the data. If we use Bayesian statistics with state-of-The-Art Markov Chain Monte Carlo (MCMC) methods for obtaining posterior distributions, this is no longer a problem. .at is, instead of the classical p-values and 95% confidence intervals, which are offen misinterpreted respectively as ""probability that the hypothesis is (in)correct"" and ""probability that the true parameter value drops within the interval is 95%,"" we can easily obtain PHjD and credible intervals which represent exactly the above. Moreover, with Bayesian tests, we can easily handle virtually any hypothesis, not just ""equality of means,"" and obtain an Expected A Posteriori (EAP) value of any statistic that we are interested in. We provide simple tools to encourage the IR community to take up paired and unpaired Bayesian tests for comparing two systems. Using a variety of TREC and NTCIR data, we compare PHjD with p-values, credible intervals with con.-dence intervals, and Bayesian EAP effect sizes with classical ones. Our results show that (a) p-values and confidence intervals can respectively be regarded as approximations of what we really want, namely, PHjD and credible intervals; and (b) sample effect sizes from classical significance tests can di.er considerably from the Bayesian EAP effect sizes, which suggests that the former can be poor estimates of population effect sizes. For both paired and unpaired tests, we propose that the IR community report the EAP, the credible interval, and the probability of hypothesis being true, not only for the raw di.erence in means but also for the effect size in terms of Glass's.δ. © 2017 Copyright held by the owner/author(s)."
,10.1109/QRS-C.2017.35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034441650&doi=10.1109%2fQRS-C.2017.35&partnerID=40&md5=ae737835efd5a1438af4884dbd1a0fda,"Residual life time estimation is significant for complex systems. In this paper, a method is proposed to estimate the residual life of products on satellite platform by fusing real time updating few failure lifetime and degradation data. The linear Wiener process is adopted to model the degradation data where the drift and diffusion parameters are both assumed to be random variables. Besides, the lifetime data is described by the inverse Gauss distribution. Through maximum likelihood function, the lifetime and degradation data are integrated. Considering the continually collected data from the satellite platform, a practical approach combined Bayesian idea is proposed to update the probability density function of the residual life time. Due to the complexity of the model, Monte Carlo Markov Chain (MCMC) is applied to estimate the parameters. Finally, an example of the infrared-sensitive single machine is provided to illustrate the effectiveness and validity of the proposed method. © 2017 IEEE."
,10.1063/1.4995920,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028319587&doi=10.1063%2f1.4995920&partnerID=40&md5=783ca27fedc3667451bcbca1220b0c5b,"Hierarchical Bayesian modeling of large point-referenced spatio-temporal data is progressively more suitable in many practical applications due to advance development of both statistical methodology and computational efficiency. We present a Bayesian spatio-temporal approach for modeling the variation of wind speed. For the analysis, Gaussian process model are adopted using Markov Chain Monte Carlo sampling technique to estimate posterior distribution. The findings show there is good evidence of variation across certain region. © 2017 Author(s)."
2,10.1063/1.4995120,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028362483&doi=10.1063%2f1.4995120&partnerID=40&md5=de96f27b26ba5fa9304884ca5185055f,"The aim of this study is to empirically investigate the performance of APARCH(1,1) volatility model with the Student-t error distribution on five foreign currency selling rates to Indonesian rupiah (IDR), including the Swiss franc (CHF), the Euro (EUR), the British pound (GBP), Japanese yen (JPY), and the US dollar (USD). Six years daily closing rates over the period of January 2010 to December 2016 for a total number of 1722 observations have analysed. The Bayesian inference using the efficient independence chain Metropolis-Hastings and adaptive random walk Metropolis methods in the Markov chain Monte Carlo (MCMC) scheme has been applied to estimate the parameters of model. According to the DIC criterion, this study has found that the APARCH(1,1) model under Student-t distribution is a better fit than the model under normal distribution for any observed rate return series. The 95% highest posterior density interval suggested the APARCH models to model the IDR/JPY and IDR/USD volatilities. In particular, the IDR/JPY and IDR/USD data, respectively, have significant negative and positive leverage effect in the rate returns. Meanwhile, the optimal power coefficient of volatility has been found to be statistically different from 2 in adopting all rate return series, save the IDR/EUR rate return series. © 2017 Author(s)."
1,10.1049/iet-com.2016.1339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029225661&doi=10.1049%2fiet-com.2016.1339&partnerID=40&md5=e091d2025d4d177b0b720cf478e5feba,"This study proposes a novel approach to joint channel estimation and detection of orthogonal frequency division multiplexing transmission over underwater acoustic (UWA) multipath channels exhibiting cluster sparsity. Unlike most sparse channel estimations, the authors exploit the cluster-sparsity characteristic of UWA channels without additional prior information. They adopt a modified spike-and-slab prior model in their non-parametric Bayesian learning framework. To avoid the need for a closed-form Bayesian estimate, they apply the Markov chain Monte Carlo technique to joint achieve channel estimation and signal detection. The proposed solution is amenable to being integrated with soft-input soft-output decoding to improve the performance through turbo iteration. Simulation results demonstrate improved bit error rate of the proposed algorithm over existing algorithms. © 2017, The Institution of Engineering and Technology."
,10.1080/03610926.2016.1161798,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018854345&doi=10.1080%2f03610926.2016.1161798&partnerID=40&md5=60482570ae871510178f13a32a7428e1,"Longitudinal data are commonly modeled with the normal mixed-effects models. Most modeling methods are based on traditional mean regression, which results in non robust estimation when suffering extreme values or outliers. Median regression is also not a best choice to estimation especially for non normal errors. Compared to conventional modeling methods, composite quantile regression can provide robust estimation results even for non normal errors. In this paper, based on a so-called pseudo composite asymmetric Laplace distribution (PCALD), we develop a Bayesian treatment to composite quantile regression for mixed-effects models. Furthermore, with the location-scale mixture representation of the PCALD, we establish a Bayesian hierarchical model and achieve the posterior inference of all unknown parameters and latent variables using Markov Chain Monte Carlo (MCMC) method. Finally, this newly developed procedure is illustrated by some Monte Carlo simulations and a case analysis of HIV/AIDS clinical data set. © 2017 Taylor & Francis Group, LLC."
1,10.1080/13696998.2017.1328423,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020247814&doi=10.1080%2f13696998.2017.1328423&partnerID=40&md5=40fc3a24079eb04e63e0cea63fbdb9e0,"Aims: This study assessed the cost-effectiveness of the subcutaneous RANKL inhibitor, denosumab, vs the intravenous bisphosphonate, zoledronic acid, for the prevention of skeletal-related events (SREs) in patients with prostate cancer, breast cancer, and other solid tumors (OST) in the Czech Republic. Materials and methods: A lifetime Markov model was developed to compare the effects of denosumab and zoledronic acid on costs (including drug costs and administration, patient management, SREs, and adverse events), quality-adjusted life-years (QALYs), and incremental cost-effectiveness ratios from a national payer perspective. Different discount rates, time horizons, SRE rates, distributions, and nature (asymptomatic vs all SREs), and the inclusion of treatment discontinuation were considered in scenario analyses. The robustness of the model was tested using deterministic and probabilistic sensitivity analyses. Results: Across tumor types, denosumab was associated with fewer SREs, improved QALYs, and higher total costs over a lifetime. The incremental cost per QALY gained for denosumab vs zoledronic acid was 382,673 CZK for prostate cancer, 408,450 CZK for breast cancer, and 608,133 CZK for OST. Incremental costs per SRE avoided for the same tumor type were 54,007 CZK, 51,765 CZK, and 94,426 CZK, respectively. In scenario analyses, the results remained similar to baseline, when different discount rates and time horizons were considered. At a non-official willingness-to-pay threshold of 1.2 million CZK, the probabilities of denosumab being cost-effective vs zoledronic acid were 0.64, 0.67, and 0.49 for prostate cancer, breast cancer, and OST, respectively. Limitations: The SRE rates used were obtained from clinical trials; studies suggest rates may be higher in clinical practice. Additional evidence on real-world SRE rates could further improve the accuracy of the modeling. Conclusions: Compared with zoledronic acid, denosumab provides a cost-effective treatment option for the prevention of SREs in patients with prostate cancer, breast cancer, and OST in the Czech Republic. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032274288&partnerID=40&md5=85361d9c3dbdf8597a5f0af54fcd0e52,"Although a large amount of wells has been extensively drilled to develop unconventional reservoirs, uncertainty associated with reservoir and fracture properties can significantly affect the evaluation of an individual well’s performance with simulations. From the history-matching point of view, this uncertainty causes non-uniqueness of solutions that should be obtained with a robust probabilistic method without overly exhausting simulation resources. Because of the challenge in this work, a proxy-based history-matching approach can deliver unique advantages of reducing the computational requirement and providing probabilistic interpretation. The work flow presented in this paper is designed to exploit full ranges of proxy-modeling benefits from prioritizing significant reservoir properties, estimating the variation of model responses, and searching multiple history-matching solutions for reliable probabilistic forecasts. A screening process is introduced at the initial stage of the work flow with design of experiment (DOE) and response-surface methodology (RSM), so the dimensions of proxy models can be reduced. In addition, the proxy models progress through the work flow by iterations that aim to improve their accuracy. This iterative work flow efficiently uses simulation resources by use of all the completed runs to provide more information about the model responses for the subsequent iterations. While the work flow is being iterated, multiple proxy models are explored for history-matching solutions by a Markov chain Monte Carlo (MCMC) algorithm. Then, history-matching solutions are used to evaluate the probability distribution of the long-term estimated ultimate recovery (EUR). Finally, an application of the work flow to a horizontal well in the Middle Bakken is presented in this paper. Copyright VC 2017 Society of Petroleum Engineers"
5,10.1061/(ASCE)WR.1943-5452.0000791,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019370206&doi=10.1061%2f%28ASCE%29WR.1943-5452.0000791&partnerID=40&md5=5e1c62bc9a666cdc0aa7473345885d7c,"A combined demand and roughness estimation is a critical step in order for the water distribution system model to represent the real system adequately. A novel two-level Markov chain Monte Carlo particle filter method for joint estimation of demand and roughness is proposed in this paper. First, an improved particle filter with ensemble Kalman filter modification to proposal density is adopted to track the non-Gaussian system dynamics and estimate demands. Then, the improved particle filter for demand estimation is nested into the Markov chain Monte Carlo simulation for roughness estimation. The method is very capable of quantifying the uncertainties associated with estimated or predicted values without requiring any assumptions of linearity and Gaussianity or any derivatives to be calculated. A strong nonlinear benchmark network with synthetically generated field data is utilized to validate the performance of this method. The results suggest that the proposed method is demonstrated to provide satisfactory demand and roughness values with reliable confidence limits. Some practical issues are also discussed to enhance the application potential of this method. © 2017 American Society of Civil Engineers."
3,10.1016/j.cageo.2017.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018753417&doi=10.1016%2fj.cageo.2017.05.001&partnerID=40&md5=56a54b9f07d2ac4ba48d7858bea022d1,"Spectral induced polarization (SIP) measurements are now widely used to infer mineralogical or hydrogeological properties from the low-frequency electrical properties of the subsurface in both mineral exploration and environmental sciences. We present an open-source program that performs fast multi-model inversion of laboratory complex resistivity measurements using Markov-chain Monte Carlo simulation. Using this stochastic method, SIP parameters and their uncertainties may be obtained from the Cole-Cole and Dias models, or from the Debye and Warburg decomposition approaches. The program is tested on synthetic and laboratory data to show that the posterior distribution of a multiple Cole-Cole model is multimodal in particular cases. The Warburg and Debye decomposition approaches yield unique solutions in all cases. It is shown that an adaptive Metropolis algorithm performs faster and is less dependent on the initial parameter values than the Metropolis-Hastings step method when inverting SIP data through the decomposition schemes. There are no advantages in using an adaptive step method for well-defined Cole-Cole inversion. Finally, the influence of measurement noise on the recovered relaxation time distribution is explored. We provide the geophysics community with a open-source platform that can serve as a base for further developments in stochastic SIP data inversion and that may be used to perform parameter analysis with various SIP models. © 2017 Elsevier Ltd"
,10.1115/1.4036153,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021898678&doi=10.1115%2f1.4036153&partnerID=40&md5=de547d0b2f105857a197295e39fb326b,"This article presents a new method of estimation of thermophysical parameters using the hybrid Monte Carlo (HMC) algorithm that synergistically combines the advantages of a Markov chain Monte Carlo (MCMC) method and molecular dynamics. The advantages of this technique over the conventional MCMC are elucidated by considering the multiparameter estimation in heat transfer. Four situations were analyzed. The first two involve a two- and a three-parameters estimation in a lumped capacitance model, third involves estimation in a distributed system, and the fourth involves estimation in a fin system. The goal is to establish the potency and usefulness of the HMC method for a wide class of engineering problems. Copyright © 2017 by ASME."
1,10.1093/gji/ggx196,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037124190&doi=10.1093%2fgji%2fggx196&partnerID=40&md5=80fc62be870122ce8df3c00772068d5f,"A system of aligned vertical fractures and fine horizontal shale layers combine to form equivalent orthorhombic media. Weak anisotropy parameters and fracture weaknesses play an important role in the description of orthorhombic anisotropy (OA). We propose a novel approach of utilizing seismic reflection amplitudes to estimate weak anisotropy parameters and fracture weaknesses from observed seismic data, based on azimuthal elastic impedance (EI). We first propose perturbation in stiffness matrix in terms of weak anisotropy parameters and fracture weaknesses, and using the perturbation and scattering function, we derive PPwave reflection coefficient and azimuthal EI for the case of an interface separating two OA media. Then we demonstrate an approach to first use a model constrained damped leastsquares algorithm to estimate azimuthal EI from partially incidence-phase-angle-stack seismic reflection data at different azimuths, and then extract weak anisotropy parameters and fracture weaknesses from the estimated azimuthal EI using a Bayesian Markov Chain Monte Carlo inversion method. In addition, a new procedure to construct rock physics effective model is presented to estimate weak anisotropy parameters and fracture weaknesses from well log interpretation results (minerals and their volumes, porosity, saturation, fracture density, etc.). Tests on synthetic and real data indicate that unknown parameters including elastic properties (P- and S-wave impedances and density), weak anisotropy parameters and fracture weaknesses can be estimated stably in the case of seismic data containing a moderate noise, and our approach can make a reasonable estimation of anisotropy in a fractured shale reservoir. © The Authors 2017."
1,10.1061/(ASCE)CO.1943-7862.0001336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018176332&doi=10.1061%2f%28ASCE%29CO.1943-7862.0001336&partnerID=40&md5=3b884613f3a2ec076b3ac88c61fd6973,"The match factor is a criterion that measures efficiency of truck and loader compatibility used in the construction industry. This factor is a function of number of trucks and loaders, truck cycle time, and loader loading time. However, these parameters are uncertain in nature because of equipment failures, climate, road conditions, and operator habits. Fluctuations in the match factor result in truck queues or idle loaders waiting, which lead to production losses or opportunity costs. In this paper, a stochastic approach is proposed to assess the risks associated with uncertain parameters in the match-factor equation. The approach is based on coupling of Markov-chain Monte Carlo simulations for a number of available equipment and Ordinary Monte Carlo simulations for loader loading and truck cycle times. Thus, the variations in the match factor over a time series are quantified in such a way as to determine equipment capacity utilization and maintenance management strategy. A case study has been carried out and the results show that the proposed approach can be used as a tool to assist production planning of material handling in the construction industry. © 2017 American Society of Civil Engineers."
5,10.1016/j.jhydrol.2017.05.051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019982892&doi=10.1016%2fj.jhydrol.2017.05.051&partnerID=40&md5=7d5654170000660eea1bd1c2cbf491f4,"We apply a stochastic Newton (SN) approach to solve a high-dimensional hydraulic inverse problem in highly heterogeneous geological media. By recognizing the connection between the cost function of deterministic optimizations and the posterior probability density of stochastic inversions, the Markov chain Monte Carlo (MCMC) sampler of SN is constructed by two parts: a deterministic part, which corresponds to a Newton step of deterministic optimization, and a stochastic part, which is a Gaussian distribution with the inverse of the local Hessian as the covariance matrix. The hybrid inverse method exploits the efficient tools for fast solution of deterministic inversions to improve the efficiency of the MCMC sampler. To address the ill-posedness of the inverse problem, a priori models, generated by a transition-probability geostatistical method, and conditioned to inter-well connection data, are used as regularization constraints. The effectiveness of the stochastic Newton method is first demonstrated by a synthetic test. The transmissivity field of the synthetic model is highly heterogeneous, and includes sharp variations. The inverse approach was then applied to a field hydraulic tomography investigation in a fractured and karstified aquifer to reconstruct its transmissivity field from a collection of real hydraulic head measurements. From the inversions, a series of transmissivity fields that produce good correlations between the inverted and the measured hydraulic heads were obtained. The inverse approach produced slightly different a posteriori transmissivity patterns for different a priori structure models of transmissivity; however, the trend and location of the high-transmissivity channels are consistent among various realizations. In addition, the uncertainty associated with each realization of the inverted transmissivity fields was quantified. © 2017"
4,10.1016/j.ress.2017.03.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016057787&doi=10.1016%2fj.ress.2017.03.006&partnerID=40&md5=ccc27504ade2b80f3998fe707313fe20,"A microstructure and deformation mechanism based fatigue crack initiation and life prediction model, which links microstructure variability of a polycrystalline material to the scatter in fatigue life, is validated using an uncertainty quantification and propagation framework. First, global sensitivity analysis (GSA) is used to identify the set of most influential parameters in the fatigue life prediction model. Following GSA, the posterior distributions of all influential parameters are calculated using a Bayesian inference framework, which is built based on a Markov chain Monte Carlo (MCMC) algorithm. The quantified uncertainties thus obtained, are propagated through the model using Monte Carlo sampling technique to make robust predictions of fatigue life. The model is validated by comparing the predictions to experimental fatigue life data. © 2017 Elsevier Ltd"
2,10.1214/16-AAP1255,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028725368&doi=10.1214%2f16-AAP1255&partnerID=40&md5=38bf489a1beb29d2973c55764d9e9abc,"Tuning the durations of the Hamiltonian flow in Hamiltonian Monte Carlo (also called Hybrid Monte Carlo) (HMC) involves a tradeoff between computational cost and sampling quality, which is typically challenging to resolve in a satisfactory way. In this article, we present and analyze a randomized HMC method (RHMC), in which these durations are i.i.d. exponential random variables whose mean is a free parameter. We focus on the small time step size limit, where the algorithm is rejection-free and the computational cost is proportional to the mean duration. In this limit, we prove that RHMC is geometrically ergodic under the same conditions that imply geometric ergodicity of the solution to underdamped Langevin equations.Moreover, in the context of a multidimensional Gaussian distribution, we prove that the sampling efficiency of RHMC, unlike that of constant duration HMC, behaves in a regular way. This regularity is also verified numerically in non-Gaussian target distributions. Finally, we suggest variants of RHMC for which the time step size is not required to be small. © Institute of Mathematical Statistics, 2017."
12,10.1016/j.microrel.2017.02.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015877453&doi=10.1016%2fj.microrel.2017.02.012&partnerID=40&md5=b8964dd1e5c6f8ca1135404913eb8ca6,"Lithium-ion batteries are widely used as power sources in various portable electronics, hybrid electric vehicles, aeronautic and aerospace engineering, etc. To ensure an uninterruptible power supply, the remaining useful life (RUL) prediction of lithium-ion batteries has attracted extensive attention in recent years. This paper proposed an improved unscented particle filter (IUPF) method for lithium-ion battery RUL prediction based on Markov chain Monte Carlo (MCMC). The method uses the MCMC to solve the problem of sample impoverishment in UPF algorithm. Additionally, the IUPF method is proposed on the basis of UPF, so it can also suppress the particle degradation existing in the standard PF algorithm. In this work, the IUPF method is introduced firstly. Then, the capacity data of lithium-ion batteries are collected and the empirical capacity degradation model is established. The proposed method is used to estimate the RUL of lithium-ion battery. The RUL prediction results demonstrate the effectiveness and advantage. © 2017 Elsevier Ltd"
,10.1109/JSTSP.2017.2722419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023194336&doi=10.1109%2fJSTSP.2017.2722419&partnerID=40&md5=e8c8997edcf476dfccae0ffdd5e8e727,"Machine learning (ML) models and algorithms can enable a personalized learning experience for students in an inexpensive and scalable manner. At the heart of ML-driven personalized learning is the automated analysis of student responses to assessment items. Existing statistical models for this task enable the estimation of student knowledge and question difficulty solely from graded response data with only minimal effort from instructors. However, most existing student-response models are generalized linear models, meaning that they characterize the probability that a student answers a question correctly through a linear combination of their knowledge and the question's difficulty with respect to each concept that is being assessed. Such models cannot characterize complicated, nonlinear student-response associations and, hence, lack human interpretability in practice. In this paper, we propose a nonlinear student-response model called Boolean logic analysis (BLAh) that models a student's binary-valued graded response to a question as the output of a Boolean logic function. We develop a Markov chain Monte Carlo inference algorithm that learns the Boolean logic functions for each question solely from graded response data. A refined BLAh model improves the identifiability, tractability, and interpretability by considering a restricted set of ordered Boolean logic functions. Experimental results on a variety of real-world educational datasets demonstrate that BLAh not only achieves best-in-class prediction performance on unobserved student responses on some datasets but also provides easily interpretable parameters when questions are tagged with metadata by domain experts, which can provide useful feedback to instructors and content designers to improve the quality of assessment items. © 2007-2012 IEEE."
,10.1177/1748006X17712121,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026917018&doi=10.1177%2f1748006X17712121&partnerID=40&md5=79e93b3f7f912d57cc99b21f5ec4b773,"To ensure a power generation level, the French national electricity supply (EDF) has to manage its producing assets by putting in place adapted preventive maintenance strategies. In this article, a fleet of identical components is considered, which are spread out all around France (one per power plant site). The components are assumed to have stochastically independent lifetimes, but they are made functionally dependent through the sharing of a common stock of spare parts. When available, these spare parts are used for both corrective and preventive replacements, with priority to corrective replacements. When the stock is empty, replacements are delayed until the arrival of new spare parts. These spare parts are expensive, and their manufacturing time is long, which makes it necessary to rigorously define their ordering process. The point of the article is to provide the decision maker with the tools to take the right decision (make or not the overhaul). To do that, two indicators are proposed, which are based on an economic variable called the net present value. The net present value stands for the difference between the cumulated discounted cash-flows of the purely corrective policy and the preventive one which including the overhaul. Piecewise deterministic Markov processes are first considered for the joint modelling of the stochastic evolution of the components, stock and ordering process with and without overhaul. The indicators are next expressed with respect to these piecewise deterministic Markov processes, which have to be numerically assessed. Instead of using the most classical Monte Carlo simulations, we here suggest alternate methods based on quasi Monte Carlo simulations, which replace the random uniform numbers of the Monte Carlo method by deterministic sequences called low-discrepancy sequences. The obtained results show a real gain of the quasi Monte Carlo methods in comparison with the Monte Carlo method. The developed tools can hence help the decision maker to take the right decision. © Institution of Mechanical Engineers."
,10.3969/j.issn.1001-506X.2017.08.32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027709156&doi=10.3969%2fj.issn.1001-506X.2017.08.32&partnerID=40&md5=65d8aaf4fa15090169ca18cd8a0e1bd8,"Weilbull distribution is the typical failure distribution in aero equipment. In order to prove the effect of descriptive statistics on objective Bayesian reliability evaluation under random right censoring of aero equipment failure data, sensitivity analysis of the objective Bayesian method is proposed. A multiple Markov chain algorithm with the dependent variable of censoring rate and sample size is designed. The algorithm estimates the scale parameter and shape parameter of Weilbull distribution with the prior information of large variance gamma distribution under the different censoring rates and sample sizes. The deviation of estimation can be judged and evaluated with mean time between failures, mean and variation factor of distribution parameters. The numerical results show that in Weilbull distribution, when the sample size is more than 10, or the censoring rate is less than 0.5, the estimation accuracy of objective Bayesian is acceptable. Otherwise, a better reliability evaluation method is explored under small samples and high censoring rates. © 2017, Editorial Office of Systems Engineering and Electronics. All right reserved."
,10.1016/j.jfluidstructs.2017.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021211820&doi=10.1016%2fj.jfluidstructs.2017.05.007&partnerID=40&md5=5bbadb0c812eccd85e652c3d6bc9332d,"In this work, Bayesian techniques are employed to quantify model-form and predictive uncertainty in the linear behavior of an elastically mounted airfoil undergoing pitching and plunging motions. The Bayesian model averaging approach is used to construct an adjusted stochastic model from different model classes for time-harmonic incompressible flows. From a set of deterministic function approximations, we construct different stochastic models, whose uncertain coefficients are calibrated using Bayesian inference with regard to the critical flutter velocity. Results show substantial reductions in the predictive uncertainties of the critical flutter speed compared to non-calibrated stochastic simulations. In particular, it is shown that an efficient adjusted model can be derived by considering a possible bias in the random error term on the posterior predictive distributions of the flutter index. © 2017 Elsevier Ltd"
1,10.1007/s11242-017-0872-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020095977&doi=10.1007%2fs11242-017-0872-6&partnerID=40&md5=5a179b9615f38efe40690d2f644f4a43,"In this work we apply a recently proposed Bayesian Markov chain Monte Carlo framework (Akbarabadi et al. in Comput Geosci 19(6):1231–1250, 2015) to quantify uncertainty in the three-dimensional permeability field of a rock core. This process establishes the credibility of a compositional two-phase flow model to describe the displacement of brine by CO 2 and CO 2 storage in saline aquifers. We investigate the predictive capabilities of the compositional model in the context of an unsteady-state CO 2-brine drainage experiment at the laboratory scale, performed at field-scale aquifer conditions. We employ forward models consisting of a system of discretized partial differential equations along with relative permeability curves obtained by a curve fitting of experimental measurements. We consider a forward model to be validated when: (1) numerical simulations reveal that the Bayesian framework has accurately characterized the core’s permeability and (2) Monte Carlo predictions show excellent agreement between measured and simulated data. A large set of numerical studies with an accurate compositional simulator shows that forward models have been successfully validated. For such models, our numerical results show that we are able to capture all the dominant features and general trends of the CO 2 saturation fields observed in the core. Our study is consistent with the design and findings of real experiments. Fluid properties, relative permeability data, measured porosity field, physical dimensions, and thermodynamic conditions are the same as those reported in Akbarabadi and Piri (Adv Water Resour 52:190–206, 2013). However, the measured saturation data are from flow experiments different from those reported in Akbarabadi and Piri (2013), and will be presented here. © 2017, Springer Science+Business Media Dordrecht."
1,10.1109/TBCAS.2017.2679039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029314673&doi=10.1109%2fTBCAS.2017.2679039&partnerID=40&md5=65cdd096c34f5a6549668c5eec8c3e12,"The brain is extremely energy efficient and remarkably robust in what it does despite the considerable variability and noise caused by the stochastic mechanisms in neurons and synapses. Computational modeling is a powerful tool that can help us gain insight into this important aspect of brain mechanism. A deep understanding and computational design tools can help develop robust neuromorphic electronic circuits and hybrid neuroelectronic systems. In this paper, we present a general modeling framework for biological neuronal circuits that systematically captures the nonstationary stochastic behavior of ion channels and synaptic processes. In this framework, fine-grained, discrete-state, continuous-time Markov chain models of both ion channels and synaptic processes are treated in a unified manner. Our modeling framework features a mechanism for the automatic generation of the corresponding coarse-grained, continuous-state, continuous-time stochastic differential equation models for neuronal variability and noise. Furthermore, we repurpose non-Monte Carlo noise analysis techniques, which were previously developed for analog electronic circuits, for the stochastic characterization of neuronal circuits both in time and frequency domain. We verify that the fast non-Monte Carlo analysis methods produce results with the same accuracy as computationally expensive Monte Carlo simulations. We have implemented the proposed techniques in a prototype simulator, where both biological neuronal and analog electronic circuits can be simulated together in a coupled manner. © 2017 IEEE."
2,10.1007/s10236-017-1074-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020411682&doi=10.1007%2fs10236-017-1074-z&partnerID=40&md5=9c3ec7a06228a69b4ab44c5c1a5cf1f5,"Bayesian estimation/inversion is commonly used to quantify and reduce modeling uncertainties in coastal ocean model, especially in the framework of parameter estimation. Based on Bayes rule, the posterior probability distribution function (pdf) of the estimated quantities is obtained conditioned on available data. It can be computed either directly, using a Markov chain Monte Carlo (MCMC) approach, or by sequentially processing the data following a data assimilation approach, which is heavily exploited in large dimensional state estimation problems. The advantage of data assimilation schemes over MCMC-type methods arises from the ability to algorithmically accommodate a large number of uncertain quantities without significant increase in the computational requirements. However, only approximate estimates are generally obtained by this approach due to the restricted Gaussian prior and noise assumptions that are generally imposed in these methods. This contribution aims at evaluating the effectiveness of utilizing an ensemble Kalman-based data assimilation method for parameter estimation of a coastal ocean model against an MCMC polynomial chaos (PC)-based scheme. We focus on quantifying the uncertainties of a coastal ocean ADvanced CIRCulation (ADCIRC) model with respect to the Manning’s n coefficients. Based on a realistic framework of observation system simulation experiments (OSSEs), we apply an ensemble Kalman filter and the MCMC method employing a surrogate of ADCIRC constructed by a non-intrusive PC expansion for evaluating the likelihood, and test both approaches under identical scenarios. We study the sensitivity of the estimated posteriors with respect to the parameters of the inference methods, including ensemble size, inflation factor, and PC order. A full analysis of both methods, in the context of coastal ocean model, suggests that an ensemble Kalman filter with appropriate ensemble size and well-tuned inflation provides reliable mean estimates and uncertainties of Manning’s n coefficients compared to the full posterior distributions inferred by MCMC. © 2017, Springer-Verlag Berlin Heidelberg."
,10.1016/j.csda.2017.02.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014835842&doi=10.1016%2fj.csda.2017.02.010&partnerID=40&md5=9b25915325c58f9342d9a4fec07c8d55,"In recent years, there has been increasing interest in Bayesian nonparametric methods due to their flexibility, and the availability of Markov chain Monte Carlo (MCMC) methods for sampling from the posterior distribution. As MCMC methods are generally time consuming for computation, there is a need for faster methods, which can be executed within a matter of seconds. A fast alternative to MCMC for sampling the well known and widely used Dirichlet process mixture (DPM) model is investigated to draw approximate independent and identically distributed samples from the posterior distribution of the latent allocations, and then to draw samples from the weights and locations conditional on the allocations. To address the order depend issue of the proposed algorithm, an optimal ordering scheme based on a sequence of optimizations is proposed to first obtain an optimal order of the data, and then run the algorithm on this ordering. The fast sampling algorithm is assisted by parallel computing using commands within MATLAB. © 2017 Elsevier B.V."
,10.1007/s00138-017-0840-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018260263&doi=10.1007%2fs00138-017-0840-8&partnerID=40&md5=952e063ecd62ca266d1645f54908ddac,"This paper presents a framework to predict the performance of multiple target tracking (MTT) techniques. The framework is based on the mathematical descriptors of point processes, the probability generating functional (p.g.fl). It is shown that conceptually the p.g.fls of MTT techniques can be interpreted as a transform that can be marginalized to an expression that encodes all the information regarding the likelihood model as well as the underlying assumptions present in a given tracking technique. In order to use this approach for tracker performance prediction in video sequences, a framework that combines video quality assessment concepts and the marginalized transform is introduced. The multiple hypothesis tracker and Markov Chain Monte Carlo data association methods are used as test cases. We introduce their transforms and perform a numerical comparison to predict their performance under identical conditions. © 2017, Springer-Verlag Berlin Heidelberg."
1,10.1177/0962280215586010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027680445&doi=10.1177%2f0962280215586010&partnerID=40&md5=16682eb9eefb1252fb62c4c103c1e78e,"Many longitudinal studies (e.g. observational studies and randomized clinical trials) have collected multiple rating scales at each visit in the form of patient-reported outcomes (PROs) in the close unit interval [0,1]. We propose a joint modeling framework to address the issues from the following data features: (1) multiple correlated PROs; (2) the presence of the boundary values of zeros and ones; (3) extreme outliers and heavy tails; (4) the PRO-dependent terminal events such as death and dropout. Our modeling framework consists of a multivariate augmented mixed-effects sub-model based on Beta rectangular distributions for the multiple longitudinal outcomes and a Cox model for the terminal events. The simulation studies suggest that in the presence of outliers, heavy tails, and dependent terminal event, our proposed models provide more accurate parameter estimates than the joint model based on Beta distributions. The proposed models are applied to the motivating Long-term Study-1 (LS-1 study, n = 1741) of Parkinson's disease patients. © The Author(s) 2017."
3,10.1007/s00170-017-0064-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010755516&doi=10.1007%2fs00170-017-0064-0&partnerID=40&md5=36ed9ba0bfa586ccaebaa4b79a677e59,"This work discusses the Bayesian parameter inference method for a mechanistic force model for machining. Bayesian inference methods have gained popularity recently owing to their intuitiveness and ease with which empirical knowledge may be combined with experimental data considering the uncertainty. The first part of the paper discusses Bayesian parameter inference and Markov Chain Monte Carlo (MCMC) methods. MCMC method effectiveness has been further analyzed by (1) changing the number of particles in MCMC estimation and (2) changing the MCMC move step size. The second part of the paper discusses two example applications as nonlinear mechanistic force model coefficient identification. The Bayesian inference scheme performs prediction of the cutting force coefficients from the training data. Using these coefficients and input parameters to the model, the cutting force is predicted. This prediction is validated using experimental data, and it is demonstrated that with very few parameter updates the predicted force converges with the measured cutting force. The paper is concluded with the discussion of future work. © 2017, Springer-Verlag London."
1,10.1007/s11432-016-0442-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022035848&doi=10.1007%2fs11432-016-0442-1&partnerID=40&md5=a17b92fd54cc4764b05ff5c450139a93,"Differential privacy (DP) has become one of the most important solutions for privacy protection in recent years. Previous studies have shown that prediction accuracy usually increases as more data mining (DM) logic is considered in the DP implementation. However, although one-step DM computation for decision tree (DT) model has been investigated, existing research has not studied the scenarios when the DP is embedded in two-step DM computation, three-step DM computation until the whole model DM computation. It is very challenging to embed DP in more than two steps of DM computation since the solution space exponentially increases with the increase of computational complexity. In this work, we propose algorithms by making use of Markov Chain Monte Carlo (MCMC) method, which can efficiently search a computationally infeasible space to embed DP into DT generation algorithm. We compare the performance when embedding DP in DT with different depths, i.e., one-step DM computation (previous work), two-step, three-step and the whole model. We find that the deep combination of DP and DT does help to increase the prediction accuracy. However, when the privacy budget is very large (e.g., ϵ = 10), this may overwhelm the complexity of DT model, and the increasing trend is not obvious. We also find that the prediction accuracy decreases with the increase of model complexity. © 2017, Science China Press and Springer-Verlag GmbH Germany."
,10.4230/LIPIcs.WABI.2017.14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028747220&doi=10.4230%2fLIPIcs.WABI.2017.14&partnerID=40&md5=fe30b6bea9487a1e8ba0caf5b90dcbbb,"Peptidic Natural Products (PNPs) are highly sought after bioactive compounds that include many antibiotic, antiviral and antitumor agents, immunosuppressors and toxins. Even though recent advancements in mass-spectrometry have led to the development of accurate sequencing methods for nonlinear (cyclic and branch-cyclic) peptides, requiring only picograms of input material, the identification of PNPs via a database search of mass spectra remains problematic. This holds particularly true when trying to evaluate the statistical significance of Peptide Spectrum Matches (PSM) especially when working with non-linear peptides that often contain non-standard amino acids, modifications and have an overall complex structure. In this paper we describe a new way of estimating the statistical significance of a PSM, defined by any peptide (including linear and non-linear), by using state-of-the-art Markov Chain Monte Carlo methods. In addition to the estimate itself our method also provides an uncertainty estimate in the form of confidence bounds, as well as an automatic simulation stopping rule that ensures that the sample size is sufficient to achieve the desired level of result accuracy. © Anastasiia Abramova and Anton Korobeynikov."
6,10.1016/j.nucengdes.2017.05.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019363804&doi=10.1016%2fj.nucengdes.2017.05.011&partnerID=40&md5=9368bf1db8a397abfe7923b27ee713a5,"Within the BEPU (Best Estimate plus Uncertainty) methodology uncertainties must be quantified in order to prove that the investigated design remains within acceptance criteria. For best-estimate system thermal-hydraulics codes like TRACE and RELAP5, significant uncertainties come from the closure laws which are used to describe transfer terms in the balance equations. The accuracy and uncertainty information of these correlations are usually unknown to the code users, which results in the user simply ignoring or describing them using expert opinion or personal judgment during uncertainty and sensitivity analysis. The purpose of this paper is to replace such ad-hoc expert judgment of the uncertainty information of TRACE physical model parameters with inverse Uncertainty Quantification (UQ) based on OECD/NRC BWR Full-size Fine-Mesh Bundle Tests (BFBT) benchmark steady-state void fraction data. Inverse UQ seeks statistical descriptions of the physical model random input parameters that are consistent with the experimental data. Inverse UQ always captures the uncertainty of its estimates rather than merely determining point estimates of the best-fit input parameters. Bayesian analysis is used to establish the inverse UQ problems based on experimental data, with systematic and rigorously derived surrogate models based on Sparse Gird Stochastic Collocation (SGSC). Global sensitivity analysis including Sobol’ indices and correlation coefficients are used to identify the important TRACE input parameters. Several adaptive Markov Chain Monte Carlo (MCMC) sampling techniques are investigated and implemented to explore the posterior probability density functions. This research solves the problem of lack of uncertainty information for TRACE physical model parameters for the closure relations. The quantified uncertainties are necessary for future uncertainty and sensitivity study of TRACE code in nuclear reactor system design and safety analysis. © 2017 Elsevier B.V."
,10.1093/annweh/wxx046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032178285&doi=10.1093%2fannweh%2fwxx046&partnerID=40&md5=8b6728cd4951908578ed9587d6fc50b2,"Objective: Direct reading instruments are valuable tools for measuring exposure as they provide real-time measurements for rapid decision making. However, their use is limited to general survey applications in part due to issues related to their performance. Moreover, statistical analysis of realtime data is complicated by autocorrelation among successive measurements, non-stationary time series, and the presence of left-censoring due to limit-of-detection (LOD). A Bayesian framework is proposed that accounts for non-stationary autocorrelation and LOD issues in exposure time-series data in order to model workplace factors that affect exposure and estimate summary statistics for tasks or other covariates of interest. Method: A spline-based approach is used to model non-stationary autocorrelation with relatively few assumptions about autocorrelation structure. Left-censoring is addressed by integrating over the left tail of the distribution. The model is fit using Markov-Chain Monte Carlo within a Bayesian paradigm. The method can flexibly account for hierarchical relationships, random effects and fixed effects of covariates. The method is implemented using the rjags package in R, and is illustrated by applying it to real-time exposure data. Estimates for task means and covariates from the Bayesian model are compared to those from conventional frequentist models including linear regression, mixed-effects, and time-series models with different autocorrelation structures. Simulations studies are also conducted to evaluate method performance. Results: Simulation studies with percent of measurements below the LOD ranging from 0 to 50% showed lowest root mean squared errors for task means and the least biased standard deviations from the Bayesian model compared to the frequentist models across all levels of LOD. In the application, task means from the Bayesian model were similar to means from the frequentist models, while the standard deviations were different. Parameter estimates for covariates were significant in some frequentist models, but in the Bayesian model their credible intervals contained zero; such discrepancies were observed in multiple datasets. Variance components from the Bayesian model reflected sub stantial autocorrelation, consistent with the frequentist models, except for the auto-regressive moving average model. Plots of means from the Bayesian model showed good fit to the observed data. Conclusion: The proposed Bayesian model provides an approach for modeling non-stationary autocorrelation in a hierarchical modeling framework to estimate task means, standard deviations, quantiles, and parameter estimates for covariates that are less biased and have better performance characteristics than some of the contemporary methods. © The Author 2017. Published by Oxford University Press on behalf of the British Occupational Hygiene Society."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030252194&partnerID=40&md5=5d6f014572a5f7b93a91d7816e25fa1f,"We propose a Bayesian approach to regression with a scalar response on vector and tensor covariates. Vectorization of the tensor prior to analysis fails to exploit the structure, often leading to poor estimation and predictive performance. We introduce a novel class of multiway shrinkage priors for tensor coefficients in the regression setting and present posterior consistency results under mild conditions. A computationally efficient Markov chain Monte Carlo algorithm is developed for posterior computation. Simulation studies illustrate substantial gains over existing tensor regression methods in terms of estimation and parameter inference. Our approach is further illustrated in a neuroimaging application. ©2017 Rajarshi Guhaniyogi and Shaan Qamar and David B. Dunson."
1,10.1016/j.csda.2017.02.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015391635&doi=10.1016%2fj.csda.2017.02.011&partnerID=40&md5=0d819d454d49f9ca3fb81be9ee9d00a6,"In biomedical studies, it is often of interest to estimate how the risk profile of an adverse event is related to the timing of an intervention. For example, in randomized controlled clinical trials of bivalent human papillomavirus (HPV) vaccine, investigators are interested to know how miscarriage rate relates to the timing of HPV vaccination. A risk window is defined as an interval for the covariate where the risk of adverse event is elevated. Existing methods cannot make simultaneous inference on both the risk window and the magnitude of the risk. A hierarchical Bayesian logistic regression model is developed to estimate the risk window of miscarriage on the time of conception with respect to vaccination. Hierarchical priors are proposed and used in Markov Chain Monte Carlo for statistical inference. The performance of the Bayesian model and two existing methods is evaluated in simulation settings with varying risk windows and relative risks. The proposed model provides both point and interval estimates for the risk window regarding to vaccination, and captures its effect modification on miscarriage risk in pregnancy. Analysis of the vaccine trial using the proposed model shows no significant evidence of an association between the HPV vaccine and miscarriage risk. The hierarchical Bayesian model is useful in general in analyzing a randomized trial or an epidemiological study in which the effect of an agent is potentially modified by a temporal factor. © 2017 Elsevier B.V."
1,10.1214/16-AAP1257,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028724242&doi=10.1214%2f16-AAP1257&partnerID=40&md5=8d165b809e4ad3d0127d1181406132a9,"We introduce new Gaussian proposals to improve the efficiency of the standard Hastings-Metropolis algorithm in Markov chain Monte Carlo (MCMC) methods, used for the sampling from a target distribution in large dimension d. The improved complexity is O(d1/5) compared to the complexity O(d1/3) of the standard approach. We prove an asymptotic diffusion limit theorem and show that the relative efficiency of the algorithm can be characterised by its overall acceptance rate (with asymptotical value 0.704), independently of the target distribution. Numerical experiments confirm our theoretical findings. © Institute of Mathematical Statistics, 2017."
1,10.1371/journal.pone.0180908,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027222934&doi=10.1371%2fjournal.pone.0180908&partnerID=40&md5=9dc8cb489c85e8537f84c373ffaa9ce6,"We present a new open source software tool called BEASTling, designed to simplify the preparation of Bayesian phylogenetic analyses of linguistic data using the BEAST 2 platform. BEASTling transforms comparatively short and human-readable configuration files into the XML files used by BEAST to specify analyses. By taking advantage of Creative Commons-licensed data from the Glottolog language catalog, BEASTling allows the user to conveniently filter datasets using names for recognised language families, to impose monophyly constraints so that inferred language trees are backward compatible with Glottolog classifications, or to assign geographic location data to languages for phylogeographic analyses. Support for the emerging cross-linguistic linked data format (CLDF) permits easy incorporation of data published in cross-linguistic linked databases into analyses. BEASTling is intended to make the power of Bayesian analysis more accessible to historical linguists without strong programming backgrounds, in the hopes of encouraging communication and collaboration between those developing computational models of language evolution (who are typically not linguists) and relevant domain experts. © 2017 Maurits et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
3,10.1111/rssc.12200,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006710869&doi=10.1111%2frssc.12200&partnerID=40&md5=acbfd157ca22de4b46552d3f4f4b97c9,"We investigate the causal relationship between climate and criminal behaviour. Considering the characteristics of integer-valued time series of criminal incidents, we propose a modified Granger causality test based on the generalized auto-regressive conditional heteroscedasticity type of integer-valued time series models to analyse the relationship between the number of crimes and the temperature as an environmental factor. More precisely, we employ the Poisson, negative binomial and log-linear Poisson integer-valued generalized auto-regressive conditional heteroscedasticity models and particularly adopt a Bayesian method for our analysis. The Bayes factors and posterior probability of the null hypothesis help to determine the causality between the variables considered. Moreover, employing an adaptive Markov chain Monte Carlo sampling scheme, we estimate model parameters and initial values. As an illustration, we evaluate our test through a simulation study and, to examine whether or not temperature affects crime activities, we apply our method to data sets categorized as sexual offences, drug offences, theft of motor vehicles, and domestic-violence-related assault in Ballina, New South Wales, Australia. The result reveals that more sexual offences, drug offences and domestic-violence-related assaults occur during the summer than in other seasons of the year. This evidence strongly advocates a causal relationship between crime and temperature. © 2016 Royal Statistical Society"
1,10.1016/j.dib.2017.05.053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021689832&doi=10.1016%2fj.dib.2017.05.053&partnerID=40&md5=eaf916e930bab2b016fdb2124f75bc4d,"The data presented in this article are related to the research article entitled “Comparison between Generalized Linear Modelling and Additive Bayesian Network; Identification of Factors associated with the Incidence of Antibodies against Leptospira interrogans sv Pomona in Meat Workers in New Zealand” (Pittavino et al., 2017) [5]. A prospective cohort study was conducted in four sheep slaughtering abattoirs in New Zealand (NZ) (Dreyfus et al., 2015) [1]. Sera were collected twice a year from 384 meat workers and tested by Microscopic Agglutination for Leptospira interrogans sv Pomona (Pomona) infection, one of the most common Leptospira serovars in humans in NZ. This article provides an extended analysis of the data, illustrating the different steps of a multivariable (i.e. generalized linear model) and especially a multivariate tool based on additive Bayesian networks (ABN) modelling. © 2017 The Authors"
,10.1016/j.compbiomed.2017.05.032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020682419&doi=10.1016%2fj.compbiomed.2017.05.032&partnerID=40&md5=9449eda88b9352beae9e010531d46d33,"Summary Mathematical models of the cardiac cell have started to include markovian representations of the ionic channels instead of the traditional Hodgkin &amp; Huxley formulations. There are many reasons for this: Markov models are not restricted to the idea of independent gates defining the channel, they allow more complex description with specific transitions between open, closed or inactivated states, and more importantly those states can be closely related to the underlying channel structure and conformational changes. Methods We used the LabVIEW® and MATLAB® programs to implement the simulator MarkoLAB that allow a dynamical 3D representation of the markovian model of the channel. The Monte Carlo simulation was used to implement the stochastic transitions among states. The user can specify the voltage protocol by setting the holding potential, the step-to voltage and the duration of the stimuli. Results The most studied feature of a channel is the current flowing through it. This happens when the channel stays in the open state, but most of the time, as revealed by the low open probability values, the channel remains on the inactive or closed states. By focusing only when the channel enters or leaves the open state we are missing most of its activity. MarkoLAB proved to be quite useful to visualize the whole behavior of the channel and not only when the channel produces a current. Such dynamic representation provides more complete information about channel kinetics and will be a powerful tool to demonstrate the effect of gene mutations or drugs on the channel function. Conclusions MarkoLAB provides an original way of visualizing the stochastic behavior of a channel. It clarifies concepts, such as recovery from inactivation, calcium- versus voltage-dependent inactivation, and tail currents. It is not restricted to ionic channels only but it can be extended to other transporters, such as exchangers and pumps. This program is intended as a didactical tool to illustrate the dynamical behavior of a channel. It has been implemented in two platforms MATLAB® and LabVIEW® to enhance the target users of this new didactical tool. The computational cost of implementing a stochastic simulation is within the range of a personal computer performance; making MarkoLAB suitable to be run during a lecture or presentation. © 2017 Elsevier Ltd"
5,10.1007/s10064-016-0869-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962157687&doi=10.1007%2fs10064-016-0869-3&partnerID=40&md5=ab4af433616687ec83bf9157f43ddc0f,"This paper presents a framework for optimization of site investigation program, within which the robustness of the site investigation program and the investigation effort are optimized. A site investigation program is judged robust if the derived statistics of the geotechnical property of interest are robust against the uncertainties caused by limited data availability and test error. In this study, a Markov chain Monte Carlo simulation-based Bayesian inference approach was used to characterize the statistics of the intended geotechnical property. The robustness of the site investigation program was formulated as a byproduct of the Bayesian inference of the geotechnical property statistics. The proposed framework for optimization of the site investigation program was implemented as a bi-objective optimization problem that considers both robustness and investigation effort. The concepts of Pareto Front and knee point were employed to aid in making an informed decision regarding selection of site investigation program. The effectiveness and significance of the proposed framework were demonstrated through a simulation study. © 2016, Springer-Verlag Berlin Heidelberg."
1,10.1007/s00438-017-1322-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019843658&doi=10.1007%2fs00438-017-1322-4&partnerID=40&md5=70e7feb601260a7a6f791a8043b387c9,"Genome-wide association studies (GWAS) have identified a large amount of single-nucleotide polymorphisms (SNPs) associated with complex traits. A recently developed linear mixed model for estimating heritability by simultaneously fitting all SNPs suggests that common variants can explain a substantial fraction of heritability, which hints at the low power of single variant analysis typically used in GWAS. Consequently, many multi-locus shrinkage models have been proposed under a Bayesian framework. However, most use Markov Chain Monte Carlo (MCMC) algorithm, which are time-consuming and challenging to apply to GWAS data. Here, we propose a fast algorithm of Bayesian adaptive lasso using variational inference (BAL-VI). Extensive simulations and real data analysis indicate that our model outperforms the well-known Bayesian lasso and Bayesian adaptive lasso models in accuracy and speed. BAL-VI can complete a simultaneous analysis of a lung cancer GWAS data with ~3400 subjects and ~570,000 SNPs in about half a day. © 2017, Springer-Verlag Berlin Heidelberg."
1,10.1016/j.sste.2017.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021141431&doi=10.1016%2fj.sste.2017.05.001&partnerID=40&md5=f200d4285fa3a0706dd20c47e9997047,"The influence of climatic variables on the dynamics of human malaria has been widely highlighted. Also, it is known that this mosquito-borne infection varies in space and time. However, when the data is spatially incomplete most popular spatio-temporal methods of analysis cannot be applied directly. In this paper, we develop a two step methodology to model the spatio-temporal dependence of malaria incidence on local rainfall, temperature, and humidity as well as the regional sea surface temperatures (SST) in the northern coast of Venezuela. First, we fit an autoregressive distributed lag model (ARDL) to the weekly data, and then, we adjust a linear separable spacial vectorial autoregressive model (VAR) to the residuals of the ARDL. Finally, the model parameters are tuned using a Markov Chain Monte Carlo (MCMC) procedure derived from the Metropolis-Hastings algorithm. Our results show that the best model to account for the variations of malaria incidence from 2001 to 2008 in 10 endemic Municipalities in North-Eastern Venezuela is a logit model that included the accumulated local precipitation in combination with the local maximum temperature of the preceding month as positive regressors. Additionally, we show that although malaria dynamics is highly heterogeneous in space, a detailed analysis of the estimated spatial parameters in our model yield important insights regarding the joint behavior of the disease incidence across the different counties in our study. © 2017 Elsevier Ltd"
1,10.6004/jnccn.2017.0136,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027285984&doi=10.6004%2fjnccn.2017.0136&partnerID=40&md5=b714ea780a3b938a0af09f9452e6c017,"Background: After completing 5 years of adjuvant tamoxifen, women with estrogen receptor (ER)-positive breast cancer benefit from 5 more years of endocrine therapy, either with tamoxifen or an aromatase inhibitor (AI). For premenopausal women, ovarian ablation (OA) would be required before starting an AI (OA/AI). According to the SOFT/TEXT studies, OA/AI improves 5-year disease-free survival compared with tamoxifen alone, suggesting that OA/AI could be superior to tamoxifen as extended endocrine therapy. The long-term costs and consequences of premature menopause from OA are unknown, but could be estimated through a cost-effectiveness analysis. Methods: A Markov chain Monte Carlo simulation model estimated the costs and benefits of 3 extended endocrine strategies in a hypothetical cohort of premenopausal women with ER-positive early breast cancer: (1) no further treatment; (2) tamoxifen for 5 years (extended tamoxifen); or (3) OA/AI for 5 years. Effectiveness was measured in years of life expectancy gain. Sensitivity analyses accounted for uncertainty surrounding various parameters. Monte Carlo simulation estimated the number of adverse events and deaths from each strategy in the US population over a 40-year period. Results: Extended tamoxifen yielded a higher average life expectancy gain than OA/AI (17.31 vs 17.06 years) at lower average cost ($3,550 vs $14,312). For 18,000 premenopausal ER-positive women, the simulation estimated 13,236, 12,557, and 11,338 deaths with no further treatment, extended tamoxifen, and OA/AI, respectively, but an additional 1,897 deaths from OA, for a total of 13,235 deaths associated with OA/AI. After 24.6 years of follow-up, more women are expected to die from OA/AI than extended tamoxifen. Conclusions: For premenopausal women with ER-positive cancer who have completed adjuvant tamoxifen, another 5 years of tamoxifen is the preferable extended endocrine strategy. The potential long-term health consequences of OA could affect overall survival when it precedes the use of an AI. © JNCCN-Journal of the National Comprehensive Cancer Network."
1,10.1016/j.meegid.2017.04.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018483200&doi=10.1016%2fj.meegid.2017.04.015&partnerID=40&md5=8272c41a419df816aee5b7e497b7a62c,"In this study, we examined the molecular evolution of the fusion protein (F) gene in human respiratory syncytial virus subgroup B (HRSV-B). First, we performed time-scale evolution analyses using the Bayesian Markov chain Monte Carlo (MCMC) method. Next, we performed genetic distance, linear B-cell epitope prediction, N-glycosylation, positive/negative selection site, and Bayesian skyline plot analyses. We also constructed a structural model of the F protein and mapped the amino acid substitutions and the predicted B-cell epitopes. The MCMC-constructed phylogenetic tree indicated that the HRSV F gene diverged from the bovine respiratory syncytial virus gene approximately 580 years ago and had a relatively low evolutionary rate (7.14 × 10− 4 substitutions/site/year). Furthermore, a common ancestor of HRSV-A and -B diverged approximately 290 years ago, while HRSV-B diverged into three clusters for approximately 60 years. The genetic similarity of the present strains was very high. Although a maximum of 11 amino acid substitutions were observed in the structural model of the F protein, only one strain possessed an amino acid substitution located within the palivizumab epitope. Four epitopes were predicted, although these did not correspond to the neutralization sites of the F protein including the palivizumab epitope. In addition, five N-glycosylation sites of the present HRSV-B strains were inferred. No positive selection sites were identified; however, many sites were found to be under negative selection. The effective population size of the gene has remained almost constant. On the basis of these results, it can be concluded that the HRSV-B F gene is highly conserved, as is the F protein of HRSV-A. Moreover, our prediction of B-cell epitopes does not show that the palivizumab reaction site may be recognized as an epitope during naturally occurring infections. © 2017 Elsevier B.V."
1,10.1177/0037549717698014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025454088&doi=10.1177%2f0037549717698014&partnerID=40&md5=6e442d8abe1b6655d2985af2cc1442b8,"This paper attempts to understand the reasons for the difference in the value of the round trip time between calculation and simulation. It is posited the main reason for the difference is the combination of two factors: the restricted car capacity and the randomness in the behavior of the elevator traffic system, thus leading to a reduced effective car loading (effectively based on a smaller number of passengers in the car). There are three sources of randomness in the behavior of the system: the randomness of the passenger destinations (thus making the value of the round trip time a random variable), the randomness of the passenger arrival (driven by a Poisson passenger arrival model), and the effect of elevator bunching (thus making the value of the interval a random variable). Using a MATLAB-based simulator, the value of the round trip time is plotted against the system loading level for the case of a single entrance and incoming traffic only. Different conditions are simulated, including constant and random passenger arrivals, as well as queues-allowed and queues-not-allowed conditions. Varying these conditions provides an essential insight into the variation of the round trip time and the reasons for it. The effect of the number of passengers boarding the elevator on the value of the round trip time (and thus on the value of the system handling capacity) is investigated in more detail. © The Author(s) 2017."
1,10.1016/j.spa.2016.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010619199&doi=10.1016%2fj.spa.2016.11.003&partnerID=40&md5=4da17573262270e967a80f3255ff2198,"We study a distributed particle filter proposed by Bolić et al. (2005). This algorithm involves m groups of M particles, with interaction between groups occurring through a “local exchange” mechanism. We establish a central limit theorem in the regime where M is fixed and m→∞. A formula we obtain for the asymptotic variance can be interpreted in terms of colliding Markov chains, enabling analytic and numerical evaluations of how the asymptotic variance behaves over time, with comparison to a benchmark algorithm consisting of m independent particle filters. We prove that subject to regularity conditions, when m is fixed both algorithms converge time-uniformly at rate M−1/2. Through use of our asymptotic variance formula we give counter-examples satisfying the same regularity conditions to show that when M is fixed neither algorithm, in general, converges time-uniformly at rate m−1/2. © 2017 Elsevier B.V."
4,10.1007/s40258-017-0311-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012871305&doi=10.1007%2fs40258-017-0311-4&partnerID=40&md5=d80cce733a5f3ac7d8b8f1e46c78213a,"Background: Chronic hepatitis B is a common, progressive disease, particularly when viral replication is detected. Oral antivirals can suppress viral replication and prevent or delay the development of cirrhosis and liver-related complications. The treatments of chronic hepatitis B cannot totally cure the disease but can prevent its progression to hepatocellular carcinoma, decreasing the levels of both morbidity and mortality. To date, there are several therapies indicated by the international guidelines as first-line treatments for the management of hepatitis B; two of the most effective are those based on either tenofovir or entecavir. Objective: The aim of this study is to evaluate the cost-effectiveness of tenofovir and entecavir in the treatment of naïve patients with chronic hepatitis B. The two treatments are compared with the “no treatment” and to one another. Methods: The cost-effectiveness analysis was conducted using a Markov model; patients entered one of the following health states: chronic hepatitis, cirrhosis (compensated or decompensated), hepatocellular carcinoma, liver transplantation or death. The analysis was carried out from the perspective of the Italian National Health Service by considering a life-time horizon with cycles lasting 1 year and with costs and QALYs (quality-adjusted life years) discounted at a rate of 3.5%. The results of the model were analysed in terms of incremental cost-effectiveness ratio (ICER). Results: ICERs for tenofovir and entecavir emerging from the comparison versus “no treatment” were equal to €10,274.73 and €16,300.44 per QALY gained, respectively, on the life-time horizon. Tenofovir was dominant in the direct comparison with entecavir, indicating more QALYs and a lower consumption of resources. The Monte Carlo simulation demonstrated that in 97% (tenofovir) and in 85% (entecavir) of the scenarios performed, the cost per QALY fell below the threshold of €30,000/QALY. The budget impact analysis showed savings for tenofovir amounting to 33% compared to entecavir in the first year on treatment and to 31% in following years. Conclusions: Entecavir and tenofovir are recommended for the treatment of patients with chronic Hepatitis B in the Italian Health System. In particular, tenofovir appeared to be the more cost-effective drug for the management of chronic hepatitis B virus (HBV) infections. These results could help decision makers and clinicians to address their decision when choosing a first-line treatment for the management of people affected by chronic HBV. © 2017, Springer International Publishing Switzerland."
1,10.1051/0004-6361/201630139,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026325057&doi=10.1051%2f0004-6361%2f201630139&partnerID=40&md5=f05ee080611b61260c680c81b08da9e8,"Aims. We present new IRAM Plateau de Bure Interferometer observations of Arp 220 in HCN, HCO+, HN13C J = 1-0, C2H N = 1-0, SiO J = 2-1, HNCO Jk,k′ = 50,4-40,4, CH3CN(6-5), CS J = 2-1 and 5-4 and 13CO J = 1-0 and 2-1 and of NGC 6240 in HCN, HCO+J = 1-0 and C2H N = 1-0. In addition, we present Atacama Large Millimeter/submill-meter Array science verification observations of Arp 220 in CS J = 4-3 and CH3CN(10-9). Various lines are used to analyse the physical conditions of the molecular gas including the [12CO]/[13CO] and [12CO]/[C18O] abundance ratios. These observations will be made available to the public. Methods. We create brightness temperature line ratio maps to present the different physical conditions across Arp 220 and NGC 6240. In addition, we use the radiative transfer code RADEX and a Monte Carlo Markov chain likelihood code to model the 12CO, 13CO and C18O lines of Arp 220 at ~2′′ (~700 pc) scales, where the 12CO and C18O measurements were obtained from literature. Results. Line ratios of optically thick lines such as 12CO show smoothly varying ratios while the line ratios of optically thin lines such as 13CO show a east-west gradient across Arp 220. The HCN/HCO+ line ratio differs between Arp 220 and NGC 6240, where Arp 220 has line ratios above 2 and NGC 6240 below 1. The radiative transfer analysis solution is consistent with a warm (~40 K), moderately dense (~103.4 cm-3) molecular gas component averaged over the two nuclei. We find [12CO]/[13CO] and [12CO]/[C18O] abundance ratios of ~90 for both. The abundance enhancement of C18O can be explained by stellar nucleosynthesis enrichment of the interstellar medium. © 2017 ESO."
4,10.1007/s40273-017-0510-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019054747&doi=10.1007%2fs40273-017-0510-8&partnerID=40&md5=6ccff0feed3c0ac6003898b8f2dcbb04,"The volume and technical complexity of both academic and commercial research using decision analytic modelling has increased rapidly over the last two decades. The range of software programs used for their implementation has also increased, but it remains true that a small number of programs account for the vast majority of cost-effectiveness modelling work. We report a comparison of four software programs: TreeAge Pro, Microsoft Excel, R and MATLAB. Our focus is on software commonly used for building Markov models and decision trees to conduct cohort simulations, given their predominance in the published literature around cost-effectiveness modelling. Our comparison uses three qualitative criteria as proposed by Eddy et al.: “transparency and validation”, “learning curve” and “capability”. In addition, we introduce the quantitative criterion of processing speed. We also consider the cost of each program to academic users and commercial users. We rank the programs based on each of these criteria. We find that, whilst Microsoft Excel and TreeAge Pro are good programs for educational purposes and for producing the types of analyses typically required by health technology assessment agencies, the efficiency and transparency advantages of programming languages such as MATLAB and R become increasingly valuable when more complex analyses are required. © 2017, Springer International Publishing Switzerland."
3,10.1007/s10596-017-9646-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017179802&doi=10.1007%2fs10596-017-9646-z&partnerID=40&md5=c106e19910880efe5795ee0329456bb1,"This work addresses the estimation of the parameters of an earthquake model by the consequent tsunami, with an application to the Chile 2010 event. We are particularly interested in the Bayesian inference of the location, the orientation, and the slip of an Okada-based model of the earthquake ocean floor displacement. The tsunami numerical model is based on the GeoClaw software while the observational data is provided by a single DARTⓇ buoy. We propose in this paper a methodology based on polynomial chaos expansion to construct a surrogate model of the wave height at the buoy location. A correlated noise model is first proposed in order to represent the discrepancy between the computational model and the data. This step is necessary, as a classical independent Gaussian noise is shown to be unsuitable for modeling the error, and to prevent convergence of the Markov Chain Monte Carlo sampler. Second, the polynomial chaos model is subsequently improved to handle the variability of the arrival time of the wave, using a preconditioned non-intrusive spectral method. Finally, the construction of a reduced model dedicated to Bayesian inference is proposed. Numerical results are presented and discussed. © 2017, Springer International Publishing Switzerland."
2,10.1177/0962280215590284,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027681882&doi=10.1177%2f0962280215590284&partnerID=40&md5=3d5476d96e138acb3db6db6ff25ff165,"Semicontinuous data featured with an excessive proportion of zeros and right-skewed continuous positive values arise frequently in practice. One example would be the substance abuse/dependence symptoms data for which a substantial proportion of subjects investigated may report zero. Two-part mixed-effects models have been developed to analyze repeated measures of semicontinuous data from longitudinal studies. In this paper, we propose a flexible two-part mixed-effects model with skew distributions for correlated semicontinuous alcohol data under the framework of a Bayesian approach. The proposed model specification consists of two mixed-effects models linked by the correlated random effects: (i) a model on the occurrence of positive values using a generalized logistic mixed-effects model (Part I); and (ii) a model on the intensity of positive values using a linear mixed-effects model where the model errors follow skew distributions including skew-t and skew-normal distributions (Part II). The proposed method is illustrated with an alcohol abuse/dependence symptoms data from a longitudinal observational study, and the analytic results are reported by comparing potential models under different random-effects structures. Simulation studies are conducted to assess the performance of the proposed models and method. © The Author(s) 2017."
,10.1007/s40430-017-0787-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024402960&doi=10.1007%2fs40430-017-0787-8&partnerID=40&md5=6a3710d8c5fcfe2bc269ecd264255d96,"Stochastic methods application is emergent in engineering field, leading designers to better solutions during product development. The stochastic characteristic of system parameters, such as geometric dimensions, operating conditions, among others, may lead to unexpected or even undesirable behavior, making it mandatory to take into account the parameters' uncertainties aiming a robust project. An approach considered here to the uncertainties distribution model is the Bayesian inference. This method gives the estimation of the stochastic parameter from previous information and observations of experimental response. After that, it is possible to proceed with the correspondent propagation on the system response. In the context of rotor dynamics, stochastic methods are not yet scattered and deterministic approaches still prevail. This work aims the use of Bayesian inference, particularly the Markov Chain Monte Carlo method, in a simple rotor-bearing system model to evaluate the influence of uncertainties in the journal bearings parameters on the overall behavior of these components. The critical parameters considered here are radial clearance and oil viscosity as function of temperature. © 2017, The Brazilian Society of Mechanical Sciences and Engineering."
1,10.1089/thy.2016.0572,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027134802&doi=10.1089%2fthy.2016.0572&partnerID=40&md5=f4029f630ac7381ab4305546f93550d6,"Background: Lenvatinib (Lenvima®) and sorafenib (Nexavar®) are the two most recently Food and Drug Administration-Approved drugs for treating radioiodine-refractory differentiated thyroid cancer (RR-DTC). Both demonstrated superior progression-free survival over placebo in their respective Phase III clinical trials. This study compared the cost-effectiveness of the two treatments with placebo from a limited societal perspective. Methods: A Markov model was developed to estimate the costs and health benefits for treatment of RR-DTC. The probabilities and survival rates were obtained from two Phase III trials: the SELECT trial comparing lenvatinib to placebo, and the DECISION trial comparing sorafenib to placebo. A bimonthly cycle length and half-cycle correction were used for a lifetime time horizon. Medical costs and utility data were obtained from RedBook, Healthcare Cost and Utilization Project, and the published literature. All costs were adjusted to US$2015, discounted at 3% annually. Then second-order Monte Carlo simulation with distributions was conducted to obtain the acceptability curve to address the uncertainty around model inputs. Results: In the base case, lenvatinib was the most cost-effective treatment compared to sorafenib (incremental cost-effectiveness ratio [ICER] = $25,275/quality-Adjusted life year [QALY]) and placebo (ICER = $40,869). Sorafenib is also cost-effective compared to placebo (ICER = $64,067/QALY). The treatment decisions were found to be sensitive to the treatment costs and the health utility associated with lenvatinib and its side effects. The acceptability curve showed lenvatinib optimal 80% of time at WTP of $100,000/QALY. Conclusions: This study suggests that lenvatinib is the optimally cost-effective treatment for RR-DTC, although both lenvatinib and sorafenib are cost-effective compared to placebo. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
1,10.1073/pnas.1619583114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026638741&doi=10.1073%2fpnas.1619583114&partnerID=40&md5=71ede27c2970d9db5ea1828526693c7f,"Precise estimation of age is essential in evolutionary anthropology, especially to infer population age structures and understand the evolution of human life history diversity. However, in small-scale societies, such as hunter-gatherer populations, time is often not referred to in calendar years, and accurate age estimation remains a challenge. We address this issue by proposing a Bayesian approach that accounts for age uncertainty inherent to fieldwork data. We developed a Gibbs sampling Markov chain Monte Carlo algorithm that produces posterior distributions of ages for each individual, based on a ranking order of individuals from youngest to oldest and age ranges for each individual. We first validate our method on 65 Agta foragers from the Philippines with known ages, and show that our method generates age estimations that are superior to previously published regression-based approaches. We then use data on 587 Agta collected during recent fieldwork to demonstrate how multiple partial age ranks coming from multiple camps of hunter-gatherers can be integrated. Finally, we exemplify how the distributions generated by our method can be used to estimate important demographic parameters in small-scale societies: here, age-specific fertility patterns. Our flexible Bayesian approach will be especially useful to improve cross-cultural life history datasets for small-scale societies for which reliable age records are difficult to acquire. © 2017, National Academy of Sciences. All rights reserved."
,10.1136/injuryprev-2016-042057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987810696&doi=10.1136%2finjuryprev-2016-042057&partnerID=40&md5=42dc076228552fbf41bd8bb24bca1362,"Background Our objective is to evaluate the costeffectiveness of investments in bike lanes using New York City’s (NYC) fiscal year 2015 investment as a case study. We also provide a generalizable model, so that localities can estimate their return on bike lane investments. Methods and findings We evaluate the costeffectiveness of bike lane construction using a two-stage model. Our regression analysis, to estimate the marginal addition of lane miles on the expansion in bike ridership, reveals that the 45.5 miles of bike lanes NYC constructed in 2015 at a cost of $8 109 511.47 may increase the probability of riding bikes by 9.32%. In the second stage, we constructed a Markov model to estimate the cost-effectiveness of bike lane construction. This model compares the status quo with the 2015 investment. We consider the reduced risk of injury and increased probability of ridership, costs associated with bike lane implementation and maintenance, and effectiveness due to physical activity and reduced pollution. We use Monte Carlo simulation and one-way sensitivity analysis to test the reliability of the base-case result. This model reveals that over the lifetime of all people in NYC, bike lane construction produces additional costs of $2.79 and gain of 0.0022 quality-adjusted life years (QALYs) per person. This results in an incremental cost-effectiveness ratio of $1297/QALY gained (95% CI −$544/QALY gained to $5038/QALY gained). Conclusions We conclude that investments in bicycle lanes come with an exceptionally good value because they simultaneously address multiple public health problems. Investments in bike lanes are more cost-effective than the majority of preventive approaches used today. © 2017, BMJ Publishing Group. All rights reserved."
,10.1002/env.2449,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020226502&doi=10.1002%2fenv.2449&partnerID=40&md5=b5243c0390643863d1c21b6fadf69057,"Predicting the occurrence, level, and duration of high air pollution concentrations exceeding a given critical level enables researchers to study the health impact of road traffic on local air quality and to inform public policy action. Precise estimates of the probabilities of occurrence and level of extreme concentrations are formidable due to the combination of complex physical and chemical processes involved. This underpins the need for developing sophisticated extreme value models, in particular allowing for nonstationarity of environmental time series. In this paper, extremes of nitrogen oxide (NO), nitrogen dioxide (NO2), and ozone (O3) concentrations are investigated using two models. Model I is based on an extended peaks-over-threshold (POT) approach developed by A. C. Davison and R. L. Smith, whereby the parameters of the underlying generalized Pareto distribution (GPD) are treated as functions of covariates (i.e., traffic and meteorological factors). The new Model II resolves the lack of threshold stability in the Davison–Smith model by constructing a special functional form for the GPD parameters. For each of the models, the effects of traffic and meteorological factors on the frequency and size of extreme values are estimated using Markov chain Monte Carlo methods. Finally, appropriate goodness-of-fit tests and model selection criteria confirm that Model II significantly outperforms Model I in estimation and forecasting of extremes. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.1007/s10614-016-9606-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981507133&doi=10.1007%2fs10614-016-9606-z&partnerID=40&md5=0b36bb9fdb5de976ba5aaf78cf39ead4,"To analyze the dynamic and asymmetric contagion reactions of financial markets during the last subprime crisis, this paper proposes a contagion reaction equation combined with the generalized auto regressive conditional heteroskedasticity process to develop a dynamic asymmetric contagion model, and then provides the Markov chain Monte Carlo estimation method of this new model. This paper then constructs an empirical study of two metals futures in China during the last subprime crisis period, applying the model to measure the impact of the contagion reactions as well as assess the model’s effectiveness. Our results show: (1) the financial contagion phenomenon is the reason why some financial markets experienced almost corresponding reactions during the subprime crisis; (2) financial contagion reactions behave conspicuously in three particular phases during the subprime crisis; (3) financial contagion reactions have predictive functions for financial market changes and can provide indicators for risk management during crisis periods. © 2016, Springer Science+Business Media New York."
,10.1016/j.spasta.2017.06.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030557488&doi=10.1016%2fj.spasta.2017.06.006&partnerID=40&md5=2642aaf9fe603e61a06c9c7db98b8571,"We consider online prediction of a latent dynamic spatiotemporal process and estimation of the associated model parameters based on noisy data. The problem is motivated by the analysis of spatial data arriving in real-time and the current parameter estimates and predictions are updated using the new data at a fixed computational cost. Estimation and prediction is performed within an empirical Bayes framework with the aid of Markov chain Monte Carlo samples. Samples for the latent spatial field are generated using a sampling importance resampling algorithm with a skewed-normal proposal and for the temporal parameters using Gibbs sampling with their full conditionals written in terms of sufficient quantities which are updated online. The spatial range parameter is estimated by a novel online implementation of an empirical Bayes method, called herein sequential empirical Bayes method. A simulation study shows that our method gives similar results as an offline Bayesian method. We also find that the skewed-normal proposal improves over the traditional Gaussian proposal. The application of our method is demonstrated for online monitoring of radiation after the Fukushima nuclear accident. © 2017 Elsevier B.V."
4,10.1007/s12035-016-9982-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976285836&doi=10.1007%2fs12035-016-9982-3&partnerID=40&md5=b226d0562a654442a4251748a0951d1e,"Desflurane, halothane, isoflurane, propofol, and sevoflurane are widely used anesthetics in pediatric anesthesia. Adverse effect including emergence agitation, postoperative nausea and vomiting, and postoperative pain are common. Prolonged extubation time and emergency time are also troubling anesthesiologists. Previous studies have noted the characteristics of various anesthetics in pediatric anesthesia, while the results were inconclusive and conflicting. In this study, we aimed at performing a comprehensive network meta-analysis concerning the emergence and recovery characteristics of pediatric anesthetics. Relevant articles were retrieved and selected according to our inclusion criteria. Network meta-analysis was performed with a random-effect model within a Bayesian framework. ORs and corresponding 95 % credible intervals were calculated by Markov chain Monte Carlo methods. Node-splitting method was used to calculate the inconsistency. Rank probabilities were assessed by the surface under the cumulative ranking curve (SUCRA). Propofol was recommended as the most efficient and safe anesthetic in pediatric anesthesia with few adverse effects. Desflurane has the highest incidence of emergence agitation and worst recovery characteristics. Halothane was regarded as an efficient anesthetic with the best recovery characteristics, while postoperative nausea and vomiting is a common adverse effect. Isoflurane was reported to be the safest concerning postoperative pain, and cases using sevoflurane in pediatric anesthesia reported the highest incidence of analgesic requirement. Our network meta-analysis demonstrated that propofol was suggested as the first choice in the clinical practice for its efficiency and safe in pediatric anesthesia. © 2016, Springer Science+Business Media New York."
3,10.1016/j.tranpol.2017.04.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019368109&doi=10.1016%2fj.tranpol.2017.04.010&partnerID=40&md5=3af8980b03980c173d3790e23edfe0c9,"To alleviate congestion, many Chinese cities have adopted either one of the two car ownership policies, namely, license plate auction or license plate lottery, to limit the number of cars on the road. In an effort to address the criticism associated with administering a single car ownership policy, cities are considering the possibility of carrying out both policies simultaneously, so that residents can choose whether to pay for the license plate through an auction or get it for free from a lottery but with a longer wait time. We study residents’ preferences toward the two car ownership policies when both are administered at the same time, a problem that has not been investigated in the literature. We then examine the influence of car ownership policies on the choice of electric cars, which is also new to the literature. Using data collected from a stated preference survey, we estimate mixed logit models using the hierarchical Bayes approach based on the Markov Chain Monte Carlo method. Results show that strong preference heterogeneity exists in respondents’ policy choice. We proceed to conduct regression analysis to explain the variations in the preferences toward license plate auction and electric cars. Our main results include: (1) We find that prospective car buyers in Beijing and Shanghai are willing to bid 27,000 yuan and 49,000 yuan to shorten their wait time to get car license plates by one year, respectively; (2) The subsidy to electric cars can be reduced by 102,000 yuan in Beijing and 85,000 yuan in Shanghai if the wait time for an electric car license plate is shortened by one year; (3) Car buyers in favor of license plate auction are those who are from high-income households, who are not buying their first cars, and who are below 30 or above 40 years old; and (4) When promoting the adoption of electric cars, policy incentives, such as making it easier to obtain an electric car license plate and providing attractive subsidies, are as important as the technological advancement electric car manufacturers strive to make, such as improving the driving range of electric cars. © 2017 Elsevier Ltd"
1,10.1088/1475-7516/2017/07/052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026809528&doi=10.1088%2f1475-7516%2f2017%2f07%2f052&partnerID=40&md5=df5078d662ccc47e8edf8f7c5ecaba59,"We make projections for measuring the black hole birth rate from the diffuse supernova neutrino background (DSNB) by future neutrino experiments, and constrain the black hole merger fraction ϵ, when combined with information on the black hole merger rate from gravitational wave experiments such as LIGO. The DSNB originates from neutrinos emitted by all the supernovae in the Universe, and is expected to be made up of two components: neutrinos from neutron-star-forming supernovae, and a sub-dominant component at higher energies from black-hole-forming ""unnovae"". We perform a Markov Chain Monte Carlo analysis of simulated data of the DSNB in an experiment similar to Hyper-Kamiokande, focusing on this second component. Since all knowledge of the neutrino emission from unnovae comes from simulations of collapsing stars, we choose two sets of priors: one where the unnovae are well-understood and one where their neutrino emission is poorly known. By combining the black hole birth rate from the DSNB with projected measurements of the black hole merger rate from LIGO, we show that the fraction of black holes which lead to binary mergers observed today ϵ could be constrained to be within the range 2 ċ 10-4 ≤ ϵ ≤ 3 ċ 10-2 at 3 σ confidence, after ten years of running an experiment like Hyper-Kamiokande. © 2017 IOP Publishing Ltd and Sissa Medialab."
1,10.1109/ICC.2017.7996951,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028333655&doi=10.1109%2fICC.2017.7996951&partnerID=40&md5=059c9d3aeee287c629035c4ba48c9bbe,"The development of low complexity, high performance spatial-multiplexing MIMO detectors continues to be an important area of research capable of increasing the spectral efficiency and capacity of wireless networks. The Markov Chain Monte Carlo (MCMC) detector has shown promise as a high performance method with low complexity growth. We present a solution to the high SNR stalling problems of previous MCMC detectors. Near-MAP performance is verified in simulation and in real-world measurements on an 8-antenna MIMO testbed using the 802.11ac WiFi protocol. This demonstration shows that the channel models predominantly used in the MCMC literature are too well-conditioned to provide an understanding of performance and complexity for indoor channels. Additional information is provided on the methods and techniques to match simulation to measurement and to construct a low cost and effective 8-antenna MIMO testbed. © 2017 IEEE."
1,10.5334/dsj-2017-037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030854398&doi=10.5334%2fdsj-2017-037&partnerID=40&md5=713288ebb43ffa0df8621fdfc5fd9063,"Incomplete data are ubiquitous in social sciences; as a consequence, available data are inefficient (ineffective) and often biased. In the literature, multiple imputation is known to be the standard method to handle missing data. While the theory of multiple imputation has been known for decades, the implementation is difficult due to the complicated nature of random draws from the posterior distribution. Thus, there are several computational algorithms in software: Data Augmentation (DA), Fully Conditional Specification (FCS), and Expectation-Maximization with Bootstrapping (EMB). Although the literature is full of comparisons between joint modeling (DA, EMB) and conditional modeling (FCS), little is known about the relative superiority between the MCMC algorithms (DA, FCS) and the non-MCMC algorithm (EMB), where MCMC stands for Markov chain Monte Carlo. Based on simulation experiments, the current study contends that EMB is a confidence proper (confidence-supporting) multiple imputation algorithm without between-imputation iterations; thus, EMB is more user-friendly than DA and FCS. © 2017 The Author(s)."
,10.1098/rsta.2016.0404,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021091114&doi=10.1098%2frsta.2016.0404&partnerID=40&md5=aff9627cbb08c101efa5f36f298af47e,"Atomistic simulations of thermal desorption spectra for effusion from bulk materials to characterize binding or trapping sites are a challenging task as large system sizes as well as extended time scales are required. Here, we introduce an approach where we combine kinetic Monte Carlo with an analytic approximation of the superbasins within the framework of absorbing Markov chains. We apply our approach to the effusion of hydrogen from BCC iron, where the diffusion within bulk grains is coarse grained using absorbingMarkov chains, which provide an exact solution of the dynamics within a superbasin. Our analytic approximation to the superbasin is transferable with respect to grain size and elliptical shapes and can be applied in simulations with constant temperature as well as constant heating rate. The resulting thermal desorption spectra are in close agreement with direct kinetic Monte Carlo simulations, but the calculations are computationally much more efficient. Our approach is thus applicable to much larger system sizes and provides a first step towards an atomistic understanding of the influence of structural features on the position and shape of peaks in thermal desorption spectra. © 2017 The Author(s) Published by the Royal Society. All rights reserved."
2,10.1109/ICC.2017.7996439,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028310441&doi=10.1109%2fICC.2017.7996439&partnerID=40&md5=3b3859c5e993b139d7fd1994fbe9d4b0,"A fronthaul bridged network has attracted attention as a way of efficiently constructing the centralized radio access network (C-RAN) architecture. If we change the functional split of C-RAN and employ time-division duplex (TDD), the data rate in fronthaul will become variable and the global synchronization of fronthaul streams will occur. This feature results in an increase in the queuing delay in fronthaul bridges among fronthaul flows. This paper proposes a novel low-latency routing scheme designed to satisfy the latency requirements in fronthaul networks with path-control protocols. The proposed scheme formulates the maximum queuing delay by defining competitive links and flows. It selects the set of paths that satisfy the latency requirements with the Markov chain Monte Carlo method using machine learning (MCMC-ML). The initial paths are selected from candidate paths using the learned solutions, and path-reselection is performed with the MCMC method. We confirmed with computer simulations that the proposed scheme can compute routes for all flows that satisfy the delay requirements. We also confirmed that the route computation is accelerated with the learned solutions, even if the flow distribution changes. © 2017 IEEE."
1,10.1145/3087801.3087815,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027891279&doi=10.1145%2f3087801.3087815&partnerID=40&md5=8aaf1b2f13039a683ac6285013bdfaec,"The local computation of Linial [FOCS'87] and Naor and Stockmeyer [STOC'93] concerns with the question of whether a locally definable distributed computing problem can be solved locally: more specifically, for a given local CSP (Constraint Satisfaction Problem) whether a CSP solution can be constructed by a distributed algorithm using local information. In this paper, we consider the problem of sampling a uniform CSP solution by distributed algorithms, and ask whether a locally definable joint distribution can be sampled from locally. More broadly, we consider sampling from Gibbs distributions induced by weighted local CSPs, especially the Markov random fields (MRFs), in the LOCAL model. We give two Markov chain based distributed algorithms which we believe to represent two fundamental approaches for sampling from Gibbs distributions via distributed algorithms. The first algorithm generically parallelizes the single-site sequential Markov chain by updating in each step the variables from a random independent set in parallel, and achieves an O(Δ log n) time upper bound in the LOCAL model, where Δ is the maximum degree, when the Dobrushin's condition for the Gibbs distribution is satisfied. The second algorithm is a novel parallel Markov chain which proposes to update all variables simultaneously yet still guarantees to converge correctly with no bias. It surprisingly parallelizes an intrinsically sequential process: stabilizing to a joint distribution with massive local dependencies, and may achieve an optimal O(log n) time upper bound independent of the maximum degree Δ under a stronger mixing condition. We also show a strong Ω(diam) lower bound for sampling: in particular for sampling independent set in graphs with maximum degree Δ ≥ 6. Independent sets are trivial to construct locally and the sampling lower bound holds even when every node is aware of the entire graph. This gives a strong separation between sampling and constructing locally checkable labelings. © 2017 Association for Computing Machinery."
1,10.1080/00949655.2017.1326119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019612859&doi=10.1080%2f00949655.2017.1326119&partnerID=40&md5=fc0effe14bfbc1092100705d2bd1ddb9,"In this article, we consider the problem of estimation of the stress–strength parameter δ = P(Y < X) based on progressively first-failure-censored samples, when X and Y both follow two-parameter generalized inverted exponential distribution with different and unknown shape and scale parameters. The maximum likelihood estimator of δ and its asymptotic confidence interval based on observed Fisher information are constructed. Two parametric bootstrap boot-p and boot-t confidence intervals are proposed. We also apply Markov Chain Monte Carlo techniques to carry out Bayes estimation procedures. Bayes estimate under squared error loss function and the HPD credible interval of δ are obtained using informative and non-informative priors. A Monte Carlo simulation study is carried out for comparing the proposed methods of estimation. Finally, the methods developed are illustrated with a couple of real data examples. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1109/ICRA.2017.7989169,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027959995&doi=10.1109%2fICRA.2017.7989169&partnerID=40&md5=47b71257726340d179ddaa09f963eb03,"Publicly available map services are widely used by humans for navigation and nowadays provide almost complete road network data. When utilizing such maps for autonomous navigation with mobile robots one is faced with the problem of inaccuracies of the map and the uncertainty about the position of the robot relative to the map. In this paper, we present a probabilistic approach to autonomous robot navigation using data from OpenStreetMap that associates tracks from Open-StreeetMap with the trails detected by the robot based on its 3D-LiDAR data. It combines semantic terrain information, derived from the 3D-LiDAR data, with a Markov-Chain Monte-Carlo technique to match the tracks from OpenStreetMap with the sensor data. This enables our robot to utilize OpenStreetMap for navigation planning and to still stay on the trails during the execution of these plans. We present the results of extensive experiments carried out in real world settings that demonstrate the robustness of our system regarding the alignment of the vehicle pose relative to the OpenStreetMap data. © 2017 IEEE."
,10.1007/s10961-017-9610-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025442487&doi=10.1007%2fs10961-017-9610-z&partnerID=40&md5=8fb77a911dd9587180228a316307078a,"Each year, the United States invests about $45 billion in research conducted by federal researchers within federal laboratories. These efforts generate extensive social benefits when results are transferred to the private sector. It is important that we effectively quantify the economic and societal impact of federal technology transfer activities to inform taxpayers and policymakers about the value of public investments in this form of research. The Argus II device, an artificial retina commercialized in the United States by Second Sight in 2013, provides a rich example of how private sector innovation can be enhanced by research collaborations with federal labs and academia. Over the 25-year journey from idea to product, Second Sight carried out research and development collaborations with six Department of Energy national laboratories and seven universities. The case of Argus II also offers valuable insight into (1) how private industry, academia, and government can work together to bring socially beneficial innovations to fruition and (2) the tradeoffs inherent in these public–private collaborations. In this paper, we use a Markov model to estimate the realized and potential future social benefits associated with Argus II. We provide an interactive tool that can be used to replicate our findings and modify assumptions using updated patient information as it becomes available. We also provide insight into the aspects of federal involvement surrounding the development of Argus II that contributed to its successful commercialization and discuss other spillover benefits from these public–private collaborations. © 2017 Springer Science+Business Media, LLC"
,10.1063/1.4992683,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026679089&doi=10.1063%2f1.4992683&partnerID=40&md5=90564fcf8e1fc5383028f4910cbdd19f,"The Item Response Theory (IRT) has become one of the most popular scoring frameworks for measurement data, frequently used in computerized adaptive testing, cognitively diagnostic assessment and test equating. According to Andrade et al. (2000), IRT can be defined as a set of mathematical models (Item Response Models - IRM) constructed to represent the probability of an individual giving the right answer to an item of a particular test. The number of Item Responsible Models available to measurement analysis has increased considerably in the last fifteen years due to increasing computer power and due to a demand for accuracy and more meaningful inferences grounded in complex data. The developments in modeling with Item Response Theory were related with developments in estimation theory, most remarkably Bayesian estimation with Markov chain Monte Carlo algorithms (Patz & Junker, 1999). The popularity of Item Response Theory has also implied numerous overviews in books and journals, and many connections between IRT and other statistical estimation procedures, such as factor analysis and structural equation modeling, have been made repeatedly (Van der Lindem & Hambleton, 1997). As stated before the Item Response Theory covers a variety of measurement models, ranging from basic one-dimensional models for dichotomously and polytomously scored items and their multidimensional analogues to models that incorporate information about cognitive sub-processes which influence the overall item response process. The aim of this work is to introduce the main concepts associated with one-dimensional models of Item Response Theory, to specify the logistic models with one, two and three parameters, to discuss some properties of these models and to present the main estimation procedures. © 2017 Author(s)."
1,10.1002/sim.7292,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017556494&doi=10.1002%2fsim.7292&partnerID=40&md5=ba593054226d97b4ed8218c79a309960,"Studies of reproductive physiology involve rapid sampling protocols that result in time series of hormone concentrations. The signature pattern in these times series is pulses of hormone release. Various statistical models for quantifying the pulsatile release features exist. Currently these models are fitted separately to each individual and the resulting estimates averaged to arrive at post hoc population-level estimates. When the signal-to-noise ratio is small or the time of observation is short (e.g., 6 h), this two-stage estimation approach can fail. This work extends the single-subject modelling framework to a population framework similar to what exists for complex pharamacokinetics data. The goal is to leverage information across subjects to more clearly identify pulse locations and improve estimation of other model parameters. This modelling extension has proven difficult because the pulse number and locations are unknown. Here, we show that simultaneously modelling a group of subjects is computationally feasible in a Bayesian framework using a birth–death Markov chain Monte Carlo estimation algorithm. Via simulation, we show that this population-based approach reduces the false positive and negative pulse detection rates and results in less biased estimates of population-level parameters of frequency, pulse size, and hormone elimination. We then apply the approach to a reproductive study in healthy women where approximately one-third of the 21 subjects in the study did not have appropriate fits using the single-subject fitting approach. Using the population model produced more precise, biologically plausible estimates of all model parameters. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
6,10.3847/1538-4357/aa77f4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026345600&doi=10.3847%2f1538-4357%2faa77f4&partnerID=40&md5=b764d0f516460993cfafc6af0787281f,"The redshifted 21 cm monopole is expected to be a powerful probe of the epoch of the first stars and galaxies (10 < z < 35). The global 21 cm signal is sensitive to the thermal and ionization state of hydrogen gas and thus provides a tracer of sources of energetic photons-primarily hot stars and accreting black holes-which ionize and heat the high redshift intergalactic medium (IGM). This paper presents a strategy for observations of the global spectrum with a realizable instrument placed in a low-altitude lunar orbit, performing night-time 40-120 MHz spectral observations, while on the farside to avoid terrestrial radio frequency interference, ionospheric corruption, and solar radio emissions. The frequency structure, uniformity over large scales, and unpolarized state of the redshifted 21 cm spectrum are distinct from the spectrally featureless, spatially varying, and polarized emission from the bright foregrounds. This allows a clean separation between the primordial signal and foregrounds. For signal extraction, we model the foreground, instrument, and 21 cm spectrum with eigenmodes calculated via Singular Value Decomposition analyses. Using a Markov Chain Monte Carlo algorithm to explore the parameter space defined by the coefficients associated with these modes, we illustrate how the spectrum can be measured and how astrophysical parameters (e.g., IGM properties, first star characteristics) can be constrained in the presence of foregrounds using the Dark Ages Radio Explorer (DARE). © 2017. The American Astronomical Society. All rights reserved."
3,10.1109/ICSE.2017.28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027707946&doi=10.1109%2fICSE.2017.28&partnerID=40&md5=4506c68a3a30c52694c37746c1d78299,"Program obfuscation is a common practice in software development to obscure source code or binary code, in order to prevent humans from understanding the purpose or logic of software. It protects intellectual property and deters malicious attacks. While tremendous efforts have been devoted to the development of various obfuscation techniques, we have relatively little knowledge on how to most effectively use them together. The biggest challenge lies in identifying the most effective combination of obfuscation techniques. This paper presents a unified framework to optimize program obfuscation. Given an input program P and a set T of obfuscation transformations, our technique can automatically identify a sequence seq = t1, t2,.., tn (&ForAll;i &Element; [1, n].Ti &Element; T), such that applying ti in order on P yields the optimal obfuscation performance. We model the process of searching for seq as a mathematical optimization problem. The key technical contributions of this paper are: (1) an obscurity language model to assess obfuscation effectiveness/optimality, and (2) a guided stochastic algorithm based on Markov chain Monte Carlo methods to search for the optimal solution seq. We have realized the framework in a tool Closure∗ for JavaScript, and evaluated it on 25 most starred JavaScript projects on GitHub (19K lines of code). Our machinery study shows that Closure∗ outperforms the well-known Google Closure Compiler by defending 26% of the attacks initiated by JSNice. Our human study also reveals that Closure∗ is practical and can reduce the human attack success rate by 30%. © 2017 IEEE."
2,10.1186/s12879-017-2592-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024858212&doi=10.1186%2fs12879-017-2592-5&partnerID=40&md5=ff6802e9fb814d921a162bd5fb3893dc,"Background: China has a high prevalence of human papillomavirus (HPV) and a consequently high burden of disease with respect to cervical cancer. The HPV vaccine has proved to be effective in preventing cervical cancer and is now a part of routine immunization programs worldwide. It has also proved to be cost effective. This study aimed to assess the cost-effectiveness of 2-, 4-, and 9-valent HPV vaccines (hereafter, HPV2, 4 or 9) combined with current screening strategies in China. Methods: A Markov model was developed for a cohort of 100,000 HPV-free girls to simulate the natural history to HPV infection. Three recommended screening methods (1. liquid-based cytology test + HPV DNA test; 2. pap smear cytology test + HPV DNA test; 3. visual inspection with acetic acid) and three types of HPV vaccination program (HPV2/4/9) were incorporated into 15 intervention options, and the incremental cost-effectiveness ratio (ICER) was calculated to determine the dominant strategies. Costs, transition probabilities and utilities were obtained from a review of the literature and national databases. One-way sensitivity analyses and threshold analyses were performed for key variables in different vaccination scenarios. Results: HPV9 combined with screening showed the highest health impact in terms of reducing HPV-related diseases and increasing the number of quality-adjusted life years (QALYs). Under the current thresholds of willingness to pay (WTP, 3 times the per capita GDP or USD$ 23,880), HPV4/9 proved highly cost effective, while HPV2 combined with screening cost more and was less cost effective. Only when screening coverage increased to 60% ~ 70% did the HPV2 and screening combination strategy become economically feasible. Conclusions: The combination of the HPV4/9 vaccine with current screening strategies for adolescent girls was highly cost-effective and had a significant impact on reducing the HPV infection-related disease burden in Mainland China. © 2017 The Author(s)."
,10.3390/e19070327,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024911892&doi=10.3390%2fe19070327&partnerID=40&md5=1626cbaf4b871abe3078e09245ddf6c6,"The coupling between dark energy and dark matter provides a possible approach to mitigate the coincidence problem of the cosmological standard model. In this paper, we assumed the interacting term was related to the Hubble parameter, energy density of dark energy, and equation of state of dark energy. The interaction rate between dark energy and dark matter was a constant parameter, which was, Q = 3Hξ(1 + wx)ρx. Based on the Markov chain Monte Carlo method, we made a global fitting on the interacting dark energy model from Planck 2015 cosmic microwave background anisotropy and observational Hubble data. We found that the observational data sets slightly favored a small interaction rate between dark energy and dark matter; however, there wasnot obvious evidence of interaction at the 1σ level. © 2017 by the authors."
3,10.1016/j.eswa.2017.02.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012974071&doi=10.1016%2fj.eswa.2017.02.003&partnerID=40&md5=a6764dcb19a34839368144c4ea265ca7,"User Experience (UX) design has become an important factor of product success. One of the important issues involved in UX design is how to evaluate UX. In this research, UX evaluation is quantitatively fulfilled by the cumulative prospect theory, in which UX is perceived from the perspective of the decision making procedure of two alternative design profiles. Furthermore, we study the influence of affective states on UX prospect evaluation through shaping affective parameters involved in UX design. To account for multiple sources of uncertainties, we develop a hierarchical Bayesian model via Markov chain Monte Carlo technique for parameter estimation under three affective states. Also, aircraft cabin interior design is studied as a case study to demonstrate the potential and feasibility of the proposed method. © 2017 Elsevier Ltd"
,10.1016/j.scitotenv.2017.03.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014555894&doi=10.1016%2fj.scitotenv.2017.03.003&partnerID=40&md5=79f25c5ee2652fbb42e5bfbf47dbf0e1,"Selecting proper rate equations for the kinetic models is essential to quantify biotransformation processes in the environment. Bayesian model selection method can be used to evaluate the candidate models. However, comparisons of all plausible models can result in high computational cost, while limiting the number of candidate models may lead to biased results. In this work, we developed an integrated Bayesian method to simultaneously perform model selection and parameter estimation by using a generalized rate equation. In the approach, the model hypotheses were represented by discrete parameters and the rate constants were represented by continuous parameters. Then Bayesian inference of the kinetic models was solved by implementing Markov Chain Monte Carlo simulation for parameter estimation with the mixed (i.e., discrete and continuous) priors. The validity of this approach was illustrated through a synthetic case and a nitrogen transformation experimental study. It showed that our method can successfully identify the plausible models and parameters, as well as uncertainties therein. Thus this method can provide a powerful tool to reveal more insightful information for the complex biotransformation processes. © 2017 Elsevier B.V."
1,10.1016/j.neuroimage.2017.04.069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019367608&doi=10.1016%2fj.neuroimage.2017.04.069&partnerID=40&md5=03600c1d3135f6d94a94f5816ea23650,"We propose a voxel-wise general linear model with autoregressive noise and heteroscedastic noise innovations (GLMH) for analyzing functional magnetic resonance imaging (fMRI) data. The model is analyzed from a Bayesian perspective and has the benefit of automatically down-weighting time points close to motion spikes in a data-driven manner. We develop a highly efficient Markov Chain Monte Carlo (MCMC) algorithm that allows for Bayesian variable selection among the regressors to model both the mean (i.e., the design matrix) and variance. This makes it possible to include a broad range of explanatory variables in both the mean and variance (e.g., time trends, activation stimuli, head motion parameters and their temporal derivatives), and to compute the posterior probability of inclusion from the MCMC output. Variable selection is also applied to the lags in the autoregressive noise process, making it possible to infer the lag order from the data simultaneously with all other model parameters. We use both simulated data and real fMRI data from OpenfMRI to illustrate the importance of proper modeling of heteroscedasticity in fMRI data analysis. Our results show that the GLMH tends to detect more brain activity, compared to its homoscedastic counterpart, by allowing the variance to change over time depending on the degree of head motion. © 2017 Elsevier Inc."
,10.1093/bioinformatics/btx248,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024488834&doi=10.1093%2fbioinformatics%2fbtx248&partnerID=40&md5=ae2d5b638aa19ae6871da877ea2d1f52,"Motivation: Epigenome-wide association studies can provide novel insights into the regulation of genes involved in traits and diseases. The rapid emergence of bisulfite-sequencing technologies enables performing such genome-wide studies at the resolution of single nucleotides. However, analysis of data produced by bisulfite-sequencing poses statistical challenges owing to low and uneven sequencing depth, as well as the presence of confounding factors. The recently introduced Mixed model Association for Count data via data AUgmentation (MACAU) can address these challenges via a generalized linear mixed model when confounding can be encoded via a single variance component. However, MACAU cannot be used in the presence of multiple variance components. Additionally, MACAU uses a computationally expensive Markov Chain Monte Carlo (MCMC) procedure, which cannot directly approximate the model likelihood. Results: We present a new method, Mixed model Association via a Laplace ApproXimation (MALAX), that is more computationally efficient than MACAU and allows to model multiple variance components. MALAX uses a Laplace approximation rather than MCMC based approximations, which enables to directly approximate the model likelihood. Through an extensive analysis of simulated and real data, we demonstrate that MALAX successfully addresses statistical challenges introduced by bisulfite-sequencing while controlling for complex sources of confounding, and can be over 50% faster than the state of the art. © 2017 The Author. Published by Oxford University Press. All rights reserved."
1,10.1093/bioinformatics/btx253,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024502073&doi=10.1093%2fbioinformatics%2fbtx253&partnerID=40&md5=ad2b930ddfa4e8fe9edaa0e73b52295e,"Motivation: Biological cells operate in a noisy regime influenced by intrinsic, extrinsic and external noise, which leads to large differences of individual cell states. Stochastic effects must be taken into account to characterize biochemical kinetics accurately. Since the exact solution of the chemical master equation, which governs the underlying stochastic process, cannot be derived for most biochemical systems, approximate methods are used to obtain a solution. Results: In this study, a method to efficiently simulate the various sources of noise simultaneously is proposed and benchmarked on several examples. The method relies on the combination of the sigma point approach to describe extrinsic and external variability and the τ,-leaping algorithm to account for the stochasticity due to probabilistic reactions. The comparison of our method to extensive Monte Carlo calculations demonstrates an immense computational advantage while losing an acceptable amount of accuracy. Additionally, the application to parameter optimization problems in stochastic biochemical reaction networks is shown, which is rarely applied due to its huge computational burden. To give further insight, a MATLAB script is provided including the proposed method applied to a simple toy example of gene expression. © 2017 The Author. Published by Oxford University Press. All rights reserved."
1,10.1103/PhysRevE.96.012406,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026447085&doi=10.1103%2fPhysRevE.96.012406&partnerID=40&md5=3092103d19a76f245fa42c42f5f01bb9,"Signaling in enzymatic networks is typically triggered by environmental fluctuations, resulting in a series of stochastic chemical reactions, leading to corruption of the signal by noise. For example, information flow is initiated by binding of extracellular ligands to receptors, which is transmitted through a cascade involving kinase-phosphatase stochastic chemical reactions. For a class of such networks, we develop a general field-theoretic approach to calculate the error in signal transmission as a function of an appropriate control variable. Application of the theory to a simple push-pull network, a module in the kinase-phosphatase cascade, recovers the exact results for error in signal transmission previously obtained using umbral calculus [Hinczewski and Thirumalai, Phys. Rev. X 4, 041017 (2014)2160-330810.1103/PhysRevX.4.041017]. We illustrate the generality of the theory by studying the minimal errors in noise reduction in a reaction cascade with two connected push-pull modules. Such a cascade behaves as an effective three-species network with a pseudointermediate. In this case, optimal information transfer, resulting in the smallest square of the error between the input and output, occurs with a time delay, which is given by the inverse of the decay rate of the pseudointermediate. Surprisingly, in these examples the minimum error computed using simulations that take nonlinearities and discrete nature of molecules into account coincides with the predictions of a linear theory. In contrast, there are substantial deviations between simulations and predictions of the linear theory in error in signal propagation in an enzymatic push-pull network for a certain range of parameters. Inclusion of second-order perturbative corrections shows that differences between simulations and theoretical predictions are minimized. Our study establishes that a field theoretic formulation of stochastic biological signaling offers a systematic way to understand error propagation in networks of arbitrary complexity. © 2017 us. Published by the American Physical Society."
,10.1109/PTC.2017.7981039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034736302&doi=10.1109%2fPTC.2017.7981039&partnerID=40&md5=bd06f4bf4209579dd878387c9b0f7acb,"An adequate modelling of the harmonic load is required to analyse the harmonic impact and perform propagation studies. Since more and more nonlinear loads are connected with the network, such as energy saving lamps, electric vehicles, photovoltaics systems etc., the accurate modelling of the harmonic injection of new appliances becomes more important. In this paper, a bottom-up stochastic model is proposed for the modelling of harmonic loads in residential networks. First a Markov Chain Monte Carlo approach is employed for household occupancy modelling with time use survey database. The occupancy, weather conditions, neighbourhood features and behavioural survey data are subsequently applied to obtain loading patterns of the household appliances. The harmonic spectra of various appliances are established based on measurements. Based on these, a harmonic load flow can be run to calculate the harmonic load of each household. Measurements of harmonic magnitude carried out in a residential low voltage network in the Netherlands are used to validate the proposed approach. © 2017 IEEE."
,10.1145/3105831.3105860,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028061039&doi=10.1145%2f3105831.3105860&partnerID=40&md5=6dd34a6caa9e6158c7aa2ebd248a0667,"Protection of copyrighted contents is a crucial activity for digital content producers in order to avoid unauthorized use of the artifacts or worse in order to prevent sensible information to be stealth (e.g. private documents of an administrative board). A common solution is to uniquely identify each copy by embedding some distinguishing features into it. This activity is usually known as fingerprinting (a.k.a. watermarking) and the embedded content is referred as fingerprinting code. In order to make this process robust against possible malicious users attacks, it is mandatory to hide the positions where the code is embedded. However, even in the case that the positions where the code is embedded are hidden to the users, a group of malicious users (referred in what follows as pirates) may establish a coalition in order to compare their copies and identify the positions where they differ as a positions where the fingerprinting code has been embedded. This kind of attack is named coalition attack. If the coalition succeeds in this identification process, pirates can then arbitrarily change the fingerprint code embedded in the distributed copy so that it does not correspond to any of the original fingerprinting codes of the pirates. However, for the purpose of designing proper protection strategies, it can be assumed that they do not know the positions of the hidden code where the bits of their codes agreed and therefore they cannot alter these positions. This assumption is referred as the marking condition. A (collusion resistant) fingerprinting code can be built by a randomized procedure to choose codewords (the code generation) and a tracing algorithm tailored for tracing one of the pirates based on all these codewords and the forged codeword read from the unauthorized copy distributed by the pirates. Obviously, we should avoid two type of errors: 1) accusing an innocent user and 2) not accusing a pirate. In this respect, the tracing algorithm fails if it falsely accuses an innocent user or outputs no accused user at all. The above mentioned errors should occur with small probability. This problem have been largely investigated in the literature and all the approaches proposed so far shares a common terminology that we introduce here in order to ease the reading of next sections. More in detail, we briefly recall the following key terms: • Alphabet size. The codewords are sequences over a fixed alphabet Σ. Usually, fingerprinting codes are built by leveraging the binary alphabet Σ = {0,1}, however larger alphabets can be used thus the size Σ of the alphabet is an important parameter; • Codelength. This parameter refers to the length of the codewords, usually denoted by n; • Number of users. Usually denoted by N, it coincides with the number of codewords. • Pirate Coalition Size. This parameter takes into account the actual size of the coalition that could be lower than the expected one (say it c), in such a case, the accusation algorithm should achieve a small error probability1; • Error probability. A code is ∈-secure against a coalition of c pirates if the probability of the error of the accusation algorithm is at most e for any set of at most c pirates performing an arbitrary pirate strategy to produce the forged codeword under the marking assumption; • Code rate. The rate R of a fingerprinting code is computed as r = log(N)/n, where the logarithm is binary. The goal of fingerprinting schemes is to find efficient and secure fingerprinting codes while taking into account the high cost of embedding every single digit of the code. In several real world applications fingerprinting codes may be short, such as the case of fingerprinting text documents due to the intrinsic difficulty of embedding information in text. However, in literature many proposal have been defined based on the seminal work of Tardos[11, 12] that state many interesting theoretical results on the generation of short codes that under some conditions guarantees the possibility of finding guilty users. Tardos fingerprinting scheme is optimum as it generates fingerprinting codes sufficient to deal with n users and c pirates with the guarantee that the probability of accusing an innocent is bounded by a constant e, and having a length which is asymptotically minimum. Moreover, the accusation algorithm allows to detect a traitor by looking only at the code they have been assigned to him, disregarding both the codes assigned to other users and the type of attack that have been performed. It is worth noticing that in literature have been defined many other approaches that slightly outperforms Tardos scheme while having the same asymptotic complexity [9, 10]. Unfortunately, Tardos based fingerprinting are not effective in accusation processes when the leveraged code is too short [1, 13]. For instance, in the case that the code has to be embedded in a textual document by applying some modification of words, phrases or generally speaking tokens appearing in the text of the document as described in [3, 5, 6], and the document is about 20 pages long it is expected that the longest fingerprint that can be embedded is at most 200 bit long. In such a case, the Tardos accusation algorithm fails in accusing any user with a suitable probability of being guilty. For instance, in the case that the maximum coalition size is 2 and the desired probability of being guilty is 90% it requires a code of length at least 800 bits to accuse an user. This code length could be impractical in many scenarios. In order to overcome the above mentioned limitations, new approaches have been proposed and one of the most interesting is joint-decoding. Joint-decoders compute the guilty probability for a set of users instead of a single one. A first proposal has been made in [7, 8], however those algorithms are tailored for small coalition and do not scale-up properly. This drawback occurs as the search space computation grows up exponentially w.r.t. the number of users (or the maximum expected number of users that we may conjecture that could form a coalition for spreading the pirated copy). To ameliorate this problem, joint-decoding has been investigated from the theoretical viewpoint in order to define efficient approaches that work properly for real life situations. In this respect, a Markov Chain Monte Carlo (MCMC) based approach has been proposed in [2]. The proposed approach leverages Gibbs sampling for estimating the marginal probability that a user joined a coalition for generating a pirated copy. However, this approach turns to be ineffective for code length greater than 1024 bit due to the low quality of the probability estimation (as noted by the authors themselves). The main limitation of the above mentioned Tardos based approaches is that they perform satisfactorily when dealing with image or video fingerprinting[4] while their use for textual documents turns to be ineffective. More in detail, textual document watermarking is prone to several types of attacks, even very simple ones like the so called cut & paste attack. This attack allows to completely strip the watermark and the corresponding fingerprinting code by simply extracting the text in the source document and inserting it in a brand new document. The latter cause the deletion of eventual watermark inserted in the source text. This type of attack causes the fingerprint of the pirated copy to be empty, thus avoiding any accuse to users by using Tardos based schemes. In order to overcome such limitations, we propose a simpler but still effective accusation model based on Metropolis-Hastings (MH) sampling. Next sections are devoted to describe our proposal. © 2017 ACM."
,10.3390/ijerph14070765,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024118513&doi=10.3390%2fijerph14070765&partnerID=40&md5=bd53d2708ddafbde0ba7747d583a371f,"This study presents an approach for obtaining realization sets of parameters for nitrogen removal in a pilot-scale waste stabilization pond (WSP) system. The proposed approach was designed for optimal parameterization, local sensitivity analysis, and global uncertainty analysis of a dynamic simulation model for the WSP by using the R software package Flexible Modeling Environment (R-FME) with the Markov chain Monte Carlo (MCMC) method. Additionally, generalized likelihood uncertainty estimation (GLUE) was integrated into the FME to evaluate the major parameters that affect the simulation outputs in the study WSP. Comprehensive modeling analysis was used to simulate and assess nine parameters and concentrations of ON-N, NH3-N and NO3-N. Results indicate that the integrated FME-GLUE-based model, with good Nash-Sutcliffe coefficients (0.53-0.69) and correlation coefficients (0.76-0.83), successfully simulates the concentrations of ON-N, NH3-N and NO3-N. Moreover, the Arrhenius constant was the only parameter sensitive to model performances of ON-N and NH3-N simulations. However, Nitrosomonas growth rate, the denitrification constant, and the maximum growth rate at 20 °C were sensitive to ON-N and NO3-N simulation, which was measured using global sensitivity. © 2017 by the authors. Licensee MDPI, Basel, Switzerland."
,10.1088/1742-6596/869/1/012073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028705069&doi=10.1088%2f1742-6596%2f869%2f1%2f012073&partnerID=40&md5=31139223b8d334e91017a363d812dec0,We present the observation of a transit of the exoplanet TrES-3b by the newly commissioned robotic telescope TRAPPIST-North located at Oukaimeden Observatory (Morocco). The obtained light curve reaches a photometric precison 600 ppm. Its Bayesian analysis with a Markov Chain Monte Carlo code enables us to refine the radius of the planet to Rp = 1.346+0.065 -0.050 RJup. These results demonstrate the high potential of TRAPPIST-North for high-photometry of exoplanet transits. © Published under licence by IOP Publishing Ltd.
6,10.1016/j.bpj.2017.05.048,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022100207&doi=10.1016%2fj.bpj.2017.05.048&partnerID=40&md5=a1f2c50bc60fa13bc10e32c1928fa2aa,"Circadian clocks must be able to entrain to time-varying signals to keep their oscillations in phase with the day-night rhythm. On the other hand, they must also exhibit input compensation: their period must remain approximately one day in different constant environments. The posttranslational oscillator of the Kai system can be entrained by transient or oscillatory changes in the ATP fraction, yet is insensitive to constant changes in this fraction. We study in three different models of this system how these two seemingly conflicting criteria are met. We find that one of these (our recently published Paijmans model) exhibits the best tradeoff between input compensation and entrainability: on the footing of equal phase-response curves, it exhibits the strongest input compensation. Performing stochastic simulations at the level of individual hexamers allows us to identify a new, to our knowledge, mechanism, which is employed by the Paijmans model to achieve input compensation: at lower ATP fraction, the individual hexamers make a shorter cycle in the phosphorylation state space, which compensates for the slower pace at which they traverse the cycle. © 2017 Biophysical Society"
1,10.12989/sem.2017.63.1.047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026235284&doi=10.12989%2fsem.2017.63.1.047&partnerID=40&md5=2e81b8d29299c06c0d57e60de18861e3,"The fundamental goal of this study is to minimize the uncertainty of the median fragility curve and to assess the structural vulnerability under earthquake excitation. Bayesian Inference with Markov Chain Monte Carlo (MCMC) simulation has been presented for efficient collapse response assessment of the independent intake water tower. The intake tower is significantly used as a diversion type of the hydropower station for maintaining power plant, reservoir and spillway tunnel. Therefore, the seismic fragility assessment of the intake tower is a pivotal component for estimating total system risk of the reservoir. In this investigation, an asymmetrical independent slender reinforced concrete structure is considered. The Bayesian Inference method provides the flexibility to integrate the prior information of collapse response data with the numerical analysis results. The preliminary information of risk data can be obtained from various sources like experiments, existing studies, and simplified linear dynamic analysis or nonlinear static analysis. The conventional lognormal model is used for plotting the fragility curve using the data from time history simulation and nonlinear static pushover analysis respectively. The Bayesian Inference approach is applied for integrating the data from both analyses with the help of MCMC simulation. The method achieves meaningful improvement of uncertainty associated with the fragility curve, and provides significant statistical and computational efficiency. © 2017 Techno-Press, Ltd."
,10.3390/ijerph14070734,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022183663&doi=10.3390%2fijerph14070734&partnerID=40&md5=dcab45d06ebd7f5e2a7f81aac401e218,"We implemented a spatial model for analysing PM10 maxima across the Mexico City metropolitan area during the period 1995-2016. We assumed that these maxima follow a non-identical generalized extreme value (GEV) distribution and modeled the trend by introducing multivariate smoothing spline functions into the probability GEV distribution. A flexible, three-stage hierarchical Bayesian approach was developed to analyse the distribution of the PM10 maxima in space and time. We evaluated the statistical model’s performance by using a simulation study. The results showed strong evidence of a positive correlation between the PM10 maxima and the longitude and latitude. The relationship between time and the PM10 maxima was negative, indicating a decreasing trend over time. Finally, a high risk of PM10 maxima presenting levels above 1000 μg/m3 (return period: 25 yr) was observed in the northwestern region of the study area. © 2017 by the authors. Licensee MDPI, Basel, Switzerland."
2,10.3390/ijerph14070735,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022023896&doi=10.3390%2fijerph14070735&partnerID=40&md5=e329f5feac462a0da2e8336596fe9241,"Considerable effort has been devoted to incorporate temporal trends in disease mapping. In this line, this work describes the importance of including the effect of the seasonality in a particular setting related with suicides. In particular, the number of suicide-related emergency calls is modeled by means of an autoregressive approach to spatio-temporal disease mapping that allows for incorporating the possible interaction between both temporal and spatial effects. Results show the importance of including seasonality effect, as there are differences between the number of suicide-related emergency calls between the four seasons of each year. © 2017 by the authors. Licensee MDPI, Basel, Switzerland."
6,10.1080/10543406.2016.1167075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974698238&doi=10.1080%2f10543406.2016.1167075&partnerID=40&md5=4a5da59bf962fee47996df1fae9a2279,We develop an efficient Markov chain Monte Carlo algorithm for the mixed-effects model for repeated measures (MMRM) and a class of pattern mixture models (PMMs) via monotone data augmentation (MDA). The proposed algorithm is particularly useful for multiple imputation in PMMs and is illustrated by the analysis of an antidepressant trial. We also describe the full data augmentation (FDA) algorithm for MMRM and PMMs and show that the marginal posterior distributions of the model parameters are the same in the MDA and FDA algorithms. © 2017 Taylor & Francis.
,10.1080/02664763.2016.1221903,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982261366&doi=10.1080%2f02664763.2016.1221903&partnerID=40&md5=54c3a94a709d05574cd50fe484a93480,"Variables taking value in (0, 1), such as rates or proportions, are frequently analyzed by researchers, for instance, political and social data, as well as the Human Development Index (HDI). However, sometimes this type of data cannot be modeled adequately using a unique distribution. In this case, we can use a mixture of distributions, which is a powerful and flexible probabilistic tool. This manuscript deals with a mixture of simplex distributions to model proportional data. A fully Bayesian approach is proposed for inference which includes a reversible-jump Markov Chain Monte Carlo procedure. The usefulness of the proposed approach is confirmed by using of the simulated mixture data from several different scenarios and by using the methodology to analyze municipal HDI data of cities (or towns) in the Northeast region and São Paulo state in Brazil. The analysis shows that among the cities in the Northeast, some appear to have a similar HDI to other cities in São Paulo state. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
1,10.1080/02664763.2016.1214692,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980369586&doi=10.1080%2f02664763.2016.1214692&partnerID=40&md5=c029266616835d18f362f3dc4d1a2265,"In this paper, the estimation of parameters for a generalized inverted exponential distribution based on the progressively first-failure type-II right-censored sample is studied. An expectation–maximization (EM) algorithm is developed to obtain maximum likelihood estimates of unknown parameters as well as reliability and hazard functions. Using the missing value principle, the Fisher information matrix has been obtained for constructing asymptotic confidence intervals. An exact interval and an exact confidence region for the parameters are also constructed. Bayesian procedures based on Markov Chain Monte Carlo methods have been developed to approximate the posterior distribution of the parameters of interest and in addition to deduce the corresponding credible intervals. The performances of the maximum likelihood and Bayes estimators are compared in terms of their mean-squared errors through the simulation study. Furthermore, Bayes two-sample point and interval predictors are obtained when the future sample is ordinary order statistics. The squared error, linear-exponential and general entropy loss functions have been considered for obtaining the Bayes estimators and predictors. To illustrate the discussed procedures, a set of real data is analyzed. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
2,10.5194/esurf-5-331-2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022025733&doi=10.5194%2fesurf-5-331-2017&partnerID=40&md5=157f70e69a816538fec9f2e5ec274494,"The rate at which low-lying sandy areas in temperate regions, such as the Campine Plateau (NE Belgium), have been eroding during the Quaternary is a matter of debate. Current knowledge on the average pace of landscape evolution in the Campine area is largely based on geological inferences and modern analogies. We performed a Bayesian inversion of an in situ-produced 10Be concentration depth profile to infer the average long-term erosion rate together with two other parameters: The surface exposure age and the inherited 10Be concentration. Compared to the latest advances in probabilistic inversion of cosmogenic radionuclide (CRN) data, our approach has the following two innovative components: It (1) uses Markov chain Monte Carlo (MCMC) sampling and (2) accounts (under certain assumptions) for the contribution of model errors to posterior uncertainty. To investigate to what extent our approach differs from the state of the art in practice, a comparison against the Bayesian inversion method implemented in the CRONUScalc program is made. Both approaches identify similar maximum a posteriori (MAP) parameter values, but posterior parameter and predictive uncertainty derived using the method taken in CRONUScalc is moderately underestimated. A simple way for producing more consistent uncertainty estimates with the CRONUScalc-like method in the presence of model errors is therefore suggested. Our inferred erosion rate of 39 ± 8. 9mmkyr-1 (1σ) is relatively large in comparison with landforms that Erode under comparable (paleo-)climates elsewhere in the world. We evaluate this value in the light of the erodibility of the substrate and sudden base level lowering during the Middle Pleistocene. A denser sampling scheme of a two-nuclide concentration depth profile would allow for better inferred erosion rate resolution, and including more uncertain parameters in the MCMC inversion. © Author(s) 2017."
4,10.1080/17415977.2016.1215446,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982845425&doi=10.1080%2f17415977.2016.1215446&partnerID=40&md5=92553813a1057dd43d0d224937d68408,"Bayesian techniques have been widely used in finite element model (FEM) updating. The attraction of these techniques is their ability to quantify and characterize the uncertainties associated with dynamic systems. In order to update an FEM, the Bayesian formulation requires the evaluation of the posterior distribution function. For large systems, this function is difficult to solve analytically. In such cases, the use of sampling techniques often provides a good approximation of this posterior distribution function. The hybrid Monte Carlo (HMC) method is a classic sampling method used to approximate high-dimensional complex problems. However, the acceptance rate of HMC is sensitive to the system size, as well as to the time step used to evaluate the molecular dynamics trajectory. The shadow HMC technique (SHMC), which is a modified version of the HMC method, was developed to improve sampling for large system sizes by drawing from a modified shadow Hamiltonian function. However, the SHMC algorithm performance is limited by the use of a non-separable modified Hamiltonian function. Moreover, two additional parameters are required for the sampling procedure, which could be computationally expensive. To overcome these weaknesses, the separable shadow HMC (S2HMC) method has been introduced. This method uses a transformation to a different parameter space to generate samples. In this paper, we analyse the application and performance of these algorithms, including the parameters used in each algorithm, their limitations and the effects on model updating. The accuracy and the efficiency of the algorithms are demonstrated by updating the finite element models of two real mechanical structures. It is observed that the S2HMC algorithm has a number of advantages over the other algorithms; for example, the S2HMC algorithm is able to efficiently sample at larger time steps while using fewer parameters than the other algorithms. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
1,10.1080/10618600.2017.1336446,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026506363&doi=10.1080%2f10618600.2017.1336446&partnerID=40&md5=b6745e772d114d9f885f5909fc9268a2,"It is common to subsample Markov chain output to reduce the storage burden. Geyer shows that discarding k − 1 out of every k observations will not improve statistical efficiency, as quantified through variance in a given computational budget. That observation is often taken to mean that thinning Markov chain Monte Carlo (MCMC) output cannot improve statistical efficiency. Here, we suppose that it costs one unit of time to advance a Markov chain and then θ &gt; 0 units of time to compute a sampled quantity of interest. For a thinned process, that cost θ is incurred less often, so it can be advanced through more stages. Here, we provide examples to show that thinning will improve statistical efficiency if θ is large and the sample autocorrelations decay slowly enough. If the lag ℓ ⩾ 1 autocorrelations of a scalar measurement satisfy ρℓ &gt; ρℓ + 1 &gt; 0, then there is always a θ &lt; ∞ at which thinning becomes more efficient for averages of that scalar. Many sample autocorrelation functions resemble first order AR(1) processes with ρℓ = ρ|ℓ| for some − 1 &lt; ρ &lt; 1. For an AR(1) process, it is possible to compute the most efficient subsampling frequency k. The optimal k grows rapidly as ρ increases toward 1. The resulting efficiency gain depends primarily on θ, not ρ. Taking k = 1 (no thinning) is optimal when ρ ⩽ 0. For ρ &gt; 0, it is optimal if and only if θ ⩽ (1 − ρ)2/(2ρ). This efficiency gain never exceeds 1 + θ. This article also gives efficiency bounds for autocorrelations bounded between those of two AR(1) processes. Supplementary materials for this article are available online. © 2017 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America."
,10.1080/16843703.2016.1226593,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020170245&doi=10.1080%2f16843703.2016.1226593&partnerID=40&md5=01a2de5c339d74ab8f3ee1f9ac07d7f5,"Degradation data provide useful information about the reliability assessment for highly reliable and long lifetime products. Motivated by laser degradation data, a new degradation modelling approach is proposed, in which degradation path has linear mean and linear standard deviation functions. In this article, the population degradation modelling and individual real time reliability assessment are discussed, and a Bayesian framework is proposed to integrate population degradation information and individual degradation data. The population degradation path is characterized by a random effect independent increment process where random effect captures unit to unit variation, and the Markov Chain Monte Carlo (MCMC) method is used to estimate the unknown parameters. To obtain individual real time reliability assessment, the parameters are updated iteratively using Bayesian theory. Based on updated results, the residual use life and individual real-time reliability evaluation are obtained. For an illustration of the usefulness and validity of the proposed model and method, a numerical example about laser data is given. © 2016 International Chinese Association of Quantitative Management."
,10.1080/00031305.2017.1305289,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031813125&doi=10.1080%2f00031305.2017.1305289&partnerID=40&md5=37da6a77046edd84e126f3b220f8e81b,"Students of statistics should be taught the ideas and methods that are widely used in practice and that will help them understand the world of statistics. Today, this means teaching them about Bayesian methods. In this article, I present ideas on teaching an undergraduate Bayesian course that uses Markov chain Monte Carlo and that can be a second course or, for strong students, a first course in statistics. © 2017 American Statistical Association."
6,10.1080/10586458.2016.1158134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981725342&doi=10.1080%2f10586458.2016.1158134&partnerID=40&md5=58a5bd0bdb133d433fabef1309a4d3a1,"We describe a Markov Chain Monte Carlo algorithm which can be used to generate naturally labeled n-element posets at random with a probability distribution of one’s choice. Implementing this algorithm for the uniform distribution, we explore the approach to the asymptotic regime in which almost every poset takes on the three-layer structure described by Kleitman and Rothschild (KR). By tracking the n-dependence of several order-invariants, among them the height of the poset, we observe an oscillatory behavior which is very unlike a monotonic approach to the KR regime. Only around n = 40 or so does this “finite size dance” appear to give way to a gradual crossover to asymptopia which lasts until n = 85, the largest n we have simulated. © 2017 Taylor & Francis."
,10.1080/01621459.2016.1260465,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032470183&doi=10.1080%2f01621459.2016.1260465&partnerID=40&md5=870a34fcdca93747e28d957ada7155fc,"Finding functional modules in gene regulation networks is an important task in systems biology. Many methods have been proposed for finding communities in static networks; however, the application of such methods is limited due to the dynamic nature of gene regulation networks. In this article, we first propose a statistical framework for detecting common modules in the Drosophila melanogaster time-varying gene regulation network. We then develop both a significance test and a robustness test for the identified modular structure. We apply an enrichment analysis to our community findings, which reveals interesting results. Moreover, we investigate the consistency property of our proposed method under a time-varying stochastic block model framework with a temporal correlation structure. Although we focus on gene regulation networks in our work, our method is general and can be applied to other time-varying networks. Supplementary materials for this article are available online. © 2017 American Statistical Association."
,10.1080/03610918.2015.1134570,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011629272&doi=10.1080%2f03610918.2015.1134570&partnerID=40&md5=3d79f6534dd8b1a03171aa386d7777c3,"This paper investigates the new prior distribution on the Unobserved-Autoregressive Conditional Heteroscedasticity (ARCH) unit root test. Monte Carlo simulations show that the sample size is seriously effective in efficiency of Bayesian test. To improve the performance of Bayesian test for unit root, we propose a new Bayesian test that is robust in the presence of stationary and nonstationary Unobserved-ARCH. The finite sample property of the proposed test statistic is evaluated using Monte Carlo studies. Applying the developed method, we test the policy of daily exchange rate of the German Marc with respect to the Greek Drachma. © 2017 Taylor & Francis Group, LLC."
3,10.1080/13696998.2017.1301943,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015203890&doi=10.1080%2f13696998.2017.1301943&partnerID=40&md5=6542a6ddfde43f6ff006f0eee4000eb6,"Background and aims: IDegLira, a fixed ratio combination of insulin degludec and glucagon-like peptide-1 receptor agonist liraglutide, utilizes the complementary mechanisms of action of these two agents to improve glycemic control with low risk of hypoglycemia and avoidance of weight gain. The aim of the present analysis was to assess the long-term cost-effectiveness of IDegLira vs liraglutide added to basal insulin, for patients with type 2 diabetes not achieving glycemic control on basal insulin in the US setting. Methods: Projections of lifetime costs and clinical outcomes were made using the IMS CORE Diabetes Model. Treatment effect data for patients receiving IDegLira and liraglutide added to basal insulin were modeled based on the outcomes of a published indirect comparison, as no head-to-head clinical trial data is currently available. Costs were accounted in 2015 US dollars ($) from a healthcare payer perspective. Results: IDegLira was associated with small improvements in quality-adjusted life expectancy compared with liraglutide added to basal insulin (8.94 vs 8.91 discounted quality-adjusted life years [QALYs]). The key driver of improved clinical outcomes was the greater reduction in glycated hemoglobin associated with IDegLira. IDegLira was associated with mean costs savings of $17,687 over patient lifetimes vs liraglutide added to basal insulin, resulting from lower treatment costs and cost savings as a result of complications avoided. Conclusions: The present long-term modeling analysis found that IDegLira was dominant vs liraglutide added to basal insulin for patients with type 2 diabetes failing to achieve glycemic control on basal insulin in the US, improving clinical outcomes and reducing direct costs. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/03610918.2015.1130837,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011590490&doi=10.1080%2f03610918.2015.1130837&partnerID=40&md5=e6ab96f1c63c9fac6f56057b5f94efa1,"We consider a Bayesian nonignorable model to accommodate a nonignorable selection mechanism for predicting small area proportions. Our main objective is to extend a model on selection bias in a previously published paper, coauthored by four authors, to accommodate small areas. These authors assume that the survey weights (or their reciprocals that we also call selection probabilities) are available, but there is no simple relation between the binary responses and the selection probabilities. To capture the nonignorable selection bias within each area, they assume that the binary responses and the selection probabilities are correlated. To accommodate the small areas, we extend their model to a hierarchical Bayesian nonignorable model and we use Markov chain Monte Carlo methods to fit it. We illustrate our methodology using a numerical example obtained from data on activity limitation in the U.S. National Health Interview Survey. We also perform a simulation study to assess the effect of the correlation between the binary responses and the selection probabilities. © 2017 Taylor & Francis Group, LLC."
,10.1080/10618600.2017.1297240,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021884581&doi=10.1080%2f10618600.2017.1297240&partnerID=40&md5=00d2edf3efc4b9d4a488c5ebf58bc654,"We present a diagnostic for monitoring convergence of a Markov chain Monte Carlo (MCMC) sampler to its target distribution. In contrast to popular existing methods, we monitor convergence to the joint target distribution directly rather than a select scalar projection. The method uses a simple nonparametric posterior approximation based on a state-space partition obtained by clustering the pooled draws from multiple chains, and convergence is determined when the estimated posterior probabilities of partition elements under each chain are sufficiently similar. This framework applies to a wide variety of problems, and generalizes directly to non-Euclidean state spaces. Our method also provides approximate high-posterior-density regions, and a characterization of differences between nonconverged chains, all with little additional computational burden. We demonstrate this approach on applications to sampling posterior distributions over Rp, graphs, and partitions. Supplementary materials for this article are available online. © 2017, In the public domain."
,10.1080/01621459.2016.1189337,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017447336&doi=10.1080%2f01621459.2016.1189337&partnerID=40&md5=7a332e642a8d62188b92ab2a48016138,"The time-average covariance matrix (TACM) ∑ : ∑kϵZ Γk, where Γk is the auto-covariance function, is an important quantity for the inference of the mean of an Rd -valued stationary process (d ⩾ 1). This article proposes two recursive estimators for Σ with optimal asymptotic mean square error (AMSE) under different strengths of serial dependence. The optimal estimator involves a batch size selection, which requires knowledge of a smoothness parameter ϒβ := ∑kϵZ |k|β, Γk for some β. This article also develops recursive estimators for ϒβ. Combining these two estimators, we obtain a fully automatic procedure for optimal online estimation for Σ. Consistency and convergence rates of the proposed estimators are derived. Applications to confidence region construction and Markov chain Monte Carlo convergence diagnosis are discussed. Supplementary materials for this article are available online. © 2017 American Statistical Association."
5,10.1080/01621459.2016.1192545,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017457839&doi=10.1080%2f01621459.2016.1192545&partnerID=40&md5=e3c58bdf228a34a992e0853c7c3f77f2,"In spite of the recent surge of interest in quantile regression, joint estimation of linear quantile planes remains a great challenge in statistics and econometrics. We propose a novel parameterization that characterizes any collection of noncrossing quantile planes over arbitrarily shaped convex predictor domains in any dimension by means of unconstrained scalar, vector and function valued parameters. Statistical models based on this parameterization inherit a fast computation of the likelihood function, enabling penalized likelihood or Bayesian approaches to model fitting. We introduce a complete Bayesian methodology by using Gaussian process prior distributions on the function valued parameters and develop a robust and efficient Markov chain Monte Carlo parameter estimation. The resulting method is shown to offer posterior consistency under mild tail and regularity conditions. We present several illustrative examples where the new method is compared against existing approaches and is found to offer better accuracy, coverage and model fit. Supplementary materials for this article are available online. © 2017 American Statistical Association."
8,10.1371/journal.pntd.0005696,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026291462&doi=10.1371%2fjournal.pntd.0005696&partnerID=40&md5=1928ba301aa628bcc6fa4874a93f090c,"The aim of this study is to model the association between weekly time series of dengue case counts and meteorological variables, in a high-incidence city of Colombia, applying Bayesian hierarchical dynamic generalized linear models over the period January 2008 to August 2015. Additionally, we evaluate the model’s short-term performance for predicting dengue cases. The methodology shows dynamic Poisson log link models including constant or time-varying coefficients for the meteorological variables. Calendar effects were modeled using constant or first- or second-order random walk time-varying coefficients. The meteorological variables were modeled using constant coefficients and first-order random walk time-varying coefficients. We applied Markov Chain Monte Carlo simulations for parameter estimation, and deviance information criterion statistic (DIC) for model selection. We assessed the short-term predictive performance of the selected final model, at several time points within the study period using the mean absolute percentage error. The results showed the best model including first-order random walk time-varying coefficients for calendar trend and first-order random walk time-varying coefficients for the meteorological variables. Besides the computational challenges, interpreting the results implies a complete analysis of the time series of dengue with respect to the parameter estimates of the meteorological effects. We found small values of the mean absolute percentage errors at one or two weeks out-of-sample predictions for most prediction points, associated with low volatility periods in the dengue counts. We discuss the advantages and limitations of the dynamic Poisson models for studying the association between time series of dengue disease and meteorological variables. The key conclusion of the study is that dynamic Poisson models account for the dynamic nature of the variables involved in the modeling of time series of dengue disease, producing useful models for decision-making in public health. © 2017 Martínez-Bello et al."
,10.1002/qre.2071,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989318600&doi=10.1002%2fqre.2071&partnerID=40&md5=86dbb07b4efc745c58334a638b02af86,"Markov chain Monte Carlo (MCMC) techniques have been extensively developed and are accepted for solving various real-world problems. However, process capabilities are rarely analyzed with the means of MCMC. This study integrates the MCMC technique into Bayesian models for assessing the well-known quality loss index Cpm for gamma and Weibull process distributions. After the MCMC iterations are completed, the quality manager can make reliable decisions via the proposed credible intervals. Furthermore, this study provides performance comparisons of the estimators of Cpm obtained by the MCMC and bootstrap techniques. Simulations show that the MCMC technique performs better than the bootstrap technique in most of the cases that were considered. Copyright © 2016 John Wiley &amp; Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd."
1,10.1134/S0361768817040053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025817868&doi=10.1134%2fS0361768817040053&partnerID=40&md5=6b32001574a3b37bea10051ffc473f55,"The paper considers a problem of multiple person tracking. We present the algorithm to automatic people tracking on surveillance videos recorded by static cameras. Proposed algorithm is an extension of approach based on tracking-by-detection of people heads and data association using Markov chain Monte Carlo (MCMC). Short track fragments (tracklets) are built by local tracking of people heads. Tracklet postprocessing and accurate results interpolation were shown to reduce number of false positives. We use position deviations of tracklets and revised entry/exit points factor to separate pedestrians from false positives. The paper presents a new method to estimate body position, that increases precision of tracker. Finally, we switched HOG-based detector to cascade one. Our evaluation shows proposed modifications significantly increase tracking quality. © 2017, Pleiades Publishing, Ltd."
1,10.1109/JPHOTOV.2017.2690876,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019005272&doi=10.1109%2fJPHOTOV.2017.2690876&partnerID=40&md5=ee6280092e9de224a3e4558d9a963a75,"This paper presents an alternative approach to obtain, from experimental measurements, physical parameters of organic solar cells associated with a given model. In order to get rid of the limitations of common fitting methods, we use a specific Markov chain Monte Carlo technique. This method is applied to a two-dimensional model of an organic solar cell. Measurements carried out under dark and one sun conditions, from two complementary cells, allow access to more reliable values of the active layer parameters. The corresponding set of parameters generates JV -curves in excellent agreement with the measurements for a range of different illumination intensities. Similar extractions are applied on temperature-dependent parameters, from experimental data acquired at various temperatures. As the simulation results reproduce the measurement data rather well, we show that this approach can also be useful to test or determine the governing law associated with some of the temperature-dependent parameters. In addition, analyzing the simulated responses of the model allows the identification of model limitations. The approach discussed in this paper, not specific to organic solar cells, can be applied to a large range of condensed matter topics. © 2011-2012 IEEE."
1,10.1016/j.jmva.2017.05.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033401268&doi=10.1016%2fj.jmva.2017.05.009&partnerID=40&md5=7e251079b22e098075609596ab73f28b,Markov chain Monte Carlo (MCMC) is a simulation method commonly used for estimating expectations with respect to a given distribution. We consider estimating the covariance matrix of the asymptotic multivariate normal distribution of a vector of sample means. Geyer (1992) developed a Monte Carlo error estimation method for estimating a univariate mean. We propose a novel multivariate version of Geyer's method that provides an asymptotically valid estimator for the covariance matrix and results in stable Monte Carlo estimates. The finite sample properties of the proposed method are investigated via simulation experiments. © 2017 Elsevier Inc.
2,10.1103/PhysRevD.96.014015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027071555&doi=10.1103%2fPhysRevD.96.014015&partnerID=40&md5=4d4d7035269ddba678620695d49346c1,"We present a new procedure to determine parton distribution functions (PDFs), based on Markov chain Monte Carlo (MCMC) methods. The aim of this paper is to show that we can replace the standard χ2 minimization by procedures grounded on statistical methods, and on Bayesian inference in particular, thus offering additional insight into the rich field of PDFs determination. After a basic introduction to these techniques, we introduce the algorithm we have chosen to implement - namely Hybrid (or Hamiltonian) Monte Carlo. This algorithm, initially developed for Lattice QCD, turns out to be very interesting when applied to PDFs determination by global analyses; we show that it allows us to circumvent the difficulties due to the high dimensionality of the problem, in particular concerning the acceptance. A first feasibility study is performed and presented, which indicates that Markov chain Monte Carlo can successfully be applied to the extraction of PDFs and of their uncertainties. © 2017 American Physical Society."
,10.1145/3071178.3071234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026386995&doi=10.1145%2f3071178.3071234&partnerID=40&md5=bf0841e7fb0bfec294f0eeb6daf44d40,"Genetic algorithms and genetic programming lend themselves well to the field of machine learning, which involves solving test case based problems. However, most traditional multi-objective selection methods work with scalar objectives, such as minimizing false negative and false positive rates, that are computed from underlying test cases. In this paper, we propose a new fuzzy selection operator that takes into account the statistical nature of machine learning problems based on test cases. Rather than use a Pareto rank or strength computed from scalar objectives, such as with NSGA2 or SPEA2, we will compute a probability of Pareto optimality. This will be accomplished through covariance estimation and Markov chain Monte Carlo simulation in order to generate probabilistic objective scores for each individual. We then compute a probability that each individual will generate a Pareto optimal solution. This probability is directly used with a roulette wheel selection technique. Our method's performance is evaluated on the evolution of a feature selection vector for a binary classification on each of eight different activities. Fuzzy selection performance varies, outperforming both NSGA2 and SPEA2 in both speed (measured in generations) and solution quality (measured by area under the curve) in some cases, while underperforming in others. © 2017 ACM."
3,10.1007/s00477-016-1279-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975297914&doi=10.1007%2fs00477-016-1279-6&partnerID=40&md5=4e1650f9b5642b8ff4339f1fdc501f29,"The creeping characteristics of drought make it possible to mitigate drought’s effects with accurate forecasting models. Drought forecasts are inevitably plagued by uncertainties, making it necessary to derive forecasts in a probabilistic framework. In this study, we proposed a new probabilistic scheme to forecast droughts that used a discrete-time finite state-space hidden Markov model (HMM) aggregated with the Representative Concentration Pathway 8.5 (RCP) precipitation projection (HMM-RCP). The standardized precipitation index (SPI) with a 3-month time scale was employed to represent the drought status over the selected stations in South Korea. The new scheme used a reversible jump Markov chain Monte Carlo algorithm for inference on the model parameters and performed an RCP precipitation projection transformed SPI (RCP-SPI) weight-corrected post-processing for the HMM-based drought forecasting to perform a probabilistic forecast of SPI at the 3-month time scale that considered uncertainties. The point forecasts which were derived as the HMM-RCP forecast mean values, as measured by forecasting skill scores, were much more accurate than those from conventional models and a climatology reference model at various lead times. We also used probabilistic forecast verification and found that the HMM-RCP provided a probabilistic forecast with satisfactory evaluation for different drought categories, even at long lead times. In a drought event analysis, the HMM-RCP accurately predicted about 71.19 % of drought events during the validation period and forecasted the mean duration with an error of less than 1.8 months and a mean severity error of <0.57. The results showed that the HMM-RCP had good potential in probabilistic drought forecasting. © 2016, Springer-Verlag Berlin Heidelberg."
,10.1016/j.csda.2017.01.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012273138&doi=10.1016%2fj.csda.2017.01.006&partnerID=40&md5=08bf4a4e60949c855ff75cb3782dce95,"A novel approach to perform unsupervised sequential learning for functional data is proposed. The goal is to extract reference shapes (referred to as templates) from noisy, deformed and censored realizations of curves and images. The proposed model generalizes the Bayesian dense deformable template model, a hierarchical model in which the template is the function to be estimated and the deformation is a nuisance, assumed to be random with a known prior distribution. The templates are estimated using a Monte Carlo version of the online Expectation–Maximization (EM) algorithm. The designed sequential inference framework is significantly more computationally efficient than equivalent batch learning algorithms, especially when the missing data is high-dimensional. Some numerical illustrations on curve registration problem and templates extraction from images are provided to support the methodology. © 2017 Elsevier B.V."
,10.1016/j.csda.2017.01.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014116003&doi=10.1016%2fj.csda.2017.01.007&partnerID=40&md5=14d2c7478a46d68c947b4c936ac01607,"The authors develop a Bayesian local influence method for semiparametric structural equation models. The effects of minor perturbations to individual observations, the prior distributions of parameters, and the sampling distribution on the statistical inference are assessed with various perturbation schemes. A Bayesian perturbation manifold is constructed to characterize such perturbation schemes. The first- and second-order influence measures are proposed to quantify the degree of minor perturbations on different aspects of a statistical model via objective functions, such as Bayes factor. Simulation studies are conducted to evaluate the empirical performance of the Bayesian local influence procedure. An application to a study of bone mineral density is presented. © 2017 Elsevier B.V."
1,10.1016/j.ijfatigue.2017.03.043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017244056&doi=10.1016%2fj.ijfatigue.2017.03.043&partnerID=40&md5=1effb7decd0370c5399b077416e0372d,"The problem minimizing the number of specimens required for fatigue data analysis is considered in this research. Assuming unknown hyperparameters described via prior distributions, a hierarchical Bayesian model with accumulated prior information was proposed to deal with this issue. One of the main advantages of hierarchical Bayesian model over the empirical Bayesian model is that the prior distributions with hierarchical structure can incorporate structural prior and subjective prior simultaneously. The probabilistic stress-cycle (P-S-N) curves are generated from the predictive distributions, involving both the randomness of parameters and the scatter of observations, and calculated by an identical hierarchical structure. The numerical calculation is done via the Gibbs sampler, which makes the whole process simple and intuitive. © 2017 Elsevier Ltd"
9,10.1061/(ASCE)EM.1943-7889.0001240,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018745372&doi=10.1061%2f%28ASCE%29EM.1943-7889.0001240&partnerID=40&md5=306544545c7347558a8e275c02558f7d,"This paper addresses the statistical uncertainties associated with the estimation of a depth-dependent trend function and spatial variation about the trend function using limited site-specific geotechnical data. Specifically, the statistical uncertainties associated with the following elements are considered: (1) the functional form (shape) of the trend function; (2) the parameters of the trend function (e.g., intercept and gradient); and (3) the random field parameters describing spatial variation about the trend function, namely standard deviation (σ) and scale of fluctuation (δ). The problem is resolved with a two-step Bayesian framework. In Step 1, a set of suitable basis functions that parameterize the trend function is selected using sparse Bayesian learning. In Step 2, an advanced Markov chain Monte Carlo method is adopted for the Bayesian analysis. The two-step approach is shown to be consistent in the well-defined sense that the resulting 95% Bayesian confidence interval (or region) contains the actual trend (or actual σ and δ) with a chance that is close to 0.95. Inconsistency can occur when the spatial variability has a large σ or a large δ relative to data record length. © 2016 American Society of Civil Engineers."
2,10.1002/biot.201600613,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013230729&doi=10.1002%2fbiot.201600613&partnerID=40&md5=41c5727bef74ffaeed30dbbc563bbe8b,"Biotechnological separation processes are routinely designed and optimized using parallel high-throughput experiments and/or serial experiments. Well-characterized processes can further be optimized using mechanistic models. In all these cases – serial/parallel experiments and modeling – iterative strategies are customarily applied for planning novel experiments/simulations based on the previously acquired knowledge. Process optimization is typically complicated by conflicting design targets, such as productivity and yield. We address these issues by introducing a novel algorithm that combines recently developed approaches for utilizing statistical regression models in multi-objective optimization. The proposed algorithm is demonstrated by simultaneous optimization of elution gradient and pooling strategy for chromatographic separation of a three-component system with respect to purity, yield, and processing time. Gaussian Process Regression Models (GPM) are used for estimating functional relationships between design variables (gradient, pooling) and performance indicators (purity, yield, time). The Pareto front is iteratively approximated by planning new experiments such as to maximize the Expected Hypervolume Improvement (EHVI) as determined from the GPM by Markov Chain Monte Carlo (MCMC) sampling. A comprehensive Monte-Carlo study with in-silico data illustrates efficiency, effectiveness and robustness of the presented Multi-Objective Global Optimization (MOGO) algorithm in determining best compromises between conflicting objectives with comparably very low experimental effort. Copyright © 2017 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim"
7,10.1007/s00477-016-1230-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010218923&doi=10.1007%2fs00477-016-1230-x&partnerID=40&md5=a2432a6cd816d2baf3bdd575de079d12,"Empirical tsunami fragility curves are developed based on a Bayesian framework by accounting for uncertainty of input tsunami hazard data in a systematic and comprehensive manner. Three fragility modeling approaches, i.e. lognormal method, binomial logistic method, and multinomial logistic method, are considered, and are applied to extensive tsunami damage data for the 2011 Tohoku earthquake. A unique aspect of this study is that uncertainty of tsunami inundation data (i.e. input hazard data in fragility modeling) is quantified by comparing two tsunami inundation/run-up datasets (one by the Ministry of Land, Infrastructure, and Transportation of the Japanese Government and the other by the Tohoku Tsunami Joint Survey group) and is then propagated through Bayesian statistical methods to assess the effects on the tsunami fragility models. The systematic implementation of the data and methods facilitates the quantitative comparison of tsunami fragility models under different assumptions. Such comparison shows that the binomial logistic method with un-binned data is preferred among the considered models; nevertheless, further investigations related to multinomial logistic regression with un-binned data are required. Finally, the developed tsunami fragility functions are integrated with building damage-loss models to investigate the influences of different tsunami fragility curves on tsunami loss estimation. Numerical results indicate that the uncertainty of input tsunami data is not negligible (coefficient of variation of 0.25) and that neglecting the input data uncertainty leads to overestimation of the model uncertainty. © 2016, The Author(s)."
1,10.1007/s11222-016-9667-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016107399&doi=10.1007%2fs11222-016-9667-9&partnerID=40&md5=de998998e32a07f8003c48bb26061021,"We consider continuous time Markovian processes where populations of individual agents interact stochastically according to kinetic rules. Despite the increasing prominence of such models in fields ranging from biology to smart cities, Bayesian inference for such systems remains challenging, as these are continuous time, discrete state systems with potentially infinite state-space. Here we propose a novel efficient algorithm for joint state/parameter posterior sampling in population Markov Jump processes. We introduce a class of pseudo-marginal sampling algorithms based on a random truncation method which enables a principled treatment of infinite state spaces. Extensive evaluation on a number of benchmark models shows that this approach achieves considerable savings compared to state of the art methods, retaining accuracy and fast convergence. We also present results on a synthetic biology data set showing the potential for practical usefulness of our work. © 2016, The Author(s)."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028889741&partnerID=40&md5=b30bf30af115a617f6ed63e12c708c99,"This paper proposed an approach of Bayesian inference in structural equation modelling (SEM) to evaluate the accident causation in underground coal mines in India. The statistics on accident events and reportable incidents has not shown the corresponding levels of improvement. In the area of major hazards control, the mining industry has emphasized mainly on past experiences and lessons learnt. However, the conventional risk management processes are not able to achieve the goal of zero accident potential (ZAP) due to a tonne of reasons. Bayesian inference SEM is necessary to develop the models and the coefficient of parameter estimation. The Markov Chain Monte Carlo sampling in the form Gibbs sampling was applied for sampling from the posterior distribution. The results revealed that all coefficients of SEM parameters are statistically significant. The Bayesian error statistics reveals that this model provides an approach to reduce accidents in underground coal mines of India."
,10.2436/20.8080.02.63,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039429624&doi=10.2436%2f20.8080.02.63&partnerID=40&md5=4012bee3c87f377c126f262fa2279727,"Aging societies have given rise to important challenges in the field of health insurance. Elderly policyholders need to be provided with fair premiums based on their individual health status, whereas insurance companies want to plan for the potential costs of tackling lifetimes above mean expectations. In this article, we focus on a large cohort of policyholders in Barcelona (Spain), aged 65 years and over. A shared-parameter joint model is proposed to analyse the relationship between annual demand for emergency claims and time until death outcomes, which are subject to left truncation. We compare different functional forms of the association between both processes, and, furthermore, we illustrate how the fitted model provides time-dynamic predictions of survival probabilities. The parameter estimation is performed under the Bayesian framework using Markov chain Monte Carlo methods."
6,10.1109/TSTE.2016.2637916,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028833608&doi=10.1109%2fTSTE.2016.2637916&partnerID=40&md5=ff36cef93d774cfe8fb1963475fe9f55,"Due to the intermittent nature of wind power, large-scale wind farm integration creates technical challenges, such as increase of peak/valley net load difference and uncertainty of generation. Energy storage is essential in providing flexibility and ensuring system reliability. Storage sizing problem is widely studied for a given demand curve, and the needed storage capacity to achieve a certain level of peak-shaving performance is not analyzed. In this paper, a probabilistic model of storage sizing with peak-shaving policy optimization under required matching probability is established to minimize net cost considering time-variant energy price. Storage is used not only for reducing energy deficit and keeping generation reliable but also for energy shifting to obtain higher profit. A cyclic nonhomogeneous Markov chain (CNHMC) steady-state analysis method is proposed, serving as a more efficient way to test probability constraint than commonly used time-consuming sequential Monte-Carlo simulation. CNHMC is used in stored power modeling representing diurnal variation of wind power and load. Probability constraint is tested by obtained analytical expression of matching probability. Numerical test shows that reliable power supply is achieved with little profit sacrifice, peak/valley net load difference decreases with little increment on storage capacity, and the proposed solution method is fast and accurate. © 2016 IEEE."
5,10.1007/s00477-016-1319-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988311257&doi=10.1007%2fs00477-016-1319-2&partnerID=40&md5=7856aa694ecc0214555f7a27eb5596a7,"Parameter uncertainty in hydrologic modeling is crucial to the flood simulation and forecasting. The Bayesian approach allows one to estimate parameters according to prior expert knowledge as well as observational data about model parameter values. This study assesses the performance of two popular uncertainty analysis (UA) techniques, i.e., generalized likelihood uncertainty estimation (GLUE) and Bayesian method implemented with the Markov chain Monte Carlo sampling algorithm, in evaluating model parameter uncertainty in flood simulations. These two methods were applied to the semi-distributed Topographic hydrologic model (TOPMODEL) that includes five parameters. A case study was carried out for a small humid catchment in the southeastern China. The performance assessment of the GLUE and Bayesian methods were conducted with advanced tools suited for probabilistic simulations of continuous variables such as streamflow. Graphical tools and scalar metrics were used to test several attributes of the simulation quality of selected flood events: deterministic accuracy and the accuracy of 95 % prediction probability uncertainty band (95PPU). Sensitivity analysis was conducted to identify sensitive parameters that largely affect the model output results. Subsequently, the GLUE and Bayesian methods were used to analyze the uncertainty of sensitive parameters and further to produce their posterior distributions. Based on their posterior parameter samples, TOPMODEL’s simulations and the corresponding UA results were conducted. Results show that the form of exponential decline in conductivity and the overland flow routing velocity were sensitive parameters in TOPMODEL in our case. Small changes in these two parameters would lead to large differences in flood simulation results. Results also suggest that, for both UA techniques, most of streamflow observations were bracketed by 95PPU with the containing ratio value larger than 80 %. In comparison, GLUE gave narrower prediction uncertainty bands than the Bayesian method. It was found that the mode estimates of parameter posterior distributions are suitable to result in better performance of deterministic outputs than the 50 % percentiles for both the GLUE and Bayesian analyses. In addition, the simulation results calibrated with Rosenbrock optimization algorithm show a better agreement with the observations than the UA’s 50 % percentiles but slightly worse than the hydrographs from the mode estimates. The results clearly emphasize the importance of using model uncertainty diagnostic approaches in flood simulations. © 2016, Springer-Verlag Berlin Heidelberg."
,10.1111/1365-2435.12844,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015198625&doi=10.1111%2f1365-2435.12844&partnerID=40&md5=f65d9e9be68de067794f8c3286acea9b,"Several dynamic models have shown that dynamics of legumes and grasses can result in periodic behaviour. These oscillations arise due to delays in nitrogen flows coupled with differences in ability to compete for light. However, long-term time series on legume dynamics that could be used to test predictions of these models are almost non-existent. We examine legume oscillations in a semi-natural mountain grassland using a long-term (Tilde; 30 years) data series on aboveground biomass of individual species and on nitrogen and phosphorus content over time. Using autocorrelation analysis, we show that there is a strong periodicity (with period of 8–9 years) of legume and grass biomass and nitrogen content in the grass biomass. These three variables are in fairly stable phase shifts relative to each other, with a grass peak followed by a peak in C : N ratio in grasses which is followed by a legume peak. Phosphorus content in either legume or grass biomass does not show synchronous cycling with legume or grass biomass or nitrogen content in grass. Fitting a dynamic linear model to the data showed that legumes affect nitrogen content in grasses, and grass biomass both affects and is affected by nitrogen content. In contrast, there is no negative effect of grasses on legumes, indicating some other process must be responsible for the legume decline. Manuring, which was occasionally applied to the plots, also does not seem to affect the cycling. Second-order term for legumes showed some evidence of self-inhibitory effects in legumes, but phosphorus content in legumes shows no support for phosphorus limitation. The most likely explanation of the legume decline should be sought elsewhere (pathogens, soil biota etc.). Synthesis. Long-term data support the existing the claim that legume dynamics are the key driver of nitrogen dynamics in nutrient-poor semi-natural grasslands. Grasses benefit from the nutrient enrichment due to legume cycling, but are a passive element and do not play a role in legume limitation. Apart from the role of nutrient cycling, these legume-driven nutrient dynamics also constitutes processes by which long-term richness of meadows is maintained. © 2017 The Authors. Functional Ecology © 2017 British Ecological Society"
4,10.1007/s10458-016-9359-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010796734&doi=10.1007%2fs10458-016-9359-z&partnerID=40&md5=8dac76656160b8f4937860699483637e,"We consider an autonomous agent facing a stochastic, partially observable, multiagent environment. In order to compute an optimal plan, the agent must accurately predict the actions of the other agents, since they influence the state of the environment and ultimately the agent’s utility. To do so, we propose a special case of interactive partially observable Markov decision process, in which the agent does not explicitly model the other agents’ beliefs and preferences, and instead represents them as stochastic processes implemented by probabilistic deterministic finite state controllers (PDFCs). The agent maintains a probability distribution over the PDFC models of the other agents, and updates this belief using Bayesian inference. Since the number of nodes of these PDFCs is unknown and unbounded, the agent places a Bayesian nonparametric prior distribution over the infinitely dimensional set of PDFCs. This allows the size of the learned models to adapt to the complexity of the observed behavior. Deriving the posterior distribution is in this case too complex to be amenable to analytical computation; therefore, we provide a Markov chain Monte Carlo algorithm that approximates the posterior beliefs over the other agents’ PDFCs, given a sequence of (possibly imperfect) observations about their behavior. Experimental results show that the learned models converge behaviorally to the true ones. We consider two settings, one in which the agent first learns, then interacts with other agents, and one in which learning and planning are interleaved. We show that the agent’s performance increases as a result of learning in both situations. Moreover, we analyze the dynamics that ensue when two agents are simultaneously learning about each other while interacting, showing in an example environment that coordination emerges naturally from our approach. Furthermore, we demonstrate how an agent can exploit the learned models to perform indirect inference over the state of the environment via the modeled agent’s actions. © 2017, The Author(s)."
1,10.1007/s11222-016-9663-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969785532&doi=10.1007%2fs11222-016-9663-0&partnerID=40&md5=a9d3eb8512f234bcd346d637ff289dca,"We present the parallel and interacting stochastic approximation annealing (PISAA) algorithm, a stochastic simulation procedure for global optimisation, that extends and improves the stochastic approximation annealing (SAA) by using population Monte Carlo ideas. The efficiency of standard SAA algorithm crucially depends on its self-adjusting mechanism which presents stability issues in high dimensional or rugged optimisation problems. The proposed algorithm involves simulating a population of SAA chains that interact each other in a manner that significantly improves the stability of the self-adjusting mechanism and the search for the global optimum in the sampling space, as well as it inherits SAA desired convergence properties when a square-root cooling schedule is used. It can be implemented in parallel computing environments in order to mitigate the computational overhead. As a result, PISAA can address complex optimisation problems that it would be difficult for SAA to satisfactory address. We demonstrate the good performance of the proposed algorithm on challenging applications including Bayesian network learning and protein folding. Our numerical comparisons suggest that PISAA outperforms the simulated annealing, stochastic approximation annealing, and annealing evolutionary stochastic approximation Monte Carlo. © 2016, Springer Science+Business Media New York."
1,10.1017/aer.2017.37,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021078954&doi=10.1017%2faer.2017.37&partnerID=40&md5=82e67d55f73997831608e978d1b8e561,"The assimilation of discrete data points with model predictions can be used to achieve a reduction in the uncertainty of the model input parameters, which generate accurate predictions. The problem investigated here involves the prediction of limit-cycle oscillations using a High-Dimensional Harmonic Balance (HDHB) method. The efficiency of the HDHB method is exploited to enable calibration of structural input parameters using a Bayesian inference technique. Markov-chain Monte Carlo is employed to sample the posterior distributions. Parameter estimation is carried out on a pitch/plunge aerofoil and two Goland wing configurations. In all cases, significant refinement was achieved in the distribution of possible structural parameters allowing better predictions of their true deterministic values. Additionally, a comparison of two approaches to extract the true values from the posterior distributions is presented. © 2017 Royal Aeronautical Society."
,10.1016/j.chinastron.2017.08.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028298202&doi=10.1016%2fj.chinastron.2017.08.008&partnerID=40&md5=cad694766df74b534abc4cf4f376de44,"Astrometry is an effective measure to detect exoplanets. It has many advantages that other detection methods do not bear, such as providing three dimensional planetary orbit, and determining planetary mass, etc. Astrometry will enrich the sample of exoplanets. As the high-precision astrometric satellite Gaia (Global Astrometry Interferometer for Astrophysics) was launched in 2013, it is predictable that there will be abundant long-period Jupiter-size planets to be discovered by Gaia. In this paper, we specify the α Centauri A, HD 62509, and GJ 876 systems, and generate the synthetic astrometric data with the single-time astrometric precision of Gaia. Then we use the Lomb-Scargle periodogram to analyze the periodical signal of planetary orbit, and use the Markov Chain Monte Carlo (MCMC) algorithm to make the orbit inversion of the planetary system, the obtained result is well coincident with the initial parameters of the planet. © 2017 Elsevier B.V."
,10.1016/j.spl.2017.02.035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015225401&doi=10.1016%2fj.spl.2017.02.035&partnerID=40&md5=589d19df413f3977931cd70d1f0530c1,"We propose a new non-iterative, very simple but accurate, Bayesian inference procedure for the stochastic volatility model. The only requirement of our approach is to solve a large, sparse linear system which we avoid by iteration. © 2017 Elsevier B.V."
1,10.1016/j.ecosta.2016.08.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032823531&doi=10.1016%2fj.ecosta.2016.08.003&partnerID=40&md5=a6247d7fd5112e004bd10b35ebab7eea,"Multivariate stochastic volatility models with leverage are expected to play important roles in financial applications such as asset allocation and risk management. However, these models suffer from two major difficulties: (1) there are too many parameters to estimate by using only daily asset returns and (2) estimated covariance matrices are not guaranteed to be positive definite. Our approach takes advantage of realized covariances to achieve the efficient estimation of parameters by incorporating additional information for the co-volatilities, and considers Cholesky decomposition to guarantee the positive definiteness of the covariance matrices. In this framework, a flexible model is proposed for stylized facts of financial markets, such as dynamic correlations and leverage effects among volatilities. By using the Bayesian approach, Markov Chain Monte Carlo implementation is described with a simple but efficient sampling scheme. Our model is applied to the data of nine U.S. stock returns, and it is compared with other models on the basis of portfolio performances. © 2016 EcoSta Econometrics and Statistics"
2,10.1007/s11222-016-9660-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969837191&doi=10.1007%2fs11222-016-9660-3&partnerID=40&md5=4b19479c6699aa2b7152b028f786fc86,"We consider the task of generating discrete-time realisations of a nonlinear multivariate diffusion process satisfying an Itô stochastic differential equation conditional on an observation taken at a fixed future time-point. Such realisations are typically termed diffusion bridges. Since, in general, no closed form expression exists for the transition densities of the process of interest, a widely adopted solution works with the Euler–Maruyama approximation, by replacing the intractable transition densities with Gaussian approximations. However, the density of the conditioned discrete-time process remains intractable, necessitating the use of computationally intensive methods such as Markov chain Monte Carlo. Designing an efficient proposal mechanism which can be applied to a noisy and partially observed system that exhibits nonlinear dynamics is a challenging problem, and is the focus of this paper. By partitioning the process into two parts, one that accounts for nonlinear dynamics in a deterministic way, and another as a residual stochastic process, we develop a class of novel constructs that bridge the residual process via a linear approximation. In addition, we adapt a recently proposed construct to a partial and noisy observation regime. We compare the performance of each new construct with a number of existing approaches, using three applications. © 2016, The Author(s)."
1,10.1016/j.ecosta.2016.08.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032827548&doi=10.1016%2fj.ecosta.2016.08.002&partnerID=40&md5=895a5431afbab5d14847a7b00a4f7400,"An efficient method for Bayesian inference in stochastic volatility models uses a linear state space representation to define a Gibbs sampler in which the volatilities are jointly updated. This method involves the choice of an offset parameter and we illustrate how its choice can have an important effect on the posterior inference. A Metropolis–Hastings algorithm is developed to robustify this approach to choice of the offset parameter. The method is illustrated on simulated data with known parameters, the daily log returns of the Eurostoxx index and a Bayesian vector autoregressive model with stochastic volatility. © 2016 EcoSta Econometrics and Statistics"
,10.1002/bimj.201600086,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016013502&doi=10.1002%2fbimj.201600086&partnerID=40&md5=58f70816b0ff4d082911cdb227c3f087,"We present a generalization of the usual (independent) mixture model to accommodate a Markovian first-order mixing distribution. We propose the data-driven reversible jump, a Markov chain Monte Carlo (MCMC) procedure, for estimating the a posteriori probability for each model in a model selection procedure and estimating the corresponding parameters. Simulated datasets show excellent performance of the proposed method in the convergence, model selection, and precision of parameters estimates. Finally, we apply the proposed method to analyze USA diabetes incidence datasets. © 2017 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim"
1,10.1016/j.ifacol.2017.08.1072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031823981&doi=10.1016%2fj.ifacol.2017.08.1072&partnerID=40&md5=b54736f14ec3a33060262cb038406eca,"Modelling and simulating loads of agricultural machinery under different variable loading conditions is an important tool for optimising the design of these machines. The developed models should be based on real conditions that the machine will face during its service life. In this paper, the load-time series from a four-rotor swather were acquired under normal working conditions, performing infield swathing and headland turning. The turning points of these loads were modelled using switching Markov chains, in order to model the switching between these two operating modes (i.e. swathing and headland turning). Based on the developed model, a total of 10,000 Monte Carlo simulations were performed, to assess the proposed methodology. The comparison between the measured and the simulated loads was performed by calculating the correlation coefficient of the power spectral density (PSD) of the two signals. The mean value and the standard deviation of the correlation coefficient when comparing the PSD of all Monte Carlo simulations with the PSD for the measured load, was 0.87 and 0.05, respectively, while the 5th and 95th percentile were 0.78 and 0.94, respectively. © 2017"
2,10.1109/TASE.2015.2443132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027683238&doi=10.1109%2fTASE.2015.2443132&partnerID=40&md5=282ef38feafac6843d3e95164da54b86,"Digital networked control systems are of growing importance in safety-critical systems and perform indispensable function in most complex systems today. Networked degradations such as transmission delay and packet dropout cause such systems to fail to satisfy performance requirements, and eventually affect the overall reliability. It is necessary to get a model to verify and evaluate the system reliability in early design phase, prior to its implementation. However, existing probabilistic models only provide partial descriptions of such coupled networks and control system. In this paper, a new stochastic model represented by linear discrete-time approach is proposed, considering data packet transmissions in both channels: controller-to-actuator and sensor-to-controller. Different from pervious works, the historical behaviors of networked degradations are modeled by multistate Markov chains with uncertainties, releasing the assumption that faults of all periods are independent of each other. The concept of domain requirements for such systems is considered here, contributing to the integration of control and reliability engineering. Methodologies for quantitatively assessing the reliability of the single- and sequential-control goal are derived from the Monte Carlo method. An example of an industrial heat exchanger digital networked control system is provided to illustrate the effectiveness of the model and method. © 2015 IEEE."
5,10.1007/s11276-016-1226-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958824933&doi=10.1007%2fs11276-016-1226-y&partnerID=40&md5=41581631e2a18df5d6181c58e83c3937,"The IEEE 802.15.4 standard has been introduced for low latency and low energy consumption in wireless sensor networks. To better support the requirements of industrial applications, where the use of this standard is limited, the low latency deterministic network (LLDN) mechanism of the IEEE 802.15.4e amendment has been proposed. In this paper, we develop a three dimensional Markov chain model for the IEEE 802.15.4e LLDN mechanism. Then, we estimate the stationary probability distribution of this chain in order to derive theoretical expressions of some performance metrics, as the reliability, energy consumption, throughput, delay and jitter. After that, we conduct a comparative study between the IEEE 802.15.4e LLDN and the IEEE 802.15.4 slotted carrier sense multiple access with collision avoidance (CSMA/CA). Numerical results show that the deterministic behavior of the LLDN mechanism significantly reduces the collision probability providing best performances in terms of reliability, energy consumption, throughput and delay compared to the IEEE 802.15.4 slotted CSMA/CA. Finally, the accuracy of our theoretical analysis is validated by Monte Carlo simulations. © 2016, Springer Science+Business Media New York."
1,10.1016/j.jvir.2017.02.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016599363&doi=10.1016%2fj.jvir.2017.02.031&partnerID=40&md5=a1f8c24ce2c90ecbc490edfd4ed7aacc,"Purpose To estimate the least costly routine exchange frequency for percutaneous nephrostomies (PCNs) placed for malignant urinary obstruction, as measured by annual hospital charges, and to estimate the financial impact of patient compliance. Materials and Methods Patients with PCNs placed for malignant urinary obstruction were studied from 2011 to 2013. Exchanges were classified as routine or due to 1 of 3 complication types: mechanical (tube dislodgment), obstruction, or infection. Representative cases were identified, and median representative charges were used as inputs for the model. Accelerated failure time and Markov chain Monte Carlo models were used to estimate distribution of exchange types and annual hospital charges under different routine exchange frequency and compliance scenarios. Results Long-term PCN management was required in 57 patients, with 87 total exchange encounters. Median representative hospital charges for pyelonephritis and obstruction were 11.8 and 9.3 times greater, respectively, than a routine exchange. The projected proportion of routine exchanges increased and the projected proportion of infection-related exchanges decreased when moving from a 90-day exchange with 50% compliance to a 60-day exchange with 75% compliance, and this was associated with a projected reduction in annual charges. Projected cost reductions resulting from increased compliance were generally greater than reductions resulting from changes in exchange frequency. Conclusions This simulation model suggests that the optimal routine exchange interval for PCN exchange in patients with malignant urinary obstruction is approximately 60 days and that the degree of reduction in charges likely depends more on patient compliance than exact exchange interval. © 2017 SIR"
1,10.1007/s10985-016-9361-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961198156&doi=10.1007%2fs10985-016-9361-4&partnerID=40&md5=cb9bca96271a98283beab7898f81362d,"Flexible incorporation of both geographical patterning and risk effects in cancer survival models is becoming increasingly important, due in part to the recent availability of large cancer registries. Most spatial survival models stochastically order survival curves from different subpopulations. However, it is common for survival curves from two subpopulations to cross in epidemiological cancer studies and thus interpretable standard survival models can not be used without some modification. Common fixes are the inclusion of time-varying regression effects in the proportional hazards model or fully nonparametric modeling, either of which destroys any easy interpretability from the fitted model. To address this issue, we develop a generalized accelerated failure time model which allows stratification on continuous or categorical covariates, as well as providing per-variable tests for whether stratification is necessary via novel approximate Bayes factors. The model is interpretable in terms of how median survival changes and is able to capture crossing survival curves in the presence of spatial correlation. A detailed Markov chain Monte Carlo algorithm is presented for posterior inference and a freely available function frailtyGAFT is provided to fit the model in the R package spBayesSurv. We apply our approach to a subset of the prostate cancer data gathered for Louisiana by the surveillance, epidemiology, and end results program of the National Cancer Institute. © 2016, Springer Science+Business Media New York."
,10.1093/ageing/afw249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021771961&doi=10.1093%2fageing%2fafw249&partnerID=40&md5=699d2f311d0a5b4a689967989c9ce943,"Objective: to measure the impact of the 'My Power of Attorney' media campaign on the number of new power of attorney (POA) registrations in Scotland.Setting: POA registrations in Scotland processed by the Office of the Public Guardian during January 2010 to June 2015.Methods: multilevel Poisson models for POA registrations nested by council and annual quarter were run using Markov chain Monte Carlo methods, adjusting for time, campaign (variable ranging between 0 and 5 dependent on intensity of campaign measured by the number of media platforms received) and offset term mid-year population estimate for those aged 25 years+/65 years+.Results: POA registrations saw a reduction between 2010 and 2011 but overall, increased between 2010 and 2015. POA registrations rose by 33.3% in Glasgow City between 2013 and 2014, when the campaign began, while the rest of Scotland saw a rise of 17.3%. When the data were modelled, Relative Risk (RR) of a POA registration increased with increasing intensity of campaign, so that in an area in receipt of the full campaign was RR = 1.31 (1.28, 1.34) that of an area with no campaign. Between council variation persisted after adjustment for campaign (Variance = 0.041 (0.011)).Conclusions: during the period of the campaign, area-level increases in POA registrations were observed associated with the 'My Power of Attorney' timing and location, in an approximate dose-response relationship with campaign intensity, suggesting that this is likely to be due to the campaign that began in Glasgow City. © The Author 2017. Published by Oxford University Press on behalf of the British Geriatrics Society.All rights reserved."
1,10.1631/jzus.B1600143,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025100922&doi=10.1631%2fjzus.B1600143&partnerID=40&md5=724296554e9dd1f8bc4304656f1badb1,"Background: Antithrombotic therapy using new oral anticoagulants (NOACs) in patients with atrial fibrillation (AF) has been generally shown to have a favorable risk-benefit profile. Since there has been dispute about the risks of gastrointestinal bleeding (GIB) and intracranial hemorrhage (ICH), we sought to conduct a systematic review and network meta-analysis using Bayesian inference to analyze the risks of GIB and ICH in AF patients taking NOACs. Methods: We analyzed data from 20 randomized controlled trials of 91 671 AF patients receiving anticoagulants, antiplatelet drugs, or placebo. Bayesian network meta-analysis of two different evidence networks was performed using a binomial likelihood model, based on a network in which different agents (and doses) were treated as separate nodes. Odds ratios (ORs) and 95% confidence intervals (CIs) were modeled using Markov chain Monte Carlo methods. Results: Indirect comparisons with the Bayesian model confirmed that aspirin+clopidogrel significantly increased the risk of GIB in AF patients compared to the placebo (OR 0.33, 95% CI 0.01–0.92). Warfarin was identified as greatly increasing the risk of ICH compared to edoxaban 30 mg (OR 3.42, 95% CI 1.22–7.24) and dabigatran 110 mg (OR 3.56, 95% CI 1.10–8.45). We further ranked the NOACs for the lowest risk of GIB (apixaban 5 mg) and ICH (apixaban 5 mg, dabigatran 110 mg, and edoxaban 30 mg). Conclusion: Bayesian network meta-analysis of treatment of nonvalvular AF patients with anticoagulants suggested that NOACs do not increase risks of GIB and/or ICH, compared to each other. © 2017, Zhejiang University and Springer-Verlag GmbH Germany."
,10.1016/j.ifacol.2017.08.2585,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044333643&doi=10.1016%2fj.ifacol.2017.08.2585&partnerID=40&md5=9dc1bceb5183c561730e8fea14097a85,"Particle Gibbs with Ancestor Sampling (PGAS) is a particle Markov chain Monte Carlo method (PMCMC) for Bayesian inference and learning. PGAS conditions on a reference-state trajectory in the underlying particle filter using ancestor sampling. In this paper, we leverage PGAS for identification of cornering-stiffness parameters in road vehicles only using production-grade sensors. The cornering-stiffness parameters are essential for describing the motion of the vehicle. We show how PGAS can be adapted to efficiently learn the stiffness parameters by conditioning on the noise-input trajectory instead of the state trajectory. We verify on a three-minute long experimental test drive that our method correctly identifies the tire-stiffness parameters. © 2017"
1,10.1002/mats.201700039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021842720&doi=10.1002%2fmats.201700039&partnerID=40&md5=1e86fdf9bac36940ae0036788e351856,"Selected aspects of copolymerization processes carried on at constant comonomer concentrations are analyzed theoretically and modeled by Monte Carlo method. It is confirmed that some combinations of initial parameters lead to stationary conditions of copolymer formation for both irreversible and reversible systems which can be regarded as the first-order Markov chain process. However, this study shows that for many copolymerization systems the stationary conditions are attained only at high number-average degree of polymerization DPn, and for some reversible copolymerizations, attaining equilibrium, stationary conditions are not observed at all. The analysis shows that the chain length distribution (CLD) for copolymerization carried out under steady state conditions at constant comonomer concentrations, equal to equilibrium concentrations for infinitely high DPn, is approximately described by the modified Bessel and exponential functions. This type of CLD is analytically proved and confirmed by the Monte Carlo simulations for the analogous homopolymerization process. © 2017 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim"
2,10.1051/0004-6361/201628986,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022204170&doi=10.1051%2f0004-6361%2f201628986&partnerID=40&md5=644533fc5742ebb497c05da1778bdb49,"The goal of population spectral synthesis (pss; also referred to as inverse, semi-empirical evolutionary- or fossil record approach) is to decipher from the spectrum of a galaxy the mass, age and metallicity of its constituent stellar populations. This technique, which is the reverse of but complementary to evolutionary synthesis, has been established as fundamental tool in extragalactic research. It has been extensively applied to large spectroscopic data sets, notably the SDSS, leading to important insights into the galaxy assembly history. However, despite significant improvements over the past decade, all current pss codes suffer from two major deficiencies that inhibit us from gaining sharp insights into the star-formation history (SFH) of galaxies and potentially introduce substantial biases in studies of their physical properties (e.g., stellar mass, mass-weighted stellar age and specific star formation rate). These are i) the neglect of nebular emission in spectral fits, consequently; ii) the lack of a mechanism that ensures consistency between the best-fitting SFH and the observed nebular emission characteristics of a star-forming (SF) galaxy (e.g., hydrogen Balmer-line luminosities and equivalent widths-EWs, shape of the continuum in the region around the Balmer and Paschen jump). In this article, we present fado (Fitting Analysis using Differential evolution Optimization) - a conceptually novel, publicly available pss tool with the distinctive capability of permitting identification of the SFH that reproduces the observed nebular characteristics of a SF galaxy. This so-far unique self-consistency concept allows us to significantly alleviate degeneracies in current spectral synthesis, thereby opening a new avenue to the exploration of the assembly history of galaxies. The innovative character of fado is further augmented by its mathematical foundation: fado is the first pss code employing genetic differential evolution optimization. This, in conjunction with various other currently unique elements in its mathematical concept and numerical realization (e.g., mid-analysis optimization of the spectral library using artificial intelligence, test for convergence through a procedure inspired by Markov chain Monte Carlo techniques, quasi-parallelization embedded within a modular architecture) results in key improvements with respect to computational efficiency and uniqueness of the best-fitting SFHs. Furthermore, fado incorporates within a single code the entire chain of pre-processing, modeling, post-processing, storage and graphical representation of the relevant output from pss, including emission-line measurements and estimates of uncertainties for all primary and secondary products from spectral synthesis (e.g., mass contributions of individual stellar populations, mass- and luminosity-weighted stellar ages and metallicities). This integrated concept greatly simplifies and accelerates a lengthy sequence of individual time-consuming steps that are generally involved in pss modeling, further enhancing the overall efficiency of the code and inviting to its automated application to large spectroscopic data sets. © ESO, 2017."
2,10.1371/journal.pone.0181929,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025842159&doi=10.1371%2fjournal.pone.0181929&partnerID=40&md5=5ab179d039de6a740cee71ee451b23a8,"Contacts across the Strait of Gibraltar in the Pleistocene have been studied in different research papers, which have demonstrated that this apparent barrier has been permeable to human and fauna movements in both directions. Our study, based on the genetic analysis of wild boar (Sus scrofa), suggests that there has been contact between Africa and Europe through the Strait of Gibraltar in the Late Pleistocene (at least in the last 90,000 years), as shown by the partial analysis of mitochondrial DNA. Cytochrome b and the control region from North African wild boar indicate a close relationship with European wild boar, and even some specimens belong to a common haplotype in Europe. The analyses suggest the transformation of the wild boar phylogeography in North Africa by the emergence of a natural communication route in times when sea levels fell due to climatic changes, and possibly through human action, since contacts coincide with both the Last Glacial period and the increasing human dispersion via the strait. © 2017 Soria-Boix et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
2,10.1115/1.4035898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014955854&doi=10.1115%2f1.4035898&partnerID=40&md5=9acd8ff3f8e02282afa67dca3ccb7bfe,"Uncertainty quantification (UQ) is an emerging field that focuses on characterizing, quantifying, and potentially reducing, the uncertainties associated with computer simulation models used in a wide range of applications. Although it has been successfully applied to computer simulation models in areas such as structural engineering, climate forecasting, and medical sciences, this powerful research area is still lagging behind in materials simulation models. These are broadly defined as physics-based predictive models developed to predict material behavior, i.e., processing-microstructure-property relations and have recently received considerable interest with the advent of emerging concepts such as Integrated Computational Materials Engineering (ICME). The need of effective tools for quantifying the uncertainties associated with materials simulation models has been identified as a high priority research area in most recent roadmapping efforts in the field. In this paper, we present one of the first efforts in conducting systematic UQ of a physics-based materials simulation model used for predicting the evolution of precipitates in advanced nickel-titanium shape-memory alloys (SMAs) subject to heat treatment. Specifically, a Bayesian calibration approach is used to conduct calibration of the precipitation model using a synthesis of experimental and computer simulation data. We focus on constructing a Gaussian process-based surrogate modeling approach for achieving this task, and then benchmark the predictive accuracy of the calibrated model with that of the model calibrated using traditional Markov chain Monte Carlo (MCMC) methods. © Copyright 2017 by ASME."
,10.1016/j.ifacol.2017.08.1183,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031798806&doi=10.1016%2fj.ifacol.2017.08.1183&partnerID=40&md5=73fc9294138b915735d076f6f29744cb,"Creating a driving cycle (DC) for the design and validation of new vehicles is an important step that will influence the efficiency, functionality and performance of the final systems. In this work, a DC synthesis method is introduced, based on multi-dimensional Markov Chain, where both the velocity and road slope are investigated. Particularly, improvements on the DC synthesis method are proposed, to reach a more realistic slope profile and more accurate fuel consumption and CO2 emission estimates. The effects of using synthesized DCs on fuel consumption are investigated considering three different vehicle models: conventional ICE, and full hybrid and mild hybrid electric vehicles. Results show that short but representative synthetic DCs will results in more realistic fuel consumption estimates (e.g. in the 5%-10% range) and in much faster simulations. Using the results of this proposed method also eliminates the need to use very simplified DCs, as the New European Driving Cycle(NEDC), or long, measured DCs. © 2017"
4,10.1016/j.ress.2017.02.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013851421&doi=10.1016%2fj.ress.2017.02.002&partnerID=40&md5=2ae6fbca6f22230eda8e5922522f3516,"Efficient life-cycle management of civil infrastructure systems under continuous deterioration can be improved by studying the sensitivity of optimised preventive maintenance decisions with respect to changes in model parameters. Sensitivity analysis in maintenance optimisation problems is important because if the calculation of the cost of preventive maintenance strategies is not sufficiently robust, the use of the maintenance model can generate optimised maintenances strategies that are not cost-effective. Probabilistic sensitivity analysis methods (particularly variance based ones), only partially respond to this issue and their use is limited to evaluating the extent to which uncertainty in each input contributes to the overall output's variance. These methods do not take account of the decision-making problem in a straightforward manner. To address this issue, we use the concept of the Expected Value of Perfect Information (EVPI) to perform decision-informed sensitivity analysis: to identify the key parameters of the problem and quantify the value of learning about certain aspects of the life-cycle management of civil infrastructure system. This approach allows us to quantify the benefits of the maintenance strategies in terms of expected costs and in the light of accumulated information about the model parameters and aspects of the system, such as the ageing process. We use a Gamma process model to represent the uncertainty associated with asset deterioration, illustrating the use of EVPI to perform sensitivity analysis on the optimisation problem for age-based and condition-based preventive maintenance strategies. The evaluation of EVPI indices is computationally demanding and Markov Chain Monte Carlo techniques would not be helpful. To overcome this computational difficulty, we approximate the EVPI indices using Gaussian process emulators. The implications of the worked numerical examples discussed in the context of analytical efficiency and organisational learning. © 2017"
6,10.1002/pst.1807,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018379336&doi=10.1002%2fpst.1807&partnerID=40&md5=b1386aed8faa3a462fcb5cd37ee64b03,"Children represent a large underserved population of “therapeutic orphans,” as an estimated 80% of children are treated off-label. However, pediatric drug development often faces substantial challenges, including economic, logistical, technical, and ethical barriers, among others. Among many efforts trying to remove these barriers, increased recent attention has been paid to extrapolation; that is, the leveraging of available data from adults or older age groups to draw conclusions for the pediatric population. The Bayesian statistical paradigm is natural in this setting, as it permits the combining (or “borrowing”) of information across disparate sources, such as the adult and pediatric data. In this paper, authored by the pediatric subteam of the Drug Information Association Bayesian Scientific Working Group and Adaptive Design Working Group, we develop, illustrate, and provide suggestions on Bayesian statistical methods that could be used to design improved pediatric development programs that use all available information in the most efficient manner. A variety of relevant Bayesian approaches are described, several of which are illustrated through 2 case studies: extrapolating adult efficacy data to expand the labeling for Remicade to include pediatric ulcerative colitis and extrapolating adult exposure-response information for antiepileptic drugs to pediatrics. Copyright © 2017 John Wiley & Sons, Ltd."
,10.1002/pst.1815,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020070905&doi=10.1002%2fpst.1815&partnerID=40&md5=7f661d896e01fd6ec0ad8eda0050f26f,"The borrowing of historical control data can be an efficient way to improve the treatment effect estimate of the current control group in a randomized clinical trial. When the historical and current control data are consistent, the borrowing of historical data can increase power and reduce Type I error rate. However, when these 2 sources of data are inconsistent, it may result in a combination of biased estimates, reduced power, and inflation of Type I error rate. In some situations, inconsistency between historical and current control data may be caused by a systematic variation in the measured baseline prognostic factors, which can be appropriately addressed through statistical modeling. In this paper, we propose a Bayesian hierarchical model that can incorporate patient-level baseline covariates to enhance the appropriateness of the exchangeability assumption between current and historical control data. The performance of the proposed method is shown through simulation studies, and its application to a clinical trial design for amyotrophic lateral sclerosis is described. The proposed method is developed for scenarios involving multiple imbalanced prognostic factors and thus has meaningful implications for clinical trials evaluating new treatments for heterogeneous diseases such as amyotrophic lateral sclerosis. Copyright © 2017 John Wiley & Sons, Ltd."
3,10.1371/journal.pcbi.1005653,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026631540&doi=10.1371%2fjournal.pcbi.1005653&partnerID=40&md5=a48ae6e1d70d1da1db21dfe84f8b6b50,"In recent years, there has been a huge rise in the number of publicly available transcriptional profiling datasets. These massive compendia comprise billions of measurements and provide a special opportunity to predict the function of unstudied genes based on co-expression to well-studied pathways. Such analyses can be very challenging, however, since biological pathways are modular and may exhibit co-expression only in specific contexts. To overcome these challenges we introduce CLIC, CLustering by Inferred Co-expression. CLIC accepts as input a pathway consisting of two or more genes. It then uses a Bayesian partition model to simultaneously partition the input gene set into coherent co-expressed modules (CEMs), while assigning the posterior probability for each dataset in support of each CEM. CLIC then expands each CEM by scanning the transcriptome for additional co-expressed genes, quantified by an integrated log-likelihood ratio (LLR) score weighted for each dataset. As a byproduct, CLIC automatically learns the conditions (datasets) within which a CEM is operative. We implemented CLIC using a compendium of 1774 mouse microarray datasets (28628 microarrays) or 1887 human microarray datasets (45158 microarrays). CLIC analysis reveals that of 910 canonical biological pathways, 30% consist of strongly co-expressed gene modules for which new members are predicted. For example, CLIC predicts a functional connection between protein C7orf55 (FMC1) and the mitochondrial ATP synthase complex that we have experimentally validated. CLIC is freely available at www.gene-clic.org. We anticipate that CLIC will be valuable both for revealing new components of biological pathways as well as the conditions in which they are active. © 2017 Li et al."
1,10.1371/journal.pone.0180331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022338184&doi=10.1371%2fjournal.pone.0180331&partnerID=40&md5=a3b97b9e0f67176530394f18693fcde7,"Sulfolobus solfataricus is a thermoacidophilic Archaeon that thrives in terrestrial hot springs (solfatares) with optimal growth at 80C and pH 2–4. It catabolizes specific carbon sources, such as D-glucose, to pyruvate via the modified Entner-Doudoroff (ED) pathway. This pathway has two parallel branches, the semi-phosphorylative and the non-phosphorylative. However, the strategy of S.solfataricus to endure in such an extreme environment in terms of robustness and adaptation is not yet completely understood. Here, we present the first dynamic mathematical model of the ED pathway parameterized with quantitative experimental data. These data consist of enzyme activities of the branched pathway at 70C and 80C and of metabolomics data at the same temperatures for the wild type and for a metabolic engineered knockout of the semi-phosphorylative branch. We use the validated model to address two questions: 1. Is this system more robust to perturbations at its optimal growth temperature? 2. Is the ED robust to deletion and perturbations? We employed a systems biology approach to answer these questions and to gain further knowledge on the emergent properties of this biological system. Specifically, we applied deterministic and stochastic approaches to study the sensitivity and robustness of the system, respectively. The mathematical model we present here, shows that: 1. Steady state metabolite concentrations of the ED pathway are consistently more robust to stochastic internal perturbations at 80C than at 70C; 2. These metabolite concentrations are highly robust when faced with the knockout of either branch. Connected with this observation, these two branches show different properties at the level of metabolite production and flux control. These new results reveal how enzyme kinetics and metabolomics synergizes with mathematical modelling to unveil new systemic properties of the ED pathway in S.solfataricus in terms of its adaptation and robustness. © 2017 Figueiredo et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
3,10.1016/j.apm.2017.03.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020286365&doi=10.1016%2fj.apm.2017.03.020&partnerID=40&md5=c03381074fa00a455858555cebf469ca,"In the model of R=P(Y<X), X and Y usually represent the strength of a system and stress applied to it. Then, R is the measure of system reliability. In this paper, Bayes estimation of R=P(Y<X) is studied under the assumption that X and Y are independent Weibull random variables with arbitrary scale and shape parameters. We show here for the first time how to compute the Bayes estimates and credible intervals for R in that case. First, a closed form expression for R is derived. Prior distributions are assumed for Weibull parameters, and the posterior distribution is presented. Next, by proposing an universal sample-based method according to the Monte Carlo Markov Chain (MCMC) method, we draw samples and compute the Bayes estimates and credible intervals for R. Through Monte Carlo simulations and two real data examples, the proposed method is demonstrated to be robust and satisfactory. © 2017 Elsevier Inc."
,10.1016/j.ifacol.2017.08.2278,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031825175&doi=10.1016%2fj.ifacol.2017.08.2278&partnerID=40&md5=3ce6b102db1d7ecad6eb4c560fa8d5f3,"Can a dynamical system paint masterpieces such as Da Vinci's Mona Lisa or Monet's Water Lilies? Moreover, can this dynamical system be chaotic in the sense that although the trajectories are sensitive to initial conditions, the same painting is created every time? Setting aside the creative aspect of painting a picture, in this work, we develop a novel algorithm to reproduce paintings and photographs. Combining ideas from ergodic theory and control theory, we construct a chaotic dynamical system with predetermined statistical properties. If one makes the spatial distribution of colors in the picture the target distribution, akin to a human, the algorithm first captures large scale features and then goes on to refine small scale features. Beyond reproducing paintings, this approach is expected to have a wide variety of applications such as uncertainty quantification, sampling for efficient inference in scalable machine learning for big data, and developing effective strategies for search and rescue. In particular, our preliminary studies demonstrate that this algorithm provides significant acceleration and higher accuracy than competing methods for Monte Carlo, Quasi Monte Carlo, Markov Chain Monte Carlo (MCMC). © 2017"
18,10.1093/sysbio/syw101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021806470&doi=10.1093%2fsysbio%2fsyw101&partnerID=40&md5=0c1415f53d4a6227a6241db2205becbe,"As the application of genomic data in phylogenetics has become routine, a number of cases have arisen where alternative data sets strongly support conflicting conclusions. This sensitivity to analytical decisions has prevented firm resolution of some of the most recalcitrant nodes in the tree of life. To better understand the causes and nature of this sensitivity,we analyzed several phylogenomic data sets using an alternativemeasure of topological support (the Bayes factor) that both demonstrates and averts several limitations of more frequently employed support measures (such asMarkov chain Monte Carlo estimates of posterior probabilities). Bayes factors reveal important, previously hidden, differences across six ""phylogenomic"" data sets collected to resolve the phylogenetic placement of turtles within Amniota. These data sets vary substantially in their support forwell-established amniote relationships, particularly in the proportion of genes that contain extreme amounts of information aswell as the proportion that strongly reject these uncontroversial relationships. All six data sets contain little information to resolve the phylogenetic placement of turtles relative to other amniotes. Bayes factors also reveal that a very small number of extremely influential genes (less than 1% of genes in a data set) can fundamentally change significant phylogenetic conclusions. In one example, these genes are shown to contain previously unrecognized paralogs. This study demonstrates both that the resolution of difficult phylogenomic problems remains sensitive to seemingly minor analysis details and that Bayes factors are a valuable tool for identifying and solving these challenges. © The Author(s) 2016. Published by Oxford University Press, on behalf of the Society of Systematic Biologists. All rights reserved."
,10.1175/JHM-D-17-0030.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023161212&doi=10.1175%2fJHM-D-17-0030.1&partnerID=40&md5=bfb952584bde7fdd5d283b688cfefd00,"Land surface models are notorious for containing many parameters that control the exchange of heat and moisture between land and atmosphere. Properly modeling the partitioning of total evapotranspiration (ET) between transpiration and evaporation is critical for accurate hydrological modeling, but depends heavily on the treatment of turbulence within and above canopies. Previous work has constrained estimates of evapotranspiration and its partitioning using statistical approaches that calibrate land surface model parameters by assimilating in situ measurements. These studies, however, are silent on the impacts of the accounting of uncertainty within the statistical calibration framework. The present study calibrates the aerodynamic, leaf boundary layer, and stomatal resistance parameters, which partially control canopy turbulent exchange and thus the evapotranspiration flux partitioning. Using an adaptive Metropolis-Hastings algorithm to construct a Markov chain of draws from the joint posterior distribution of these resistance parameters, an ensemble of model realizations is generated, in which latent and sensible heat fluxes and top soil layer temperature are optimized. A set of five calibration experiments demonstrate that model performance is sensitive to the accounting of various sources of uncertainty in the field observations and model output and that it is critical to account for model structural uncertainty. After calibration, the modeled fluxes and top soil layer temperature are largely free from bias, and this calibration approach successfully informs and characterizes uncertainty in these parameters, which is essential for model improvement and development. The key points of this paper are 1) a Markov chain Monte Carlo calibration approach successfully improves modeled turbulent fluxes; 2) ET partitioning estimates hinge on the representation of uncertainties in the model and data; and 3) despite these inherent uncertainties, constrained posterior estimates of ET partitioning emerge. © 2017 American Meteorological Society."
,10.1016/j.econlet.2017.04.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018365794&doi=10.1016%2fj.econlet.2017.04.011&partnerID=40&md5=11445899840db335f392d05ee81fa358,"We consider Bayesian inference about the mean of a binary variable that is subject to misclassification error. If the error probabilities are not known, or cannot be estimated, the parameter is only partially identified. For several reasonable and intuitive prior distributions of the misclassification probabilities, we derive new analytical expressions for the posterior distribution. Our results circumvent the need for Markov chain Monte Carlo simulation. The priors we use lead to regions in the identified set that are a posteriori more likely than others. © 2017 Elsevier B.V."
,10.1016/j.ifacol.2017.08.632,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031814034&doi=10.1016%2fj.ifacol.2017.08.632&partnerID=40&md5=d19a4617d0bc95da933b0aa6f5f46ba6,"There has been a strong interest in emergency planning in response to an attack or accidental release of harmful chemical, biological, radiological or nuclear substances. Under such circumstances, it is of paramount importance to determine the location and release rate of the hazardous source to forecast the future harm it may cause and employ methods to minimize the disturbance. In this paper, a sensor data collection strategy is proposed whereby an autonomous mobile sensor is guided to address such a problem with a high degree of accuracy and in a short amount of time. First, the parameters of the release source are estimated using the Markov chain Monte Carlo sampling approach. The most informative manoeuvre from the set of possible choices is then selected using the concept of maximum entropy sampling. Numerical simulations demonstrate the superior performance of the proposed algorithm compared to traditional approaches in terms of estimation accuracy and the number of measurements required. © 2017"
1,10.1016/j.ifacol.2017.08.1555,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031810110&doi=10.1016%2fj.ifacol.2017.08.1555&partnerID=40&md5=5c04066dd5d86710fb99a7a62dbde8b1,"We propose a method for nonparametric identification of Hammerstein models with Gaussian-process models for the impulse response of the linear block and for the input nonlinearity. Interpreting the Gaussian-processes as prior distributions, we can estimate the unknowns using the posterior means given the data. To estimate the hyperparameters we set up an iterative scheme, reminiscent of the expectation-maximization method, where the posterior expectation of the complete likelihood is iteratively maximized. In the Hammerstein case, the posterior density is intractable because, in general, it does not admit a closed form expression. In this work, we propose two approximation approaches to estimate the posterior mean. In the first, we make a particle approximation of the posterior using Markov Chain Monte Carlo. In the second, we use a variational Bayes approach with a mean-field hypothesis. We validate the proposed methods on synthetic datasets of Hammerstein systems. © 2017"
22,10.3847/1538-3881/aa71ef,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024408952&doi=10.3847%2f1538-3881%2faa71ef&partnerID=40&md5=bc505427fc32987081d1c32eb13ac4eb,"We conduct a uniform analysis of the transit timing variations (TTVs) of 145 planets from 55 Kepler multiplanet systems to infer planet masses and eccentricities. Eighty of these planets do not have previously reported mass and eccentricity measurements. We employ two complementary methods to fit TTVs: Markov chain Monte Carlo simulations based on N-body integration, and an analytic fitting approach. Mass measurements of 49 planets, including 12 without previously reported masses, meet our criterion for classification as robust. Using mass and radius measurements, we infer the masses of planets' gaseous envelopes for both our TTV sample and transiting planets with radial velocity observations. Insight from analytic TTV formulae allows us to partially circumvent degeneracies inherent to inferring eccentricities from TTV observations. We find that planet eccentricities are generally small, typically a few percent, but in many instances are nonzero. © 2017. The American Astronomical Society. All rights reserved."
56,10.1093/sysbio/syx037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017541124&doi=10.1093%2fsysbio%2fsyx037&partnerID=40&md5=0885faf3b78d0b8b3ae29c376609da72,"Bayesian analysis of macroevolutionary mixtures (BAMM) is a statistical framework that uses reversible jump Markov chain Monte Carlo to infer complex macroevolutionary dynamics of diversification and phenotypic evolution on phylogenetic trees. A recent article by Moore et al. (MEA) reported a number of theoretical and practical concerns with BAMM. Major claims from MEA are that (i) BAMM's likelihood function is incorrect, because it does not account for unobserved rate shifts; (ii) the posterior distribution on the number of rate shifts is overly sensitive to the prior; and (iii) diversification rate estimates from BAMM are unreliable. Here, we show that these and other conclusions from MEA are generally incorrect or unjustified. We first demonstrate that MEA's numerical assessment of the BAMM likelihood is compromised by their use of an invalid likelihood function. We then show that ""unobserved rate shifts"" appear to be irrelevant for biologically plausible parameterizations of the diversification process. We find that the purportedly extreme prior sensitivity reported byMEA cannot be replicated with standard usage of BAMMv2.5, or with any other version when conventional Bayesian model selection is performed. Finally, we demonstrate that BAMM performs very well at estimating diversification rate variation across the ∼20% of simulated trees inMEA's data set for which it is theoretically possible to infer rate shifts with confidence. Due to ascertainment bias, the remaining 80% of their purportedly variable-rate phylogenies are statistically indistinguishable from those produced by a constant-rate birth-death process and were thus poorly suited for the summary statistics used in their performance assessment.We demonstrate that inferences about diversification rates have been accurate and consistent across all major previous releases of the BAMM software.We recognize an acute need to address the theoretical foundations of rate-shift models for phylogenetic trees, and we expect BAMM and other modeling frameworks to improve in response to mathematical and computational innovations. However, we remain optimistic that that the imperfect tools currently available to comparative biologists have provided and will continue to provide important insights into the diversification of life on Earth. © The Author(s) 2016. Published by Oxford University Press, on behalf of the Society of Systematic Biologists. All rights reserved."
,10.1007/s11222-016-9674-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975260806&doi=10.1007%2fs11222-016-9674-x&partnerID=40&md5=fcbd2d30d0b3cdce812b4021b2b2b96e,"Complex biological processes are usually experimented along time among a collection of individuals, longitudinal data are then available. The statistical challenge is to better understand the underlying biological mechanisms. A standard statistical approach is mixed-effects model where the regression function is highly-developed to describe precisely the biological processes (solutions of multi-dimensional ordinary differential equations or of partial differential equation). A classical estimation method relies on coupling a stochastic version of the EM algorithm with a Monte Carlo Markov Chain algorithm. This algorithm requires many evaluations of the regression function. This is clearly prohibitive when the solution is numerically approximated with a time-consuming solver. In this paper a meta-model relying on a Gaussian process emulator is proposed to approximate the regression function, that leads to what is called a mixed meta-model. The uncertainty of the meta-model approximation can be incorporated in the model. A control on the distance between the maximum likelihood estimates of the mixed meta-model and the maximum likelihood estimates of the exact mixed model is guaranteed. Eventually, numerical simulations are performed to illustrate the efficiency of this approach. © 2016, Springer Science+Business Media New York."
,10.1109/IPDPSW.2017.127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028062086&doi=10.1109%2fIPDPSW.2017.127&partnerID=40&md5=87a33a4cbc0716928879cdc2453afca5,"Coalescent genealogy samplers are effective tools for the study of population genetics. They are used to estimate the historical parameters of a population based upon the sampling of present-day genetic information. A popular approach employs Markov chain Monte Carlo (MCMC) methods. While effective, these methods are very computationally intensive, often taking weeks to run. Although attempts have been made to leverage parallelism in an effort to reduce runtimes, they have not resulted in scalable solutions. Due to the inherently sequential nature of MCMC methods, their performance has suffered diminishing returns when applied to large-scale computing clusters. In the interests of reduced runtimes and higher quality solutions, a more sophisticated form of parallelism is required. This paper describes a novel way to apply a recently discovered generalization of MCMC for this purpose. The new approach exploits the multiple-proposal mechanism of the generalized method to enable the desired scalable parallelism while maintaining the accuracy of the original technique. © 2017 IEEE."
,10.1109/FCCM.2017.9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027713187&doi=10.1109%2fFCCM.2017.9&partnerID=40&md5=88631ad0b65100d84dcacdaf33f2000c,"Markov Chain Monte Carlo (MCMC) based methods have been the main tool for Bayesian Inference for some years now, and recently they find increasing applications in modern statistics and machine learning. Nevertheless, with the availability of large datasets and increasing complexity of Bayesian models, MCMC methods are becoming prohibitively expensive for real-world problems. At the heart of these methods, lies the computation of likelihood functions that requires access to all input data points in each iteration of the method. Current approaches, based on data subsampling, aim to accelerate these algorithms by reducing the number of the data points for likelihood evaluations at each MCMC iteration. However the existing work doesn't consider the properties of modern memory hierarchies, but treats the memory as one monolithic storage space. This paper proposes a communication-aware MCMC framework that takes into account the underlying performance of the memory subsystem. The framework is based on a novel subsampling algorithm that utilises an unbiased likelihood estimator based on Probability Proportional-to-Size (PPS) sampling, allowing information on the performance of the memory system to be taken into account during the sampling stage. The proposed MCMC sampler is mapped to an FPGA device and its performance is evaluated using the Bayesian logistic regression model on MNIST dataset. The proposed system achieves a 3.37x speed up over a highly optimised traditional FPGA design, therefore the risk in the estimates based on the generated samples is largely decreased. © 2017 IEEE."
,10.1002/sim.7278,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014880678&doi=10.1002%2fsim.7278&partnerID=40&md5=8a45181fef21d8c4a68bf798288fa8f8,"An important statistical task in disease mapping problems is to identify divergent regions with unusually high or low risk of disease. Leave-one-out cross-validatory (LOOCV) model assessment is the gold standard for estimating predictive p-values that can flag such divergent regions. However, actual LOOCV is time-consuming because one needs to rerun a Markov chain Monte Carlo analysis for each posterior distribution in which an observation is held out as a test case. This paper introduces a new method, called integrated importance sampling (iIS), for estimating LOOCV predictive p-values with only Markov chain samples drawn from the posterior based on a full data set. The key step in iIS is that we integrate away the latent variables associated the test observation with respect to their conditional distribution without reference to the actual observation. By following the general theory for importance sampling, the formula used by iIS can be proved to be equivalent to the LOOCV predictive p-value. We compare iIS and other three existing methods in the literature with two disease mapping datasets. Our empirical results show that the predictive p-values estimated with iIS are almost identical to the predictive p-values estimated with actual LOOCV and outperform those given by the existing three methods, namely, the posterior predictive checking, the ordinary importance sampling, and the ghosting method by Marshall and Spiegelhalter (2003). Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
1,10.1109/FCCM.2017.56,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027717985&doi=10.1109%2fFCCM.2017.56&partnerID=40&md5=01ca9b9ef22e334efb3c6ef994ea7a9c,"Markov Chain Monte Carlo (MCMC) algorithms are used to obtain samples from any target probability distribution and are widely used in stochastic processing techniques. Stochastic processing techniques such as machine learning and image processing need to compute large amounts of data in real-time, thus high throughput MCMC samplers are of utmost importance. Parallel Tempering (PT) MCMC has proven better mixing and convergence for high-dimensional and multi-modal distributions compared to other popular MCMC algorithms. In this paper, we employ a special case of Dth order Markov chains to modify the PT-MCMC algorithm, named 'Multiple Parallel Tempering' (MPT). The modification converts one MCMC sampler into multiple independent samplers that generate and interleave their samples on one output line each clock cycle. A fully scalable and pipelined hardware accelerator for the PT and proposed MPT sampler is designed and implemented on Artix-7 Xilinx FPGA for chain numbers of 1, 2, and 8. The post-place and route FPGA implementation results indicate that the throughput of the proposed MPT sampler for chain numbers 1, 2, and 8 achieves 31x, 31x, and 28x respectively higher as compared to PT sampler with the same chain number configuration. © 2017 IEEE."
,10.1002/sim.7255,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013677589&doi=10.1002%2fsim.7255&partnerID=40&md5=02ea8106df2bffc03850ddaef27545c6,"A normality assumption is typically adopted for the random effects in a clustered or longitudinal data analysis using a linear mixed model. However, such an assumption is not always realistic, and it may lead to potential biases of the estimates, especially when variable selection is taken into account. Furthermore, flexibility of nonparametric assumptions (e.g., Dirichlet process) on these random effects may potentially cause centering problems, leading to difficulty of interpretation of fixed effects and variable selection. Motivated by these problems, we proposed a Bayesian method for fixed and random effects selection in nonparametric random effects models. We modeled the regression coefficients via centered latent variables which are distributed as probit stick-breaking scale mixtures. By using the mixture priors for centered latent variables along with covariance decomposition, we could avoid the aforementioned problems and allow efficient selection of fixed and random effects from the model. We demonstrated the advantages of our proposed approach over other competing alternatives through a simulated example and also via an illustrative application to a data set from a periodontal disease study. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd."
,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028975755&partnerID=40&md5=1a613190893683fd15626b6ddad04fda,"It is important to consider the changing states in hedging. The Markov regime-switching dynamic correlation multivariate stochastic volatility (MRS-DC-MSV) model was proposed to solve this issue. DC-MSV model and MRS-DC-MSV model were used to calculate the time-varying hedging ratios and compare the hedging performance. The Markov chain Monte Carlo (MCMC) method was used to estimate the parameters. The results showed that, there were obviously two economic states in Chinese financial market. Two models all did well in hedging, but the performance of MRS-DC-MSV model was better. It could reduce risk by nearly 90%. Thus, in the hedging period, changing states is a factor that cannot be neglected. Copyright © 2017 Editorial Board of Journal of Donghua University, Shanghai, China."
,10.3389/fmicb.2017.01139,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021775606&doi=10.3389%2ffmicb.2017.01139&partnerID=40&md5=ab4bdb0185568fdc8b56f46e34461921,"Different techniques are available for assessing differences in virulence of bacterial foodborne pathogens. The use of animal models or human volunteers is not expedient for various reasons; the use of epidemiological data is often hampered by lack of crucial data. In this paper, we describe a static, sequential gastrointestinal tract (GIT) model system in which foodborne pathogens are exposed to simulated gastric and intestinal contents of the human digestive tract, including the interaction of pathogens with the intestinal epithelium. The system can be employed with any foodborne bacterial pathogens. Five strains of Salmonella Heidelberg and one strain of Salmonella Typhimurium were used to assess the robustness of the system. Four S. Heidelberg strains originated from an outbreak, the fifth S. Heidelberg strain and the S. Typhimurium strain originated from routine meat inspections. Data from plate counts, collected for determining the numbers of surviving bacteria in each stage, were used to quantify both the experimental uncertainty and biological variability of pathogen survival throughout the system. For this, a hierarchical Bayesian framework using Markov chain Monte Carlo (MCMC) was employed. The model system is able to distinguish serovars/strains for in vitro infectivity when accounting for within strain biological variability and experimental uncertainty. © 2017 Wijnands, Teunis, Kuijpers, Delfgou-Van Asch and Pielaat."
,10.1103/PhysRevE.95.062135,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022182648&doi=10.1103%2fPhysRevE.95.062135&partnerID=40&md5=1a777a7a5b909b42186291ebac835e12,"The use of stochastic models to study the dynamics of infectious diseases is an important tool to understand the epidemiological process. For several directly transmitted diseases, reinfection is a relevant process, which can be expressed by endogenous reactivation of the pathogen or by exogenous reinfection due to direct contact with an infected individual (with smaller reinfection rate σβ than infection rate β). In this paper, we examine the stochastic susceptible, infected, recovered, infected (SIRI) model simulating the endogenous reactivation by a spontaneous reaction, while exogenous reinfection by a catalytic reaction. Analyzing the mean-field approximations of a site and pairs of sites, and Monte Carlo (MC) simulations for the particular case of exogenous reinfection, we obtained continuous phase transitions involving endemic, epidemic, and no transmission phases for the simple approach; the approach of pairs is better to describe the phase transition from endemic phase (susceptible, infected, susceptible (SIS)-like model) to epidemic phase (susceptible, infected, and removed or recovered (SIR)-like model) considering the comparison with MC results; the reinfection increases the peaks of outbreaks until the system reaches endemic phase. For the particular case of endogenous reactivation, the approach of pairs leads to a continuous phase transition from endemic phase (SIS-like model) to no transmission phase. Finally, there is no phase transition when both effects are taken into account. We hope the results of this study can be generalized for the susceptible, exposed, infected, and removed or recovered (SEIRIE) model, for which the state exposed (infected but not infectious), describing more realistically transmitted diseases such as tuberculosis. In future work, we also intend to investigate the effect of network topology on phase transitions when the SIRI model describes both transmitted diseases (σ<1) and social contagions (σ>1). © 2017 American Physical Society."
,10.1109/SIU.2017.7960520,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026287266&doi=10.1109%2fSIU.2017.7960520&partnerID=40&md5=094171694d441bf3e7597fbbf8096c85,"We propose a Monte Carlo Markov Chain (MCMC) based method for image registration. We formulate the image registration problem within a Bayesian framework and generate samples from the resulting posterior density of the registration parameters using MCMC. Thus, posterior density is characterized through the samples that are drawn with the MCMC principle. When the posterior density is multimodal, samples from different modes of the posterior lead to different and meaningful solutions for the image registration problem. We perform experiments on pairs of test images which may admit multiple registration solutions. Preliminary results demonstrate the potential of the proposed approach. © 2017 IEEE."
1,10.1364/OE.25.015441,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021347269&doi=10.1364%2fOE.25.015441&partnerID=40&md5=fdf0951ddb7f658c27c69b5cd1d7e77b,We investigate the influence of the Mo-layer thickness on the EUV reflectance of Mo/Si mirrors with a set of unpolished and interface-polished Mo/Si/C multilayer mirrors. The Mo-layer thickness is varied in the range from 1.7 nm to 3.05 nm. We use a novel combination of specular and di-use intensity measurements to determine the interface roughness throughout the multilayer stack and do not rely on scanning probe measurements at the surface only. The combination of EUV and X-ray reflectivity measurements and near-normal incidence EUV di-use scattering allows to reconstruct the Mo layer thicknesses and to determine the interface roughness power spectral density. The data analysis is conducted by applying a matrix method for the specular reflection and the distorted-wave Born approximation for di-use scattering. We introduce the Markov-chain Monte Carlo method into the field in order to determine the respective confidence intervals for all reconstructed parameters. We unambiguously detect a threshold thickness for Mo in both sample sets where the specular reflectance goes through a local minimum correlated with a distinct increase in di-use scatter. We attribute that to the known appearance of an amorphous-To-crystallization transition at a certain thickness threshold which is altered in our sample system by the polishing. © 2017 Optical Society of America.
3,10.1186/s12876-017-0639-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021255935&doi=10.1186%2fs12876-017-0639-0&partnerID=40&md5=7919c75d04640633697f36bc828e7be7,"Background: Controversies persist regarding the effect of prokinetics for the treatment of functional dyspepsia (FD). This study aimed to assess the comparative efficacy of prokinetic agents for the treatment of FD. Methods: Randomized controlled trials (RCTs) of prokinetics for the treatment of FD were identified from core databases. Symptom response rates were extracted and analyzed using odds ratios (ORs). A Bayesian network meta-analysis was performed using the Markov chain Monte Carlo method in WinBUGS and NetMetaXL. Results: In total, 25 RCTs, which included 4473 patients with FD who were treated with 6 different prokinetics or placebo, were identified and analyzed. Metoclopramide showed the best surface under the cumulative ranking curve (SUCRA) probability (92.5%), followed by trimebutine (74.5%) and mosapride (63.3%). However, the therapeutic efficacy of metoclopramide was not significantly different from that of trimebutine (OR:1.32, 95% credible interval: 0.27-6.06), mosapride (OR: 1.99, 95% credible interval: 0.87-4.72), or domperidone (OR: 2.04, 95% credible interval: 0.92-4.60). Metoclopramide showed better efficacy than itopride (OR: 2.79, 95% credible interval: 1.29-6.21) and acotiamide (OR: 3.07, 95% credible interval: 1.43-6.75). Domperidone (SUCRA probability 62.9%) showed better efficacy than itopride (OR: 1.37, 95% credible interval: 1.07-1.77) and acotiamide (OR: 1.51, 95% credible interval: 1.04-2.18). Conclusions: Metoclopramide, trimebutine, mosapride, and domperidone showed better efficacy for the treatment of FD than itopride or acotiamide. Considering the adverse events related to metoclopramide or domperidone, the short-term use of these agents or the alternative use of trimebutine or mosapride could be recommended for the symptomatic relief of FD. © 2017 The Author(s)."
6,10.1186/s12918-017-0433-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021207880&doi=10.1186%2fs12918-017-0433-1&partnerID=40&md5=d366595b20a37db902e18cfcf7f2b71e,"Background: In quantitative biology, mathematical models are used to describe and analyze biological processes. The parameters of these models are usually unknown and need to be estimated from experimental data using statistical methods. In particular, Markov chain Monte Carlo (MCMC) methods have become increasingly popular as they allow for a rigorous analysis of parameter and prediction uncertainties without the need for assuming parameter identifiability or removing non-identifiable parameters. A broad spectrum of MCMC algorithms have been proposed, including single- and multi-chain approaches. However, selecting and tuning sampling algorithms suited for a given problem remains challenging and a comprehensive comparison of different methods is so far not available. Results: We present the results of a thorough benchmarking of state-of-the-art single- and multi-chain sampling methods, including Adaptive Metropolis, Delayed Rejection Adaptive Metropolis, Metropolis adjusted Langevin algorithm, Parallel Tempering and Parallel Hierarchical Sampling. Different initialization and adaptation schemes are considered. To ensure a comprehensive and fair comparison, we consider problems with a range of features such as bifurcations, periodical orbits, multistability of steady-state solutions and chaotic regimes. These problem properties give rise to various posterior distributions including uni- and multi-modal distributions and non-normally distributed mode tails. For an objective comparison, we developed a pipeline for the semi-automatic comparison of sampling results. Conclusion: The comparison of MCMC algorithms, initialization and adaptation schemes revealed that overall multi-chain algorithms perform better than single-chain algorithms. In some cases this performance can be further increased by using a preceding multi-start local optimization scheme. These results can inform the selection of sampling methods and the benchmark collection can serve for the evaluation of new algorithms. Furthermore, our results confirm the need to address exploration quality of MCMC chains before applying the commonly used quality measure of effective sample size to prevent false analysis conclusions. © 2017 The Author(s)."
,10.1109/TCSVT.2017.2718225,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021811747&doi=10.1109%2fTCSVT.2017.2718225&partnerID=40&md5=fb1c825688c03e002d0566eb2d67d1af,"Correspondence problems are challenging due to the complexity of real-world scenes. One way to solve this problem is to improve the graph matching (GM) process, which is flexible for matching non-rigid objects. GM can be classified into three categories that correspond with the variety of object functions: first-order, second-order and, high-order matching. Graph and hypergraph matching have been proposed separately in previous works. The former is equivalent to second-order GM, and the latter is equivalent to high-order GM, but we use the terms second- and high-order GM to unify the terminology in this paper. Second- and high-order GM fit well with different types of problems; the key goal for these processes is to find better-optimized algorithms. Because the optimal problems for second- and high-order GM are different, we propose two novel optimized algorithms for them in this paper. (1) For second-order GM, we first introduce a K-Nearest-Neighbor-Pooling Matching (KNNPM) method that integrates feature pooling into GM and reduces the complexity. Meanwhile, we evaluate each matching candidate using discriminative weights on its k-nearest neighbors (KNN) by taking locality as well as sparsity into consideration. (2) High-order GM introduces numerous outliers because precision is rarely considered in related methods. Therefore, we propose a sub-pattern structure to construct a robust high-order GM method that better integrates geometric information. To narrow the search space and solve the optimization problem, a new prior strategy and a cell-algorithm-based Markov Chain Monte Carlo (MCMC) framework are proposed, respectively. In addition, experiments demonstrate the robustness and improvements of these algorithms with respect to matching accuracy compared with other state-of-the-art algorithms. IEEE"
1,10.1080/03610926.2015.1116581,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014796745&doi=10.1080%2f03610926.2015.1116581&partnerID=40&md5=08f6b6316ad58ab94eafe6bdd343e4f8,"In this paper, we consider shared gamma frailty model with the reversed hazard rate (RHR) with two different baseline distributions, namely the generalized inverse Rayleigh and the exponentiated Gumbel distributions. With these two baseline distributions we propose two different shared frailty models. We develop the Bayesian estimation procedure using Markov Chain Monte Carlo technique to estimate the parameters involved in these models. We present a simulation study to compare the true values of the parameters with the estimated values. A search of the literature suggests that currently no work has been done for these two baseline distributions with a shared gamma frailty with the RHR so far. We also apply these two models by using a real life bivariate survival data set of Australian twin data given by Duffy et a1. (1990) and a better model is suggested for the data. © 2017 Taylor & Francis Group, LLC."
3,10.1109/ICASSP.2017.7952555,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023757502&doi=10.1109%2fICASSP.2017.7952555&partnerID=40&md5=e0da1eb3df7851fb56b112e4f432a477,"Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods have become popular in modern data analysis problems due to their computational efficiency. Even though they have proved useful for many statistical models, the application of SG-MCMC to non-negative matrix factorization (NMF) models has not yet been extensively explored. In this study, we develop two parallel SG-MCMC algorithms for a broad range of NMF models. We exploit the conditional independence structure of the NMF models and utilize a stratified sub-sampling approach for enabling parallelization. We illustrate the proposed algorithms on an image restoration task and report encouraging results. © 2017 IEEE."
16,10.1002/2017GL073159,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020099128&doi=10.1002%2f2017GL073159&partnerID=40&md5=29a396104ad6e15a842f3d0aff73d071,"The Juno microwave radiometer measured the thermal emission from Jupiter's atmosphere from the cloud tops at about 1 bar to as deep as a hundred bars of pressure during its first flyby over Jupiter (PJ1). The nadir brightness temperatures show that the Equatorial Zone is likely to be an ideal adiabat, which allows a determination of the deep ammonia abundance in the range 362+33 -33 ppm. The combination of Markov chain Monte Carlo method and Tikhonov regularization is studied to invert Jupiter's global ammonia distribution assuming a prescribed temperature profile. The result shows (1) that ammonia is depleted globally down to 50–60 bars except within a few degrees of the equator, (2) the North Equatorial Belt is more depleted in ammonia than elsewhere, and (3) the ammonia concentration shows a slight inversion starting from about 7 bars to 2 bars. These results are robust regardless of the choice of water abundance. ©2017. American Geophysical Union. All Rights Reserved."
6,10.1007/978-3-319-12385-1_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034737632&doi=10.1007%2f978-3-319-12385-1_7&partnerID=40&md5=322729bb51f77890b097e43ff0251fcf,"These lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the Bayesian approach to inverse problems in differential equations. This approach is fundamental in the quantification of uncertainty within applications involving the blending of mathematical models with data. The finite-dimensional situation is described first, along with some motivational examples. Then the development of probability measures on separable Banach space is undertaken, using a random series over an infinite set of functions to construct draws; these probability measures are used as priors in the Bayesian approach to inverse problems. Regularity of draws from the priors is studied in the natural Sobolev or Besov spaces implied by the choice of functions in the random series construction, and the Kolmogorov continuity theorem is used to extend regularity considerations to the space of Hölder continuous functions. Bayes' theorem is derived in this prior setting, and here interpreted as finding conditions under which the posterior is absolutely continuous with respect to the prior, and determining a formula for the Radon-Nikodym derivative in terms of the likelihood of the data. Having established the form of the posterior, we then describe various properties common to it in the infinite-dimensional setting. These properties include well-posedness, approximation theory, and the existence of maximum a posteriori estimators. We then describe measure-preserving dynamics, again on the infinite-dimensional space, including Markov chain Monte Carlo and sequential Monte Carlo methods, and measure-preserving reversible stochastic differential equations. By formulating the theory and algorithms on the underlying infinite-dimensional space, we obtain a framework suitable for rigorous analysis of the accuracy of reconstructions, of computational complexity, as well as naturally constructing algorithms which perform well under mesh refinement, since they are inherently well defined in infinite dimensions."
1,10.1109/ICASSP.2017.7952876,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023758943&doi=10.1109%2fICASSP.2017.7952876&partnerID=40&md5=cb8283102db21a81f5fcaed9b6ad9d48,"Particle filters are among the most effective filtering algorithms for nonlinear and non-Gaussian models. When the state dimension is high, they are known to suffer from weight degeneracy. Sequential Markov chain Monte Carlo (SMCMC) methods have been proposed as an alternative sequential inference technique that can perform better in high dimensional state spaces. In this paper, we propose to construct a composite Metropolis-Hastings (MH) kernel within the SMCMC framework using invertible particle flow. Simulation results show that the proposed kernel significantly increases the acceptance rate and improves estimation accuracy compared with state-of-the-art filtering algorithms, in high dimensional simulation examples. © 2017 IEEE."
,10.1007/978-3-319-12385-1_69,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034732177&doi=10.1007%2f978-3-319-12385-1_69&partnerID=40&md5=ba2b31e748bfabfd1a8afa3c155aa06d,"Cubic splines are commonly used in numerical analysis. It has also become popular in the analysis of computer experiments, thanks to its adoption by the software JMP 8.0.2 2010. In this chapter, a Bayesian version of the cubic spline method is proposed, in which the random function that represents prior uncertainty about y is taken to be a specific stationary Gaussian process and y is the output of the computer experiment. A Markov chain Monte Carlo (MCMC) procedure is developed for updating the prior given the observed y values. Simulation examples and a real data application are given to show that the proposed Bayesian method performs better than the frequentist cubic spline method and the standard method based on the Gaussian correlation function."
,10.1007/978-3-319-12385-1_67,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034750088&doi=10.1007%2f978-3-319-12385-1_67&partnerID=40&md5=d55ad5b4f1670413006d0d5a352b12dc,"A salient task in uncertainty quantification (UQ) is to study the dependence of a quantity of interest (QoI) on input variables representing system uncertainties. Relying on linear expansions of the QoI in orthogonal polynomial bases of inputs, polynomial chaos expansions (PCEs) are now among the widely used methods in UQ. When there exists a smoothness in the solution being approximated, the PCE exhibits sparsity in that a small fraction of expansion coefficients are significant. By exploiting this sparsity, compressive sampling, also known as compressed sensing, provides a natural framework for accurate PCE using relatively few evaluations of the QoI and in a manner that does not require intrusion into legacy solvers. The PCE possesses a rich structure between the QoI being approximated, the polynomials, and input variables used to perform the approximation and where the QoI is evaluated. In this chapter insights are provided into this structure, summarizing a portion of the current literature on PCE via compressive sampling within the context of UQ."
2,10.1007/978-3-319-12385-1_56,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021140868&doi=10.1007%2f978-3-319-12385-1_56&partnerID=40&md5=91ccaccf4436c140a0f77af82b989ebc,"The UQ Toolkit (UQTk) is a collection of tools for uncertainty quantification, ranging from intrusive and nonintrusive forward propagation of uncertainty to inverse problems and sensitivity analysis. This chapter first outlines the UQTk design philosophy, followed by an overview of the available methods and the way they are implemented in UQTk. The second part of this chapter is a detailed example that illustrates a UQ workflow from surrogate construction, and calibration, to forward propagation and attribution."
,10.1007/978-3-319-12385-1_49,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034776680&doi=10.1007%2f978-3-319-12385-1_49&partnerID=40&md5=9e7f28c0cc9bac28210584a29df07131,"This chapter describes the use of the Predictive Capability Maturity Model (PCMM) (Oberkampf et al., Predictive capability maturity model for computational modeling and simulation. Technical report, SAND2007-5948, Sandia National Laboratories, 2007) applied to a nuclear reactor simulation. The application and PCMM will be discussed relative to review by the Nuclear Regulatory Commission. In a regulatory environment, one takes on the role of a lawyer presenting evidence to a judge with a prosecuting attorney allowed to cross-examine. In this type of ""hostile"" environment, a structured process that logically presents the evidence is helpful. In addition, many simulations are now multi-scale, multi-physics, and multicode. For this level of complexity, it is easy to get lost in the details. The PCMM method has been adapted for this multi-physics multi-code software. Since the key is to provide the regulator with confidence that the software is capable of predicting the quantity of interest (QoI) with a well-quantified uncertainty, the PCMM approach is a natural solution."
,10.1007/978-3-319-12385-1_53,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034763370&doi=10.1007%2f978-3-319-12385-1_53&partnerID=40&md5=c5d5cdd60ba60613921a38a607bc73db,"This chapter gives an overview of the capabilities of the PSUADE (acronym for Problem Solving Environment for Uncertainty Analysis and Design Exploration) software package, which has been developed to support the many operations involved in a typical nonintrusive (i.e., simulation codes are to be treated as ""black boxes"") uncertainty quantification (UQ) study, such as sample generation, ensemble simulations, and analysis of simulation results. Specifically, the software enables users to perform detailed UQ analysis such as uncertainty analysis (for computing statistical moments and probability distributions), sensitivity analysis (e.g., variance decomposition), parameter screening or down- selection, response surface analysis, statistical inferences, and optimization under uncertainty. In addition to a rich suite of UQ capabilities accessible via either batch or command line processing, PSUADE also provides many tools for data manipulation and visualization, which may be useful for more immersive data analysis. PSUADE is a public domain software that has been released under the LGPL license since 2007."
2,10.1007/978-3-319-12385-1_58,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031915105&doi=10.1007%2f978-3-319-12385-1_58&partnerID=40&md5=f7140778bb4f81ea590adebcde4df8dd,"The Gaussian Process Models for Simulation Analysis (GPMSA) package is a set of functions written in the Matlab programming language aimed at emulating a computer model of a system being studied, calibrating this computer model to observations of the system, and giving predictions of the expected system response. Collectively, these capabilities comprise uncertainty quantification (UQ) in model-supported inference. This chapter will first discuss some background and motivation for the GPMSA code, then demonstrate the code's function interfaces in the context of a series of illustrative example problems."
1,10.1007/978-3-319-12385-1_57,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034760420&doi=10.1007%2f978-3-319-12385-1_57&partnerID=40&md5=10d06a3e5a6f8160f5833c02d1b9633b,"The Parallel C++ Statistical Library for the Quantification of Uncertainty for Estimation, Simulation, and Optimization (QUESO) is a collection of statistical algorithms and programming constructs supporting research into the quantification of uncertainty of models and their predictions. QUESO is primarily focused on solving statistical inverse problems using Bayes' theorem, which expresses a distribution of possible values for a set of uncertain parameters (the posterior distribution) in terms of the existing knowledge of the system (the prior) and noisy observations of a physical process, represented by a likelihood distribution. The posterior distribution is not often known analytically and so requires computational methods. It is typical to compute probabilities and moments from the posterior distribution, but this is often a high-dimensional object, and standard Riemann-type methods for quadrature become prohibitively expensive. The approach QUESO takes in this regard is to rely on Markov chain Monte Carlo (MCMC) methods which are well suited to evaluating quantities such as probabilities and moments of high-dimensional probability distributions. QUESO's intended use is as tool to assist and facilitate coupling uncertainty quantification to a specific application called a *forward problem. While many libraries presently exist that solve Bayesian inference problems, QUESO is a specialized piece of software primarily designed to solve such problems by utilizing parallel environments demanded by large-scale forward problems. QUESO is written in C++, uses MPI, and utilizes libraries already available to the scientific community. Thus, the target audience of this library are researchers who have solid background in Bayesian methods, are comfortable with UNIX concepts and the command line, and have knowledge of a programming language, preferably C/C++."
,10.3390/ijerph14060648,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021125811&doi=10.3390%2fijerph14060648&partnerID=40&md5=3afd3b47f88d63c7de43ce989d69a479,"Cardiovascular disease (CVD) and associated behavioural and metabolic risk factors constitute a major public health concern at a global level. Many reports worldwide have documented different risk profiles for populations with demographic variations. The objective of this study was to examine geographic variations in the top leading cardio metabolic and behavioural risk factors in Luxembourg, in order to provide an overall picture of CVD burden across the country. The analysis conducted was based on data from the nationwide ORISCAV-LUX survey, including 1432 subjects, aged 18-69 years. A self-reported questionnaire, physical examination and blood sampling were performed. Age and sex-adjusted risk profile maps were generated using multivariate Bayesian geo-additive regression models, based on Markov Chain Monte Carlo techniques and were used to evaluate the significance of the spatial effects on the distribution of a range of cardio metabolic risk factors, namely smoking, high body mass index (BMI), high blood pressure, high fasting plasma glucose, alcohol use, high total cholesterol, low glomerular filtration rate, and physical inactivity. Higher prevalence of smoking was observed in the northern regions, higher overweight/obesity and abdominal obesity clustered in the central belt, whereas hypertension was spotted particularly in the southern part of the country. Maps revealed that subjects residing in Luxembourg canton were significantly less likely to be hypertensive or overweight/obese, whereas they were less likely to practice physical activity of ≥8000 Metabolic Equivalent of Task (MET)-min/week. These patterns were also observed at the municipality level in Luxembourg. Statistically, there were non-significant spatial patterns regarding smoking, diabetes, total serum cholesterol and low glomerular filtration rate risk distribution. This comprehensive risk profile mapping showed remarkable geographic variations in cardio metabolic and behavioural risk factors. Considering the prominent burden of CVD this research provides opportunities for tailored interventions and may help to better fight against this escalating public health problem. © 2017 by the authors. Licensee MDPI, Basel, Switzerland."
,10.1109/ICASSP.2017.7953064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023767613&doi=10.1109%2fICASSP.2017.7953064&partnerID=40&md5=8f25ce6e88549f89278ea8491fd18525,The object of this paper is to introduce a new estimation algorithm specifically designed for the latent high-order autoregressive models. It implements the concept of the filter-based maximum likelihood. Our approach is fully deterministic and is less computationally demanding than the traditional Monte Carlo Markov chain techniques. The simulation experiments and real-world data processing confirm the interest of our approach. © 2017 IEEE.
1,10.1016/j.cma.2017.01.042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017390579&doi=10.1016%2fj.cma.2017.01.042&partnerID=40&md5=315e4e019792025512daa54401cc13fd,"Bayesian model selection is augmented with automatic relevance determination (ARD) to perform model reduction of complex dynamical systems modelled by nonlinear, stochastic ordinary differential equations (ODE). Given noisy measurement data, a parametrically flexible model is envisioned to represent the dynamical system. A Bayesian model selection problem is posed to find the best model nested under the envisioned model. This model selection problem is transferred from the model space to hyper-parameter space by regularizing the parameter posterior space through a parametrized prior distribution called the ARD prior. The resulting joint prior pdf is the combination of parametrized ARD priors assigned to parameters whose relevance to the system dynamics is questionable and the known prior pdf for parameters whose relevance is known a priori. The hyper-parameter of each ARD prior explicitly represents the relevance of the corresponding model parameter. The hyper-parameters are estimated using the measurement data by performing evidence maximization or type-II maximum likelihood. Superfluous model parameters are switched off during evidence maximization by the corresponding ARD prior, forcing the model parameter to be irrelevant for prediction purposes. An efficient numerical implementation for evidence computation using Markov Chain Monte Carlo sampling of the parameter posterior distribution is presented for the case when the analytical evaluation of evidence is not possible. The ARD approach is validated with synthetic measurements generated from a nonlinear, unsteady aeroelastic oscillator consisting of a NACA0012 airfoil undergoing limit cycle oscillation. A set of intentionally flexible stochastic ODEs having different state-space formulation is proposed to model the synthetic data. ARD is used to obtain an optimal nested model corresponding to each proposed model. The optimal nested model with the maximum posterior model probability is chosen as the overall optimal model. ARD provides a flexible Bayesian platform to find the optimal nested model by eliminating the need to propose candidate nested models and its prior pdfs. © 2017 Elsevier B.V."
,10.1109/TSP.2017.2684747,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019161661&doi=10.1109%2fTSP.2017.2684747&partnerID=40&md5=91857fa7aac9a909843c4334b1c15afa,"Time-varying mixture models are useful for representing complex, dynamic distributions. Components in the mixture model can appear and disappear, and persisting components can evolve. This allows great flexibility in streaming data applications where the model can be adjusted as new data arrives. Fitting a mixture model with computational guarantees which can meet real-time requirements is challenging with existing algorithms, especially when the model order can vary with time. Existing approximate inference methods may require multiple restarts to search for a good local solution. Monte-Carlo methods can be used to jointly estimate the model order and model parameters, but when the distribution of each mixand has a high-dimensional parameter space, they suffer from the curse of dimensionality and and from slow convergence. This paper proposes a generative model for time-varying mixture models, tailored for mixtures of discrete-time Markov chains. A novel, deterministic inference procedure is introduced and is shown to be suitable for applications requiring real-time estimation, and the method is guaranteed to converge at each time step. As a motivating application, we model and predict traffic patterns in a transportation network. Experiments illustrate the performance of the scheme and offer insights regarding tuning of the algorithm parameters. The experiments also investigate the predictive power of the proposed model compared to less complex models and demonstrate the superiority of the mixture model approach for prediction of traffic routes in real data. © 1991-2012 IEEE."
4,10.1093/bioinformatics/btx088,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021317839&doi=10.1093%2fbioinformatics%2fbtx088&partnerID=40&md5=2a792e16d6c033df94586e097289c158,"Motivation: Advances in sequencing technology continue to deliver increasingly large molecular sequence datasets that are often heavily partitioned in order to accurately model the underlying evolutionary processes. In phylogenetic analyses, partitioning strategies involve estimating conditionally independent models of molecular evolution for different genes and different positions within those genes, requiring a large number of evolutionary parameters that have to be estimated, leading to an increased computational burden for such analyses. The past two decades have also seen the rise of multi-core processors, both in the central processing unit (CPU) and Graphics processing unit processor markets, enabling massively parallel computations that are not yet fully exploited by many software packages for multipartite analyses. Results: We here propose a Markov chain Monte Carlo (MCMC) approach using an adaptive multivariate transition kernel to estimate in parallel a large number of parameters, split across partitioned data, by exploiting multi-core processing. Across several real-world examples, we demonstrate that our approach enables the estimation of these multipartite parameters more efficiently than standard approaches that typically use a mixture of univariate transition kernels. In one case, when estimating the relative rate parameter of the non-coding partition in a heterochronous dataset, MCMC integration efficiency improves by > 14-fold. Availability and Implementation: Our implementation is part of the BEAST code base, a widely used open source software package to perform Bayesian phylogenetic inference. © The Author 2017. Published by Oxford University Press. All rights reserved."
3,10.1103/PhysRevD.95.123507,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022341145&doi=10.1103%2fPhysRevD.95.123507&partnerID=40&md5=1dfd737228bc6701f70296737c84f0fe,"Future galaxy surveys promise to probe local primordial non-Gaussianity at unprecedented precision, σ(fNL) 1. We study the implications for multifield inflation by considering spectator models, where inflation is driven by the inflaton field, but the primordial perturbations are (partially) generated by a second, spectator field. We perform a Markov chain Monte Carlo likelihood analysis using Planck data to study quantitative predictions for fNL and other observables for a range of such spectator models. We show that models where the primordial perturbations are dominated by the spectator field, while fine-tuned within the broader parameter space, typically predict fNL of order unity. Therefore, upcoming galaxy clustering measurements will constitute a stringent test of whether or not the generation of primordial perturbations and the accelerated expansion in the inflationary universe are due to separate phenomena. © 2017 American Physical Society."
15,10.1103/PhysRevD.95.123540,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022339047&doi=10.1103%2fPhysRevD.95.123540&partnerID=40&md5=722f35c2e30c1f7272892e68b42750cd,"In a model of the late-time cosmic acceleration within the framework of generalized Proca theories, there exists a de Sitter attractor preceded by the dark energy equation of state wDE=-1-s, where s is a positive constant. We run the Markov-chain-Monte Carlo code to confront the model with the observational data of the cosmic microwave background (CMB), baryon acoustic oscillations, supernovae type Ia, and local measurements of the Hubble expansion rate for the background cosmological solutions and obtain the bound s=0.254-0.097+0.118 at 95% confidence level (C.L.). Existence of the additional parameter s to those in the Λ-cold-dark-matter (ΛCDM) model allows to reduce tensions of the Hubble constant H0 between the CMB and the low-redshift measurements. Including the cosmic growth data of redshift-space distortions in the galaxy power spectrum and taking into account no-ghost and stability conditions of cosmological perturbations, we find that the bound on s is shifted to s=0.16-0.08+0.08 (95% C.L.) and hence the model with s>0 is still favored over the ΛCDM model. Apart from the quantities s,H0 and the today's matter density parameter Ωm0, the constraints on other model parameters associated with perturbations are less stringent, reflecting the fact that there are different sets of parameters that give rise to a similar cosmic expansion and growth history. © 2017 American Physical Society."
17,10.1103/PhysRevD.95.123512,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022346673&doi=10.1103%2fPhysRevD.95.123512&partnerID=40&md5=8f667b44263965d0f11b0e777f8046e5,"The next-generation weak lensing surveys (i.e., LSST, Euclid, and WFIRST) will require exquisite control over systematic effects. In this paper, we address shear calibration and present the most realistic forecast to date for LSST/Euclid/WFIRST and CMB lensing from a stage 4 CMB experiment (""CMB S4""). We use the cosmolike code to simulate a joint analysis of all the two-point functions of galaxy density, galaxy shear, and CMB lensing convergence. We include the full Gaussian and non-Gaussian covariances and explore the resulting joint likelihood with Monte Carlo Markov chains. We constrain shear calibration biases while simultaneously varying cosmological parameters, galaxy biases, and photometric redshift uncertainties. We find that CMB lensing from CMB S4 enables the calibration of the shear biases down to 0.2%-3% in ten tomographic bins for LSST (below the ∼0.5% requirements in most tomographic bins), down to 0.4%-2.4% in ten bins for Euclid, and 0.6%-3.2% in ten bins for WFIRST. For a given lensing survey, the method works best at high redshift where shear calibration is otherwise most challenging. This self-calibration is robust to Gaussian photometric redshift uncertainties and to a reasonable level of intrinsic alignment. It is also robust to changes in the beam and the effectiveness of the component separation of the CMB experiment, and slowly dependent on its depth, making it possible with third-generation CMB experiments such as AdvACT and SPT-3G, as well as the Simons Observatory. © 2017 American Physical Society."
2,10.1145/3062341.3062375,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025116056&doi=10.1145%2f3062341.3062375&partnerID=40&md5=5bc3fcbc24964be5e295698b34504fa4,"The problem of probabilistic modeling and inference, at a high-level, can be viewed as constructing a (model, query, inference) tuple, where an inference algorithm implements a query on a model. Notably, the derivation of inference algorithms can be a difficult and error-prone task. Hence, researchers have explored how ideas from probabilistic pro- gramming can be applied. In the context of constructing these tuples, probabilistic programming can be seen as taking a language-based approach to probabilistic modeling and inference. For instance, by using (1) appropriate languages for expressing models and queries and (2) devising inference techniques that operate on encodings of models (and queries) as program expressions, the task of inference can be automated. In this paper, we describe a compiler that transforms a probabilistic model written in a restricted modeling language and a query for posterior samples given observed data into a Markov Chain Monte Carlo (MCMC) inference algorithm that implements the query. The compiler uses a sequence of intermediate languages (ILs) that guide it in gradually and successively refining a declarative specification of a probabilistic model and the query into an executable MCMC inference algorithm. The compilation strategy produces composable MCMC algorithms for execution on a CPU or GPU. Copyright is held by the owner/author(s)."
,10.1088/1755-1315/69/1/012155,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021843162&doi=10.1088%2f1755-1315%2f69%2f1%2f012155&partnerID=40&md5=d76787ad99b17e335e239bfa2f9952dd,"Aiming at the problem of the parameter estimation of multiple unresolved targets within the radar beam, using the joint bin processing model, a method of jointly estimating the number and the position of the targets is proposed based on reversible jump Markov Chain Monte Carlo (RJ-MCMC). Reasonable assumptions of the prior distributions and Bayesian theory are adopted to obtain the posterior probability density function of the estimated parameters from the conditional likelihood function of the observation, and then the acceptance ratios of the birth, death and update moves are given. During the update move, a hybrid Metropolis-Hastings (MH) sampling algorithm is used to make a better exploration of the parameter space. The simulation results show that this new method outperforms the method of ML-MLD [11] proposed by X.Zhang for similar estimation accuracy is achieved while fewer sub-pulses are needed. © Published under licence by IOP Publishing Ltd."
,10.1088/1742-6596/855/1/012061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023626066&doi=10.1088%2f1742-6596%2f855%2f1%2f012061&partnerID=40&md5=2a225de7176012c5d6acc344e20d39d4,"Hierarchical data structures are common throughout many areas of research. Beforehand, the existence of this type of data was less noticed in the analysis. The appropriate statistical analysis to handle this type of data is the hierarchical linear model (HLM). This article will focus only on random intercept model (RIM), as a subclass of HLM. This model assumes that the intercept of models in the lowest level are varied among those models, and their slopes are fixed. The differences of intercepts were suspected affected by some variables in the upper level. These intercepts, therefore, are regressed against those upper level variables as predictors. The purpose of this paper would demonstrate a proven work of the proposed two level RIM of the modeling on per capita household expenditure in Maluku Utara, which has five characteristics in the first level and three characteristics of districts/cities in the second level. The per capita household expenditure data in the first level were captured by the three parameters Gamma distribution. The model, therefore, would be more complex due to interaction of many parameters for representing the hierarchical structure and distribution pattern of the data. To simplify the estimation processes of parameters, the computational Bayesian method couple with Markov Chain Monte Carlo (MCMC) algorithm and its Gibbs Sampling are employed. © Published under licence by IOP Publishing Ltd."
,10.1088/1742-6596/855/1/012026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023601360&doi=10.1088%2f1742-6596%2f855%2f1%2f012026&partnerID=40&md5=e0c47d094a6a8425eea1f1dcf8e1516e,"Stocks are known as the financial instruments traded in the capital market which have a high level of risk. Their risks are indicated by their uncertainty of their return which have to be accepted by investors in the future. The higher the risk to be faced, the higher the return would be gained. Therefore, the measurements need to be made against the risk. Value at Risk (VaR) as the most popular risk measurement method, is frequently ignore when the pattern of return is not uni-modal Normal. The calculation of the risks using VaR method with the Normal Mixture Autoregressive (MNAR) approach has been considered. This paper proposes VaR method couple with the Mixture Laplace Autoregressive (MLAR) that would be implemented for analysing the first three biggest capitalization Islamic stock return in JII, namely PT. Astra International Tbk (ASII), PT. Telekomunikasi Indonesia Tbk (TLMK), and PT. Unilever Indonesia Tbk (UNVR). Parameter estimation is performed by employing Bayesian Markov Chain Monte Carlo (MCMC) approaches. © Published under licence by IOP Publishing Ltd."
1,10.1088/1742-6596/855/1/012054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023603623&doi=10.1088%2f1742-6596%2f855%2f1%2f012054&partnerID=40&md5=1ef8e45c4027a0d84723fc505371d5b3,"Regression analysis (statistical analmodelling) are among statistical methods which are frequently needed in analyzing quantitative data, especially to model relationship between response and explanatory variables. Nowadays, statistical models have been developed into various directions to model various type and complex relationship of data. Rich varieties of advanced and recent statistical modelling are mostly available on open source software (one of them is R). However, these advanced statistical modelling, are not very friendly to novice R users, since they are based on programming script or command line interface. Our research aims to developed web interface (based on R and shiny), so that most recent and advanced statistical modelling are readily available, accessible and applicable on web. We have previously made interface in the form of e-tutorial for several modern and advanced statistical modelling on R especially for independent responses (including linear models/LM, generalized linier models/GLM, generalized additive model/GAM and generalized additive model for location scale and shape/GAMLSS). In this research we unified them in the form of data analysis, including model using Computer Intensive Statistics (Bootstrap and Markov Chain Monte Carlo/ MCMC). All are readily accessible on our online Virtual Statistics Laboratory. The web (interface) make the statistical modeling becomes easier to apply and easier to compare them in order to find the most appropriate model for the data. © Published under licence by IOP Publishing Ltd."
1,10.1080/02664763.2016.1214245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979647976&doi=10.1080%2f02664763.2016.1214245&partnerID=40&md5=36217d538ac1c25ec54bab031ebb77f0,"In this paper, point and interval estimations for the parameters of the exponentiated exponential (EE) distribution are studied based on progressive first-failure-censored data. The Bayes estimates are computed based on squared error and Linex loss functions and using Markov Chain Monte Carlo (MCMC) algorithm. Also, based on this censoring scheme, approximate confidence intervals for the parameters of EE distribution are developed. Monte Carlo simulation study is carried out to compare the performances of the different methods by computing the estimated risks (ERs), as well as Akaike's information criteria (AIC) and Bayesian information criteria (BIC) of the estimates. Finally, a real data set is introduced and analyzed using EE and Weibull distributions. A comparison is carried out between the mentioned models based on the corresponding Kolmogorov–Smirnov (K–S) test statistic to emphasize that the EE model fits the data with the same efficiency as the other model. Point and interval estimation of all parameters are studied based on this real data set as illustrative example. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/02664763.2016.1204596,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979021789&doi=10.1080%2f02664763.2016.1204596&partnerID=40&md5=721f999866c1a5e5b7559ba12215732d,"This paper conducts simulation-based comparison of several stochastic volatility models with leverage effects. Two new variants of asymmetric stochastic volatility models, which are subject to a logarithmic transformation on the squared asset returns, are proposed. The leverage effect is introduced into the model through correlation either between the innovations of the observation equation and the latent process, or between the logarithm of squared asset returns and the latent process. Suitable Markov Chain Monte Carlo algorithms are developed for parameter estimation and model comparison. Simulation results show that our proposed formulation of the leverage effect and the accompanying inference methods give rise to reasonable parameter estimates. Applications to two data sets uncover a negative correlation (which can be interpreted as a leverage effect) between the observed returns and volatilities, and a negative correlation between the logarithm of squared returns and volatilities. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
7,10.1093/mnras/stx420,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017282801&doi=10.1093%2fmnras%2fstx420&partnerID=40&md5=ac2283b10ae44cd1b070a3a8ed7c8e95,"The recent low value of Planck Collaboration XLVII integrated optical depth to Thomson scattering suggests that the reionization occurred fairly suddenly, disfavouring extended reionization scenarios. This will have a significant impact on the 21 cm power spectrum. Using a seminumerical framework, we improve our model from instantaneous to include time-integrated ionization and recombination effects, and find that this leads to more sudden reionization. It also yields larger HII bubbles that lead to an order of magnitude more 21 cm power on large scales, while suppressing the small-scale ionization power. Local fluctuations in the neutral hydrogen density play the dominant role in boosting the 21 cm power spectrum on large scales, while recombinations are subdominant. We use a Monte Carlo Markov chain approach to constrain our model to observations of the star formation rate functions at z = 6, 7, 8 from Bouwens et al., the Planck Collaboration XLVII optical depth measurements and the Becker & Bolton ionizing emissivity data at z ~ 5.We then use this constrained model to perform 21 cm forecasting for Low Frequency Array, Hydrogen Epoch of Reionization Array and Square Kilometre Array in order to determine how well such data can characterize the sources driving reionization. We find that the Mock 21 cm power spectrum alone can somewhat constrain the halo mass dependence of ionizing sources, the photon escape fraction and ionizing amplitude, but combining the Mock 21 cm data with other current observations enables us to separately constrain all these parameters. Our framework illustrates how the future 21 cm data can play a key role in understanding the sources and topology of reionization as observations improve. © 2017 The Authors. Published by Oxford University Press on behalf of the Royal Astronomical Society."
10,10.3847/1538-4357/aa73d9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021118683&doi=10.3847%2f1538-4357%2faa73d9&partnerID=40&md5=e25f8cf11211b4a98d1e2486799708a9,"In this paper we collect 19 hydrogen-deficient superluminous supernovae (SLSNe) and fit their light curves, temperature evolution, and velocity evolution based on the magnetar-powered model. To obtain the best-fitting parameters, we incorporate the Markov chain Monte Carlo approach. We get rather good fits for seven events (χ2/dof = 0.24-0.96) and good fits for another seven events (χ 2/dof = 1.37-3.13). We find that the initial periods (P 0) and magnetic strength (B p) of the magnetars that supposedly power these SLSNe are in the range of ∼1.2-8.3 ms and G, respectively; the inferred masses of the ejecta of these SLSNe are between 1 and , and the values of the gamma-ray opacity are between 0.01 and 0.82 cm2 g-1. We also calculate the fraction of the initial rotational energy of the magnetars harbored in the centers of the remnants of these SLSNe that is converted to the kinetic energy of the ejecta and find that the fraction is ∼19%-97% for different values of P 0 and B p, indicating that the acceleration effect cannot be neglected. Moreover, we find that the initial kinetic energies of most of these SLSNe are so small (≲2 × 1051 erg) that they can be easily explained by the neutrino-driven mechanism. These results can help clarify some important issues related to the energy-source mechanisms and explosion mechanisms and reveal the nature of SLSNe. © 2017. The American Astronomical Society. All rights reserved."
3,10.1016/j.jsv.2017.03.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014538746&doi=10.1016%2fj.jsv.2017.03.001&partnerID=40&md5=5279161e2c2ee74bff5288548a873f79,"In this paper an offline approach for output-only Bayesian identification of stochastic nonlinear systems is presented. The approach is based on a re-parameterization of the joint posterior distribution of the parameters that define a postulated state-space stochastic model class. In the re-parameterization the state predictive distribution is included, marginalized, and estimated recursively in a state estimation step using an unscented Kalman filter, bypassing state augmentation as required by existing online methods. In applications expectations of functions of the parameters are of interest, which requires the evaluation of potentially high-dimensional integrals; Markov chain Monte Carlo is adopted to sample the posterior distribution and estimate the expectations. The proposed approach is suitable for nonlinear systems subjected to non-stationary inputs whose realization is unknown, and that are modeled as stochastic processes. Numerical verification and experimental validation examples illustrate the effectiveness and advantages of the approach, including: (i) an increased numerical stability with respect to augmented-state unscented Kalman filtering, avoiding divergence of the estimates when the forcing input is unmeasured; (ii) the ability to handle arbitrary prior and posterior distributions. The experimental validation of the approach is conducted using data from a large-scale structure tested on a shake table. It is shown that the approach is robust to inherent modeling errors in the description of the system and forcing input, providing accurate prediction of the dynamic response when the excitation history is unknown. © 2017 Elsevier Ltd"
1,10.1088/1475-7516/2017/06/015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021676975&doi=10.1088%2f1475-7516%2f2017%2f06%2f015&partnerID=40&md5=95c527afc4f2b60b93b9eb3f5918e379,"The KATRIN experiment aims to determine the absolute neutrino mass by measuring the endpoint region of the tritium β-spectrum. As a large-scale experiment with a sharp energy resolution, high source luminosity and low background it may also be capable of testing certain theories of neutrino interactions beyond the standard model (SM). An example of a non-SM interaction are right-handed currents mediated by right-handed W bosons in the left-right symmetric model (LRSM). In this extension of the SM, an additional SU(2)R symmetry in the high-energy limit is introduced, which naturally includes sterile neutrinos and predicts the seesaw mechanism. In tritium β decay, this leads to an additional term from interference between left- and right-handed interactions, which enhances or suppresses certain regions near the endpoint of the beta spectrum. In this work, the sensitivity of KATRIN to right-handed currents is estimated for the scenario of a light sterile neutrino with a mass of some eV. This analysis has been performed with a Bayesian analysis using Markov Chain Monte Carlo (MCMC). The simulations show that, in principle, KATRIN will be able to set sterile neutrino mass-dependent limits on the interference strength. The sensitivity is significantly increased if the Q value of the β decay can be sufficiently constrained. However, the sensitivity is not high enough to improve current upper limits from right-handed W boson searches at the LHC. © 2017 IOP Publishing Ltd and Sissa Medialab srl ."
19,10.1103/PhysRevB.95.241104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026897832&doi=10.1103%2fPhysRevB.95.241104&partnerID=40&md5=6af4a91997fc3736ce92bcca3a0d3b02,"We develop the self-learning Monte Carlo (SLMC) method, a general-purpose numerical method recently introduced to simulate many-body systems, for studying interacting fermion systems. Our method uses a highly efficient update algorithm, which we design and dub ""cumulative update"", to generate new candidate configurations in the Markov chain based on a self-learned bosonic effective model. From a general analysis and a numerical study of the double exchange model as an example, we find that the SLMC with cumulative update drastically reduces the computational cost of the simulation, while remaining statistically exact. Remarkably, its computational complexity is far less than the conventional algorithm with local updates. © 2017 American Physical Society."
4,10.1088/1751-8121/aa7231,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020775264&doi=10.1088%2f1751-8121%2faa7231&partnerID=40&md5=0e7b7687703054644124a59a523ac8f3,"We implement a scale-free version of the pivot algorithm and use it to sample pairs of three-dimensional self-avoiding walks, for the purpose of efficiently calculating an observable that corresponds to the probability that pairs of self-avoiding walks remain self-avoiding when they are concatenated. We study the properties of this Markov chain, and then use it to find the critical exponent γ for self-avoiding walks to unprecedented accuracy. Our final estimate for γ is . © 2017 IOP Publishing Ltd."
9,10.1371/journal.pbio.2001323,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021646961&doi=10.1371%2fjournal.pbio.2001323&partnerID=40&md5=76dde2cb1138be9368fa242fe759dd95,"When deciding between alternative options, a rational agent chooses on the basis of the desirability of each outcome, including associated costs. As different options typically result in different actions, the effort associated with each action is an essential cost parameter. How do humans discount physical effort when deciding between movements? We used an action-selection task to characterize how subjective effort depends on the parameters of arm transport movements and controlled for potential confounding factors such as delay discounting and performance. First, by repeatedly asking subjects to choose between 2 arm movements of different amplitudes or durations, performed against different levels of force, we identified parameter combinations that subjects experienced as identical in effort (isoeffort curves). Movements with a long duration were judged more effortful than short-duration movements against the same force, while movement amplitudes did not influence effort. Biomechanics of the movements also affected effort, as movements towards the body midline were preferred to movements away from it. Second, by introducing movement repetitions, we further determined that the cost function for choosing between effortful movements had a quadratic relationship with force, while choices were made on the basis of the logarithm of these costs. Our results show that effort-based action selection during reaching cannot easily be explained by metabolic costs. Instead, force-loaded reaches, a widely occurring natural behavior, imposed an effort cost for decision making similar to cost functions in motor control. Our results thereby support the idea that motor control and economic choice are governed by partly overlapping optimization principles. © 2017 Morel et al."
1,10.1016/j.bpj.2017.04.035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020263192&doi=10.1016%2fj.bpj.2017.04.035&partnerID=40&md5=662994204b71210dfd440ee781cd9d81,"Large-conductance Ca2+-dependent K+ (BKCa) channels are important regulators of electrical activity. These channels colocalize and form ion channel complexes with voltage-dependent Ca2+ (CaV) channels. Recent stochastic simulations of the BKCa-CaV complex with 1:1 stoichiometry have given important insight into the local control of BKCa channels by fluctuating nanodomains of Ca2+. However, such Monte Carlo simulations are computationally expensive, and are therefore not suitable for large-scale simulations of cellular electrical activity. In this work we extend the stochastic model to more realistic BKCa-CaV complexes with 1:n stoichiometry, and analyze the single-complex model with Markov chain theory. From the description of a single BKCa-CaV complex, using arguments based on timescale analysis, we derive a concise model of whole-cell BKCa currents, which can readily be analyzed and inserted into models of cellular electrical activity. We illustrate the usefulness of our results by inserting our BKCa description into previously published whole-cell models, and perform simulations of electrical activity in various cell types, which show that BKCa-CaV stoichiometry can affect whole-cell behavior substantially. Our work provides a simple formulation for the whole-cell BKCa current that respects local interactions in BKCa-CaV complexes, and indicates how local-global coupling of ion channels may affect cell behavior. © 2017 Biophysical Society"
1,10.1080/17415977.2016.1209749,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978957729&doi=10.1080%2f17415977.2016.1209749&partnerID=40&md5=f5cde5732adce414bf2e56e16b5b03f2,"The present work addresses the problem of structural damage identification built on the statistical inversion approach. Here, the damage state of the structure is continuously described by a cohesion parameter, which is spatially discretized by the finite element method. The inverse problem of damage identification is then posed as the determination of the posterior probability densities of the nodal cohesion parameters. The Markov Chain Monte Carlo method, implemented with the Metropolis–Hastings algorithm, is considered in order to approximate the posterior probabilities by drawing samples from the desired joint posterior probability density function. With this approach, prior information on the sought parameters can be used and the uncertainty concerning the known values of the material properties can be quantified in the estimation of the cohesion parameters. The assessment of the proposed approach has been performed by means of numerical simulations on a simply supported Euler–Bernoulli beam. The damage identification and assessment are performed considering time domain response data. Different damage scenarios and noise levels were addressed, demonstrating the feasibility of the proposed approach. © 2016 Informa UK Limited, trading as Taylor & Francis Group."
,10.1080/2150704X.2017.1306139,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034607530&doi=10.1080%2f2150704X.2017.1306139&partnerID=40&md5=2f367912e45000fb1cb789e268b0bfeb,"Building rooftop extraction is one of the most challenging tasks in the field of remote sensing image analysis. Existing methods usually perform poorly due to the complexity of the object and background. In this letter, we propose a novel framework for rooftop localization and geometric structure recovery via combining the strength of top-down and bottom-up methods. Specifically, a novel energy function combining the region term, the shape term, and the penalty term is proposed to eliminate the effect of unclosed contours and other disturbances such as park lots and shadows. In order to take advantage of bottom-up cues, a new penalty term is proposed, in which the position is determined by the directional spatial relationship between building and its shadow, and the orientation of a possible rooftop is estimated by the spatial context. A simulated annealing (SA) algorithm is applied to optimizing the function, which is fused with Markov chain Monte Carlo (MCMC) technique, and special transition kernels are designed in order to achieve convergent extraction results and get rid of local minimum. Experiments on IKONOS images demonstrate the robustness and accuracy of our method. © 2017 Informa UK Limited, trading as Taylor & Francis Group."
8,10.1016/j.jhydrol.2017.03.073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018514592&doi=10.1016%2fj.jhydrol.2017.03.073&partnerID=40&md5=ffc3978b5d3e6c8d43355877a73e9ce0,"This paper presents a Bayesian approach using Metropolis–Hastings Markov Chain Monte Carlo algorithm and applies this method for daily river flow rate forecast and uncertainty quantification for Zhujiachuan River using data collected from Qiaotoubao Gage Station and other 13 gage stations in Zhujiachuan watershed in China. The proposed method is also compared with the conventional maximum likelihood estimation (MLE) for parameter estimation and quantification of associated uncertainties. While the Bayesian method performs similarly in estimating the mean value of daily flow rate, it performs over the conventional MLE method on uncertainty quantification, providing relatively narrower reliable interval than the MLE confidence interval and thus more precise estimation by using the related information from regional gage stations. The Bayesian MCMC method might be more favorable in the uncertainty analysis and risk management. © 2017 Elsevier B.V."
,10.1109/TASC.2016.2642834,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015221043&doi=10.1109%2fTASC.2016.2642834&partnerID=40&md5=e868ff5202ec597bbeb332b54f5eda15,"The next generation of radiation detectors in high precision Cosmology, Astronomy, and particle-astrophysics experiments will rely heavily on superconducting microwave resonators and kinetic inductance devices. Understanding the physics of energy loss in these devices, in particular at low temperatures and powers, is vital. We present a comprehensive analysis framework, using Markov Chain Monte Carlo methods, to characterize loss due to two-level system in concert with quasi-particle dynamics in thin-film Nb resonators in the GHz range. © 2002-2011 IEEE."
2,10.1007/s11263-016-0967-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032342092&doi=10.1007%2fs11263-016-0967-5&partnerID=40&md5=f854fc7afcfbae4059cd087ee4ec38bc,"We present a novel fully probabilistic method to interpret a single face image with the 3D Morphable Model. The new method is based on Bayesian inference and makes use of unreliable image-based information. Rather than searching a single optimal solution, we infer the posterior distribution of the model parameters given the target image. The method is a stochastic sampling algorithm with a propose-and-verify architecture based on the Metropolis–Hastings algorithm. The stochastic method can robustly integrate unreliable information and therefore does not rely on feed-forward initialization. The integrative concept is based on two ideas, a separation of proposal moves and their verification with the model (Data-Driven Markov Chain Monte Carlo), and filtering with the Metropolis acceptance rule. It does not need gradients and is less prone to local optima than standard fitters. We also introduce a new collective likelihood which models the average difference between the model and the target image rather than individual pixel differences. The average value shows a natural tendency towards a normal distribution, even when the individual pixel-wise difference is not Gaussian. We employ the new fitting method to calculate posterior models of 3D face reconstructions from single real-world images. A direct application of the algorithm with the 3D Morphable Model leads us to a fully automatic face recognition system with competitive performance on the Multi-PIE database without any database adaptation. © 2016 Springer Science+Business Media New York"
3,10.1007/s11263-016-0967-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028240219&doi=10.1007%2fs11263-016-0967-5&partnerID=40&md5=77d362cf06ebb4646cd34c71dcf3a2e2,"We present a novel fully probabilistic method to interpret a single face image with the 3D Morphable Model. The new method is based on Bayesian inference and makes use of unreliable image-based information. Rather than searching a single optimal solution, we infer the posterior distribution of the model parameters given the target image. The method is a stochastic sampling algorithm with a propose-and-verify architecture based on the Metropolis–Hastings algorithm. The stochastic method can robustly integrate unreliable information and therefore does not rely on feed-forward initialization. The integrative concept is based on two ideas, a separation of proposal moves and their verification with the model (Data-Driven Markov Chain Monte Carlo), and filtering with the Metropolis acceptance rule. It does not need gradients and is less prone to local optima than standard fitters. We also introduce a new collective likelihood which models the average difference between the model and the target image rather than individual pixel differences. The average value shows a natural tendency towards a normal distribution, even when the individual pixel-wise difference is not Gaussian. We employ the new fitting method to calculate posterior models of 3D face reconstructions from single real-world images. A direct application of the algorithm with the 3D Morphable Model leads us to a fully automatic face recognition system with competitive performance on the Multi-PIE database without any database adaptation. © 2016, Springer Science+Business Media New York."
7,10.1016/j.engstruct.2017.03.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014946192&doi=10.1016%2fj.engstruct.2017.03.001&partnerID=40&md5=72638591e5759c2bea9db15c788d62dc,"This paper reports the step-by-step procedures for identification of the rail-sleeper-ballast system with the use of the measured vibration data of an in situ sleeper on an existing ballasted track. The rail-sleeper-ballast modeling method, which has been used for modal-based model updating, was used to fit the measured time-domain vibration from the field test. However, the match between the measured and model-predicted responses was not good at some measured locations. Based on the observed discrepancy, the rail-sleeper-ballast modeling method was modified in this paper for suitable use in time-domain model updating. Based on the field test data and the modified modeling method, this study puts forward the time-domain Markov chain Monte Carlo (MCMC)-based Bayesian model updating and model class selection method for identification of the rail-sleeper-ballast system. MCMC was used to ensure that the proposed method can be applied even when the problem is unidentifiable. The proposed method identified the distribution of railway ballast stiffness under low-amplitude vibration and the “equivalent” rail stiffness and mass using impact hammer test data. The model updating results confirmed that the ballast stiffness under the sleeper was uniform, which implies that there was no ballast damage under the tested sleeper. Based on the proposed method, a comprehensive study was carried out to quantify the posterior uncertainties of the identified ballast stiffness when different amounts of measured information were used for model updating. The results showed that the uncertainty of the identified ballast stiffness was at an acceptable level even when using the measured data from only one sensor. © 2017 Elsevier Ltd"
